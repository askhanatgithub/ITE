{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d54e5b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "import string\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "#import cPickle as pickle\n",
    "from GPy.util import pca\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from sklearn import svm\n",
    "from scipy.linalg import sqrtm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "#from sklearn.utils import resample\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ad205993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_type,file_num):\n",
    "\n",
    "    if(data_type=='train'):\n",
    "        data=pd.read_csv(f\"Dataset/IHDP_a/ihdp_npci_train_{file_num}.csv\")\n",
    "        #data=pd.read_csv(f\"Data/ihdp_npci_train_{file_num}.csv\",index_col=False)\n",
    "    else:\n",
    "        data = pd.read_csv(f\"Dataset/IHDP_a/ihdp_npci_test_{file_num}.csv\")\n",
    "        #data = pd.read_csv(f\"Data/ihdp_npci_test_{file_num}.csv\",index_col=False)\n",
    "\n",
    "    x_data=pd.concat([data.iloc[:,0], data.iloc[:, 1:30]], axis = 1)\n",
    " \n",
    "   \n",
    "    y_data=data.iloc[:, 1]\n",
    "    \n",
    "    return x_data,y_data\n",
    "\n",
    "def cal_pehe(y_1,y_0,test_x):\n",
    "    #pehe...\n",
    "    cate_pred=y_1-y_0\n",
    "    #cate_pred=y1-y0\n",
    "    cate_true=test_x.iloc[:,4]-test_x.iloc[:,3] #Hill's noiseless true values\n",
    "    cate_err=np.mean( np.square( ( (cate_true) - (cate_pred) ) ) )\n",
    "    return np.sqrt(cate_err)\n",
    "\n",
    "def cal_pehe_2(data,model):\n",
    "    #data,y=get_data('test',i)\n",
    "\n",
    "    data=data.to_numpy()\n",
    "    data=torch.from_numpy(data.astype(np.float32))\n",
    "\n",
    "\n",
    "\n",
    "    concat_pred=model(data[:,5:30])\n",
    "    #print(concat_pred)\n",
    "    #dont forget to rescale the outcome before estimation!\n",
    "    #y0_pred = data['y_scaler'].inverse_transform(concat_pred[:, 0].reshape(-1, 1))\n",
    "    #y1_pred = data['y_scaler'].inverse_transform(concat_pred[:, 1].reshape(-1, 1))\n",
    "    cate_pred=concat_pred[:,1]-concat_pred[:,0]\n",
    "    cate_true=data[:,4]-data[:,3] #Hill's noiseless true values\n",
    "\n",
    "\n",
    "    cate_err=torch.mean( torch.square( ( (cate_true) - (cate_pred) ) ) )\n",
    "\n",
    "    return torch.sqrt(cate_err).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2eb48305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GPLVM(input_dim,Yn,kr):\n",
    "    input_dim = input_dim\n",
    "    # define covariance function to be used\n",
    "    if(kr=='RBF'):\n",
    "        kernel = GPy.kern.RBF(input_dim, ARD=True)\n",
    "    else:\n",
    "        kernel = GPy.kern.Linear(input_dim, ARD=True)\n",
    "        \n",
    "    # setup the GPLVM model\n",
    "    m = GPy.models.GPLVM(Yn, input_dim=input_dim, kernel=kernel)\n",
    "    # optimize for 1000 iterations\n",
    "    m.optimize(messages=1, max_iters=1000) \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f6e71457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(x_data, y_data):\n",
    "    x_train_sr=x_data[x_data['treatment']==0]\n",
    "    y_train_sr=y_data[x_data['treatment']==0]\n",
    "    x_train_tr=x_data[x_data['treatment']==1]\n",
    "    y_train_tr=y_data[x_data['treatment']==1]\n",
    "    \n",
    "    \n",
    "    return x_train_sr,y_train_sr,x_train_tr,y_train_tr\n",
    "    \n",
    "\n",
    "def SA(sr,tr,lat_dim):\n",
    "    pca_src_ = PCA(lat_dim)\n",
    "    pca_tgt_ = PCA(lat_dim)\n",
    "    pca_src_.fit(sr)\n",
    "    pca_tgt_.fit(tr)\n",
    "    Xs=pca_src_.components_.transpose()\n",
    "    Xt=pca_tgt_.components_.transpose()\n",
    "    \n",
    "    sr_aligned=sr@(Xs @ Xs.transpose()@Xt);\n",
    "    tr_projected = tr@Xt;\n",
    "    #tr_projected = tr@(Xt @ Xt.transpose()@Xs);\n",
    "    \n",
    "    \n",
    "    return sr_aligned, tr_projected,pca_src_,pca_tgt_\n",
    "\n",
    "def SA_FA(sr,tr,lat_dim):\n",
    "    \n",
    "    pca_src_  = FactorAnalysis(n_components=lat_dim, random_state=0)\n",
    "    pca_tgt_ = FactorAnalysis(n_components=lat_dim, random_state=0)\n",
    "    #pca_src_ = PCA(lat_dim)\n",
    "    #pca_tgt_ = PCA(lat_dim)\n",
    "    pca_src_.fit(sr)\n",
    "    pca_tgt_.fit(tr)\n",
    "    Xs=pca_src_.components_.transpose()\n",
    "    Xt=pca_tgt_.components_.transpose()\n",
    "    \n",
    "    sr_aligned=sr@(Xs @ Xs.transpose()@Xt);\n",
    "    tr_projected = tr@Xt;\n",
    "    #tr_projected = tr@(Xt @ Xt.transpose()@Xs);\n",
    "    \n",
    "    \n",
    "    return sr_aligned, tr_projected,pca_src_,pca_tgt_\n",
    "\n",
    "def coral(Xs,Xt,lamda_):\n",
    "    \n",
    "  \n",
    "        cov_Xs = np.cov(Xs,rowvar=False)\n",
    "        cov_Xt = np.cov(Xt,rowvar=False)\n",
    "        \n",
    "          \n",
    "        Cs_ = cov_Xs + lambda_ * np.eye(Xs.shape[1])\n",
    "        Ct_ = cov_Xt + lambda_ * np.eye(Xt.shape[1])\n",
    "        \n",
    "        Cs_sqrt_inv = np.linalg.inv(sqrtm(Cs_))\n",
    "        Ct_sqrt = sqrtm(Ct_)\n",
    "        \n",
    "        if np.iscomplexobj(Cs_sqrt_inv):\n",
    "            Cs_sqrt_inv = Cs_sqrt_inv.real\n",
    "        if np.iscomplexobj(Ct_sqrt):\n",
    "            Ct_sqrt = Ct_sqrt.real\n",
    "        \n",
    "        Xs_emb = np.matmul(Xs, Cs_sqrt_inv)\n",
    "        Xs_emb = np.matmul(Xs_emb, Ct_sqrt)\n",
    "        \n",
    "        return Xs_emb,Xt\n",
    "    \n",
    "    \n",
    "\n",
    "def coral_FA(Xs,Xt,lamda_,dim):\n",
    "    \n",
    "    \n",
    "        pca_src_  = FactorAnalysis(n_components=dim, random_state=0)\n",
    "        pca_tgt_ = FactorAnalysis(n_components=dim, random_state=0)\n",
    "        pca_src_.fit(Xs)\n",
    "        pca_tgt_.fit(Xt)\n",
    "        cov_Xs=pca_src_.get_covariance()\n",
    "        cov_Xt=pca_tgt_.get_covariance()\n",
    "        #cov_Xs = np.cov(Xs,rowvar=False)\n",
    "        #cov_Xt = np.cov(Xt,rowvar=False)\n",
    "        \n",
    "          \n",
    "        Cs_ = cov_Xs + lambda_ * np.eye(Xs.shape[1])\n",
    "        Ct_ = cov_Xt + lambda_ * np.eye(Xt.shape[1])\n",
    "        \n",
    "        Cs_sqrt_inv = np.linalg.inv(sqrtm(Cs_))\n",
    "        Ct_sqrt = sqrtm(Ct_)\n",
    "        \n",
    "        if np.iscomplexobj(Cs_sqrt_inv):\n",
    "            Cs_sqrt_inv = Cs_sqrt_inv.real\n",
    "        if np.iscomplexobj(Ct_sqrt):\n",
    "            Ct_sqrt = Ct_sqrt.real\n",
    "        \n",
    "        Xs_emb = np.matmul(Xs, Cs_sqrt_inv)\n",
    "        Xs_emb = np.matmul(Xs_emb, Ct_sqrt)\n",
    "        \n",
    "        return Xs_emb,Xt\n",
    "    \n",
    "    \n",
    "def coral_GPLVM(Xs,Xt,lamda_, lat_dim,kr):\n",
    "    \n",
    "       # if (train==True):\n",
    "            \n",
    "        \n",
    "        msr=train_GPLVM(lat_dim,Xs.to_numpy(),kr)\n",
    "        mtr=train_GPLVM(lat_dim,Xt.to_numpy(),kr)\n",
    "\n",
    "        Xs=msr.X\n",
    "        Xt=mtr.X\n",
    "        cov_Xs = np.cov(Xs,rowvar=False)\n",
    "        cov_Xt = np.cov(Xt,rowvar=False)\n",
    "        \n",
    "        Cs_ = cov_Xs + lambda_ * np.eye(Xs.shape[1])\n",
    "        Ct_ = cov_Xt + lambda_ * np.eye(Xt.shape[1])\n",
    "        \n",
    "        Cs_sqrt_inv = np.linalg.inv(sqrtm(Cs_))\n",
    "        Ct_sqrt = sqrtm(Ct_)\n",
    "        \n",
    "        if np.iscomplexobj(Cs_sqrt_inv):\n",
    "            Cs_sqrt_inv = Cs_sqrt_inv.real\n",
    "        if np.iscomplexobj(Ct_sqrt):\n",
    "            Ct_sqrt = Ct_sqrt.real\n",
    "        \n",
    "        Xs_emb = np.matmul(Xs, Cs_sqrt_inv)\n",
    "        Xs_emb = np.matmul(Xs_emb, Ct_sqrt)\n",
    "        \n",
    "        return Xs_emb,Xt\n",
    "    \n",
    "    \n",
    "def coral_GPLVM_FA(Xs,Xt,lamda_, lat_dim,kr):\n",
    "    \n",
    "       # if (train==True):\n",
    "          \n",
    "            \n",
    "        pca_src_  = FactorAnalysis(n_components=lat_dim, random_state=0)\n",
    "        pca_tgt_ = FactorAnalysis(n_components=lat_dim, random_state=0)\n",
    "        \n",
    "        Xs=pca_src_.fit(Xs)\n",
    "        Xt=pca_tgt_.fit(Xt)\n",
    "        \n",
    "        Xs=pca_src_.transform(Xs)\n",
    "        Xt=pca_tgt_.transform(Xt)\n",
    "        \n",
    "        #msr=train_GPLVM(lat_dim,Xs.to_numpy(),kr)\n",
    "        #mtr=train_GPLVM(lat_dim,Xt.to_numpy(),kr)\n",
    "\n",
    "        #Xs=msr.X\n",
    "        #Xt=mtr.X\n",
    "        \n",
    "        cov_Xs = np.cov(Xs,rowvar=False)\n",
    "        cov_Xt = np.cov(Xt,rowvar=False)\n",
    "        \n",
    "        Cs_ = cov_Xs + lambda_ * np.eye(Xs.shape[1])\n",
    "        Ct_ = cov_Xt + lambda_ * np.eye(Xt.shape[1])\n",
    "        \n",
    "        Cs_sqrt_inv = np.linalg.inv(sqrtm(Cs_))\n",
    "        Ct_sqrt = sqrtm(Ct_)\n",
    "        \n",
    "        if np.iscomplexobj(Cs_sqrt_inv):\n",
    "            Cs_sqrt_inv = Cs_sqrt_inv.real\n",
    "        if np.iscomplexobj(Ct_sqrt):\n",
    "            Ct_sqrt = Ct_sqrt.real\n",
    "        \n",
    "        Xs_emb = np.matmul(Xs, Cs_sqrt_inv)\n",
    "        Xs_emb = np.matmul(Xs_emb, Ct_sqrt)\n",
    "        \n",
    "        return Xs_emb,Xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6b844b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling SA\n",
    "#fil_num=50\n",
    "input_dim=20\n",
    "kr='Linear' #RBF\n",
    "pehe_error=[]\n",
    "reg_0 = svm.SVR()\n",
    "reg_1=svm.SVR()\n",
    "\n",
    "for i in range(1,101):\n",
    "    x_data,y_data=get_data('train',i)\n",
    "    test_x,test_y=get_data('test',i)\n",
    "    x_sr,y_sr,x_tr,y_tr=upsample(x_data, y_data)\n",
    "    \n",
    "    #x_sr_tran=x_sr.iloc[:,5:30].to_numpy().transpose()\n",
    "    #x_tr_tran=x_tr.iloc[:,5:30].to_numpy().transpose()\n",
    "    #sr_aligned,emd_tr,msr,mtr=SA(x_sr_tran,x_tr_tran,input_dim,kr)\n",
    "    sr,tr,pca_sr,pca_tr=SA(x_sr.iloc[:,5:30],x_tr.iloc[:,5:30],input_dim)\n",
    "    \n",
    "    # Test data transformation\n",
    "    test_sr=test_x[test_x['treatment']==0]\n",
    "    #test_sr_y=test_y[test_x['treatment']==1]\n",
    "    test_tr=test_x[test_x['treatment']==1]\n",
    "    #test_tr_y=test_y[test_x['treatment']==0]\n",
    "    \n",
    "    sr_test=test_sr.iloc[:,5:30].to_numpy()\n",
    "    tr_test=test_tr.iloc[:,5:30].to_numpy()\n",
    "    \n",
    "    Xs_test=pca_sr.components_.transpose()\n",
    "    Xt_test=pca_tr.components_.transpose()\n",
    "    \n",
    "    sr_aligned_test=sr_test@(Xs_test @ Xs_test.transpose()@Xt_test);\n",
    "    tr_projected_test= tr_test@Xt_test;\n",
    "    #tr_projected_test= tr_test@(Xt_test @ Xt_test.transpose()@Xs_test);\n",
    "    \n",
    "    \n",
    "    test_complt=pd.concat([test_sr.iloc[:,0:5],test_tr.iloc[:,0:5]], axis=0)\n",
    "    test_transformed_complt=np.concatenate((sr_aligned_test,tr_projected_test), axis=0)\n",
    "    \n",
    "    # Selectio bias measure\n",
    "    trans_comp=np.concatenate((sr,tr), axis=0)\n",
    "    t_comp=np.concatenate((x_sr.iloc[:,0],x_tr.iloc[:,0]), axis=0)\n",
    "    clf = LogisticRegression(random_state=0).fit(trans_comp, t_comp)\n",
    "    #p=clf.predict_proba(test_transformed_complt)\n",
    "    p=clf.predict_proba(trans_comp)\n",
    "    \n",
    "    \n",
    "    reg_0.fit(sr.values, y_sr.values)\n",
    "    y_0=reg_0.predict(test_transformed_complt)\n",
    "    reg_1.fit(tr.values, y_tr.values)\n",
    "    y_1=reg_1.predict(test_transformed_complt)\n",
    "    pehe=cal_pehe(y_1,y_0,test_complt)\n",
    "    pehe_error.append(pehe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a80ff405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1300980368868832"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pehe_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "adac60b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0324575950071806"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(pehe_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "157c53d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_prob=[1-np.array(t_comp).mean(), np.array(t_comp).mean()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "613cd896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34976332912943686"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(((p[:,1] - true_prob[1])**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3f2e424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling coral/coral_GPLVM\n",
    "#fil_num=50\n",
    "input_dim=25\n",
    "kr='Linear' #RBF\n",
    "pehe_error=[]\n",
    "lambda_=1\n",
    "batch_size=32\n",
    "epochs=300\n",
    "reg_0 = svm.SVR()#MLPRegressor(random_state=1, max_iter=1000,shuffle=False, solver='adam',learning_rate_init=0.01,early_stopping=True,hidden_layer_sizes=(100,100))\n",
    "reg_1=svm.SVR()#MLPRegressor(random_state=1, max_iter=1000,shuffle=False,solver='adam',learning_rate_init=0.01,early_stopping=True,hidden_layer_sizes=(100,100))#svm.SVR()\n",
    "\n",
    "for i in range(1,101):\n",
    "    x_data,y_data=get_data('train',i)\n",
    "    test_x,test_y=get_data('test',i)\n",
    "    x_sr,y_sr,x_tr,y_tr=upsample(x_data, y_data)\n",
    "    \n",
    "    #x_sr_tran=x_sr.iloc[:,5:30].to_numpy().transpose()\n",
    "    #x_tr_tran=x_tr.iloc[:,5:30].to_numpy().transpose()\n",
    "    #sr_aligned,emd_tr,msr,mtr=SA(x_sr_tran,x_tr_tran,input_dim,kr)\n",
    "    sr,tr=coral(x_sr.iloc[:,5:30],x_tr.iloc[:,5:30],lambda_)\n",
    "    #sr,tr=coral_GPLVM_FA(x_sr.iloc[:,5:30],x_tr.iloc[:,5:30],lambda_,input_dim,kr)\n",
    "    \n",
    "    # Test data transformation\n",
    "    test_sr=test_x[test_x['treatment']==0]\n",
    "    #test_sr_y=test_y[test_x['treatment']==1]\n",
    "    test_tr=test_x[test_x['treatment']==1]\n",
    "    #test_tr_y=test_y[test_x['treatment']==0]\n",
    "    \n",
    "    sr_test=test_sr.iloc[:,5:30]\n",
    "    tr_test=test_tr.iloc[:,5:30]\n",
    "    \n",
    "    \n",
    "    # Test Transformation\n",
    "    \n",
    "    sr_t, tr_t=coral(sr_test,tr_test,lambda_)\n",
    "    #sr_t, tr_t=coral_GPLVM_FA(sr_test,tr_test,lambda_,input_dim,kr)\n",
    "    \n",
    "    test_complt=pd.concat([test_sr.iloc[:,0:5],test_tr.iloc[:,0:5]], axis=0)\n",
    "    test_transformed_complt=np.concatenate((sr_t,tr_t), axis=0)\n",
    "    trans_comp=np.concatenate((sr,tr), axis=0)\n",
    "    untransformed_data=np.concatenate((x_sr.iloc[:,5:30],x_tr.iloc[:,5:30]), axis=0)\n",
    "    t_comp=np.concatenate((x_sr.iloc[:,0],x_tr.iloc[:,0]), axis=0)\n",
    "    \n",
    "    \n",
    "    # train NN for Regression task\n",
    "    # combine train data\n",
    "    \"\"\"\n",
    "    s_train_data=pd.concat([x_sr.iloc[:,0:5],sr],axis=1)\n",
    "    t_train_data=pd.concat([x_tr.iloc[:,0:5],tr],axis=1)\n",
    "    com_x=pd.concat([s_train_data,t_train_data],axis=0)\n",
    "    com_y=pd.concat([y_sr,y_tr],axis=0)\n",
    "    model=train_Regressor(com_x,com_y,batch_size,epochs)\n",
    "    test_data=pd.concat([test_complt,test_transformed_complt],axis=1)\n",
    "    pehe=cal_pehe_2(test_data,model)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    reg_0.fit(sr.values, y_sr.values)\n",
    "    y_0=reg_0.predict(test_transformed_complt)\n",
    "    \n",
    "    # calculation about about P(t|x)\n",
    "    #trans_comp=np.concatenate((x_sr.iloc[:,5:30],x_tr.iloc[:,5:30]), axis=0)\n",
    "    clf = LogisticRegression(random_state=0).fit(trans_comp, t_comp)\n",
    "    \n",
    "    #p=clf.predict_proba(test_transformed_complt)\n",
    "    p=clf.predict_proba(trans_comp)\n",
    "    \n",
    "    reg_1.fit(tr.values, y_tr.values)\n",
    "    y_1=reg_1.predict(test_transformed_complt)\n",
    "    pehe=cal_pehe(y_1,y_0,test_complt)  \n",
    "    pehe_error.append(pehe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "78be0274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.09799355646089"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pehe_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f6132b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.003280612852779"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(pehe_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "439d350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_prob=[1-np.array(t_comp).mean(), np.array(t_comp).mean()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c9f62683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15314371597193827"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(((p[:,1] - true_prob[1])**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "c8602174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#t_sr_data=msr.infer_newX(test_sr.iloc[:,5:30].to_numpy())\n",
    "#emd_sr_test=np.array(t_sr_data[0])\n",
    "#t_tr_data=mtr.infer_newX(test_tr.iloc[:,5:30].to_numpy())\n",
    "#emd_tr_test=np.array(t_tr_data[0])\n",
    "\n",
    "#sr_aligned_test=(emd_sr_test @ emd_sr_test.transpose()@emd_tr_test);\n",
    "#test_complt=pd.concat([test_sr.iloc[:,0:5],test_tr.iloc[:,0:5]], axis=0)\n",
    "#test_embd_complt=np.concatenate((sr_aligned_test,emd_tr_test), axis=0)\n",
    "\n",
    "#reg_1.fit(sr_aligned, y_sr)\n",
    "#y_1=reg_1.predict(test_embd_complt)\n",
    "#reg_0.fit(emd_tr, y_tr)\n",
    "#y_0=reg_0.predict(test_embd_complt)\n",
    "#pehe=cal_pehe(y_1,y_0,test_complt)\n",
    "#pehe_error.append(pehe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a913c44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Principal Component Analysis\\nfrom numpy import array\\nfrom sklearn.decomposition import PCA\\n# define a matrix\\nA = array([[1, 2,7,11], [3, 4,8,12], [5, 6,9,13]])\\nprint(A)\\n# create the PCA instance\\npca = PCA(2)\\n# fit on data\\npca.fit(A)\\n# access values and vectors\\nprint(pca.components_)\\n#print(pca.explained_variance_)\\n# transform data\\n#B = pca.transform(A)\\n#print(B)\\n\\n'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Principal Component Analysis\n",
    "from numpy import array\n",
    "from sklearn.decomposition import PCA\n",
    "# define a matrix\n",
    "A = array([[1, 2,7,11], [3, 4,8,12], [5, 6,9,13]])\n",
    "print(A)\n",
    "# create the PCA instance\n",
    "pca = PCA(2)\n",
    "# fit on data\n",
    "pca.fit(A)\n",
    "# access values and vectors\n",
    "print(pca.components_)\n",
    "#print(pca.explained_variance_)\n",
    "# transform data\n",
    "#B = pca.transform(A)\n",
    "#print(B)\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "848a0553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsvd = np.linalg.svd\\nA = array([[1, 2,7,11], [3, 4,8,12], [5, 6,9,13]])\\nprint(A)\\nData = A - A.mean(0)\\nData.round(2)\\nU, S, Vt = svd(Data)\\n\\n'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "svd = np.linalg.svd\n",
    "A = array([[1, 2,7,11], [3, 4,8,12], [5, 6,9,13]])\n",
    "print(A)\n",
    "Data = A - A.mean(0)\n",
    "Data.round(2)\n",
    "U, S, Vt = svd(Data)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fb65ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
