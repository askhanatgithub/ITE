{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "52542919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f163aff2a50>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from sklearn import preprocessing\n",
    "#import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "#from geomloss import SamplesLoss\n",
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.functional import normalize\n",
    "#from torchmetrics.classification import BinaryAccuracy\n",
    "#from torchmetrics.classification import BinaryF1Score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e50bd733",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TarNet(nn.Module):\n",
    "    def __init__(self,params):\n",
    "        super(TarNet, self).__init__()\n",
    "        self.encoder1 = nn.Linear(25, params['RL11'])\n",
    "        self.encoder2 = nn.Linear(params['RL11'], params['RL21'])\n",
    "        self.encoder3 = nn.Linear(params['RL21'], params['RL32'])\n",
    "\n",
    "        self.regressor1_y0 = nn.Sequential(\n",
    "            nn.Linear(params['RL32'], params['RG012']),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=.01),\n",
    "        )\n",
    "        self.regressor2_y0 = nn.Sequential(\n",
    "            nn.Linear(params['RG012'], params['RG022']),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=.01),\n",
    "        )\n",
    "        self.regressorO_y0 = nn.Linear(params['RG022'], 1)\n",
    "\n",
    "        self.regressor1_y1 = nn.Sequential(\n",
    "            nn.Linear(params['RL32'], params['RG112']),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=.01),\n",
    "        )\n",
    "        self.regressor2_y1 = nn.Sequential(\n",
    "            nn.Linear(params['RG112'], params['RG122']),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=.01),\n",
    "        )\n",
    "        self.regressorO_y1 = nn.Linear(params['RG122'], 1)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = nn.functional.elu(self.encoder1(inputs))\n",
    "        x = nn.functional.elu(self.encoder2(x))\n",
    "        phi = nn.functional.elu(self.encoder3(x))\n",
    "\n",
    "        out_y0 = self.regressor1_y0(phi)\n",
    "        out_y0 = self.regressor2_y0(out_y0)\n",
    "        y0 = self.regressorO_y0(out_y0)\n",
    "\n",
    "        out_y1 = self.regressor1_y1(phi)\n",
    "        out_y1 = self.regressor2_y1(out_y1)\n",
    "        y1 = self.regressorO_y1(out_y1)\n",
    "\n",
    "        concat = torch.cat((y0, y1), 1)\n",
    "        return concat,phi\n",
    "\n",
    "    '''\n",
    "    class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, 50)\n",
    "        self.linear_2 = torch.nn.Linear(50, output_dim)\n",
    "    def forward(self, x):\n",
    "        out_1=self.linear(x)\n",
    "        out_2=self.linear_2(out_1)\n",
    "        outputs = torch.sigmoid(out_2)\n",
    "        return outputs\n",
    "    \n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "70c58121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial,i):\n",
    "\n",
    "    params = {\n",
    "          'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
    "          'optimizer': trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\"]),\n",
    "          'batch_size':trial.suggest_int('batch_size', 8, 256),\n",
    "          'RL11':trial.suggest_int('RL11', 16, 512),\n",
    "          'RL21': trial.suggest_int('RL21', 16, 512),\n",
    "          'RL32': trial.suggest_int('RL32', 16, 512),\n",
    "          'RG012':trial.suggest_int('RG012', 16, 512),\n",
    "        'RG022':trial.suggest_int('RG022', 16, 512),\n",
    "        'RG112':trial.suggest_int('RG112', 16, 512),\n",
    "        'RG122':trial.suggest_int('RG122', 16, 512),\n",
    "          \n",
    "          }\n",
    "\n",
    "    model = TarNet(params)\n",
    "\n",
    "    pehe,model= train_evaluate(params, model, trial,i)\n",
    "\n",
    "    return pehe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "48800d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X.astype(np.float32))\n",
    "        self.y = torch.from_numpy(y.astype(np.float32))\n",
    "        self.len = self.X.shape[0]\n",
    "       \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "   \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d80cf340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_type,file_num):\n",
    "\n",
    "    if(data_type=='train'):\n",
    "        data=pd.read_csv(f\"Dataset/IHDP_a/ihdp_npci_train_{file_num}.csv\")\n",
    "    else:\n",
    "        data = pd.read_csv(f\"Dataset/IHDP_a/ihdp_npci_test_{file_num}.csv\")\n",
    "\n",
    "    x_data=pd.concat([data.iloc[:,0], data.iloc[:, 1:30]], axis = 1)\n",
    "    #x_data=data.iloc[:, 5:30]\n",
    "    y_data=data.iloc[:, 1]\n",
    "    return x_data,y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2319f810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(x_data,y_data,batch_size):\n",
    "\n",
    "    x_train_sr=x_data[x_data['treatment']==0]\n",
    "    y_train_sr=y_data[x_data['treatment']==0]\n",
    "    x_train_tr=x_data[x_data['treatment']==1]\n",
    "    y_train_tr=y_data[x_data['treatment']==1]\n",
    "\n",
    "\n",
    "    train_data_sr = Data(np.array(x_train_sr), np.array(y_train_sr))\n",
    "    train_dataloader_sr = DataLoader(dataset=train_data_sr, batch_size=batch_size)\n",
    "\n",
    "    train_data_tr = Data(np.array(x_train_tr), np.array(y_train_tr))\n",
    "    train_dataloader_tr = DataLoader(dataset=train_data_tr, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    return train_dataloader_sr, train_dataloader_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b92cc291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_loss(concat_true, concat_pred):\n",
    "    #computes a standard MSE loss for TARNet\n",
    "    y_true = concat_true[:, 0] #get individual vectors\n",
    "    t_true = concat_true[:, 1]\n",
    "\n",
    "    y0_pred = concat_pred[:, 0]\n",
    "    y1_pred = concat_pred[:, 1]\n",
    "\n",
    "    #Each head outputs a prediction for both potential outcomes\n",
    "    #We use t_true as a switch to only calculate the factual loss\n",
    "    loss0 = torch.sum((1. - t_true) * torch.square(y_true - y0_pred))\n",
    "    loss1 = torch.sum(t_true * torch.square(y_true - y1_pred))\n",
    "    #note Shi uses tf.reduce_sum for her losses instead of tf.reduce_mean.\n",
    "    #They should be equivalent but it's possible that having larger gradients accelerates convergence.\n",
    "    #You can always try changing it!\n",
    "    return loss0 + loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "536f4a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_pehe(data,y,model):\n",
    "    #data,y=get_data('test',i)\n",
    "\n",
    "    data=data.to_numpy()\n",
    "    data=torch.from_numpy(data.astype(np.float32))\n",
    "\n",
    "\n",
    "\n",
    "    concat_pred,phi=model(data[:,5:30])\n",
    "    #dont forget to rescale the outcome before estimation!\n",
    "    #y0_pred = data['y_scaler'].inverse_transform(concat_pred[:, 0].reshape(-1, 1))\n",
    "    #y1_pred = data['y_scaler'].inverse_transform(concat_pred[:, 1].reshape(-1, 1))\n",
    "    cate_pred=concat_pred[:,1]-concat_pred[:,0]\n",
    "    cate_true=data[:,4]-data[:,3] #Hill's noiseless true values\n",
    "\n",
    "\n",
    "    cate_err=torch.mean( torch.square( ( (cate_true) - (cate_pred) ) ) )\n",
    "\n",
    "    return torch.sqrt(cate_err).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5226d04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_cal(X_data,y_data,net):\n",
    "    \n",
    "    x_train_sr=X_data[X_data['treatment']==0]\n",
    "    y_train_sr=y_data[X_data['treatment']==0]\n",
    "    x_train_tr=X_data[X_data['treatment']==1]\n",
    "    y_train_tr=y_data[X_data['treatment']==1]\n",
    "    xs_t=x_train_sr.iloc[:,0].to_numpy()\n",
    "    xt_t=x_train_tr.iloc[:,0].to_numpy()\n",
    "    \n",
    "    xs=x_train_sr.iloc[:,5:30].to_numpy()\n",
    "    xt=x_train_tr.iloc[:,5:30].to_numpy()\n",
    "    xs_t=torch.from_numpy(xs_t.astype(np.float32))\n",
    "    xt_t=torch.from_numpy(xt_t.astype(np.float32))\n",
    "    y_train_sr=y_train_sr.to_numpy()\n",
    "    y_train_tr=y_train_tr.to_numpy()\n",
    "    xs=torch.from_numpy(xs.astype(np.float32))\n",
    "    xt=torch.from_numpy(xt.astype(np.float32))\n",
    "    \n",
    "    y_train_sr=torch.from_numpy(y_train_sr.astype(np.float32))\n",
    "    y_train_tr=torch.from_numpy(y_train_tr.astype(np.float32))\n",
    "    \n",
    "    \n",
    "    input_data=torch.cat((xs,xt),0)\n",
    "    true_y=torch.unsqueeze(torch.cat((y_train_sr,y_train_tr),0), dim=1)\n",
    "    true_t=torch.unsqueeze(torch.cat((xs_t,xt_t),0), dim=1)\n",
    "    \n",
    "    \n",
    "    concat_true=torch.cat((true_y,true_t),1)\n",
    "    concat_pred,phi=net(input_data)\n",
    "    loss=regression_loss(concat_true, concat_pred)\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "63dce250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_log_reg(X_train,y_train,phi_model,phi_size):\n",
    "    epochs = 400\n",
    "    input_dim = 25 # Two inputs x1 and x2 \n",
    "    output_dim = 1 # Single binary output \n",
    "    learning_rate = 0.1\n",
    "    model = LogisticRegression(phi_size,output_dim)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    #x_data,y_data=get_data('train',1)\n",
    "    #X_train, X_val,y_train, y_val = train_test_split(x_data,y_data ,random_state=42, test_size=0.20)\n",
    "    #batch_loss=0\n",
    "    train_loss=[]\n",
    "    #val_loss=[]\n",
    "    batch_size=32\n",
    "    for i in range(epochs):\n",
    "            batch_loss=0\n",
    "\n",
    "            train_dataloader_sr, train_dataloader_tr=get_dataloader(X_train,y_train,batch_size)\n",
    "\n",
    "            for batch_idx, (train_source_data, train_target_data) in enumerate(zip(train_dataloader_sr, train_dataloader_tr)):\n",
    "\n",
    "                xs,ys=train_source_data\n",
    "                xt,yt=train_target_data\n",
    "\n",
    "                xs_train=xs[:,5:30]\n",
    "                xt_train=xt[:,5:30]\n",
    "\n",
    "                train_x=torch.cat((xs_train,xt_train),0)\n",
    "                #train_y=torch.unsqueeze(torch.cat((ys,yt),0), dim=1)\n",
    "                true_t=torch.unsqueeze(torch.cat((xs[:,0],xt[:,0]),0), dim=1)\n",
    "                #concat_true=torch.cat((train_y,true_t),1)\n",
    "                concat_pred,phi=phi_model(train_x)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "               # Setting our stored gradients equal to zero\n",
    "                outputs = model(phi)\n",
    "                loss = criterion(outputs, true_t) # [200,1] -squeeze-> [200]\n",
    "                loss.backward() # Computes the gradient of the given tensor w.r.t. graph leaves \n",
    "                batch_loss=batch_loss+loss.item()\n",
    "                optimizer.step() \n",
    "            train_loss.append(batch_loss)\n",
    "            \n",
    "    #X_train_n=X_train.to_numpy()\n",
    "    #X_train_n=torch.from_numpy(X_train_n.astype(np.float32))\n",
    "    #train_probs=model(X_train_n[:,5:30])\n",
    "    #return train_probs,1-train_probs\n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c05852e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_MSE=nn.MSELoss()\n",
    "#criterion_reg=nn.MSELoss()\n",
    "#criterion_reg=regression_loss(concat_true,concat_pred)\n",
    "epochs=300\n",
    "#batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bcb7c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss=[]\n",
    "val_loss=[]\n",
    "pehe_error=[]\n",
    "batch_loss=0\n",
    "num_files=2\n",
    "lambda_=0.3\n",
    "def train_evaluate(param, model, trial,file_num):\n",
    "    #for nf in range(1,num_files):\n",
    "    x_data,y_data=get_data('train',file_num)\n",
    "    X_train, X_val,y_train, y_val = train_test_split(x_data,y_data ,\n",
    "                                       random_state=42, \n",
    "                                       test_size=0.20)\n",
    "    \n",
    "    #net=TarNet(25,.01)\n",
    "    #opt_net = torch.optim.Adam(net.parameters(), lr=1e-4)\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    optimizer = getattr(optim, param['optimizer'])(model.parameters(), lr= param['learning_rate'])\n",
    "    \n",
    "   \n",
    "        #model = model.cuda()\n",
    "    #model = model\n",
    "        #criterion = criterion.cuda()\n",
    "\n",
    "    for ep in range(1,epochs+1 ):\n",
    "        #print('epoch',ep)\n",
    "        batch_loss=0\n",
    "        #X_train_n=X_train.to_numpy()\n",
    "        #X_train_n=torch.from_numpy(X_train_n.astype(np.float32))\n",
    "        #concat_pred_b,phi_b=model(X_train_n[:,5:30])\n",
    "        #logistic_reg=train_log_reg(X_train,y_train,model,param['RL32'])\n",
    "        \n",
    "        #------------------------------\n",
    "        \n",
    "        t_com=X_train.iloc[:,0].to_numpy()\n",
    "        x_data_com=X_train.iloc[:,5:30].to_numpy()\n",
    "        x_data_com = torch.from_numpy(x_data_com.astype(np.float32))\n",
    "        concat_pred_com,phi_com=model(x_data_com)\n",
    "\n",
    "        torch.nan_to_num(phi_com,nan=0.0)\n",
    "        phi_com=phi_com.detach().numpy()\n",
    "        \n",
    "        \n",
    "        clf = LogisticRegression(max_iter=400,random_state=0).fit(phi_com, t_com) #\n",
    "        #clf = LogisticRegression(random_state=0).fit(trans_comp, t_comp)\n",
    "        \n",
    "        #----------------------------\n",
    "        \n",
    "\n",
    "        train_dataloader_sr, train_dataloader_tr=get_dataloader(X_train,y_train,param['batch_size'])\n",
    "\n",
    "        for batch_idx, (train_source_data, train_target_data) in enumerate(zip(train_dataloader_sr, train_dataloader_tr)):\n",
    "\n",
    "            xs,ys=train_source_data\n",
    "            xt,yt=train_target_data\n",
    "\n",
    "            xs_train=xs[:,5:30]\n",
    "            xt_train=xt[:,5:30]\n",
    "\n",
    "            train_x=torch.cat((xs_train,xt_train),0)\n",
    "            train_y=torch.unsqueeze(torch.cat((ys,yt),0), dim=1)\n",
    "            true_t=torch.unsqueeze(torch.cat((xs[:,0],xt[:,0]),0), dim=1)\n",
    "            concat_true=torch.cat((train_y,true_t),1)\n",
    "            concat_pred,phi=model(train_x)\n",
    "            \n",
    "            \n",
    "            # p(t|x)\n",
    "            \n",
    "            #t_com=t_batch\n",
    "            \n",
    "            #print(phi_com)\n",
    "            #phi_com[np.isnan(phi_com)] = 0\n",
    "            t_batch=torch.cat((xs[:,0],xt[:,0]),0).numpy()\n",
    "            #t_comp=torch.cat((xs[:,0],xt[:,0]),0).detach().numpy()\n",
    "            trans_comp=phi.detach().numpy()\n",
    "            p=clf.predict_proba(trans_comp)\n",
    "            #p_1=logistic_reg(phi)\n",
    "            #p_0=1-p_1\n",
    "            #true_prob=[1-t_comp.mean(), t_comp.mean()]\n",
    "            true_prob=[1-t_batch.mean(), t_batch.mean()]\n",
    "\n",
    "            #scaler = preprocessing.StandardScaler().fit(phi_com)\n",
    "            #tranformed=scaler.transform(phi_com)\n",
    "\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            #source_mse=criterion_reg(y0,ys)\n",
    "            #target_mse=criterion_reg(y1,yt)\n",
    "            ptx=torch.unsqueeze(torch.from_numpy(p[1].astype(np.float32)), dim=1)\n",
    "            #ptx=p_1\n",
    "            pt=torch.empty(ptx.shape[0],1).fill_(true_prob[1])\n",
    "            \n",
    "            sel_loss=torch.sum(torch.abs(ptx-pt))\n",
    "            #May be there is another way\n",
    "            combined_loss=(lambda_)*regression_loss(concat_true,concat_pred)+(1-lambda_)*sel_loss # think about tradeoff and other\n",
    "            # loss function, check in paper\n",
    "            #print('Training loss: ',combined_loss.item())\n",
    "            # backward propagation\n",
    "            combined_loss.backward()\n",
    "            losss=batch_loss+combined_loss.item()\n",
    "\n",
    "            # optimize\n",
    "            optimizer.step()\n",
    "        #train_loss.append(loss_cal(X_train,y_train,net))\n",
    "        #val_loss.append(loss_cal(X_val,y_val,net))\n",
    "        train_loss.append(losss)\n",
    "        \n",
    "        # Add prune mechanism\n",
    "        #trial.report(accuracy, ep)\n",
    "\n",
    "        #if trial.should_prune():\n",
    "        #   raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "    #return cal_pehe(X_val,y_val,model),model\n",
    "    return loss_cal(X_val,y_val,model),model\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "19615b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-19 18:32:56,110]\u001b[0m A new study created in memory with name: no-name-c4c9911f-dfdf-4f1d-a730-65d3fed7e59f\u001b[0m\n",
      "/tmp/ipykernel_342389/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-19 18:33:29,444]\u001b[0m Trial 0 finished with value: 1191.3082275390625 and parameters: {'learning_rate': 2.368863950364079e-05, 'optimizer': 'Adam', 'batch_size': 157, 'RL11': 93, 'RL21': 93, 'RL32': 44, 'RG012': 446, 'RG022': 314, 'RG112': 367, 'RG122': 26}. Best is trial 0 with value: 1191.3082275390625.\u001b[0m\n",
      "/tmp/ipykernel_342389/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-19 18:34:35,615]\u001b[0m Trial 1 finished with value: 278.57965087890625 and parameters: {'learning_rate': 9.330606024425662e-05, 'optimizer': 'Adam', 'batch_size': 53, 'RL11': 107, 'RL21': 167, 'RL32': 276, 'RG012': 230, 'RG022': 160, 'RG112': 320, 'RG122': 85}. Best is trial 1 with value: 278.57965087890625.\u001b[0m\n",
      "/tmp/ipykernel_342389/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-19 18:35:26,630]\u001b[0m Trial 2 finished with value: 347.8929443359375 and parameters: {'learning_rate': 1.9594972058679154e-05, 'optimizer': 'SGD', 'batch_size': 203, 'RL11': 115, 'RL21': 271, 'RL32': 310, 'RG012': 39, 'RG022': 317, 'RG112': 100, 'RG122': 48}. Best is trial 1 with value: 278.57965087890625.\u001b[0m\n",
      "/tmp/ipykernel_342389/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-19 18:36:54,720]\u001b[0m Trial 3 finished with value: 216.5089111328125 and parameters: {'learning_rate': 8.889667907018936e-05, 'optimizer': 'Adam', 'batch_size': 83, 'RL11': 64, 'RL21': 356, 'RL32': 234, 'RG012': 76, 'RG022': 262, 'RG112': 33, 'RG122': 467}. Best is trial 3 with value: 216.5089111328125.\u001b[0m\n",
      "/tmp/ipykernel_342389/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-19 18:38:04,599]\u001b[0m Trial 4 finished with value: 419.8470458984375 and parameters: {'learning_rate': 1.8145961353490253e-05, 'optimizer': 'Adam', 'batch_size': 137, 'RL11': 287, 'RL21': 107, 'RL32': 497, 'RG012': 401, 'RG022': 482, 'RG112': 460, 'RG122': 313}. Best is trial 3 with value: 216.5089111328125.\u001b[0m\n",
      "/tmp/ipykernel_342389/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-19 18:38:45,045]\u001b[0m Trial 5 finished with value: 263.43646240234375 and parameters: {'learning_rate': 8.35361075531177e-05, 'optimizer': 'SGD', 'batch_size': 19, 'RL11': 177, 'RL21': 209, 'RL32': 150, 'RG012': 427, 'RG022': 193, 'RG112': 155, 'RG122': 285}. Best is trial 3 with value: 216.5089111328125.\u001b[0m\n",
      "/tmp/ipykernel_342389/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-19 18:39:15,635]\u001b[0m Trial 6 finished with value: 605.9613037109375 and parameters: {'learning_rate': 1.3833249975219966e-05, 'optimizer': 'Adam', 'batch_size': 253, 'RL11': 399, 'RL21': 114, 'RL32': 18, 'RG012': 421, 'RG022': 367, 'RG112': 378, 'RG122': 399}. Best is trial 3 with value: 216.5089111328125.\u001b[0m\n",
      "/tmp/ipykernel_342389/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-19 18:39:49,895]\u001b[0m Trial 7 finished with value: 1508.822998046875 and parameters: {'learning_rate': 1.1858906685575266e-05, 'optimizer': 'Adam', 'batch_size': 222, 'RL11': 325, 'RL21': 180, 'RL32': 47, 'RG012': 170, 'RG022': 177, 'RG112': 378, 'RG122': 332}. Best is trial 3 with value: 216.5089111328125.\u001b[0m\n",
      "/tmp/ipykernel_342389/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-19 18:40:49,388]\u001b[0m Trial 8 finished with value: 224.3376007080078 and parameters: {'learning_rate': 7.712811947156355e-05, 'optimizer': 'Adam', 'batch_size': 185, 'RL11': 394, 'RL21': 294, 'RL32': 399, 'RG012': 261, 'RG022': 275, 'RG112': 228, 'RG122': 28}. Best is trial 3 with value: 216.5089111328125.\u001b[0m\n",
      "/tmp/ipykernel_342389/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-19 18:42:05,006]\u001b[0m Trial 9 finished with value: 430.0090637207031 and parameters: {'learning_rate': 1.2820100418916903e-05, 'optimizer': 'SGD', 'batch_size': 86, 'RL11': 268, 'RL21': 467, 'RL32': 139, 'RG012': 219, 'RG022': 391, 'RG112': 129, 'RG122': 54}. Best is trial 3 with value: 216.5089111328125.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files completed so far =  1\n"
     ]
    }
   ],
   "source": [
    "pehe_total=[]\n",
    "for i in range(1,2):\n",
    "    func = lambda trial: objective(trial, i)\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "    study.optimize(func, n_trials=10)\n",
    "    best_trial = study.best_trial\n",
    "    print('Files completed so far = ',i)\n",
    "    best_model=TarNet(study.best_trial.params)\n",
    "    train_loss.clear()\n",
    "    best_val,model=train_evaluate(study.best_trial.params, best_model, study.best_trial,i)\n",
    "    data,y=get_data('test',i)\n",
    "    pehe=cal_pehe(data,y,model)\n",
    "\n",
    "    pehe_total.append(pehe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a97b7951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7116636037826538\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(pehe_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ac88ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"v3_Ours_sel_loss_1_50_(IHDPa-Hyper_val_300ep_outsample).csv\", pehe_total,delimiter =\", \", fmt ='% s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a351929c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7473039031028748,\n",
       " 0.6983595490455627,\n",
       " 0.9662187099456787,\n",
       " 0.7418673634529114,\n",
       " 0.8065203428268433,\n",
       " 1.1107197999954224,\n",
       " 0.4231216311454773,\n",
       " 0.5315221548080444,\n",
       " 0.8781972527503967,\n",
       " 1.4119138717651367,\n",
       " 1.3753641843795776,\n",
       " 0.5007650256156921,\n",
       " 0.6108106374740601,\n",
       " 1.3233189582824707,\n",
       " 0.8348250389099121,\n",
       " 0.6897068023681641,\n",
       " 0.662876546382904,\n",
       " 0.5743263959884644,\n",
       " 0.8990809321403503,\n",
       " 1.0691567659378052,\n",
       " 0.7262060642242432,\n",
       " 0.7096218466758728,\n",
       " 0.6990131139755249,\n",
       " 0.5855550169944763,\n",
       " 0.48488813638687134,\n",
       " 0.9634784460067749,\n",
       " 0.963718593120575,\n",
       " 0.7788012027740479,\n",
       " 1.54386305809021,\n",
       " 0.6514948010444641,\n",
       " 0.4629833400249481,\n",
       " 0.7672489285469055,\n",
       " 0.6974350810050964,\n",
       " 0.778927743434906,\n",
       " 0.6609624624252319,\n",
       " 0.48041534423828125,\n",
       " 0.45375707745552063,\n",
       " 0.48724886775016785,\n",
       " 1.1568771600723267,\n",
       " 2.1152191162109375,\n",
       " 1.7150038480758667,\n",
       " 0.6617791056632996,\n",
       " 0.32774782180786133,\n",
       " 0.9956676959991455,\n",
       " 0.753425121307373,\n",
       " 0.7662010192871094,\n",
       " 0.8731015920639038,\n",
       " 1.3374296426773071,\n",
       " 2.6201539039611816,\n",
       " 3.133744239807129]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pehe_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05dc239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e536b889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9699601031313039"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pehe_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb8e4efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for key, value in best_trial.params.items():\n",
    "#    print(\"{}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f0b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ate_pred=torch.mean(cate_pred)\n",
    "#print(\"Estimated ATE (True is 4):\", ate_pred.detach().numpy(),'\\n\\n')\n",
    "\n",
    "#print(\"Individualized CATE Estimates: BLUE\")\n",
    "#print(pd.Series(cate_pred.detach().numpy()).plot.kde(color='blue'))\n",
    "#print(\"Individualized CATE True: Green\")\n",
    "#print(pd.Series(cate_true.detach().numpy()).plot.kde(color='green'))\n",
    "\n",
    "#print(\"\\nError CATE Estimates: RED\")\n",
    "#print(pd.Series(cate_pred.detach().numpy()-cate_true.detach().numpy()).plot.kde(color='red'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
