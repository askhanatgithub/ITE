{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52542919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmad/envs/RL/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f307fa72a50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from sklearn import preprocessing\n",
    "#import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "#from geomloss import SamplesLoss\n",
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.functional import normalize\n",
    "#from torchmetrics.classification import BinaryAccuracy\n",
    "#from torchmetrics.classification import BinaryF1Score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e50bd733",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TarNet(nn.Module):\n",
    "    def __init__(self,params):\n",
    "        super(TarNet, self).__init__()\n",
    "        self.encoder1 = nn.Linear(25, params['RL11'])\n",
    "        self.encoder2 = nn.Linear(params['RL11'], params['RL21'])\n",
    "        self.encoder3 = nn.Linear(params['RL21'], params['RL32'])\n",
    "\n",
    "        self.regressor1_y0 = nn.Sequential(\n",
    "            nn.Linear(params['RL32'], params['RG012']),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=.01),\n",
    "        )\n",
    "        self.regressor2_y0 = nn.Sequential(\n",
    "            nn.Linear(params['RG012'], params['RG022']),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=.01),\n",
    "        )\n",
    "        self.regressorO_y0 = nn.Linear(params['RG022'], 1)\n",
    "\n",
    "        self.regressor1_y1 = nn.Sequential(\n",
    "            nn.Linear(params['RL32'], params['RG112']),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=.01),\n",
    "        )\n",
    "        self.regressor2_y1 = nn.Sequential(\n",
    "            nn.Linear(params['RG112'], params['RG122']),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=.01),\n",
    "        )\n",
    "        self.regressorO_y1 = nn.Linear(params['RG122'], 1)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = nn.functional.elu(self.encoder1(inputs))\n",
    "        x = nn.functional.elu(self.encoder2(x))\n",
    "        phi = nn.functional.elu(self.encoder3(x))\n",
    "\n",
    "        out_y0 = self.regressor1_y0(phi)\n",
    "        out_y0 = self.regressor2_y0(out_y0)\n",
    "        y0 = self.regressorO_y0(out_y0)\n",
    "\n",
    "        out_y1 = self.regressor1_y1(phi)\n",
    "        out_y1 = self.regressor2_y1(out_y1)\n",
    "        y1 = self.regressorO_y1(out_y1)\n",
    "\n",
    "        concat = torch.cat((y0, y1), 1)\n",
    "        return concat,phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70c58121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial,i):\n",
    "\n",
    "    params = {\n",
    "          'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
    "          'optimizer': trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\"]),\n",
    "          'batch_size':trial.suggest_int('batch_size', 8, 256),\n",
    "          'RL11':trial.suggest_int('RL11', 16, 512),\n",
    "          'RL21': trial.suggest_int('RL21', 16, 512),\n",
    "          'RL32': trial.suggest_int('RL32', 16, 512),\n",
    "          'RG012':trial.suggest_int('RG012', 16, 512),\n",
    "        'RG022':trial.suggest_int('RG022', 16, 512),\n",
    "        'RG112':trial.suggest_int('RG112', 16, 512),\n",
    "        'RG122':trial.suggest_int('RG122', 16, 512),\n",
    "          \n",
    "          }\n",
    "\n",
    "    model = TarNet(params)\n",
    "\n",
    "    pehe,model= train_evaluate(params, model, trial,i)\n",
    "\n",
    "    return pehe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48800d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X.astype(np.float32))\n",
    "        self.y = torch.from_numpy(y.astype(np.float32))\n",
    "        self.len = self.X.shape[0]\n",
    "       \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "   \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d80cf340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_type,file_num):\n",
    "\n",
    "    if(data_type=='train'):\n",
    "        data=pd.read_csv(f\"Dataset/IHDP_a/ihdp_npci_train_{file_num}.csv\")\n",
    "    else:\n",
    "        data = pd.read_csv(f\"Dataset/IHDP_a/ihdp_npci_test_{file_num}.csv\")\n",
    "\n",
    "    x_data=pd.concat([data.iloc[:,0], data.iloc[:, 1:30]], axis = 1)\n",
    "    #x_data=data.iloc[:, 5:30]\n",
    "    y_data=data.iloc[:, 1]\n",
    "    return x_data,y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2319f810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(x_data,y_data,batch_size):\n",
    "\n",
    "    x_train_sr=x_data[x_data['treatment']==0]\n",
    "    y_train_sr=y_data[x_data['treatment']==0]\n",
    "    x_train_tr=x_data[x_data['treatment']==1]\n",
    "    y_train_tr=y_data[x_data['treatment']==1]\n",
    "\n",
    "\n",
    "    train_data_sr = Data(np.array(x_train_sr), np.array(y_train_sr))\n",
    "    train_dataloader_sr = DataLoader(dataset=train_data_sr, batch_size=batch_size)\n",
    "\n",
    "    train_data_tr = Data(np.array(x_train_tr), np.array(y_train_tr))\n",
    "    train_dataloader_tr = DataLoader(dataset=train_data_tr, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    return train_dataloader_sr, train_dataloader_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b92cc291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_loss(concat_true, concat_pred):\n",
    "    #computes a standard MSE loss for TARNet\n",
    "    y_true = concat_true[:, 0] #get individual vectors\n",
    "    t_true = concat_true[:, 1]\n",
    "\n",
    "    y0_pred = concat_pred[:, 0]\n",
    "    y1_pred = concat_pred[:, 1]\n",
    "\n",
    "    #Each head outputs a prediction for both potential outcomes\n",
    "    #We use t_true as a switch to only calculate the factual loss\n",
    "    loss0 = torch.sum((1. - t_true) * torch.square(y_true - y0_pred))\n",
    "    loss1 = torch.sum(t_true * torch.square(y_true - y1_pred))\n",
    "    #note Shi uses tf.reduce_sum for her losses instead of tf.reduce_mean.\n",
    "    #They should be equivalent but it's possible that having larger gradients accelerates convergence.\n",
    "    #You can always try changing it!\n",
    "    return loss0 + loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "536f4a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_pehe(data,y,model):\n",
    "    #data,y=get_data('test',i)\n",
    "\n",
    "    data=data.to_numpy()\n",
    "    data=torch.from_numpy(data.astype(np.float32))\n",
    "\n",
    "\n",
    "\n",
    "    concat_pred,phi=model(data[:,5:30])\n",
    "    #dont forget to rescale the outcome before estimation!\n",
    "    #y0_pred = data['y_scaler'].inverse_transform(concat_pred[:, 0].reshape(-1, 1))\n",
    "    #y1_pred = data['y_scaler'].inverse_transform(concat_pred[:, 1].reshape(-1, 1))\n",
    "    cate_pred=concat_pred[:,1]-concat_pred[:,0]\n",
    "    cate_true=data[:,4]-data[:,3] #Hill's noiseless true values\n",
    "\n",
    "\n",
    "    cate_err=torch.mean( torch.square( ( (cate_true) - (cate_pred) ) ) )\n",
    "\n",
    "    return torch.sqrt(cate_err).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5226d04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_cal(X_data,y_data,net):\n",
    "    \n",
    "    x_train_sr=X_data[X_data['treatment']==0]\n",
    "    y_train_sr=y_data[X_data['treatment']==0]\n",
    "    x_train_tr=X_data[X_data['treatment']==1]\n",
    "    y_train_tr=y_data[X_data['treatment']==1]\n",
    "    xs_t=x_train_sr.iloc[:,0].to_numpy()\n",
    "    xt_t=x_train_tr.iloc[:,0].to_numpy()\n",
    "    \n",
    "    xs=x_train_sr.iloc[:,5:30].to_numpy()\n",
    "    xt=x_train_tr.iloc[:,5:30].to_numpy()\n",
    "    xs_t=torch.from_numpy(xs_t.astype(np.float32))\n",
    "    xt_t=torch.from_numpy(xt_t.astype(np.float32))\n",
    "    y_train_sr=y_train_sr.to_numpy()\n",
    "    y_train_tr=y_train_tr.to_numpy()\n",
    "    xs=torch.from_numpy(xs.astype(np.float32))\n",
    "    xt=torch.from_numpy(xt.astype(np.float32))\n",
    "    \n",
    "    y_train_sr=torch.from_numpy(y_train_sr.astype(np.float32))\n",
    "    y_train_tr=torch.from_numpy(y_train_tr.astype(np.float32))\n",
    "    \n",
    "    \n",
    "    input_data=torch.cat((xs,xt),0)\n",
    "    true_y=torch.unsqueeze(torch.cat((y_train_sr,y_train_tr),0), dim=1)\n",
    "    true_t=torch.unsqueeze(torch.cat((xs_t,xt_t),0), dim=1)\n",
    "    \n",
    "    \n",
    "    concat_true=torch.cat((true_y,true_t),1)\n",
    "    concat_pred,phi=net(input_data)\n",
    "    loss=regression_loss(concat_true, concat_pred)\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c05852e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_MSE=nn.MSELoss()\n",
    "#criterion_reg=nn.MSELoss()\n",
    "#criterion_reg=regression_loss(concat_true,concat_pred)\n",
    "epochs=300\n",
    "#batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcb7c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss=[]\n",
    "val_loss=[]\n",
    "pehe_error=[]\n",
    "batch_loss=0\n",
    "num_files=2\n",
    "def train_evaluate(param, model, trial,file_num):\n",
    "    #for nf in range(1,num_files):\n",
    "    x_data,y_data=get_data('train',file_num)\n",
    "    X_train, X_val,y_train, y_val = train_test_split(x_data,y_data ,\n",
    "                                       random_state=42, \n",
    "                                       test_size=0.20)\n",
    "    \n",
    "    #net=TarNet(25,.01)\n",
    "    #opt_net = torch.optim.Adam(net.parameters(), lr=1e-4)\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    optimizer = getattr(optim, param['optimizer'])(model.parameters(), lr= param['learning_rate'])\n",
    "    \n",
    "   \n",
    "        #model = model.cuda()\n",
    "    #model = model\n",
    "        #criterion = criterion.cuda()\n",
    "\n",
    "    for ep in range(1,epochs+1 ):\n",
    "        #print('epoch',ep)\n",
    "        batch_loss=0\n",
    "\n",
    "        train_dataloader_sr, train_dataloader_tr=get_dataloader(X_train,y_train,param['batch_size'])\n",
    "\n",
    "        for batch_idx, (train_source_data, train_target_data) in enumerate(zip(train_dataloader_sr, train_dataloader_tr)):\n",
    "\n",
    "            xs,ys=train_source_data\n",
    "            xt,yt=train_target_data\n",
    "\n",
    "            xs_train=xs[:,5:30]\n",
    "            xt_train=xt[:,5:30]\n",
    "\n",
    "            train_x=torch.cat((xs_train,xt_train),0)\n",
    "            train_y=torch.unsqueeze(torch.cat((ys,yt),0), dim=1)\n",
    "            true_t=torch.unsqueeze(torch.cat((xs[:,0],xt[:,0]),0), dim=1)\n",
    "            concat_true=torch.cat((train_y,true_t),1)\n",
    "            concat_pred,phi=model(train_x)\n",
    "            \n",
    "            \n",
    "            # p(t|x)\n",
    "            t_batch=torch.cat((xs[:,0],xt[:,0]),0).numpy()\n",
    "            #t_com=t_batch\n",
    "            t_com=X_train.iloc[:,0].to_numpy()\n",
    "            x_data_com=X_train.iloc[:,5:30].to_numpy()\n",
    "            x_data_com = torch.from_numpy(x_data_com.astype(np.float32))\n",
    "            concat_pred_com,phi_com=model(x_data_com)\n",
    "            \n",
    "            torch.nan_to_num(phi_com,nan=0.0)\n",
    "            phi_com=phi_com.detach().numpy()\n",
    "            #print(phi_com)\n",
    "            #phi_com[np.isnan(phi_com)] = 0\n",
    "            \n",
    "            \n",
    "            \n",
    "            #scaler = preprocessing.StandardScaler().fit(phi_com)\n",
    "            #tranformed=scaler.transform(phi_com)\n",
    "            clf = LogisticRegression(max_iter=300,random_state=0).fit(phi_com, t_com) #\n",
    "            #clf = LogisticRegression(random_state=0).fit(trans_comp, t_comp)\n",
    "            #t_comp=torch.cat((xs[:,0],xt[:,0]),0).detach().numpy()\n",
    "            trans_comp=phi.detach().numpy()\n",
    "            p=clf.predict_proba(trans_comp)\n",
    "            #true_prob=[1-t_comp.mean(), t_comp.mean()]\n",
    "            true_prob=[1-t_batch.mean(), t_batch.mean()]\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            #source_mse=criterion_reg(y0,ys)\n",
    "            #target_mse=criterion_reg(y1,yt)\n",
    "            ptx=torch.unsqueeze(torch.from_numpy(p[:,1].astype(np.float32)), dim=1)\n",
    "            pt=torch.empty(ptx.shape[0],1).fill_(true_prob[1])\n",
    "            \n",
    "            sel_loss=torch.sum(torch.abs(ptx-pt))\n",
    "            #May be there is another way\n",
    "            combined_loss=regression_loss(concat_true,concat_pred)+sel_loss # think about tradeoff and other\n",
    "            # loss function, check in paper\n",
    "            #print('Training loss: ',combined_loss.item())\n",
    "            # backward propagation\n",
    "            combined_loss.backward()\n",
    "            losss=batch_loss+combined_loss.item()\n",
    "\n",
    "            # optimize\n",
    "            optimizer.step()\n",
    "        #train_loss.append(loss_cal(X_train,y_train,net))\n",
    "        #val_loss.append(loss_cal(X_val,y_val,net))\n",
    "        train_loss.append(losss)\n",
    "        \n",
    "        # Add prune mechanism\n",
    "        #trial.report(accuracy, ep)\n",
    "\n",
    "        #if trial.should_prune():\n",
    "        #   raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "    #return cal_pehe(X_val,y_val,model),model\n",
    "    return loss_cal(X_val,y_val,model),model\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19615b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-18 16:53:52,197]\u001b[0m A new study created in memory with name: no-name-8bec25a5-e723-4e8e-ad89-e23efeae174d\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 16:54:16,387]\u001b[0m Trial 0 finished with value: 1191.39697265625 and parameters: {'learning_rate': 2.368863950364079e-05, 'optimizer': 'Adam', 'batch_size': 157, 'RL11': 93, 'RL21': 93, 'RL32': 44, 'RG012': 446, 'RG022': 314, 'RG112': 367, 'RG122': 26}. Best is trial 0 with value: 1191.39697265625.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 16:55:37,690]\u001b[0m Trial 1 finished with value: 274.985595703125 and parameters: {'learning_rate': 9.330606024425662e-05, 'optimizer': 'Adam', 'batch_size': 53, 'RL11': 107, 'RL21': 167, 'RL32': 276, 'RG012': 230, 'RG022': 160, 'RG112': 320, 'RG122': 85}. Best is trial 1 with value: 274.985595703125.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 16:56:19,029]\u001b[0m Trial 2 finished with value: 233.07455444335938 and parameters: {'learning_rate': 1.9594972058679154e-05, 'optimizer': 'SGD', 'batch_size': 203, 'RL11': 115, 'RL21': 271, 'RL32': 310, 'RG012': 39, 'RG022': 317, 'RG112': 100, 'RG122': 48}. Best is trial 2 with value: 233.07455444335938.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 16:57:44,922]\u001b[0m Trial 3 finished with value: 223.94309997558594 and parameters: {'learning_rate': 8.889667907018936e-05, 'optimizer': 'Adam', 'batch_size': 83, 'RL11': 64, 'RL21': 356, 'RL32': 234, 'RG012': 76, 'RG022': 262, 'RG112': 33, 'RG122': 467}. Best is trial 3 with value: 223.94309997558594.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 16:58:35,247]\u001b[0m Trial 4 finished with value: 450.94903564453125 and parameters: {'learning_rate': 1.8145961353490253e-05, 'optimizer': 'Adam', 'batch_size': 137, 'RL11': 287, 'RL21': 107, 'RL32': 497, 'RG012': 401, 'RG022': 482, 'RG112': 460, 'RG122': 313}. Best is trial 3 with value: 223.94309997558594.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:02:15,499]\u001b[0m Trial 5 finished with value: 254.02560424804688 and parameters: {'learning_rate': 8.35361075531177e-05, 'optimizer': 'SGD', 'batch_size': 19, 'RL11': 177, 'RL21': 209, 'RL32': 150, 'RG012': 427, 'RG022': 193, 'RG112': 155, 'RG122': 285}. Best is trial 3 with value: 223.94309997558594.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:02:44,127]\u001b[0m Trial 6 finished with value: 620.3011474609375 and parameters: {'learning_rate': 1.3833249975219966e-05, 'optimizer': 'Adam', 'batch_size': 253, 'RL11': 399, 'RL21': 114, 'RL32': 18, 'RG012': 421, 'RG022': 367, 'RG112': 378, 'RG122': 399}. Best is trial 3 with value: 223.94309997558594.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:03:14,233]\u001b[0m Trial 7 finished with value: 1479.54150390625 and parameters: {'learning_rate': 1.1858906685575266e-05, 'optimizer': 'Adam', 'batch_size': 222, 'RL11': 325, 'RL21': 180, 'RL32': 47, 'RG012': 170, 'RG022': 177, 'RG112': 378, 'RG122': 332}. Best is trial 3 with value: 223.94309997558594.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:04:01,459]\u001b[0m Trial 8 finished with value: 224.42156982421875 and parameters: {'learning_rate': 7.712811947156355e-05, 'optimizer': 'Adam', 'batch_size': 185, 'RL11': 394, 'RL21': 294, 'RL32': 399, 'RG012': 261, 'RG022': 275, 'RG112': 228, 'RG122': 28}. Best is trial 3 with value: 223.94309997558594.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:05:23,077]\u001b[0m Trial 9 finished with value: 244.91830444335938 and parameters: {'learning_rate': 1.2820100418916903e-05, 'optimizer': 'SGD', 'batch_size': 86, 'RL11': 268, 'RL21': 467, 'RL32': 139, 'RG012': 219, 'RG022': 391, 'RG112': 129, 'RG122': 54}. Best is trial 3 with value: 223.94309997558594.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files completed so far =  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-18 17:06:49,491]\u001b[0m A new study created in memory with name: no-name-53fd931b-c298-4970-ac4a-68a399ea1e21\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:07:27,081]\u001b[0m Trial 0 finished with value: 965.4296875 and parameters: {'learning_rate': 2.368863950364079e-05, 'optimizer': 'Adam', 'batch_size': 157, 'RL11': 93, 'RL21': 93, 'RL32': 44, 'RG012': 446, 'RG022': 314, 'RG112': 367, 'RG122': 26}. Best is trial 0 with value: 965.4296875.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:08:44,722]\u001b[0m Trial 1 finished with value: 182.2666473388672 and parameters: {'learning_rate': 9.330606024425662e-05, 'optimizer': 'Adam', 'batch_size': 53, 'RL11': 107, 'RL21': 167, 'RL32': 276, 'RG012': 230, 'RG022': 160, 'RG112': 320, 'RG122': 85}. Best is trial 1 with value: 182.2666473388672.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:09:25,113]\u001b[0m Trial 2 finished with value: 184.16384887695312 and parameters: {'learning_rate': 1.9594972058679154e-05, 'optimizer': 'SGD', 'batch_size': 203, 'RL11': 115, 'RL21': 271, 'RL32': 310, 'RG012': 39, 'RG022': 317, 'RG112': 100, 'RG122': 48}. Best is trial 1 with value: 182.2666473388672.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:10:49,309]\u001b[0m Trial 3 finished with value: 186.93569946289062 and parameters: {'learning_rate': 8.889667907018936e-05, 'optimizer': 'Adam', 'batch_size': 83, 'RL11': 64, 'RL21': 356, 'RL32': 234, 'RG012': 76, 'RG022': 262, 'RG112': 33, 'RG122': 467}. Best is trial 1 with value: 182.2666473388672.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:11:37,319]\u001b[0m Trial 4 finished with value: 231.24481201171875 and parameters: {'learning_rate': 1.8145961353490253e-05, 'optimizer': 'Adam', 'batch_size': 137, 'RL11': 287, 'RL21': 107, 'RL32': 497, 'RG012': 401, 'RG022': 482, 'RG112': 460, 'RG122': 313}. Best is trial 1 with value: 182.2666473388672.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:14:19,750]\u001b[0m Trial 5 finished with value: 189.25653076171875 and parameters: {'learning_rate': 8.35361075531177e-05, 'optimizer': 'SGD', 'batch_size': 19, 'RL11': 177, 'RL21': 209, 'RL32': 150, 'RG012': 427, 'RG022': 193, 'RG112': 155, 'RG122': 285}. Best is trial 1 with value: 182.2666473388672.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:14:44,062]\u001b[0m Trial 6 finished with value: 424.28021240234375 and parameters: {'learning_rate': 1.3833249975219966e-05, 'optimizer': 'Adam', 'batch_size': 253, 'RL11': 399, 'RL21': 114, 'RL32': 18, 'RG012': 421, 'RG022': 367, 'RG112': 378, 'RG122': 399}. Best is trial 1 with value: 182.2666473388672.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:15:10,496]\u001b[0m Trial 7 finished with value: 843.2476806640625 and parameters: {'learning_rate': 1.1858906685575266e-05, 'optimizer': 'Adam', 'batch_size': 222, 'RL11': 325, 'RL21': 180, 'RL32': 47, 'RG012': 170, 'RG022': 177, 'RG112': 378, 'RG122': 332}. Best is trial 1 with value: 182.2666473388672.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:15:53,103]\u001b[0m Trial 8 finished with value: 203.75741577148438 and parameters: {'learning_rate': 7.712811947156355e-05, 'optimizer': 'Adam', 'batch_size': 185, 'RL11': 394, 'RL21': 294, 'RL32': 399, 'RG012': 261, 'RG022': 275, 'RG112': 228, 'RG122': 28}. Best is trial 1 with value: 182.2666473388672.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:17:13,238]\u001b[0m Trial 9 finished with value: 190.616455078125 and parameters: {'learning_rate': 1.2820100418916903e-05, 'optimizer': 'SGD', 'batch_size': 86, 'RL11': 268, 'RL21': 467, 'RL32': 139, 'RG012': 219, 'RG022': 391, 'RG112': 129, 'RG122': 54}. Best is trial 1 with value: 182.2666473388672.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files completed so far =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-18 17:18:32,214]\u001b[0m A new study created in memory with name: no-name-e398f940-a1cb-4b76-8bbe-ebbe2ca09824\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:18:58,549]\u001b[0m Trial 0 finished with value: 867.0318603515625 and parameters: {'learning_rate': 2.368863950364079e-05, 'optimizer': 'Adam', 'batch_size': 157, 'RL11': 93, 'RL21': 93, 'RL32': 44, 'RG012': 446, 'RG022': 314, 'RG112': 367, 'RG122': 26}. Best is trial 0 with value: 867.0318603515625.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:20:20,217]\u001b[0m Trial 1 finished with value: 197.13330078125 and parameters: {'learning_rate': 9.330606024425662e-05, 'optimizer': 'Adam', 'batch_size': 53, 'RL11': 107, 'RL21': 167, 'RL32': 276, 'RG012': 230, 'RG022': 160, 'RG112': 320, 'RG122': 85}. Best is trial 1 with value: 197.13330078125.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:20:59,697]\u001b[0m Trial 2 finished with value: 184.19537353515625 and parameters: {'learning_rate': 1.9594972058679154e-05, 'optimizer': 'SGD', 'batch_size': 203, 'RL11': 115, 'RL21': 271, 'RL32': 310, 'RG012': 39, 'RG022': 317, 'RG112': 100, 'RG122': 48}. Best is trial 2 with value: 184.19537353515625.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:22:27,045]\u001b[0m Trial 3 finished with value: 177.32366943359375 and parameters: {'learning_rate': 8.889667907018936e-05, 'optimizer': 'Adam', 'batch_size': 83, 'RL11': 64, 'RL21': 356, 'RL32': 234, 'RG012': 76, 'RG022': 262, 'RG112': 33, 'RG122': 467}. Best is trial 3 with value: 177.32366943359375.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:23:16,571]\u001b[0m Trial 4 finished with value: 211.90188598632812 and parameters: {'learning_rate': 1.8145961353490253e-05, 'optimizer': 'Adam', 'batch_size': 137, 'RL11': 287, 'RL21': 107, 'RL32': 497, 'RG012': 401, 'RG022': 482, 'RG112': 460, 'RG122': 313}. Best is trial 3 with value: 177.32366943359375.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:26:36,647]\u001b[0m Trial 5 finished with value: 184.4178466796875 and parameters: {'learning_rate': 8.35361075531177e-05, 'optimizer': 'SGD', 'batch_size': 19, 'RL11': 177, 'RL21': 209, 'RL32': 150, 'RG012': 427, 'RG022': 193, 'RG112': 155, 'RG122': 285}. Best is trial 3 with value: 177.32366943359375.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:27:02,209]\u001b[0m Trial 6 finished with value: 327.00140380859375 and parameters: {'learning_rate': 1.3833249975219966e-05, 'optimizer': 'Adam', 'batch_size': 253, 'RL11': 399, 'RL21': 114, 'RL32': 18, 'RG012': 421, 'RG022': 367, 'RG112': 378, 'RG122': 399}. Best is trial 3 with value: 177.32366943359375.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:27:28,157]\u001b[0m Trial 7 finished with value: 468.07000732421875 and parameters: {'learning_rate': 1.1858906685575266e-05, 'optimizer': 'Adam', 'batch_size': 222, 'RL11': 325, 'RL21': 180, 'RL32': 47, 'RG012': 170, 'RG022': 177, 'RG112': 378, 'RG122': 332}. Best is trial 3 with value: 177.32366943359375.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:28:17,455]\u001b[0m Trial 8 finished with value: 183.32162475585938 and parameters: {'learning_rate': 7.712811947156355e-05, 'optimizer': 'Adam', 'batch_size': 185, 'RL11': 394, 'RL21': 294, 'RL32': 399, 'RG012': 261, 'RG022': 275, 'RG112': 228, 'RG122': 28}. Best is trial 3 with value: 177.32366943359375.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:29:36,901]\u001b[0m Trial 9 finished with value: 188.4711456298828 and parameters: {'learning_rate': 1.2820100418916903e-05, 'optimizer': 'SGD', 'batch_size': 86, 'RL11': 268, 'RL21': 467, 'RL32': 139, 'RG012': 219, 'RG022': 391, 'RG112': 129, 'RG122': 54}. Best is trial 3 with value: 177.32366943359375.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files completed so far =  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-18 17:31:01,689]\u001b[0m A new study created in memory with name: no-name-3b9db501-55ce-4e5e-af1f-a8ec9d6b0e9a\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:31:38,522]\u001b[0m Trial 0 finished with value: 514.27734375 and parameters: {'learning_rate': 2.368863950364079e-05, 'optimizer': 'Adam', 'batch_size': 157, 'RL11': 93, 'RL21': 93, 'RL32': 44, 'RG012': 446, 'RG022': 314, 'RG112': 367, 'RG122': 26}. Best is trial 0 with value: 514.27734375.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:32:54,098]\u001b[0m Trial 1 finished with value: 206.55368041992188 and parameters: {'learning_rate': 9.330606024425662e-05, 'optimizer': 'Adam', 'batch_size': 53, 'RL11': 107, 'RL21': 167, 'RL32': 276, 'RG012': 230, 'RG022': 160, 'RG112': 320, 'RG122': 85}. Best is trial 1 with value: 206.55368041992188.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:33:33,148]\u001b[0m Trial 2 finished with value: 168.74258422851562 and parameters: {'learning_rate': 1.9594972058679154e-05, 'optimizer': 'SGD', 'batch_size': 203, 'RL11': 115, 'RL21': 271, 'RL32': 310, 'RG012': 39, 'RG022': 317, 'RG112': 100, 'RG122': 48}. Best is trial 2 with value: 168.74258422851562.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:34:59,071]\u001b[0m Trial 3 finished with value: 179.77066040039062 and parameters: {'learning_rate': 8.889667907018936e-05, 'optimizer': 'Adam', 'batch_size': 83, 'RL11': 64, 'RL21': 356, 'RL32': 234, 'RG012': 76, 'RG022': 262, 'RG112': 33, 'RG122': 467}. Best is trial 2 with value: 168.74258422851562.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:35:47,405]\u001b[0m Trial 4 finished with value: 198.05908203125 and parameters: {'learning_rate': 1.8145961353490253e-05, 'optimizer': 'Adam', 'batch_size': 137, 'RL11': 287, 'RL21': 107, 'RL32': 497, 'RG012': 401, 'RG022': 482, 'RG112': 460, 'RG122': 313}. Best is trial 2 with value: 168.74258422851562.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:39:11,602]\u001b[0m Trial 5 finished with value: 194.9930419921875 and parameters: {'learning_rate': 8.35361075531177e-05, 'optimizer': 'SGD', 'batch_size': 19, 'RL11': 177, 'RL21': 209, 'RL32': 150, 'RG012': 427, 'RG022': 193, 'RG112': 155, 'RG122': 285}. Best is trial 2 with value: 168.74258422851562.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:39:38,493]\u001b[0m Trial 6 finished with value: 213.5443878173828 and parameters: {'learning_rate': 1.3833249975219966e-05, 'optimizer': 'Adam', 'batch_size': 253, 'RL11': 399, 'RL21': 114, 'RL32': 18, 'RG012': 421, 'RG022': 367, 'RG112': 378, 'RG122': 399}. Best is trial 2 with value: 168.74258422851562.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:40:01,821]\u001b[0m Trial 7 finished with value: 336.9329528808594 and parameters: {'learning_rate': 1.1858906685575266e-05, 'optimizer': 'Adam', 'batch_size': 222, 'RL11': 325, 'RL21': 180, 'RL32': 47, 'RG012': 170, 'RG022': 177, 'RG112': 378, 'RG122': 332}. Best is trial 2 with value: 168.74258422851562.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:40:45,360]\u001b[0m Trial 8 finished with value: 170.376220703125 and parameters: {'learning_rate': 7.712811947156355e-05, 'optimizer': 'Adam', 'batch_size': 185, 'RL11': 394, 'RL21': 294, 'RL32': 399, 'RG012': 261, 'RG022': 275, 'RG112': 228, 'RG122': 28}. Best is trial 2 with value: 168.74258422851562.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:42:04,906]\u001b[0m Trial 9 finished with value: 166.4828338623047 and parameters: {'learning_rate': 1.2820100418916903e-05, 'optimizer': 'SGD', 'batch_size': 86, 'RL11': 268, 'RL21': 467, 'RL32': 139, 'RG012': 219, 'RG022': 391, 'RG112': 129, 'RG122': 54}. Best is trial 9 with value: 166.4828338623047.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files completed so far =  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-18 17:43:24,693]\u001b[0m A new study created in memory with name: no-name-d4bc5987-7062-4d0f-b005-728e2043bd1f\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:43:50,347]\u001b[0m Trial 0 finished with value: 693.5117797851562 and parameters: {'learning_rate': 2.368863950364079e-05, 'optimizer': 'Adam', 'batch_size': 157, 'RL11': 93, 'RL21': 93, 'RL32': 44, 'RG012': 446, 'RG022': 314, 'RG112': 367, 'RG122': 26}. Best is trial 0 with value: 693.5117797851562.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:45:09,816]\u001b[0m Trial 1 finished with value: 186.9933319091797 and parameters: {'learning_rate': 9.330606024425662e-05, 'optimizer': 'Adam', 'batch_size': 53, 'RL11': 107, 'RL21': 167, 'RL32': 276, 'RG012': 230, 'RG022': 160, 'RG112': 320, 'RG122': 85}. Best is trial 1 with value: 186.9933319091797.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:45:51,228]\u001b[0m Trial 2 finished with value: 182.89584350585938 and parameters: {'learning_rate': 1.9594972058679154e-05, 'optimizer': 'SGD', 'batch_size': 203, 'RL11': 115, 'RL21': 271, 'RL32': 310, 'RG012': 39, 'RG022': 317, 'RG112': 100, 'RG122': 48}. Best is trial 2 with value: 182.89584350585938.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:47:14,500]\u001b[0m Trial 3 finished with value: 170.63943481445312 and parameters: {'learning_rate': 8.889667907018936e-05, 'optimizer': 'Adam', 'batch_size': 83, 'RL11': 64, 'RL21': 356, 'RL32': 234, 'RG012': 76, 'RG022': 262, 'RG112': 33, 'RG122': 467}. Best is trial 3 with value: 170.63943481445312.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:48:02,828]\u001b[0m Trial 4 finished with value: 231.50355529785156 and parameters: {'learning_rate': 1.8145961353490253e-05, 'optimizer': 'Adam', 'batch_size': 137, 'RL11': 287, 'RL21': 107, 'RL32': 497, 'RG012': 401, 'RG022': 482, 'RG112': 460, 'RG122': 313}. Best is trial 3 with value: 170.63943481445312.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:51:24,610]\u001b[0m Trial 5 finished with value: 173.64581298828125 and parameters: {'learning_rate': 8.35361075531177e-05, 'optimizer': 'SGD', 'batch_size': 19, 'RL11': 177, 'RL21': 209, 'RL32': 150, 'RG012': 427, 'RG022': 193, 'RG112': 155, 'RG122': 285}. Best is trial 3 with value: 170.63943481445312.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:51:51,313]\u001b[0m Trial 6 finished with value: 349.7547302246094 and parameters: {'learning_rate': 1.3833249975219966e-05, 'optimizer': 'Adam', 'batch_size': 253, 'RL11': 399, 'RL21': 114, 'RL32': 18, 'RG012': 421, 'RG022': 367, 'RG112': 378, 'RG122': 399}. Best is trial 3 with value: 170.63943481445312.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:52:21,344]\u001b[0m Trial 7 finished with value: 531.318359375 and parameters: {'learning_rate': 1.1858906685575266e-05, 'optimizer': 'Adam', 'batch_size': 222, 'RL11': 325, 'RL21': 180, 'RL32': 47, 'RG012': 170, 'RG022': 177, 'RG112': 378, 'RG122': 332}. Best is trial 3 with value: 170.63943481445312.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:53:09,444]\u001b[0m Trial 8 finished with value: 164.86383056640625 and parameters: {'learning_rate': 7.712811947156355e-05, 'optimizer': 'Adam', 'batch_size': 185, 'RL11': 394, 'RL21': 294, 'RL32': 399, 'RG012': 261, 'RG022': 275, 'RG112': 228, 'RG122': 28}. Best is trial 8 with value: 164.86383056640625.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:54:29,582]\u001b[0m Trial 9 finished with value: 202.15386962890625 and parameters: {'learning_rate': 1.2820100418916903e-05, 'optimizer': 'SGD', 'batch_size': 86, 'RL11': 268, 'RL21': 467, 'RL32': 139, 'RG012': 219, 'RG022': 391, 'RG112': 129, 'RG122': 54}. Best is trial 8 with value: 164.86383056640625.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files completed so far =  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-18 17:55:18,341]\u001b[0m A new study created in memory with name: no-name-102472a9-b393-4c1f-b298-02dcfd586da0\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:55:42,952]\u001b[0m Trial 0 finished with value: 2611.76220703125 and parameters: {'learning_rate': 2.368863950364079e-05, 'optimizer': 'Adam', 'batch_size': 157, 'RL11': 93, 'RL21': 93, 'RL32': 44, 'RG012': 446, 'RG022': 314, 'RG112': 367, 'RG122': 26}. Best is trial 0 with value: 2611.76220703125.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:56:59,099]\u001b[0m Trial 1 finished with value: 224.34677124023438 and parameters: {'learning_rate': 9.330606024425662e-05, 'optimizer': 'Adam', 'batch_size': 53, 'RL11': 107, 'RL21': 167, 'RL32': 276, 'RG012': 230, 'RG022': 160, 'RG112': 320, 'RG122': 85}. Best is trial 1 with value: 224.34677124023438.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:57:39,747]\u001b[0m Trial 2 finished with value: 202.8003387451172 and parameters: {'learning_rate': 1.9594972058679154e-05, 'optimizer': 'SGD', 'batch_size': 203, 'RL11': 115, 'RL21': 271, 'RL32': 310, 'RG012': 39, 'RG022': 317, 'RG112': 100, 'RG122': 48}. Best is trial 2 with value: 202.8003387451172.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:59:06,410]\u001b[0m Trial 3 finished with value: 242.07806396484375 and parameters: {'learning_rate': 8.889667907018936e-05, 'optimizer': 'Adam', 'batch_size': 83, 'RL11': 64, 'RL21': 356, 'RL32': 234, 'RG012': 76, 'RG022': 262, 'RG112': 33, 'RG122': 467}. Best is trial 2 with value: 202.8003387451172.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "\u001b[32m[I 2023-05-18 17:59:55,349]\u001b[0m Trial 4 finished with value: 841.6948852539062 and parameters: {'learning_rate': 1.8145961353490253e-05, 'optimizer': 'Adam', 'batch_size': 137, 'RL11': 287, 'RL21': 107, 'RL32': 497, 'RG012': 401, 'RG022': 482, 'RG112': 460, 'RG122': 313}. Best is trial 2 with value: 202.8003387451172.\u001b[0m\n",
      "/tmp/ipykernel_76918/2216607761.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n"
     ]
    }
   ],
   "source": [
    "pehe_total=[]\n",
    "for i in range(1,51):\n",
    "    func = lambda trial: objective(trial, i)\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "    study.optimize(func, n_trials=10)\n",
    "    best_trial = study.best_trial\n",
    "    print('Files completed so far = ',i)\n",
    "    best_model=TarNet(study.best_trial.params)\n",
    "    train_loss.clear()\n",
    "    best_val,model=train_evaluate(study.best_trial.params, best_model, study.best_trial,i)\n",
    "    data,y=get_data('test',i)\n",
    "    pehe=cal_pehe(data,y,model)\n",
    "\n",
    "    pehe_total.append(pehe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97b7951",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(pehe_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ac88ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"v2_Ours_sel_loss_1_50_(IHDPa-Hyper_val_300ep_outsample).csv\", pehe_total,delimiter =\", \", fmt ='% s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a351929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pehe_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05dc239",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e536b889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9699601031313039"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pehe_total[0:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6757e606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7243529558181763,\n",
       " 1.0974527597427368,\n",
       " 0.9394102096557617,\n",
       " 0.5717839002609253,\n",
       " 0.9096927046775818,\n",
       " 1.1487631797790527,\n",
       " 0.3492894768714905,\n",
       " 0.7218011021614075,\n",
       " 0.8568695187568665,\n",
       " 1.5668202638626099,\n",
       " 0.9571043252944946,\n",
       " 0.8009145259857178,\n",
       " 0.6833164691925049,\n",
       " 1.4453110694885254,\n",
       " 0.8261359930038452,\n",
       " 0.8569546341896057,\n",
       " 0.5389509201049805,\n",
       " 0.44863349199295044,\n",
       " 0.9471801519393921,\n",
       " 1.5991759300231934,\n",
       " 0.6929744482040405,\n",
       " 0.6652252674102783,\n",
       " 0.784331738948822,\n",
       " 0.6690612435340881,\n",
       " 0.6766694188117981,\n",
       " 0.9668810367584229,\n",
       " 0.9014957547187805,\n",
       " 0.7697464823722839,\n",
       " 1.324318528175354,\n",
       " 0.621450662612915,\n",
       " 0.37054741382598877,\n",
       " 0.6356532573699951,\n",
       " 0.7808274626731873,\n",
       " 0.6976386308670044,\n",
       " 0.6815195679664612,\n",
       " 0.5998408198356628,\n",
       " 0.4749812185764313,\n",
       " 0.3917040228843689,\n",
       " 1.1139411926269531,\n",
       " 1.7644257545471191,\n",
       " 1.743551254272461,\n",
       " 0.5670009255409241,\n",
       " 0.36650827527046204,\n",
       " 0.9405661225318909,\n",
       " 0.6831498742103577,\n",
       " 0.5027104616165161,\n",
       " 0.9520105719566345,\n",
       " 1.125625729560852,\n",
       " 3.6237738132476807,\n",
       " 2.6148533821105957,\n",
       " 0.9421584606170654,\n",
       " 0.5472754240036011,\n",
       " 0.6155757904052734,\n",
       " 1.835038661956787,\n",
       " 0.7135471701622009,\n",
       " 1.44832444190979,\n",
       " 0.8698838949203491,\n",
       " 1.7813423871994019,\n",
       " 0.3835853338241577,\n",
       " 0.4076337218284607,\n",
       " 0.8243339657783508,\n",
       " 0.4084721505641937,\n",
       " 0.6304378509521484,\n",
       " 0.32720643281936646,\n",
       " 0.5986371636390686,\n",
       " 0.805038332939148,\n",
       " 1.7026782035827637,\n",
       " 0.48341014981269836,\n",
       " 1.3114203214645386,\n",
       " 0.9163693785667419,\n",
       " 1.525949478149414,\n",
       " 1.2066518068313599,\n",
       " 1.4079869985580444,\n",
       " 0.7255473136901855,\n",
       " 1.2749381065368652,\n",
       " 0.8955796360969543,\n",
       " 0.6445927023887634,\n",
       " 1.5023322105407715,\n",
       " 1.0977782011032104,\n",
       " 1.7908669710159302,\n",
       " 1.037284016609192,\n",
       " 2.0626187324523926,\n",
       " 0.8138039112091064,\n",
       " 1.2322604656219482,\n",
       " 0.4311870038509369,\n",
       " 1.0688645839691162,\n",
       " 0.6668272018432617,\n",
       " 3.063505172729492,\n",
       " 0.830393373966217,\n",
       " 0.6545764803886414,\n",
       " 0.9326821565628052,\n",
       " 0.587329089641571,\n",
       " 0.3504660725593567,\n",
       " 0.3771876394748688,\n",
       " 0.841709554195404,\n",
       " 1.8900080919265747,\n",
       " 1.1114263534545898,\n",
       " 0.851322591304779,\n",
       " 0.9051071405410767,\n",
       " nan]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pehe_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb8e4efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for key, value in best_trial.params.items():\n",
    "#    print(\"{}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f0b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ate_pred=torch.mean(cate_pred)\n",
    "#print(\"Estimated ATE (True is 4):\", ate_pred.detach().numpy(),'\\n\\n')\n",
    "\n",
    "#print(\"Individualized CATE Estimates: BLUE\")\n",
    "#print(pd.Series(cate_pred.detach().numpy()).plot.kde(color='blue'))\n",
    "#print(\"Individualized CATE True: Green\")\n",
    "#print(pd.Series(cate_true.detach().numpy()).plot.kde(color='green'))\n",
    "\n",
    "#print(\"\\nError CATE Estimates: RED\")\n",
    "#print(pd.Series(cate_pred.detach().numpy()-cate_true.detach().numpy()).plot.kde(color='red'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
