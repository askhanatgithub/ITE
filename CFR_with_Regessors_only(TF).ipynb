{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4be27e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np #numpy is the numerical computing package in python\n",
    "import datetime #we'll use dates to label our logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c65da35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def make_tarnet(input_dim, reg_l2):\n",
    "    '''\n",
    "    The first argument is the column dimension of our data.\n",
    "    It needs to be specified because the functional API creates a static computational graph\n",
    "    The second argument is the strength of regularization we'll apply to the output layers\n",
    "    '''\n",
    "    x = Input(shape=(input_dim,), name='input')\n",
    "\n",
    "    # REPRESENTATION\n",
    "    #in TF2/Keras it is idiomatic to instantiate a layer and pass its inputs on the same line unless the layer will be reused\n",
    "    #Note that we apply no regularization to the representation layers \n",
    "    phi = Dense(units=200, activation='elu', kernel_initializer='RandomNormal',name='phi_1')(x)\n",
    "    phi = Dense(units=200, activation='elu', kernel_initializer='RandomNormal',name='phi_2')(phi)\n",
    "    phi = Dense(units=200, activation='elu', kernel_initializer='RandomNormal',name='phi_3')(phi)\n",
    "\n",
    "    # HYPOTHESIS\n",
    "    y0_hidden = Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_1')(phi)\n",
    "    y1_hidden = Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_1')(phi)\n",
    "\n",
    "    # second layer\n",
    "    y0_hidden = Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_2')(y0_hidden)\n",
    "    y1_hidden = Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_2')(y1_hidden)\n",
    "\n",
    "    # third\n",
    "    y0_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y0_predictions')(y0_hidden)\n",
    "    y1_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y1_predictions')(y1_hidden)\n",
    "\n",
    "    #a convenience \"layer\" that concatenates arrays as columns in a matrix\n",
    "    concat_pred = Concatenate(1)([y0_predictions, y1_predictions])\n",
    "    #the declarations above have specified the computational graph of our network, now we instantiate it\n",
    "    model = Model(inputs=x, outputs=concat_pred)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7855de13",
   "metadata": {},
   "outputs": [],
   "source": [
    " tarnet_model=make_tarnet(25,.01)\n",
    "\n",
    "#print(tarnet_model.summary())\n",
    "#tf.keras.utils.plot_model(tarnet_model, show_shapes=True, show_layer_names=True, to_file='tarnet.png')\n",
    "\n",
    "#from IPython.display import Image # this just Jupyter notebook stuff\n",
    "#Image(retina=True, filename='tarnet.png')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b3db962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# every loss function in TF2 takes 2 arguments, a vector of true values and a vector predictions\n",
    "def regression_loss(concat_true, concat_pred):\n",
    "    #computes a standard MSE loss for TARNet\n",
    "    y_true = concat_true[:, 0] #get individual vectors\n",
    "    t_true = concat_true[:, 1]\n",
    "\n",
    "    y0_pred = concat_pred[:, 0]\n",
    "    y1_pred = concat_pred[:, 1]\n",
    "\n",
    "    #Each head outputs a prediction for both potential outcomes\n",
    "    #We use t_true as a switch to only calculate the factual loss\n",
    "    loss0 = tf.reduce_sum((1. - t_true) * tf.square(y_true - y0_pred))\n",
    "    loss1 = tf.reduce_sum(t_true * tf.square(y_true - y1_pred))\n",
    "    #note Shi uses tf.reduce_sum for her losses instead of tf.reduce_mean.\n",
    "    #They should be equivalent but it's possible that having larger gradients accelerates convergence.\n",
    "    #You can always try changing it!\n",
    "    return loss0 + loss1\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ddd0ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.train.npz\n",
    "#!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.test.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cd5a0b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_IHDP_data(training_data,testing_data,i=0):\n",
    "    with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
    "        train_data=np.load(trf); test_data=np.load(tef)\n",
    "        y=np.concatenate(   (train_data['yf'][:,i],   test_data['yf'][:,i])).astype('float32') #most GPUs only compute 32-bit floats\n",
    "        t=np.concatenate(   (train_data['t'][:,i],    test_data['t'][:,i])).astype('float32')\n",
    "        x=np.concatenate(   (train_data['x'][:,:,i],  test_data['x'][:,:,i]),axis=0).astype('float32')\n",
    "        mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
    "        mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
    " \n",
    "        data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "        data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "        data['y']=data['y'].reshape(-1,1)\n",
    "        \n",
    "        #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "        data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "        data['ys'] = data['y_scaler'].transform(data['y'])\n",
    " \n",
    "    return data\n",
    "\n",
    "\n",
    "def load_IHDP_data_n(training_data,testing_data,i,dt):\n",
    "    \n",
    "    if(dt=='train'):\n",
    "        \n",
    "        with open(training_data,'rb') as trf:\n",
    "            train_data=np.load(trf)\n",
    "            y=train_data['yf'][:,i].astype('float32') #most GPUs only compute 32-bit floats\n",
    "            t=train_data['t'][:,i].astype('float32')\n",
    "            x=train_data['x'][:,:,i].astype('float32')\n",
    "            mu_0=np.concatenate((train_data['mu0'][:,i]).astype('float32')\n",
    "            mu_1=np.concatenate((train_data['mu1'][:,i]).astype('float32')\n",
    "\n",
    "            data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "            data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "            data['y']=data['y'].reshape(-1,1)\n",
    "\n",
    "            #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "            data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "            data['ys'] = data['y_scaler'].transform(data['y'])\n",
    "    else:\n",
    "           \n",
    "        with open(testing_data,'rb') as tef:\n",
    "            test_data=np.load(tef)\n",
    "            y=  test_data['yf'][:,i].astype('float32') #most GPUs only compute 32-bit floats\n",
    "            t= test_data['t'][:,i].astype('float32')\n",
    "            x= test_data['x'][:,:,i].astype('float32')\n",
    "            mu_0= test_data['mu0'][:,i].astype('float32')\n",
    "            mu_1= test_data['mu1'][:,i].astype('float32')\n",
    "\n",
    "            data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "            data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "            data['y']=data['y'].reshape(-1,1)\n",
    "\n",
    "            #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "            data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "            data['ys'] = data['y_scaler'].transform(data['y'])\n",
    "\n",
    "    return data\n",
    " \n",
    "def cal_pehe(i,model):\n",
    "\n",
    "    data=load_IHDP_data_n('./ihdp_npci_1-100.train.npz','./ihdp_npci_1-100.test.npz',i,'test')\n",
    "\n",
    "    concat_pred=model.predict(data['x'])\n",
    "    #dont forget to rescale the outcome before estimation!\n",
    "    y0_pred = data['y_scaler'].inverse_transform(concat_pred[:, 0].reshape(-1, 1))\n",
    "    y1_pred = data['y_scaler'].inverse_transform(concat_pred[:, 1].reshape(-1, 1))\n",
    "\n",
    "\n",
    "\n",
    "    cate_pred=y1_pred-y0_pred\n",
    "    cate_true=data['mu_1']-data['mu_0'] #Hill's noiseless true values\n",
    "    #ate_pred=tf.reduce_mean(cate_pred)\n",
    "    #print(\"Estimated ATE (True is 4):\", ate_pred.numpy(),'\\n\\n')\n",
    "\n",
    "    #print(\"Individualized CATE Estimates: BLUE\")\n",
    "    #print(pd.Series(cate_pred.squeeze()).plot.kde(color='blue'))\n",
    "\n",
    "    #print(\"Individualized CATE True: Green\")\n",
    "    #print(pd.Series(cate_true.squeeze()).plot.kde(color='green'))\n",
    "\n",
    "    #print(\"\\nError CATE Estimates: RED\")\n",
    "    #print(pd.Series(cate_pred.squeeze()-cate_true.squeeze()).plot.kde(color='red'))\n",
    "\n",
    "\n",
    "    cate_err=tf.reduce_mean( tf.square( ( (data['mu_1']-data['mu_0']) - (y1_pred-y0_pred) ) ) )\n",
    "\n",
    "    return tf.math.sqrt(cate_err)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b1cdfbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "pehe_loss=[]\n",
    "val_split=0.2\n",
    "batch_size=100\n",
    "verbose=1\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    " #we'll use both y and t to compute the loss\n",
    "sgd_callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='val_loss', patience=40, min_delta=0.), \n",
    "        #40 is Shi's recommendation for this dataset, but you should tune for your data \n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=0., cooldown=0, min_lr=0),\n",
    "    ]\n",
    "#optimzier hyperparameters\n",
    "sgd_lr = 1e-5\n",
    "momentum = 0.9\n",
    "#tarnet_model.compile(optimizer=SGD(lr=sgd_lr, momentum=momentum, nesterov=True),\n",
    "                    #loss=regression_loss,\n",
    "                    #metrics=regression_loss)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,100):\n",
    "   \n",
    "    \n",
    "    data=load_IHDP_data_n('./ihdp_npci_1-100.train.npz','./ihdp_npci_1-100.test.npz',i,'train')\n",
    "    tarnet_model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                    loss=regression_loss,\n",
    "                    metrics=regression_loss)\n",
    "    \n",
    "    yt = np.concatenate([data['ys'], data['t']], 1)\n",
    "\n",
    "    tarnet_model.fit(x=data['x'],y=yt,\n",
    "                    callbacks=sgd_callbacks,\n",
    "                    validation_split=val_split,\n",
    "                    epochs=300,\n",
    "                    batch_size=batch_size,\n",
    "                    verbose=verbose)\n",
    "    \n",
    "    pehe_loss.append(cal_pehe(i,tarnet_model))\n",
    "    \n",
    "print(\"DONE!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6c31298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(pehe_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23441e30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
