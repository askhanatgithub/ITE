{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fcd509dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np #numpy is the numerical computing package in python\n",
    "import datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e344ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.train.npz\n",
    "#!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.test.npz \n",
    "def load_IHDP_data(training_data,testing_data,i=1):\n",
    "    with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
    "        train_data=np.load(trf); test_data=np.load(tef)\n",
    "        y=np.concatenate(   (train_data['yf'][:,i],   test_data['yf'][:,i])).astype('float32') #most GPUs only compute 32-bit floats\n",
    "        t=np.concatenate(   (train_data['t'][:,i],    test_data['t'][:,i])).astype('float32')\n",
    "        x=np.concatenate(   (train_data['x'][:,:,i],  test_data['x'][:,:,i]),axis=0).astype('float32')\n",
    "        mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
    "        mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
    "\n",
    "        data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "        data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "        data['y']=data['y'].reshape(-1,1)\n",
    "        \n",
    "        #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "        data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "        data['ys'] = data['y_scaler'].transform(data['y'])\n",
    "\n",
    "    return data\n",
    "\n",
    "def load_IHDP_data_n(training_data,testing_data,i,dt):\n",
    "    \n",
    "    if(dt=='train'):\n",
    "        \n",
    "        with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
    "            train_data=np.load(trf); test_data=np.load(tef)\n",
    "            y=train_data['yf'][:,i].astype('float32') #most GPUs only compute 32-bit floats\n",
    "            t=train_data['t'][:,i].astype('float32')\n",
    "            x=train_data['x'][:,:,i].astype('float32')\n",
    "            mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
    "            mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
    "\n",
    "            data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "            data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "            data['y']=data['y'].reshape(-1,1)\n",
    "\n",
    "            #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "            data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "            data['ys'] = data['y_scaler'].transform(data['y'])\n",
    "    else:\n",
    "           \n",
    "        with open(testing_data,'rb') as tef:\n",
    "            test_data=np.load(tef)\n",
    "            y=  test_data['yf'][:,i].astype('float32') #most GPUs only compute 32-bit floats\n",
    "            t= test_data['t'][:,i].astype('float32')\n",
    "            x= test_data['x'][:,:,i].astype('float32')\n",
    "            mu_0= test_data['mu0'][:,i].astype('float32')\n",
    "            mu_1= test_data['mu1'][:,i].astype('float32')\n",
    "\n",
    "            data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "            data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "            data['y']=data['y'].reshape(-1,1)\n",
    "\n",
    "            #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "            data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "            data['ys'] = data['y_scaler'].transform(data['y'])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def cal_pehe(i,model):\n",
    "\n",
    "    data=load_IHDP_data_n('./ihdp_npci_1-100.train.npz','./ihdp_npci_1-100.test.npz',i,'test')\n",
    "\n",
    "    concat_pred=model.predict(data['x'])\n",
    "    #dont forget to rescale the outcome before estimation!\n",
    "    y0_pred = data['y_scaler'].inverse_transform(concat_pred[:, 0].reshape(-1, 1))\n",
    "    y1_pred = data['y_scaler'].inverse_transform(concat_pred[:, 1].reshape(-1, 1))\n",
    "\n",
    "\n",
    "\n",
    "    cate_pred=y1_pred-y0_pred\n",
    "    cate_true=data['mu_1']-data['mu_0'] #Hill's noiseless true values\n",
    "    #ate_pred=tf.reduce_mean(cate_pred)\n",
    "    #print(\"Estimated ATE (True is 4):\", ate_pred.numpy(),'\\n\\n')\n",
    "\n",
    "    #print(\"Individualized CATE Estimates: BLUE\")\n",
    "    #print(pd.Series(cate_pred.squeeze()).plot.kde(color='blue'))\n",
    "\n",
    "    #print(\"Individualized CATE True: Green\")\n",
    "    #print(pd.Series(cate_true.squeeze()).plot.kde(color='green'))\n",
    "\n",
    "    #print(\"\\nError CATE Estimates: RED\")\n",
    "    #print(pd.Series(cate_pred.squeeze()-cate_true.squeeze()).plot.kde(color='red'))\n",
    "\n",
    "\n",
    "    cate_err=tf.reduce_mean( tf.square( ( (data['mu_1']-data['mu_0']) - (y1_pred-y0_pred) ) ) )\n",
    "\n",
    "    return tf.math.sqrt(cate_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "115253ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pdist2sq(x,y):\n",
    "    x2 = tf.reduce_sum(x ** 2, axis=-1, keepdims=True)\n",
    "    y2 = tf.reduce_sum(y ** 2, axis=-1, keepdims=True)\n",
    "    dist = x2 + tf.transpose(y2, (1, 0)) - 2. * x @ tf.transpose(y, (1, 0))\n",
    "    return dist\n",
    "\n",
    "from tensorflow.keras.losses import Loss\n",
    "\n",
    "class CFRNet_Loss(Loss):\n",
    "  #initialize instance attributes\n",
    "    def __init__(self, alpha=1.,sigma=1.):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha # balances regression loss and MMD IPM\n",
    "        self.rbf_sigma=sigma #for gaussian kernel\n",
    "        self.name='cfrnet_loss'\n",
    "      \n",
    "    def split_pred(self,concat_pred):\n",
    "      #generic helper to make sure we dont make mistakes\n",
    "        preds={}\n",
    "        preds['y0_pred'] = concat_pred[:, 0]\n",
    "        preds['y1_pred'] = concat_pred[:, 1]\n",
    "        preds['phi'] = concat_pred[:, 2:]\n",
    "        return preds\n",
    "\n",
    "    def rbf_kernel(self, x, y):\n",
    "        return tf.exp(-pdist2sq(x,y)/tf.square(self.rbf_sigma))\n",
    "\n",
    "    def calc_mmdsq(self, Phi, t):\n",
    "        Phic, Phit =tf.dynamic_partition(Phi,tf.cast(tf.squeeze(t),tf.int32),2)\n",
    "\n",
    "        Kcc = self.rbf_kernel(Phic,Phic)\n",
    "        Kct = self.rbf_kernel(Phic,Phit)\n",
    "        Ktt = self.rbf_kernel(Phit,Phit)\n",
    "\n",
    "        m = tf.cast(tf.shape(Phic)[0],Phi.dtype)\n",
    "        n = tf.cast(tf.shape(Phit)[0],Phi.dtype)\n",
    "\n",
    "        mmd = 1.0/(m*(m-1.0))*(tf.reduce_sum(Kcc))\n",
    "        mmd = mmd + 1.0/(n*(n-1.0))*(tf.reduce_sum(Ktt))\n",
    "        mmd = mmd - 2.0/(m*n)*tf.reduce_sum(Kct)\n",
    "        return mmd * tf.ones_like(t)\n",
    "\n",
    "    def mmdsq_loss(self, concat_true,concat_pred):\n",
    "        t_true = concat_true[:, 1]\n",
    "        p=self.split_pred(concat_pred)\n",
    "        mmdsq_loss = tf.reduce_mean(self.calc_mmdsq(p['phi'],t_true))\n",
    "        return mmdsq_loss\n",
    "\n",
    "    def regression_loss(self,concat_true,concat_pred):\n",
    "        y_true = concat_true[:, 0]\n",
    "        t_true = concat_true[:, 1]\n",
    "        p = self.split_pred(concat_pred)\n",
    "        loss0 = tf.reduce_mean((1. - t_true) * tf.square(y_true - p['y0_pred']))\n",
    "        loss1 = tf.reduce_mean(t_true * tf.square(y_true - p['y1_pred']))\n",
    "        return loss0+loss1\n",
    "\n",
    "    def cfr_loss(self,concat_true,concat_pred):\n",
    "        lossR = self.regression_loss(concat_true,concat_pred)\n",
    "        lossIPM = self.mmdsq_loss(concat_true,concat_pred)\n",
    "        return lossR + self.alpha * lossIPM\n",
    "\n",
    "      #return lossR + self.alpha * lossIPM\n",
    "\n",
    "  #compute loss\n",
    "    def call(self, concat_true, concat_pred):        \n",
    "        return self.cfr_loss(concat_true,concat_pred)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d1fe26a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "#https://towardsdatascience.com/implementing-macro-f1-score-in-keras-what-not-to-do-e9f1aa04029d\n",
    "class Base_Metrics(Callback):\n",
    "    def __init__(self,data, verbose=0):   \n",
    "        super(Base_Metrics, self).__init__()\n",
    "        self.data=data #feed the callback the full dataset\n",
    "        self.verbose=verbose\n",
    "\n",
    "        #needed for PEHEnn; Called in self.find_ynn\n",
    "        self.data['o_idx']=tf.range(self.data['t'].shape[0])\n",
    "        self.data['c_idx']=self.data['o_idx'][self.data['t'].squeeze()==0] #These are the indices of the control units\n",
    "        self.data['t_idx']=self.data['o_idx'][self.data['t'].squeeze()==1] #These are the indices of the treated units\n",
    "    \n",
    "    def split_pred(self,concat_pred):\n",
    "        preds={}\n",
    "        preds['y0_pred'] = self.data['y_scaler'].inverse_transform(concat_pred[:, 0].reshape(-1, 1))\n",
    "        preds['y1_pred'] = self.data['y_scaler'].inverse_transform(concat_pred[:, 1].reshape(-1, 1))\n",
    "        preds['phi'] = concat_pred[:, 2:]\n",
    "        return preds\n",
    "\n",
    "    def find_ynn(self, Phi):\n",
    "        #helper for PEHEnn\n",
    "        PhiC, PhiT =tf.dynamic_partition(Phi,tf.cast(tf.squeeze(self.data['t']),tf.int32),2) #separate control and treated reps\n",
    "        dists=tf.sqrt(pdist2sq(PhiC,PhiT)) #calculate squared distance then sqrt to get euclidean\n",
    "        yT_nn_idx=tf.gather(self.data['c_idx'],tf.argmin(dists,axis=0),1) #get c_idxs of smallest distances for treated units\n",
    "        yC_nn_idx=tf.gather(self.data['t_idx'],tf.argmin(dists,axis=1),1) #get t_idxs of smallest distances for control units\n",
    "        yT_nn=tf.gather(self.data['y'],yT_nn_idx,1) #now use these to retrieve y values\n",
    "        yC_nn=tf.gather(self.data['y'],yC_nn_idx,1)\n",
    "        y_nn=tf.dynamic_stitch([self.data['t_idx'],self.data['c_idx']],[yT_nn,yC_nn]) #stitch em back up!\n",
    "        return y_nn\n",
    "\n",
    "    def PEHEnn(self,concat_pred):\n",
    "        p = self.split_pred(concat_pred)\n",
    "        y_nn = self.find_ynn(p['phi']) #now its 3 plus because \n",
    "        cate_nn_err=tf.reduce_mean( tf.square( (1-2*self.data['t']) * (y_nn-self.data['y']) - (p['y1_pred']-p['y0_pred']) ) )\n",
    "        return cate_nn_err\n",
    "\n",
    "    def ATE(self,concat_pred):\n",
    "        p = self.split_pred(concat_pred)\n",
    "        return p['y1_pred']-p['y0_pred']\n",
    "\n",
    "    def PEHE(self,concat_pred):\n",
    "        #simulation only\n",
    "        p = self.split_pred(concat_pred)\n",
    "        cate_err=tf.reduce_mean( tf.square( ( (self.data['mu_1']-self.data['mu_0']) - (p['y1_pred']-p['y0_pred']) ) ) )\n",
    "        return cate_err \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        concat_pred=self.model.predict(self.data['x'])\n",
    "        #Calculate Empirical Metrics        \n",
    "        ate_pred=tf.reduce_mean(self.ATE(concat_pred)); tf.summary.scalar('ate', data=ate_pred, step=epoch)\n",
    "        pehe_nn=self.PEHEnn(concat_pred); tf.summary.scalar('cate_nn_err', data=tf.sqrt(pehe_nn), step=epoch)\n",
    "        \n",
    "        #Simulation Metrics\n",
    "        ate_true=tf.reduce_mean(self.data['mu_1']-self.data['mu_0'])\n",
    "        ate_err=tf.abs(ate_true-ate_pred); tf.summary.scalar('ate_err', data=ate_err, step=epoch)\n",
    "        pehe =self.PEHE(concat_pred); tf.summary.scalar('cate_err', data=tf.sqrt(pehe), step=epoch)\n",
    "        out_str=f' — ate_err: {ate_err:.4f}  — cate_err: {tf.sqrt(pehe):.4f} — cate_nn_err: {tf.sqrt(pehe_nn):.4f} '\n",
    "        \n",
    "        if self.verbose > 0: print(out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2b788c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.metrics import binary_accuracy\n",
    "from tensorflow.keras.losses import Loss\n",
    "\n",
    "def make_tarnet(input_dim, reg_l2):\n",
    "\n",
    "    x = Input(shape=(input_dim,), name='input')\n",
    "\n",
    "    # representation\n",
    "    phi = Dense(units=25, activation='elu', kernel_initializer='RandomNormal',name='phi_1')(x)\n",
    "    phi = Dense(units=25, activation='elu', kernel_initializer='RandomNormal',name='phi_2')(phi)\n",
    "\n",
    "    # HYPOTHESIS\n",
    "    y0_hidden = Dense(units=25, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_1')(phi)\n",
    "    y1_hidden = Dense(units=25, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_1')(phi)\n",
    "\n",
    "    # second layer\n",
    "    y0_hidden = Dense(units=25, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_2')(y0_hidden)\n",
    "    y1_hidden = Dense(units=25, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_2')(y1_hidden)\n",
    "\n",
    "    # third\n",
    "    y0_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y0_predictions')(y0_hidden)\n",
    "    y1_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y1_predictions')(y1_hidden)\n",
    "\n",
    "    concat_pred = Concatenate(1)([y0_predictions, y1_predictions,phi])\n",
    "    model = Model(inputs=x, outputs=concat_pred)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcd666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "#Colab command to allow us to run Colab in TF2\n",
    "%load_ext tensorboard \n",
    "pehe_loss=[]\n",
    "val_split=0.2\n",
    "batch_size=100\n",
    "verbose=1\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    "\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/ \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0)\n",
    "\n",
    "#let's try ADAM this time\n",
    "adam_callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='val_loss', patience=2, min_delta=0.),\n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=1e-8, cooldown=0, min_lr=0),\n",
    "        tensorboard_callback,\n",
    "        Base_Metrics(data,verbose=verbose)\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "cfrnet_loss=CFRNet_Loss(alpha=1.0)\n",
    "\n",
    "\n",
    "for i in range(1,101):\n",
    "   \n",
    "    cfrnet_model=make_tarnet(data['x'].shape[1],.01)\n",
    "    data=load_IHDP_data_n('./ihdp_npci_1-100.train.npz','./ihdp_npci_1-100.test.npz',i,'train')\n",
    "    yt = np.concatenate([data['ys'], data['t']], 1)\n",
    "\n",
    "    cfrnet_model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                          loss=cfrnet_loss,\n",
    "                     metrics=[cfrnet_loss,cfrnet_loss.regression_loss,cfrnet_loss.mmdsq_loss])\n",
    "\n",
    "    cfrnet_model.fit(x=data['x'],y=yt,\n",
    "                     callbacks=adam_callbacks,\n",
    "                      validation_split=val_split,\n",
    "                      epochs=500,\n",
    "                      batch_size=batch_size,\n",
    "                      verbose=verbose)\n",
    "    \n",
    "    pehe_loss.append(cal_pehe(i,cfrnet_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ac5364",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pehe_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc12473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(pehe_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0d75b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#concat_pred=cfrnet_model.predict(data['x'])\n",
    "#dont forget to rescale the outcome before estimation!\n",
    "#y0_pred = data['y_scaler'].inverse_transform(concat_pred[:, 0].reshape(-1, 1))\n",
    "#y1_pred = data['y_scaler'].inverse_transform(concat_pred[:, 1].reshape(-1, 1))\n",
    "\n",
    "\n",
    "\n",
    "#cate_pred=y1_pred-y0_pred\n",
    "#cate_true=data['mu_1']-data['mu_0'] #Hill's noiseless true values\n",
    "#ate_pred=tf.reduce_mean(cate_pred)\n",
    "#print(\"Estimated ATE (True is 4):\", ate_pred.numpy(),'\\n\\n')\n",
    "\n",
    "#print(\"Individualized CATE Estimates: BLUE\")\n",
    "#print(pd.Series(cate_pred.squeeze()).plot.kde(color='blue'))\n",
    "\n",
    "#print(\"Individualized CATE True: Green\")\n",
    "#print(pd.Series(cate_true.squeeze()).plot.kde(color='green'))\n",
    "\n",
    "#print(\"\\nError CATE Estimates: RED\")\n",
    "#print(pd.Series(cate_pred.squeeze()-cate_true.squeeze()).plot.kde(color='red'))\n",
    "\n",
    "     \n",
    "#cate_err=tf.reduce_mean( tf.square( ( (data['mu_1']-data['mu_0']) - (y1_pred-y0_pred) ) ) )\n",
    "\n",
    "#print('pehe on training:',tf.math.sqrt(cate_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6b7cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e07753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
