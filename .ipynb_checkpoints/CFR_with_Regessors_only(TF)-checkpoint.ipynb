{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4be27e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np #numpy is the numerical computing package in python\n",
    "import datetime #we'll use dates to label our logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c65da35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def make_tarnet(input_dim, reg_l2):\n",
    "    '''\n",
    "    The first argument is the column dimension of our data.\n",
    "    It needs to be specified because the functional API creates a static computational graph\n",
    "    The second argument is the strength of regularization we'll apply to the output layers\n",
    "    '''\n",
    "    x = Input(shape=(input_dim,), name='input')\n",
    "\n",
    "    # REPRESENTATION\n",
    "    #in TF2/Keras it is idiomatic to instantiate a layer and pass its inputs on the same line unless the layer will be reused\n",
    "    #Note that we apply no regularization to the representation layers \n",
    "    phi = Dense(units=200, activation='elu', kernel_initializer='RandomNormal',name='phi_1')(x)\n",
    "    phi = Dense(units=200, activation='elu', kernel_initializer='RandomNormal',name='phi_2')(phi)\n",
    "    phi = Dense(units=200, activation='elu', kernel_initializer='RandomNormal',name='phi_3')(phi)\n",
    "\n",
    "    # HYPOTHESIS\n",
    "    y0_hidden = Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_1')(phi)\n",
    "    y1_hidden = Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_1')(phi)\n",
    "\n",
    "    # second layer\n",
    "    y0_hidden = Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_2')(y0_hidden)\n",
    "    y1_hidden = Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_2')(y1_hidden)\n",
    "\n",
    "    # third\n",
    "    y0_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y0_predictions')(y0_hidden)\n",
    "    y1_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y1_predictions')(y1_hidden)\n",
    "\n",
    "    #a convenience \"layer\" that concatenates arrays as columns in a matrix\n",
    "    concat_pred = Concatenate(1)([y0_predictions, y1_predictions])\n",
    "    #the declarations above have specified the computational graph of our network, now we instantiate it\n",
    "    model = Model(inputs=x, outputs=concat_pred)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7855de13",
   "metadata": {},
   "outputs": [],
   "source": [
    " tarnet_model=make_tarnet(25,.01)\n",
    "\n",
    "#print(tarnet_model.summary())\n",
    "#tf.keras.utils.plot_model(tarnet_model, show_shapes=True, show_layer_names=True, to_file='tarnet.png')\n",
    "\n",
    "#from IPython.display import Image # this just Jupyter notebook stuff\n",
    "#Image(retina=True, filename='tarnet.png')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b3db962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# every loss function in TF2 takes 2 arguments, a vector of true values and a vector predictions\n",
    "def regression_loss(concat_true, concat_pred):\n",
    "    #computes a standard MSE loss for TARNet\n",
    "    y_true = concat_true[:, 0] #get individual vectors\n",
    "    t_true = concat_true[:, 1]\n",
    "\n",
    "    y0_pred = concat_pred[:, 0]\n",
    "    y1_pred = concat_pred[:, 1]\n",
    "\n",
    "    #Each head outputs a prediction for both potential outcomes\n",
    "    #We use t_true as a switch to only calculate the factual loss\n",
    "    loss0 = tf.reduce_sum((1. - t_true) * tf.square(y_true - y0_pred))\n",
    "    loss1 = tf.reduce_sum(t_true * tf.square(y_true - y1_pred))\n",
    "    #note Shi uses tf.reduce_sum for her losses instead of tf.reduce_mean.\n",
    "    #They should be equivalent but it's possible that having larger gradients accelerates convergence.\n",
    "    #You can always try changing it!\n",
    "    return loss0 + loss1\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ddd0ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.train.npz\n",
    "#!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.test.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cd5a0b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_IHDP_data(training_data,testing_data,i=0):\n",
    "    with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
    "        train_data=np.load(trf); test_data=np.load(tef)\n",
    "        y=np.concatenate(   (train_data['yf'][:,i],   test_data['yf'][:,i])).astype('float32') #most GPUs only compute 32-bit floats\n",
    "        t=np.concatenate(   (train_data['t'][:,i],    test_data['t'][:,i])).astype('float32')\n",
    "        x=np.concatenate(   (train_data['x'][:,:,i],  test_data['x'][:,:,i]),axis=0).astype('float32')\n",
    "        mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
    "        mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
    " \n",
    "        data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "        data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "        data['y']=data['y'].reshape(-1,1)\n",
    "        \n",
    "        #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "        data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "        data['ys'] = data['y_scaler'].transform(data['y'])\n",
    " \n",
    "    return data\n",
    "\n",
    "\n",
    "def load_IHDP_data_n(training_data,testing_data,i,dt):\n",
    "    \n",
    "    if(dt=='train'):\n",
    "        \n",
    "        with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
    "            train_data=np.load(trf); test_data=np.load(tef)\n",
    "            y=train_data['yf'][:,i].astype('float32') #most GPUs only compute 32-bit floats\n",
    "            t=train_data['t'][:,i].astype('float32')\n",
    "            x=train_data['x'][:,:,i].astype('float32')\n",
    "            mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
    "            mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
    "\n",
    "            data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "            data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "            data['y']=data['y'].reshape(-1,1)\n",
    "\n",
    "            #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "            data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "            data['ys'] = data['y_scaler'].transform(data['y'])\n",
    "    else:\n",
    "           \n",
    "        with open(testing_data,'rb') as tef:\n",
    "            test_data=np.load(tef)\n",
    "            y=  test_data['yf'][:,i].astype('float32') #most GPUs only compute 32-bit floats\n",
    "            t= test_data['t'][:,i].astype('float32')\n",
    "            x= test_data['x'][:,:,i].astype('float32')\n",
    "            mu_0= test_data['mu0'][:,i].astype('float32')\n",
    "            mu_1= test_data['mu1'][:,i].astype('float32')\n",
    "\n",
    "            data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "            data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "            data['y']=data['y'].reshape(-1,1)\n",
    "\n",
    "            #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "            data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "            data['ys'] = data['y_scaler'].transform(data['y'])\n",
    "\n",
    "    return data\n",
    " \n",
    "def cal_pehe(i,model):\n",
    "\n",
    "    data=load_IHDP_data_n('./ihdp_npci_1-100.train.npz','./ihdp_npci_1-100.test.npz',i,'test')\n",
    "\n",
    "    concat_pred=model.predict(data['x'])\n",
    "    #dont forget to rescale the outcome before estimation!\n",
    "    y0_pred = data['y_scaler'].inverse_transform(concat_pred[:, 0].reshape(-1, 1))\n",
    "    y1_pred = data['y_scaler'].inverse_transform(concat_pred[:, 1].reshape(-1, 1))\n",
    "\n",
    "\n",
    "\n",
    "    cate_pred=y1_pred-y0_pred\n",
    "    cate_true=data['mu_1']-data['mu_0'] #Hill's noiseless true values\n",
    "    #ate_pred=tf.reduce_mean(cate_pred)\n",
    "    #print(\"Estimated ATE (True is 4):\", ate_pred.numpy(),'\\n\\n')\n",
    "\n",
    "    #print(\"Individualized CATE Estimates: BLUE\")\n",
    "    #print(pd.Series(cate_pred.squeeze()).plot.kde(color='blue'))\n",
    "\n",
    "    #print(\"Individualized CATE True: Green\")\n",
    "    #print(pd.Series(cate_true.squeeze()).plot.kde(color='green'))\n",
    "\n",
    "    #print(\"\\nError CATE Estimates: RED\")\n",
    "    #print(pd.Series(cate_pred.squeeze()-cate_true.squeeze()).plot.kde(color='red'))\n",
    "\n",
    "\n",
    "    cate_err=tf.reduce_mean( tf.square( ( (data['mu_1']-data['mu_0']) - (y1_pred-y0_pred) ) ) )\n",
    "\n",
    "    return tf.math.sqrt(cate_err)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b1cdfbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 26ms/step - loss: 92.0159 - regression_loss: 80.9787 - val_loss: 71.8339 - val_regression_loss: 55.7214 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 67.3831 - regression_loss: 59.0302 - val_loss: 55.2516 - val_regression_loss: 41.8684 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 51.8945 - regression_loss: 45.0948 - val_loss: 43.7015 - val_regression_loss: 32.0366 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 43.1608 - regression_loss: 36.3134 - val_loss: 36.9555 - val_regression_loss: 25.9060 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 37.8191 - regression_loss: 30.8728 - val_loss: 34.6427 - val_regression_loss: 23.3421 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 34.9710 - regression_loss: 28.9556 - val_loss: 34.7677 - val_regression_loss: 22.9204 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 35.3798 - regression_loss: 28.2201 - val_loss: 33.8725 - val_regression_loss: 22.1656 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 32.7998 - regression_loss: 26.9894 - val_loss: 32.5614 - val_regression_loss: 21.2356 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31.9879 - regression_loss: 25.6201 - val_loss: 31.7046 - val_regression_loss: 20.6148 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30.6270 - regression_loss: 24.5752 - val_loss: 31.2010 - val_regression_loss: 20.2255 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.9993 - regression_loss: 23.7459 - val_loss: 30.9170 - val_regression_loss: 19.9603 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.5676 - regression_loss: 22.9874 - val_loss: 30.4110 - val_regression_loss: 19.5516 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.6376 - regression_loss: 22.4284 - val_loss: 30.3076 - val_regression_loss: 19.3586 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.8989 - regression_loss: 21.8243 - val_loss: 30.3002 - val_regression_loss: 19.2511 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.7355 - regression_loss: 21.5593 - val_loss: 30.1173 - val_regression_loss: 19.0902 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.2595 - regression_loss: 21.1127 - val_loss: 29.9896 - val_regression_loss: 19.0372 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.8461 - regression_loss: 20.9629 - val_loss: 30.0714 - val_regression_loss: 19.0517 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.7067 - regression_loss: 20.6707 - val_loss: 30.1605 - val_regression_loss: 19.0611 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.7845 - regression_loss: 20.6351 - val_loss: 29.9852 - val_regression_loss: 18.9432 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.6208 - regression_loss: 20.4292 - val_loss: 29.9250 - val_regression_loss: 18.8776 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.2420 - regression_loss: 20.3159 - val_loss: 29.9013 - val_regression_loss: 18.8599 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.2649 - regression_loss: 20.2693 - val_loss: 29.8958 - val_regression_loss: 18.8249 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.3320 - regression_loss: 20.3106 - val_loss: 29.9459 - val_regression_loss: 18.8502 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.1806 - regression_loss: 20.2494 - val_loss: 29.5805 - val_regression_loss: 18.6746 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.4411 - regression_loss: 20.2306 - val_loss: 30.1256 - val_regression_loss: 18.9755 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.3072 - regression_loss: 20.0530 - val_loss: 29.7573 - val_regression_loss: 18.7588 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.2585 - regression_loss: 19.9736 - val_loss: 29.7714 - val_regression_loss: 18.7376 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.4557 - regression_loss: 19.9468 - val_loss: 29.6613 - val_regression_loss: 18.6501 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.2768 - regression_loss: 19.9057 - val_loss: 29.9117 - val_regression_loss: 18.8079 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.7235 - regression_loss: 19.9948 - val_loss: 29.8359 - val_regression_loss: 18.8301 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.7210 - regression_loss: 20.0159 - val_loss: 30.0529 - val_regression_loss: 18.9096 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.8924 - regression_loss: 19.7737 - val_loss: 29.6312 - val_regression_loss: 18.6662 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 27.9832 - regression_loss: 23.3405\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.1983 - regression_loss: 19.8808 - val_loss: 29.8593 - val_regression_loss: 18.8004 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.7150 - regression_loss: 19.6929 - val_loss: 30.0282 - val_regression_loss: 18.8921 - lr: 5.0000e-05\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.0660 - regression_loss: 19.7201 - val_loss: 29.8600 - val_regression_loss: 18.7634 - lr: 5.0000e-05\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.7483 - regression_loss: 19.6706 - val_loss: 29.6964 - val_regression_loss: 18.6905 - lr: 5.0000e-05\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.6253 - regression_loss: 19.6853 - val_loss: 29.6808 - val_regression_loss: 18.6643 - lr: 5.0000e-05\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.3071 - regression_loss: 19.6561 - val_loss: 29.9223 - val_regression_loss: 18.8027 - lr: 5.0000e-05\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.5210 - regression_loss: 19.6327 - val_loss: 29.9026 - val_regression_loss: 18.8169 - lr: 5.0000e-05\n",
      "Epoch 40/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 28.0162 - regression_loss: 23.3855\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.9161 - regression_loss: 19.6268 - val_loss: 29.8216 - val_regression_loss: 18.7676 - lr: 5.0000e-05\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.2365 - regression_loss: 19.5921 - val_loss: 29.8944 - val_regression_loss: 18.7987 - lr: 2.5000e-05\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.5013 - regression_loss: 19.5452 - val_loss: 29.8529 - val_regression_loss: 18.7672 - lr: 2.5000e-05\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.3777 - regression_loss: 19.5287 - val_loss: 29.7828 - val_regression_loss: 18.7300 - lr: 2.5000e-05\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.5013 - regression_loss: 19.5308 - val_loss: 29.7659 - val_regression_loss: 18.7180 - lr: 2.5000e-05\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.9894 - regression_loss: 19.5294 - val_loss: 29.8003 - val_regression_loss: 18.7378 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.0394 - regression_loss: 19.5218 - val_loss: 29.8585 - val_regression_loss: 18.7763 - lr: 2.5000e-05\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.2897 - regression_loss: 19.5267 - val_loss: 29.9532 - val_regression_loss: 18.8330 - lr: 2.5000e-05\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.4558 - regression_loss: 19.5198 - val_loss: 29.8307 - val_regression_loss: 18.7622 - lr: 2.5000e-05\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.4304 - regression_loss: 19.5004 - val_loss: 29.7919 - val_regression_loss: 18.7403 - lr: 2.5000e-05\n",
      "Epoch 50/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 27.8082 - regression_loss: 23.1859\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.6441 - regression_loss: 19.4842 - val_loss: 29.8399 - val_regression_loss: 18.7618 - lr: 2.5000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.3466 - regression_loss: 19.4816 - val_loss: 29.8216 - val_regression_loss: 18.7520 - lr: 1.2500e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.2128 - regression_loss: 19.4635 - val_loss: 29.8654 - val_regression_loss: 18.7745 - lr: 1.2500e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.5254 - regression_loss: 19.4586 - val_loss: 29.8462 - val_regression_loss: 18.7607 - lr: 1.2500e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.6366 - regression_loss: 19.4755 - val_loss: 29.8835 - val_regression_loss: 18.7895 - lr: 1.2500e-05\n",
      "Epoch 55/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 31.2349 - regression_loss: 26.6150\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.2446 - regression_loss: 19.4606 - val_loss: 29.8542 - val_regression_loss: 18.7730 - lr: 1.2500e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.5704 - regression_loss: 19.4453 - val_loss: 29.8366 - val_regression_loss: 18.7613 - lr: 6.2500e-06\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.6028 - regression_loss: 19.4423 - val_loss: 29.8413 - val_regression_loss: 18.7652 - lr: 6.2500e-06\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.3791 - regression_loss: 19.4427 - val_loss: 29.8518 - val_regression_loss: 18.7702 - lr: 6.2500e-06\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.5193 - regression_loss: 19.4499 - val_loss: 29.8738 - val_regression_loss: 18.7829 - lr: 6.2500e-06\n",
      "Epoch 60/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 27.4808 - regression_loss: 22.8620\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.4083 - regression_loss: 19.4385 - val_loss: 29.8628 - val_regression_loss: 18.7775 - lr: 6.2500e-06\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.3304 - regression_loss: 19.4321 - val_loss: 29.8552 - val_regression_loss: 18.7729 - lr: 3.1250e-06\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.5214 - regression_loss: 19.4307 - val_loss: 29.8584 - val_regression_loss: 18.7754 - lr: 3.1250e-06\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.1587 - regression_loss: 19.4300 - val_loss: 29.8516 - val_regression_loss: 18.7705 - lr: 3.1250e-06\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.5752 - regression_loss: 19.4299 - val_loss: 29.8449 - val_regression_loss: 18.7665 - lr: 3.1250e-06\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 24ms/step - loss: 43.4532 - regression_loss: 36.2311 - val_loss: 31.6198 - val_regression_loss: 23.0477 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 35.6331 - regression_loss: 29.2047 - val_loss: 28.1985 - val_regression_loss: 19.8913 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 32.4066 - regression_loss: 25.9792 - val_loss: 26.8463 - val_regression_loss: 18.5719 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30.0895 - regression_loss: 23.9540 - val_loss: 26.2286 - val_regression_loss: 17.9740 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.9377 - regression_loss: 22.5793 - val_loss: 25.6313 - val_regression_loss: 17.4228 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.7014 - regression_loss: 21.6061 - val_loss: 25.1859 - val_regression_loss: 16.9553 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.0159 - regression_loss: 20.8953 - val_loss: 24.6212 - val_regression_loss: 16.3743 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.1222 - regression_loss: 20.2310 - val_loss: 24.1437 - val_regression_loss: 15.9226 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.4499 - regression_loss: 19.7586 - val_loss: 24.0407 - val_regression_loss: 15.7623 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.0449 - regression_loss: 19.3412 - val_loss: 23.7907 - val_regression_loss: 15.5010 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.4864 - regression_loss: 18.9947 - val_loss: 23.5346 - val_regression_loss: 15.2743 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.5457 - regression_loss: 18.6451 - val_loss: 23.4657 - val_regression_loss: 15.1438 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.1507 - regression_loss: 18.3460 - val_loss: 23.4904 - val_regression_loss: 15.1278 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.2281 - regression_loss: 18.1150 - val_loss: 23.1730 - val_regression_loss: 14.8400 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.6759 - regression_loss: 17.9233 - val_loss: 23.1125 - val_regression_loss: 14.7759 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.5893 - regression_loss: 17.7793 - val_loss: 23.1499 - val_regression_loss: 14.7834 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.2975 - regression_loss: 17.5486 - val_loss: 23.0053 - val_regression_loss: 14.5999 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.3191 - regression_loss: 17.3773 - val_loss: 22.9901 - val_regression_loss: 14.5659 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.0643 - regression_loss: 17.2279 - val_loss: 23.0782 - val_regression_loss: 14.6322 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.9965 - regression_loss: 17.1197 - val_loss: 23.0291 - val_regression_loss: 14.5859 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.8631 - regression_loss: 16.9912 - val_loss: 22.8922 - val_regression_loss: 14.4574 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.5533 - regression_loss: 16.8924 - val_loss: 22.7915 - val_regression_loss: 14.3678 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.4488 - regression_loss: 16.8087 - val_loss: 22.7395 - val_regression_loss: 14.3019 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.6735 - regression_loss: 16.8565 - val_loss: 22.8564 - val_regression_loss: 14.3942 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.3166 - regression_loss: 16.7527 - val_loss: 22.8340 - val_regression_loss: 14.3541 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.4303 - regression_loss: 16.6384 - val_loss: 22.8192 - val_regression_loss: 14.3237 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.2309 - regression_loss: 16.5051 - val_loss: 22.7755 - val_regression_loss: 14.2721 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.2504 - regression_loss: 16.4821 - val_loss: 22.7354 - val_regression_loss: 14.2593 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.5647 - regression_loss: 16.4024 - val_loss: 22.6406 - val_regression_loss: 14.1600 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.9683 - regression_loss: 16.2981 - val_loss: 22.7506 - val_regression_loss: 14.2378 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.8144 - regression_loss: 16.3002 - val_loss: 22.6818 - val_regression_loss: 14.1962 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.7196 - regression_loss: 16.2494 - val_loss: 22.6953 - val_regression_loss: 14.1998 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.8271 - regression_loss: 16.1477 - val_loss: 22.7652 - val_regression_loss: 14.2532 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 25.0184 - regression_loss: 20.5220\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.8958 - regression_loss: 16.1728 - val_loss: 22.8399 - val_regression_loss: 14.3246 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.6903 - regression_loss: 16.0964 - val_loss: 22.7383 - val_regression_loss: 14.2297 - lr: 5.0000e-05\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.5519 - regression_loss: 16.0246 - val_loss: 22.7558 - val_regression_loss: 14.2394 - lr: 5.0000e-05\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.2606 - regression_loss: 16.0116 - val_loss: 22.7531 - val_regression_loss: 14.2294 - lr: 5.0000e-05\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.5570 - regression_loss: 16.0033 - val_loss: 22.6639 - val_regression_loss: 14.1587 - lr: 5.0000e-05\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.1919 - regression_loss: 16.0177 - val_loss: 22.7304 - val_regression_loss: 14.2030 - lr: 5.0000e-05\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.6661 - regression_loss: 16.0261 - val_loss: 22.6351 - val_regression_loss: 14.1396 - lr: 5.0000e-05\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.6831 - regression_loss: 15.9542 - val_loss: 22.7655 - val_regression_loss: 14.2354 - lr: 5.0000e-05\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.3285 - regression_loss: 15.9646 - val_loss: 22.8404 - val_regression_loss: 14.3019 - lr: 5.0000e-05\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.7121 - regression_loss: 15.9259 - val_loss: 22.7398 - val_regression_loss: 14.2184 - lr: 5.0000e-05\n",
      "Epoch 44/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 19.3448 - regression_loss: 14.8699\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.2585 - regression_loss: 15.9113 - val_loss: 22.7518 - val_regression_loss: 14.2244 - lr: 5.0000e-05\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.6449 - regression_loss: 15.8821 - val_loss: 22.7778 - val_regression_loss: 14.2353 - lr: 2.5000e-05\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.5401 - regression_loss: 15.8612 - val_loss: 22.7428 - val_regression_loss: 14.2053 - lr: 2.5000e-05\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.4178 - regression_loss: 15.8560 - val_loss: 22.7829 - val_regression_loss: 14.2406 - lr: 2.5000e-05\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.3707 - regression_loss: 15.8388 - val_loss: 22.7564 - val_regression_loss: 14.2251 - lr: 2.5000e-05\n",
      "Epoch 49/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 22.1259 - regression_loss: 17.6569\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.4057 - regression_loss: 15.8323 - val_loss: 22.7455 - val_regression_loss: 14.2124 - lr: 2.5000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.5422 - regression_loss: 15.8423 - val_loss: 22.7343 - val_regression_loss: 14.2034 - lr: 1.2500e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.9472 - regression_loss: 15.8076 - val_loss: 22.7438 - val_regression_loss: 14.2111 - lr: 1.2500e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.1539 - regression_loss: 15.8067 - val_loss: 22.7469 - val_regression_loss: 14.2154 - lr: 1.2500e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.0200 - regression_loss: 15.8111 - val_loss: 22.7743 - val_regression_loss: 14.2349 - lr: 1.2500e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.4378 - regression_loss: 15.8108 - val_loss: 22.7705 - val_regression_loss: 14.2318 - lr: 1.2500e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.5052 - regression_loss: 15.8034 - val_loss: 22.7704 - val_regression_loss: 14.2329 - lr: 1.2500e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.6744 - regression_loss: 15.7926 - val_loss: 22.7781 - val_regression_loss: 14.2407 - lr: 1.2500e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.3488 - regression_loss: 15.7958 - val_loss: 22.7697 - val_regression_loss: 14.2347 - lr: 1.2500e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.0792 - regression_loss: 15.7882 - val_loss: 22.7744 - val_regression_loss: 14.2353 - lr: 1.2500e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.5052 - regression_loss: 15.7821 - val_loss: 22.7874 - val_regression_loss: 14.2461 - lr: 1.2500e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.2998 - regression_loss: 15.7821 - val_loss: 22.7821 - val_regression_loss: 14.2419 - lr: 1.2500e-05\n",
      "Epoch 61/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 26.3125 - regression_loss: 21.8499\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.4606 - regression_loss: 15.7787 - val_loss: 22.7778 - val_regression_loss: 14.2375 - lr: 1.2500e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.4976 - regression_loss: 15.7684 - val_loss: 22.7747 - val_regression_loss: 14.2347 - lr: 6.2500e-06\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.3158 - regression_loss: 15.7661 - val_loss: 22.7673 - val_regression_loss: 14.2277 - lr: 6.2500e-06\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.0379 - regression_loss: 15.7735 - val_loss: 22.7677 - val_regression_loss: 14.2282 - lr: 6.2500e-06\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.0953 - regression_loss: 15.7646 - val_loss: 22.7659 - val_regression_loss: 14.2287 - lr: 6.2500e-06\n",
      "Epoch 66/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 23.5750 - regression_loss: 19.1139\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.3341 - regression_loss: 15.7642 - val_loss: 22.7691 - val_regression_loss: 14.2318 - lr: 6.2500e-06\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.2454 - regression_loss: 15.7566 - val_loss: 22.7748 - val_regression_loss: 14.2358 - lr: 3.1250e-06\n",
      "Epoch 68/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 21.2903 - regression_loss: 15.7547 - val_loss: 22.7743 - val_regression_loss: 14.2356 - lr: 3.1250e-06\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.9103 - regression_loss: 15.7547 - val_loss: 22.7801 - val_regression_loss: 14.2409 - lr: 3.1250e-06\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.9288 - regression_loss: 15.7546 - val_loss: 22.7831 - val_regression_loss: 14.2439 - lr: 3.1250e-06\n",
      "Epoch 71/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 17.1512 - regression_loss: 12.6909\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.2661 - regression_loss: 15.7544 - val_loss: 22.7806 - val_regression_loss: 14.2418 - lr: 3.1250e-06\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.2149 - regression_loss: 15.7492 - val_loss: 22.7809 - val_regression_loss: 14.2420 - lr: 1.5625e-06\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.0960 - regression_loss: 15.7490 - val_loss: 22.7828 - val_regression_loss: 14.2433 - lr: 1.5625e-06\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.2191 - regression_loss: 15.7490 - val_loss: 22.7829 - val_regression_loss: 14.2427 - lr: 1.5625e-06\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8518 - regression_loss: 15.7491 - val_loss: 22.7837 - val_regression_loss: 14.2434 - lr: 1.5625e-06\n",
      "Epoch 76/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 21.8650 - regression_loss: 17.4051\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.3618 - regression_loss: 15.7479 - val_loss: 22.7825 - val_regression_loss: 14.2425 - lr: 1.5625e-06\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.3344 - regression_loss: 15.7469 - val_loss: 22.7819 - val_regression_loss: 14.2422 - lr: 7.8125e-07\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.9278 - regression_loss: 15.7464 - val_loss: 22.7822 - val_regression_loss: 14.2424 - lr: 7.8125e-07\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.3612 - regression_loss: 15.7463 - val_loss: 22.7830 - val_regression_loss: 14.2433 - lr: 7.8125e-07\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.0923 - regression_loss: 15.7461 - val_loss: 22.7831 - val_regression_loss: 14.2434 - lr: 7.8125e-07\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 65.1595 - regression_loss: 56.8248 - val_loss: 51.1294 - val_regression_loss: 38.2986 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 55.5605 - regression_loss: 46.8434 - val_loss: 45.7605 - val_regression_loss: 33.7016 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 46.2470 - regression_loss: 40.2009 - val_loss: 42.4006 - val_regression_loss: 30.4022 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 42.9554 - regression_loss: 35.7914 - val_loss: 39.9588 - val_regression_loss: 28.1053 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 38.7631 - regression_loss: 31.8866 - val_loss: 37.3409 - val_regression_loss: 26.2628 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 35.2552 - regression_loss: 28.7293 - val_loss: 34.6435 - val_regression_loss: 24.2963 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 32.3800 - regression_loss: 26.0191 - val_loss: 31.9653 - val_regression_loss: 22.0060 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.1980 - regression_loss: 23.3095 - val_loss: 29.4243 - val_regression_loss: 20.0078 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.1721 - regression_loss: 21.2498 - val_loss: 26.8459 - val_regression_loss: 18.3525 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.8506 - regression_loss: 19.1662 - val_loss: 25.2983 - val_regression_loss: 17.1651 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.5472 - regression_loss: 17.9117 - val_loss: 23.8984 - val_regression_loss: 16.3275 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.6522 - regression_loss: 16.8675 - val_loss: 22.9246 - val_regression_loss: 15.6810 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.8906 - regression_loss: 16.1704 - val_loss: 22.1440 - val_regression_loss: 15.2279 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.2436 - regression_loss: 15.7709 - val_loss: 21.6282 - val_regression_loss: 14.9210 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.9141 - regression_loss: 15.5198 - val_loss: 21.6331 - val_regression_loss: 14.7786 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.4849 - regression_loss: 15.0760 - val_loss: 20.9456 - val_regression_loss: 14.4298 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6538 - regression_loss: 15.0584 - val_loss: 20.9064 - val_regression_loss: 14.2190 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.4279 - regression_loss: 14.7623 - val_loss: 20.8584 - val_regression_loss: 14.1061 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.8978 - regression_loss: 14.5701 - val_loss: 20.3021 - val_regression_loss: 13.9012 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.7724 - regression_loss: 14.5329 - val_loss: 20.4852 - val_regression_loss: 13.8375 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.5430 - regression_loss: 14.3863 - val_loss: 20.1029 - val_regression_loss: 13.6656 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.7469 - regression_loss: 14.3104 - val_loss: 20.1175 - val_regression_loss: 13.5159 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.3613 - regression_loss: 14.1402 - val_loss: 20.0585 - val_regression_loss: 13.4933 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.1714 - regression_loss: 13.9438 - val_loss: 19.9356 - val_regression_loss: 13.3653 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.4134 - regression_loss: 13.9434 - val_loss: 19.8420 - val_regression_loss: 13.2371 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.1420 - regression_loss: 13.7373 - val_loss: 19.6655 - val_regression_loss: 13.1297 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.1359 - regression_loss: 13.7298 - val_loss: 19.6328 - val_regression_loss: 13.0832 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.6407 - regression_loss: 13.5955 - val_loss: 19.5360 - val_regression_loss: 13.0502 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.9066 - regression_loss: 13.5918 - val_loss: 19.6590 - val_regression_loss: 12.9808 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.1075 - regression_loss: 13.4584 - val_loss: 19.2739 - val_regression_loss: 12.8177 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.5948 - regression_loss: 13.3835 - val_loss: 19.2716 - val_regression_loss: 12.7933 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.3828 - regression_loss: 13.3065 - val_loss: 19.4471 - val_regression_loss: 12.7679 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.5646 - regression_loss: 13.3197 - val_loss: 19.2638 - val_regression_loss: 12.7145 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.6779 - regression_loss: 13.1747 - val_loss: 19.4069 - val_regression_loss: 12.6944 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.5024 - regression_loss: 13.1415 - val_loss: 18.9908 - val_regression_loss: 12.4546 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.1346 - regression_loss: 13.1705 - val_loss: 19.0156 - val_regression_loss: 12.4103 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.5477 - regression_loss: 13.1479 - val_loss: 19.0636 - val_regression_loss: 12.4746 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.3753 - regression_loss: 12.9523 - val_loss: 19.0373 - val_regression_loss: 12.4453 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.0880 - regression_loss: 12.9203 - val_loss: 18.7477 - val_regression_loss: 12.2815 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.9813 - regression_loss: 12.8364 - val_loss: 18.8538 - val_regression_loss: 12.2364 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.9941 - regression_loss: 12.8517 - val_loss: 18.6855 - val_regression_loss: 12.2047 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.0125 - regression_loss: 12.7988 - val_loss: 18.7681 - val_regression_loss: 12.1842 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.6945 - regression_loss: 12.7276 - val_loss: 18.5447 - val_regression_loss: 12.1053 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.0828 - regression_loss: 12.7994 - val_loss: 18.4778 - val_regression_loss: 11.9988 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.7622 - regression_loss: 12.5981 - val_loss: 18.4307 - val_regression_loss: 11.9816 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.0124 - regression_loss: 12.6564 - val_loss: 18.5429 - val_regression_loss: 12.0154 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.9068 - regression_loss: 12.5381 - val_loss: 18.4917 - val_regression_loss: 11.9774 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 19.6029 - regression_loss: 15.2868\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.6953 - regression_loss: 12.4481 - val_loss: 18.4421 - val_regression_loss: 11.9102 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4270 - regression_loss: 12.4360 - val_loss: 18.2251 - val_regression_loss: 11.8326 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.4234 - regression_loss: 12.3640 - val_loss: 18.2981 - val_regression_loss: 11.8184 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.3385 - regression_loss: 12.3323 - val_loss: 18.3209 - val_regression_loss: 11.8320 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.6728 - regression_loss: 12.3359 - val_loss: 18.2880 - val_regression_loss: 11.7681 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.4031 - regression_loss: 12.3023 - val_loss: 18.1553 - val_regression_loss: 11.7105 - lr: 5.0000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.4443 - regression_loss: 12.2829 - val_loss: 18.0897 - val_regression_loss: 11.7126 - lr: 5.0000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.3890 - regression_loss: 12.2488 - val_loss: 18.2211 - val_regression_loss: 11.7365 - lr: 5.0000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.2820 - regression_loss: 12.2404 - val_loss: 18.1564 - val_regression_loss: 11.7087 - lr: 5.0000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.3170 - regression_loss: 12.2135 - val_loss: 18.1686 - val_regression_loss: 11.7276 - lr: 5.0000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.1718 - regression_loss: 12.2030 - val_loss: 18.0529 - val_regression_loss: 11.6413 - lr: 5.0000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.2887 - regression_loss: 12.1902 - val_loss: 18.1190 - val_regression_loss: 11.6387 - lr: 5.0000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.3282 - regression_loss: 12.1594 - val_loss: 18.0003 - val_regression_loss: 11.6297 - lr: 5.0000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.1467 - regression_loss: 12.1631 - val_loss: 18.0792 - val_regression_loss: 11.6346 - lr: 5.0000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.1776 - regression_loss: 12.1030 - val_loss: 18.1060 - val_regression_loss: 11.6522 - lr: 5.0000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.2229 - regression_loss: 12.1036 - val_loss: 17.9987 - val_regression_loss: 11.6198 - lr: 5.0000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.1309 - regression_loss: 12.1058 - val_loss: 17.9813 - val_regression_loss: 11.5554 - lr: 5.0000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.2129 - regression_loss: 12.0505 - val_loss: 17.9645 - val_regression_loss: 11.5454 - lr: 5.0000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.3235 - regression_loss: 12.0803 - val_loss: 17.9515 - val_regression_loss: 11.5349 - lr: 5.0000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.0314 - regression_loss: 12.0280 - val_loss: 17.9162 - val_regression_loss: 11.4871 - lr: 5.0000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.3234 - regression_loss: 12.1048 - val_loss: 17.9780 - val_regression_loss: 11.5705 - lr: 5.0000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.1779 - regression_loss: 12.0054 - val_loss: 17.8680 - val_regression_loss: 11.5155 - lr: 5.0000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.2241 - regression_loss: 12.0837 - val_loss: 17.9835 - val_regression_loss: 11.4776 - lr: 5.0000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.0773 - regression_loss: 11.9747 - val_loss: 17.8204 - val_regression_loss: 11.4415 - lr: 5.0000e-05\n",
      "Epoch 72/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.1416 - regression_loss: 13.8678\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 17.0399 - regression_loss: 11.9480 - val_loss: 17.7901 - val_regression_loss: 11.4479 - lr: 5.0000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.8274 - regression_loss: 11.9318 - val_loss: 17.7941 - val_regression_loss: 11.4301 - lr: 2.5000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.0748 - regression_loss: 11.8814 - val_loss: 17.7893 - val_regression_loss: 11.4130 - lr: 2.5000e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.0539 - regression_loss: 11.8781 - val_loss: 17.7854 - val_regression_loss: 11.4037 - lr: 2.5000e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.1251 - regression_loss: 11.9019 - val_loss: 17.8441 - val_regression_loss: 11.4240 - lr: 2.5000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.9923 - regression_loss: 11.8861 - val_loss: 17.7613 - val_regression_loss: 11.4243 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 17.3788 - regression_loss: 13.1111\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.8617 - regression_loss: 11.8737 - val_loss: 17.7189 - val_regression_loss: 11.3779 - lr: 2.5000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8768 - regression_loss: 11.8381 - val_loss: 17.7496 - val_regression_loss: 11.3666 - lr: 1.2500e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.9566 - regression_loss: 11.8354 - val_loss: 17.7526 - val_regression_loss: 11.3668 - lr: 1.2500e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.0214 - regression_loss: 11.8244 - val_loss: 17.7229 - val_regression_loss: 11.3554 - lr: 1.2500e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9482 - regression_loss: 11.8148 - val_loss: 17.7242 - val_regression_loss: 11.3616 - lr: 1.2500e-05\n",
      "Epoch 83/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 20.3921 - regression_loss: 16.1269\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.8459 - regression_loss: 11.8108 - val_loss: 17.7180 - val_regression_loss: 11.3610 - lr: 1.2500e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8083 - regression_loss: 11.8043 - val_loss: 17.7321 - val_regression_loss: 11.3632 - lr: 6.2500e-06\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.6992 - regression_loss: 11.7999 - val_loss: 17.7247 - val_regression_loss: 11.3566 - lr: 6.2500e-06\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7044 - regression_loss: 11.7999 - val_loss: 17.7233 - val_regression_loss: 11.3611 - lr: 6.2500e-06\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.6109 - regression_loss: 11.7959 - val_loss: 17.7145 - val_regression_loss: 11.3586 - lr: 6.2500e-06\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7116 - regression_loss: 11.7931 - val_loss: 17.7201 - val_regression_loss: 11.3593 - lr: 6.2500e-06\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8652 - regression_loss: 11.7954 - val_loss: 17.7020 - val_regression_loss: 11.3530 - lr: 6.2500e-06\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7315 - regression_loss: 11.7903 - val_loss: 17.7103 - val_regression_loss: 11.3558 - lr: 6.2500e-06\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7053 - regression_loss: 11.7840 - val_loss: 17.7108 - val_regression_loss: 11.3496 - lr: 6.2500e-06\n",
      "Epoch 92/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.8173 - regression_loss: 8.5543\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9730 - regression_loss: 11.7835 - val_loss: 17.7193 - val_regression_loss: 11.3497 - lr: 6.2500e-06\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9822 - regression_loss: 11.7824 - val_loss: 17.7134 - val_regression_loss: 11.3477 - lr: 3.1250e-06\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5949 - regression_loss: 11.7795 - val_loss: 17.7156 - val_regression_loss: 11.3465 - lr: 3.1250e-06\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.6515 - regression_loss: 11.7762 - val_loss: 17.7099 - val_regression_loss: 11.3442 - lr: 3.1250e-06\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9379 - regression_loss: 11.7766 - val_loss: 17.7034 - val_regression_loss: 11.3444 - lr: 3.1250e-06\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9671 - regression_loss: 11.7737 - val_loss: 17.6966 - val_regression_loss: 11.3418 - lr: 3.1250e-06\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7120 - regression_loss: 11.7740 - val_loss: 17.7011 - val_regression_loss: 11.3418 - lr: 3.1250e-06\n",
      "Epoch 99/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 19.7480 - regression_loss: 15.4858\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9811 - regression_loss: 11.7725 - val_loss: 17.6977 - val_regression_loss: 11.3387 - lr: 3.1250e-06\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8763 - regression_loss: 11.7700 - val_loss: 17.6937 - val_regression_loss: 11.3375 - lr: 1.5625e-06\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7930 - regression_loss: 11.7687 - val_loss: 17.6940 - val_regression_loss: 11.3384 - lr: 1.5625e-06\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9099 - regression_loss: 11.7682 - val_loss: 17.6960 - val_regression_loss: 11.3393 - lr: 1.5625e-06\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.6985 - regression_loss: 11.7687 - val_loss: 17.6947 - val_regression_loss: 11.3385 - lr: 1.5625e-06\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5830 - regression_loss: 11.7684 - val_loss: 17.6951 - val_regression_loss: 11.3381 - lr: 1.5625e-06\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9514 - regression_loss: 11.7679 - val_loss: 17.6943 - val_regression_loss: 11.3371 - lr: 1.5625e-06\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7777 - regression_loss: 11.7675 - val_loss: 17.6928 - val_regression_loss: 11.3376 - lr: 1.5625e-06\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7413 - regression_loss: 11.7664 - val_loss: 17.6892 - val_regression_loss: 11.3366 - lr: 1.5625e-06\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5597 - regression_loss: 11.7663 - val_loss: 17.6869 - val_regression_loss: 11.3352 - lr: 1.5625e-06\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5832 - regression_loss: 11.7649 - val_loss: 17.6870 - val_regression_loss: 11.3344 - lr: 1.5625e-06\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7831 - regression_loss: 11.7643 - val_loss: 17.6907 - val_regression_loss: 11.3364 - lr: 1.5625e-06\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8452 - regression_loss: 11.7642 - val_loss: 17.6914 - val_regression_loss: 11.3367 - lr: 1.5625e-06\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7922 - regression_loss: 11.7636 - val_loss: 17.6900 - val_regression_loss: 11.3359 - lr: 1.5625e-06\n",
      "Epoch 113/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.7487 - regression_loss: 11.4874\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9908 - regression_loss: 11.7632 - val_loss: 17.6872 - val_regression_loss: 11.3335 - lr: 1.5625e-06\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8773 - regression_loss: 11.7613 - val_loss: 17.6870 - val_regression_loss: 11.3328 - lr: 7.8125e-07\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8996 - regression_loss: 11.7609 - val_loss: 17.6881 - val_regression_loss: 11.3330 - lr: 7.8125e-07\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8903 - regression_loss: 11.7611 - val_loss: 17.6881 - val_regression_loss: 11.3323 - lr: 7.8125e-07\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9698 - regression_loss: 11.7607 - val_loss: 17.6865 - val_regression_loss: 11.3320 - lr: 7.8125e-07\n",
      "Epoch 118/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 19.5610 - regression_loss: 15.2999\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8864 - regression_loss: 11.7613 - val_loss: 17.6869 - val_regression_loss: 11.3319 - lr: 7.8125e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8292 - regression_loss: 11.7595 - val_loss: 17.6870 - val_regression_loss: 11.3321 - lr: 3.9062e-07\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.9028 - regression_loss: 11.7599 - val_loss: 17.6878 - val_regression_loss: 11.3321 - lr: 3.9062e-07\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7847 - regression_loss: 11.7595 - val_loss: 17.6877 - val_regression_loss: 11.3318 - lr: 3.9062e-07\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8235 - regression_loss: 11.7590 - val_loss: 17.6875 - val_regression_loss: 11.3319 - lr: 3.9062e-07\n",
      "Epoch 123/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 14.7173 - regression_loss: 10.4563\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7820 - regression_loss: 11.7590 - val_loss: 17.6867 - val_regression_loss: 11.3317 - lr: 3.9062e-07\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7555 - regression_loss: 11.7586 - val_loss: 17.6862 - val_regression_loss: 11.3313 - lr: 1.9531e-07\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5778 - regression_loss: 11.7585 - val_loss: 17.6861 - val_regression_loss: 11.3313 - lr: 1.9531e-07\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8567 - regression_loss: 11.7586 - val_loss: 17.6861 - val_regression_loss: 11.3315 - lr: 1.9531e-07\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9394 - regression_loss: 11.7584 - val_loss: 17.6862 - val_regression_loss: 11.3315 - lr: 1.9531e-07\n",
      "Epoch 128/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 19.6388 - regression_loss: 15.3778\n",
      "Epoch 128: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7183 - regression_loss: 11.7584 - val_loss: 17.6859 - val_regression_loss: 11.3313 - lr: 1.9531e-07\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8231 - regression_loss: 11.7582 - val_loss: 17.6859 - val_regression_loss: 11.3314 - lr: 9.7656e-08\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8072 - regression_loss: 11.7582 - val_loss: 17.6858 - val_regression_loss: 11.3313 - lr: 9.7656e-08\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8302 - regression_loss: 11.7581 - val_loss: 17.6859 - val_regression_loss: 11.3313 - lr: 9.7656e-08\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7805 - regression_loss: 11.7580 - val_loss: 17.6858 - val_regression_loss: 11.3313 - lr: 9.7656e-08\n",
      "Epoch 133/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 19.9966 - regression_loss: 15.7356\n",
      "Epoch 133: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7401 - regression_loss: 11.7581 - val_loss: 17.6856 - val_regression_loss: 11.3312 - lr: 9.7656e-08\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8957 - regression_loss: 11.7580 - val_loss: 17.6855 - val_regression_loss: 11.3312 - lr: 4.8828e-08\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7984 - regression_loss: 11.7579 - val_loss: 17.6856 - val_regression_loss: 11.3312 - lr: 4.8828e-08\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9379 - regression_loss: 11.7579 - val_loss: 17.6856 - val_regression_loss: 11.3312 - lr: 4.8828e-08\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.6424 - regression_loss: 11.7579 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 4.8828e-08\n",
      "Epoch 138/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 19.4028 - regression_loss: 15.1418\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7214 - regression_loss: 11.7579 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 4.8828e-08\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9721 - regression_loss: 11.7578 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 2.4414e-08\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8445 - regression_loss: 11.7578 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 2.4414e-08\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8712 - regression_loss: 11.7578 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 2.4414e-08\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7892 - regression_loss: 11.7578 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 2.4414e-08\n",
      "Epoch 143/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.2931 - regression_loss: 14.0321\n",
      "Epoch 143: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7454 - regression_loss: 11.7578 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 2.4414e-08\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8307 - regression_loss: 11.7578 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 1.2207e-08\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9887 - regression_loss: 11.7578 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 1.2207e-08\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7216 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 1.2207e-08\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5945 - regression_loss: 11.7578 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 1.2207e-08\n",
      "Epoch 148/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 16.6322 - regression_loss: 12.3712\n",
      "Epoch 148: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.0795 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 1.2207e-08\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9199 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 6.1035e-09\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9553 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 6.1035e-09\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9263 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 6.1035e-09\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9783 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 6.1035e-09\n",
      "Epoch 153/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.8933 - regression_loss: 11.6323\n",
      "Epoch 153: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8571 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 6.1035e-09\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7261 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 3.0518e-09\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8249 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 3.0518e-09\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7856 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 3.0518e-09\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8465 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 3.0518e-09\n",
      "Epoch 158/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 20.6704 - regression_loss: 16.4094\n",
      "Epoch 158: ReduceLROnPlateau reducing learning rate to 1.5258788677030566e-09.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9290 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 3.0518e-09\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8333 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 1.5259e-09\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8402 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 1.5259e-09\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8105 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 1.5259e-09\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7519 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 1.5259e-09\n",
      "Epoch 163/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 19.9344 - regression_loss: 15.6734\n",
      "Epoch 163: ReduceLROnPlateau reducing learning rate to 7.629394338515283e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.0264 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 1.5259e-09\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8296 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 7.6294e-10\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7092 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 7.6294e-10\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5948 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 7.6294e-10\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8422 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 7.6294e-10\n",
      "Epoch 168/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.3169 - regression_loss: 11.0560\n",
      "Epoch 168: ReduceLROnPlateau reducing learning rate to 3.8146971692576415e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9237 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 7.6294e-10\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9132 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 3.8147e-10\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9342 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 3.8147e-10\n",
      "Epoch 171/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9316 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 3.8147e-10\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7951 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 3.8147e-10\n",
      "Epoch 173/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 14.8430 - regression_loss: 10.5821\n",
      "Epoch 173: ReduceLROnPlateau reducing learning rate to 1.9073485846288207e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8220 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 3.8147e-10\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.0378 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 1.9073e-10\n",
      "Epoch 175/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8834 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 1.9073e-10\n",
      "Epoch 176/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8679 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 1.9073e-10\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9064 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 1.9073e-10\n",
      "Epoch 178/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 21.3037 - regression_loss: 17.0427\n",
      "Epoch 178: ReduceLROnPlateau reducing learning rate to 9.536742923144104e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9123 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 1.9073e-10\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7322 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 9.5367e-11\n",
      "Epoch 180/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8216 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 9.5367e-11\n",
      "Epoch 181/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8609 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 9.5367e-11\n",
      "Epoch 182/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9280 - regression_loss: 11.7577 - val_loss: 17.6855 - val_regression_loss: 11.3311 - lr: 9.5367e-11\n",
      "3/3 [==============================] - 0s 982us/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 115.9930 - regression_loss: 104.5422 - val_loss: 76.8731 - val_regression_loss: 64.0813 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 84.5201 - regression_loss: 75.3958 - val_loss: 57.3347 - val_regression_loss: 46.7895 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 69.5934 - regression_loss: 60.8619 - val_loss: 45.8670 - val_regression_loss: 36.6541 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 58.0977 - regression_loss: 51.1697 - val_loss: 38.7161 - val_regression_loss: 30.5050 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 52.6368 - regression_loss: 45.2894 - val_loss: 34.3285 - val_regression_loss: 26.5809 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 46.5922 - regression_loss: 40.2706 - val_loss: 31.4599 - val_regression_loss: 23.8765 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 43.4358 - regression_loss: 35.8937 - val_loss: 28.5469 - val_regression_loss: 21.2644 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 38.5383 - regression_loss: 31.9856 - val_loss: 25.6947 - val_regression_loss: 18.8424 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 35.3211 - regression_loss: 28.5966 - val_loss: 23.8171 - val_regression_loss: 17.0599 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.0312 - regression_loss: 25.7170 - val_loss: 22.2334 - val_regression_loss: 15.5192 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.0740 - regression_loss: 23.1907 - val_loss: 20.3661 - val_regression_loss: 13.9556 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.6946 - regression_loss: 21.0170 - val_loss: 19.6405 - val_regression_loss: 13.1625 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.4557 - regression_loss: 19.7245 - val_loss: 18.9713 - val_regression_loss: 12.4623 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.1248 - regression_loss: 18.6577 - val_loss: 18.0451 - val_regression_loss: 11.7844 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.6149 - regression_loss: 17.8729 - val_loss: 18.0619 - val_regression_loss: 11.6559 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.6025 - regression_loss: 17.2927 - val_loss: 17.6226 - val_regression_loss: 11.2972 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.9618 - regression_loss: 16.7006 - val_loss: 17.3519 - val_regression_loss: 11.0722 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.4926 - regression_loss: 16.2559 - val_loss: 17.0090 - val_regression_loss: 10.8154 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9042 - regression_loss: 15.9269 - val_loss: 16.8319 - val_regression_loss: 10.5880 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.1895 - regression_loss: 15.6741 - val_loss: 16.5979 - val_regression_loss: 10.3937 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.3613 - regression_loss: 15.1917 - val_loss: 16.5506 - val_regression_loss: 10.3207 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.2852 - regression_loss: 14.9765 - val_loss: 16.5548 - val_regression_loss: 10.3168 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.9379 - regression_loss: 14.9313 - val_loss: 16.2994 - val_regression_loss: 10.0762 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.3148 - regression_loss: 14.9445 - val_loss: 16.4110 - val_regression_loss: 10.1029 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.8075 - regression_loss: 14.5495 - val_loss: 16.1804 - val_regression_loss: 10.0353 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.0602 - regression_loss: 13.9567 - val_loss: 16.3972 - val_regression_loss: 10.0443 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1514 - regression_loss: 14.0519 - val_loss: 15.8259 - val_regression_loss: 9.6769 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.0585 - regression_loss: 13.6971 - val_loss: 15.9933 - val_regression_loss: 9.7531 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.4802 - regression_loss: 13.4311 - val_loss: 15.8341 - val_regression_loss: 9.6638 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.3468 - regression_loss: 13.2814 - val_loss: 15.7130 - val_regression_loss: 9.5555 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.3523 - regression_loss: 13.0792 - val_loss: 15.7001 - val_regression_loss: 9.5057 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.7612 - regression_loss: 12.9203 - val_loss: 15.5831 - val_regression_loss: 9.3938 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.7093 - regression_loss: 12.8485 - val_loss: 15.4585 - val_regression_loss: 9.2519 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.8941 - regression_loss: 12.8720 - val_loss: 15.3320 - val_regression_loss: 9.1698 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4154 - regression_loss: 12.4065 - val_loss: 15.5538 - val_regression_loss: 9.2849 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2858 - regression_loss: 12.3950 - val_loss: 15.3429 - val_regression_loss: 9.1818 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3107 - regression_loss: 12.2297 - val_loss: 15.2121 - val_regression_loss: 9.0147 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.0432 - regression_loss: 12.0729 - val_loss: 15.0866 - val_regression_loss: 8.9274 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.1214 - regression_loss: 12.0553 - val_loss: 15.1459 - val_regression_loss: 8.9302 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.6397 - regression_loss: 11.6869 - val_loss: 14.9998 - val_regression_loss: 8.8436 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5416 - regression_loss: 11.7038 - val_loss: 15.0336 - val_regression_loss: 8.8370 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.4752 - regression_loss: 11.5430 - val_loss: 14.8142 - val_regression_loss: 8.6782 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5565 - regression_loss: 11.4447 - val_loss: 14.6605 - val_regression_loss: 8.5374 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.2798 - regression_loss: 11.2399 - val_loss: 14.7291 - val_regression_loss: 8.5980 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.2193 - regression_loss: 11.2502 - val_loss: 14.8099 - val_regression_loss: 8.6492 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.0782 - regression_loss: 10.9978 - val_loss: 14.4440 - val_regression_loss: 8.3102 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.9664 - regression_loss: 10.8833 - val_loss: 14.4015 - val_regression_loss: 8.2638 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.7338 - regression_loss: 10.7917 - val_loss: 14.3867 - val_regression_loss: 8.2864 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5907 - regression_loss: 10.6350 - val_loss: 14.3195 - val_regression_loss: 8.2148 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.2728 - regression_loss: 10.5144 - val_loss: 14.1953 - val_regression_loss: 8.0978 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.2506 - regression_loss: 10.3705 - val_loss: 14.1440 - val_regression_loss: 8.0822 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.1870 - regression_loss: 10.4161 - val_loss: 14.0536 - val_regression_loss: 7.9459 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.9889 - regression_loss: 10.2079 - val_loss: 14.1255 - val_regression_loss: 8.0193 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.2319 - regression_loss: 10.1381 - val_loss: 14.0822 - val_regression_loss: 8.0270 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.5440 - regression_loss: 10.0285 - val_loss: 14.1073 - val_regression_loss: 7.9632 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.8737 - regression_loss: 10.0648 - val_loss: 13.6675 - val_regression_loss: 7.6186 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.1260 - regression_loss: 10.2888 - val_loss: 14.1961 - val_regression_loss: 8.0043 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.8629 - regression_loss: 9.9428 - val_loss: 13.7746 - val_regression_loss: 7.7376 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.7213 - regression_loss: 9.8603 - val_loss: 13.6674 - val_regression_loss: 7.6213 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.5319 - regression_loss: 9.8135 - val_loss: 13.7280 - val_regression_loss: 7.6371 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.3625 - regression_loss: 9.6462 - val_loss: 13.5189 - val_regression_loss: 7.5076 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.0398 - regression_loss: 9.4138 - val_loss: 13.6175 - val_regression_loss: 7.5805 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1494 - regression_loss: 9.4405 - val_loss: 13.4079 - val_regression_loss: 7.4245 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.2316 - regression_loss: 9.3662 - val_loss: 13.5222 - val_regression_loss: 7.4709 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.9754 - regression_loss: 9.4179 - val_loss: 13.3945 - val_regression_loss: 7.3947 - lr: 1.0000e-04\n",
      "Epoch 66/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 14.2050 - regression_loss: 9.3911 - val_loss: 13.5415 - val_regression_loss: 7.5083 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8769 - regression_loss: 9.1793 - val_loss: 13.3142 - val_regression_loss: 7.3521 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8344 - regression_loss: 9.0774 - val_loss: 13.3190 - val_regression_loss: 7.2906 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7446 - regression_loss: 9.0422 - val_loss: 13.2819 - val_regression_loss: 7.2813 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6268 - regression_loss: 9.0349 - val_loss: 13.3045 - val_regression_loss: 7.3428 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7026 - regression_loss: 8.9999 - val_loss: 13.2755 - val_regression_loss: 7.2576 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.4320 - regression_loss: 8.9594 - val_loss: 13.2916 - val_regression_loss: 7.3066 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.4916 - regression_loss: 8.9174 - val_loss: 13.1708 - val_regression_loss: 7.1534 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.4779 - regression_loss: 8.8377 - val_loss: 13.2456 - val_regression_loss: 7.3039 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7844 - regression_loss: 9.0176 - val_loss: 13.1527 - val_regression_loss: 7.2167 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0282 - regression_loss: 8.7665 - val_loss: 13.1394 - val_regression_loss: 7.1635 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.2651 - regression_loss: 8.7002 - val_loss: 12.9622 - val_regression_loss: 7.0588 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.2089 - regression_loss: 8.6547 - val_loss: 13.3015 - val_regression_loss: 7.3095 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.2640 - regression_loss: 8.5841 - val_loss: 12.8968 - val_regression_loss: 6.9764 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1291 - regression_loss: 8.5777 - val_loss: 13.0559 - val_regression_loss: 7.1040 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.2469 - regression_loss: 11.2183\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1703 - regression_loss: 8.4715 - val_loss: 13.0371 - val_regression_loss: 7.1060 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0915 - regression_loss: 8.4512 - val_loss: 13.0295 - val_regression_loss: 7.0848 - lr: 5.0000e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0759 - regression_loss: 8.3794 - val_loss: 12.9109 - val_regression_loss: 7.0102 - lr: 5.0000e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9094 - regression_loss: 8.3620 - val_loss: 12.9616 - val_regression_loss: 7.0395 - lr: 5.0000e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0563 - regression_loss: 8.3420 - val_loss: 12.9507 - val_regression_loss: 7.0368 - lr: 5.0000e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0089 - regression_loss: 8.3336 - val_loss: 12.9428 - val_regression_loss: 7.0217 - lr: 5.0000e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8255 - regression_loss: 8.3398 - val_loss: 12.9286 - val_regression_loss: 7.0074 - lr: 5.0000e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0985 - regression_loss: 8.2782 - val_loss: 12.8637 - val_regression_loss: 6.9464 - lr: 5.0000e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8452 - regression_loss: 8.2927 - val_loss: 12.8250 - val_regression_loss: 6.9226 - lr: 5.0000e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7985 - regression_loss: 8.2525 - val_loss: 12.8224 - val_regression_loss: 6.9321 - lr: 5.0000e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7795 - regression_loss: 8.2326 - val_loss: 12.8990 - val_regression_loss: 7.0005 - lr: 5.0000e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8178 - regression_loss: 8.2231 - val_loss: 12.8641 - val_regression_loss: 6.9691 - lr: 5.0000e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5386 - regression_loss: 8.2179 - val_loss: 12.7721 - val_regression_loss: 6.9059 - lr: 5.0000e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9352 - regression_loss: 8.2241 - val_loss: 12.9468 - val_regression_loss: 7.0359 - lr: 5.0000e-05\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8722 - regression_loss: 8.1961 - val_loss: 12.8142 - val_regression_loss: 6.9253 - lr: 5.0000e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7187 - regression_loss: 8.1711 - val_loss: 12.9546 - val_regression_loss: 7.0088 - lr: 5.0000e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7875 - regression_loss: 8.1064 - val_loss: 12.7576 - val_regression_loss: 6.8936 - lr: 5.0000e-05\n",
      "Epoch 98/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.1697 - regression_loss: 8.1670\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5984 - regression_loss: 8.1119 - val_loss: 12.8308 - val_regression_loss: 6.9481 - lr: 5.0000e-05\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5428 - regression_loss: 8.0473 - val_loss: 12.7996 - val_regression_loss: 6.9265 - lr: 2.5000e-05\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6585 - regression_loss: 8.0384 - val_loss: 12.7678 - val_regression_loss: 6.8978 - lr: 2.5000e-05\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6295 - regression_loss: 8.0365 - val_loss: 12.7575 - val_regression_loss: 6.8935 - lr: 2.5000e-05\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5375 - regression_loss: 8.0352 - val_loss: 12.7944 - val_regression_loss: 6.9041 - lr: 2.5000e-05\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5805 - regression_loss: 8.0125 - val_loss: 12.8112 - val_regression_loss: 6.9249 - lr: 2.5000e-05\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5962 - regression_loss: 7.9957 - val_loss: 12.7634 - val_regression_loss: 6.8954 - lr: 2.5000e-05\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4930 - regression_loss: 8.0197 - val_loss: 12.7723 - val_regression_loss: 6.9060 - lr: 2.5000e-05\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5298 - regression_loss: 8.0040 - val_loss: 12.8353 - val_regression_loss: 6.9454 - lr: 2.5000e-05\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6658 - regression_loss: 8.0081 - val_loss: 12.8154 - val_regression_loss: 6.9235 - lr: 2.5000e-05\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6924 - regression_loss: 7.9840 - val_loss: 12.7165 - val_regression_loss: 6.8580 - lr: 2.5000e-05\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3649 - regression_loss: 7.9772 - val_loss: 12.7637 - val_regression_loss: 6.9023 - lr: 2.5000e-05\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3536 - regression_loss: 7.9497 - val_loss: 12.8463 - val_regression_loss: 6.9560 - lr: 2.5000e-05\n",
      "Epoch 111/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2434 - regression_loss: 7.9555 - val_loss: 12.7964 - val_regression_loss: 6.9003 - lr: 2.5000e-05\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4553 - regression_loss: 7.9571 - val_loss: 12.7278 - val_regression_loss: 6.8673 - lr: 2.5000e-05\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5178 - regression_loss: 7.9388 - val_loss: 12.7591 - val_regression_loss: 6.8925 - lr: 2.5000e-05\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4258 - regression_loss: 7.9216 - val_loss: 12.7538 - val_regression_loss: 6.8905 - lr: 2.5000e-05\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5051 - regression_loss: 7.9251 - val_loss: 12.6900 - val_regression_loss: 6.8459 - lr: 2.5000e-05\n",
      "Epoch 116/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.5174 - regression_loss: 8.5284\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5991 - regression_loss: 7.9044 - val_loss: 12.7217 - val_regression_loss: 6.8733 - lr: 2.5000e-05\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4463 - regression_loss: 7.8820 - val_loss: 12.7478 - val_regression_loss: 6.8856 - lr: 1.2500e-05\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4114 - regression_loss: 7.8844 - val_loss: 12.7653 - val_regression_loss: 6.8978 - lr: 1.2500e-05\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4679 - regression_loss: 7.8746 - val_loss: 12.7296 - val_regression_loss: 6.8719 - lr: 1.2500e-05\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4628 - regression_loss: 7.8707 - val_loss: 12.6903 - val_regression_loss: 6.8479 - lr: 1.2500e-05\n",
      "Epoch 121/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.1205 - regression_loss: 8.1336\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3645 - regression_loss: 7.8752 - val_loss: 12.7326 - val_regression_loss: 6.8757 - lr: 1.2500e-05\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2926 - regression_loss: 7.8558 - val_loss: 12.7412 - val_regression_loss: 6.8762 - lr: 6.2500e-06\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4309 - regression_loss: 7.8536 - val_loss: 12.7432 - val_regression_loss: 6.8737 - lr: 6.2500e-06\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4431 - regression_loss: 7.8519 - val_loss: 12.7484 - val_regression_loss: 6.8778 - lr: 6.2500e-06\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3213 - regression_loss: 7.8483 - val_loss: 12.7298 - val_regression_loss: 6.8646 - lr: 6.2500e-06\n",
      "Epoch 126/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.1848 - regression_loss: 8.1990\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5128 - regression_loss: 7.8495 - val_loss: 12.7213 - val_regression_loss: 6.8622 - lr: 6.2500e-06\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4270 - regression_loss: 7.8416 - val_loss: 12.7248 - val_regression_loss: 6.8663 - lr: 3.1250e-06\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4787 - regression_loss: 7.8423 - val_loss: 12.7323 - val_regression_loss: 6.8682 - lr: 3.1250e-06\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3774 - regression_loss: 7.8401 - val_loss: 12.7433 - val_regression_loss: 6.8773 - lr: 3.1250e-06\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3081 - regression_loss: 7.8370 - val_loss: 12.7349 - val_regression_loss: 6.8718 - lr: 3.1250e-06\n",
      "Epoch 131/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.1233 - regression_loss: 9.1381\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4956 - regression_loss: 7.8362 - val_loss: 12.7332 - val_regression_loss: 6.8709 - lr: 3.1250e-06\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4137 - regression_loss: 7.8339 - val_loss: 12.7324 - val_regression_loss: 6.8704 - lr: 1.5625e-06\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2518 - regression_loss: 7.8343 - val_loss: 12.7263 - val_regression_loss: 6.8661 - lr: 1.5625e-06\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4267 - regression_loss: 7.8332 - val_loss: 12.7255 - val_regression_loss: 6.8655 - lr: 1.5625e-06\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5434 - regression_loss: 7.8324 - val_loss: 12.7252 - val_regression_loss: 6.8649 - lr: 1.5625e-06\n",
      "Epoch 136/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.9666 - regression_loss: 7.9817\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4089 - regression_loss: 7.8317 - val_loss: 12.7288 - val_regression_loss: 6.8678 - lr: 1.5625e-06\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3348 - regression_loss: 7.8299 - val_loss: 12.7288 - val_regression_loss: 6.8679 - lr: 7.8125e-07\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2269 - regression_loss: 7.8301 - val_loss: 12.7270 - val_regression_loss: 6.8663 - lr: 7.8125e-07\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4109 - regression_loss: 7.8299 - val_loss: 12.7279 - val_regression_loss: 6.8671 - lr: 7.8125e-07\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3593 - regression_loss: 7.8297 - val_loss: 12.7285 - val_regression_loss: 6.8676 - lr: 7.8125e-07\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3988 - regression_loss: 7.8292 - val_loss: 12.7271 - val_regression_loss: 6.8665 - lr: 7.8125e-07\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2798 - regression_loss: 7.8301 - val_loss: 12.7284 - val_regression_loss: 6.8668 - lr: 7.8125e-07\n",
      "Epoch 143/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 14.4459 - regression_loss: 10.4611\n",
      "Epoch 143: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3288 - regression_loss: 7.8292 - val_loss: 12.7252 - val_regression_loss: 6.8651 - lr: 7.8125e-07\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3301 - regression_loss: 7.8281 - val_loss: 12.7266 - val_regression_loss: 6.8663 - lr: 3.9062e-07\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3429 - regression_loss: 7.8279 - val_loss: 12.7259 - val_regression_loss: 6.8658 - lr: 3.9062e-07\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4499 - regression_loss: 7.8278 - val_loss: 12.7255 - val_regression_loss: 6.8653 - lr: 3.9062e-07\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3984 - regression_loss: 7.8278 - val_loss: 12.7266 - val_regression_loss: 6.8661 - lr: 3.9062e-07\n",
      "Epoch 148/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.0089 - regression_loss: 9.0243\n",
      "Epoch 148: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3323 - regression_loss: 7.8276 - val_loss: 12.7264 - val_regression_loss: 6.8664 - lr: 3.9062e-07\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4853 - regression_loss: 7.8274 - val_loss: 12.7259 - val_regression_loss: 6.8660 - lr: 1.9531e-07\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4241 - regression_loss: 7.8270 - val_loss: 12.7259 - val_regression_loss: 6.8660 - lr: 1.9531e-07\n",
      "Epoch 151/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3363 - regression_loss: 7.8270 - val_loss: 12.7257 - val_regression_loss: 6.8659 - lr: 1.9531e-07\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.3996 - regression_loss: 7.8270 - val_loss: 12.7262 - val_regression_loss: 6.8662 - lr: 1.9531e-07\n",
      "Epoch 153/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.9546 - regression_loss: 8.9700\n",
      "Epoch 153: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5097 - regression_loss: 7.8269 - val_loss: 12.7258 - val_regression_loss: 6.8659 - lr: 1.9531e-07\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3539 - regression_loss: 7.8267 - val_loss: 12.7259 - val_regression_loss: 6.8660 - lr: 9.7656e-08\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3296 - regression_loss: 7.8269 - val_loss: 12.7255 - val_regression_loss: 6.8658 - lr: 9.7656e-08\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 102.1558 - regression_loss: 91.1555 - val_loss: 65.0097 - val_regression_loss: 50.8250 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 58.6664 - regression_loss: 50.6888 - val_loss: 41.1208 - val_regression_loss: 30.1187 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 40.7610 - regression_loss: 34.9685 - val_loss: 33.4768 - val_regression_loss: 23.3053 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 37.9518 - regression_loss: 30.8646 - val_loss: 30.1498 - val_regression_loss: 20.2854 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 34.2276 - regression_loss: 27.8871 - val_loss: 28.5773 - val_regression_loss: 18.9946 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.7043 - regression_loss: 26.0736 - val_loss: 27.1407 - val_regression_loss: 17.9391 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.3427 - regression_loss: 24.3678 - val_loss: 26.4298 - val_regression_loss: 17.4950 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.2941 - regression_loss: 23.1942 - val_loss: 26.1384 - val_regression_loss: 17.3415 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.2730 - regression_loss: 22.4695 - val_loss: 25.9662 - val_regression_loss: 17.2245 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.2072 - regression_loss: 21.7970 - val_loss: 25.5368 - val_regression_loss: 16.8811 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.4429 - regression_loss: 21.4069 - val_loss: 25.2880 - val_regression_loss: 16.6173 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.1727 - regression_loss: 21.0453 - val_loss: 24.9487 - val_regression_loss: 16.2618 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.1229 - regression_loss: 20.5859 - val_loss: 24.7431 - val_regression_loss: 16.0813 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.1310 - regression_loss: 20.5884 - val_loss: 24.5530 - val_regression_loss: 15.9009 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.3092 - regression_loss: 20.1956 - val_loss: 24.5645 - val_regression_loss: 15.9112 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.3511 - regression_loss: 19.8935 - val_loss: 24.5240 - val_regression_loss: 15.8947 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.0151 - regression_loss: 19.8531 - val_loss: 24.3283 - val_regression_loss: 15.7535 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.9478 - regression_loss: 19.5844 - val_loss: 24.3701 - val_regression_loss: 15.7808 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.7113 - regression_loss: 19.4419 - val_loss: 24.2782 - val_regression_loss: 15.6878 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.0691 - regression_loss: 19.3610 - val_loss: 24.0627 - val_regression_loss: 15.4813 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.7307 - regression_loss: 19.3138 - val_loss: 24.2459 - val_regression_loss: 15.6189 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.0319 - regression_loss: 19.0604 - val_loss: 24.0837 - val_regression_loss: 15.5430 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.0844 - regression_loss: 18.9950 - val_loss: 23.9655 - val_regression_loss: 15.4699 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.1788 - regression_loss: 18.8959 - val_loss: 24.1299 - val_regression_loss: 15.6067 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.0001 - regression_loss: 18.7835 - val_loss: 23.9775 - val_regression_loss: 15.4849 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.1466 - regression_loss: 18.6812 - val_loss: 23.9068 - val_regression_loss: 15.4171 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.9076 - regression_loss: 18.6628 - val_loss: 23.8871 - val_regression_loss: 15.3642 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.6699 - regression_loss: 18.5857 - val_loss: 23.8739 - val_regression_loss: 15.3646 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.8531 - regression_loss: 18.4234 - val_loss: 23.8544 - val_regression_loss: 15.3313 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.8678 - regression_loss: 18.5766 - val_loss: 23.8475 - val_regression_loss: 15.3206 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.7518 - regression_loss: 18.3411 - val_loss: 23.7993 - val_regression_loss: 15.3136 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.3198 - regression_loss: 18.3053 - val_loss: 23.8934 - val_regression_loss: 15.3147 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.6861 - regression_loss: 18.2009 - val_loss: 23.6919 - val_regression_loss: 15.1808 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.6017 - regression_loss: 18.1572 - val_loss: 23.6608 - val_regression_loss: 15.2066 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.7799 - regression_loss: 17.9758 - val_loss: 23.7690 - val_regression_loss: 15.2583 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.2670 - regression_loss: 17.9240 - val_loss: 23.6830 - val_regression_loss: 15.1777 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.3022 - regression_loss: 17.8563 - val_loss: 23.6767 - val_regression_loss: 15.1585 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.3510 - regression_loss: 17.7779 - val_loss: 23.5758 - val_regression_loss: 15.1035 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.7890 - regression_loss: 17.6826 - val_loss: 23.5425 - val_regression_loss: 15.0473 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.3928 - regression_loss: 17.6200 - val_loss: 23.5852 - val_regression_loss: 15.1167 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.8115 - regression_loss: 17.5805 - val_loss: 23.5904 - val_regression_loss: 15.0841 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.0331 - regression_loss: 17.5621 - val_loss: 23.5760 - val_regression_loss: 15.0548 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 23.2784 - regression_loss: 19.3601\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.5283 - regression_loss: 17.4570 - val_loss: 23.4813 - val_regression_loss: 15.0005 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.1834 - regression_loss: 17.3991 - val_loss: 23.4507 - val_regression_loss: 14.9685 - lr: 5.0000e-05\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.5380 - regression_loss: 17.3132 - val_loss: 23.3752 - val_regression_loss: 14.9130 - lr: 5.0000e-05\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.7502 - regression_loss: 17.2999 - val_loss: 23.3680 - val_regression_loss: 14.9133 - lr: 5.0000e-05\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.3655 - regression_loss: 17.2393 - val_loss: 23.4073 - val_regression_loss: 14.9214 - lr: 5.0000e-05\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.4007 - regression_loss: 17.2600 - val_loss: 23.3866 - val_regression_loss: 14.8980 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 20.7922 - regression_loss: 16.8796\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.4091 - regression_loss: 17.2010 - val_loss: 23.3895 - val_regression_loss: 14.8868 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.4460 - regression_loss: 17.1508 - val_loss: 23.3907 - val_regression_loss: 14.8952 - lr: 2.5000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.8903 - regression_loss: 17.1315 - val_loss: 23.3578 - val_regression_loss: 14.8760 - lr: 2.5000e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.7013 - regression_loss: 17.1004 - val_loss: 23.3424 - val_regression_loss: 14.8642 - lr: 2.5000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.8622 - regression_loss: 17.1170 - val_loss: 23.3879 - val_regression_loss: 14.9067 - lr: 2.5000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.2822 - regression_loss: 17.1135 - val_loss: 23.3323 - val_regression_loss: 14.8608 - lr: 2.5000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.3196 - regression_loss: 17.0873 - val_loss: 23.3671 - val_regression_loss: 14.8823 - lr: 2.5000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.2282 - regression_loss: 17.0467 - val_loss: 23.3565 - val_regression_loss: 14.8814 - lr: 2.5000e-05\n",
      "Epoch 57/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 29.8058 - regression_loss: 25.8971\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.2597 - regression_loss: 17.0565 - val_loss: 23.3242 - val_regression_loss: 14.8557 - lr: 2.5000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.6090 - regression_loss: 17.0074 - val_loss: 23.3164 - val_regression_loss: 14.8509 - lr: 1.2500e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.3784 - regression_loss: 16.9972 - val_loss: 23.3313 - val_regression_loss: 14.8551 - lr: 1.2500e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.1483 - regression_loss: 17.0035 - val_loss: 23.3134 - val_regression_loss: 14.8332 - lr: 1.2500e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.1587 - regression_loss: 16.9846 - val_loss: 23.2971 - val_regression_loss: 14.8204 - lr: 1.2500e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.1418 - regression_loss: 16.9668 - val_loss: 23.3087 - val_regression_loss: 14.8328 - lr: 1.2500e-05\n",
      "Epoch 63/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 21.8990 - regression_loss: 17.9917\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.1952 - regression_loss: 16.9760 - val_loss: 23.3140 - val_regression_loss: 14.8387 - lr: 1.2500e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.8990 - regression_loss: 16.9492 - val_loss: 23.3101 - val_regression_loss: 14.8354 - lr: 6.2500e-06\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.9166 - regression_loss: 16.9520 - val_loss: 23.3098 - val_regression_loss: 14.8381 - lr: 6.2500e-06\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.1238 - regression_loss: 16.9467 - val_loss: 23.3031 - val_regression_loss: 14.8332 - lr: 6.2500e-06\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.0110 - regression_loss: 16.9382 - val_loss: 23.2951 - val_regression_loss: 14.8255 - lr: 6.2500e-06\n",
      "Epoch 68/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 25.9732 - regression_loss: 22.0666\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.0346 - regression_loss: 16.9341 - val_loss: 23.3088 - val_regression_loss: 14.8348 - lr: 6.2500e-06\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.8474 - regression_loss: 16.9304 - val_loss: 23.3097 - val_regression_loss: 14.8364 - lr: 3.1250e-06\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.5389 - regression_loss: 16.9232 - val_loss: 23.3090 - val_regression_loss: 14.8350 - lr: 3.1250e-06\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.6106 - regression_loss: 16.9205 - val_loss: 23.3071 - val_regression_loss: 14.8341 - lr: 3.1250e-06\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.2395 - regression_loss: 16.9205 - val_loss: 23.3059 - val_regression_loss: 14.8310 - lr: 3.1250e-06\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.8701 - regression_loss: 16.9176 - val_loss: 23.3097 - val_regression_loss: 14.8343 - lr: 3.1250e-06\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.2153 - regression_loss: 16.9159 - val_loss: 23.3094 - val_regression_loss: 14.8339 - lr: 3.1250e-06\n",
      "Epoch 75/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 23.9987 - regression_loss: 20.0925\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.6829 - regression_loss: 16.9134 - val_loss: 23.3104 - val_regression_loss: 14.8348 - lr: 3.1250e-06\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.3310 - regression_loss: 16.9091 - val_loss: 23.3096 - val_regression_loss: 14.8341 - lr: 1.5625e-06\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.9446 - regression_loss: 16.9084 - val_loss: 23.3082 - val_regression_loss: 14.8334 - lr: 1.5625e-06\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.0353 - regression_loss: 16.9079 - val_loss: 23.3053 - val_regression_loss: 14.8310 - lr: 1.5625e-06\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.7014 - regression_loss: 16.9061 - val_loss: 23.3042 - val_regression_loss: 14.8300 - lr: 1.5625e-06\n",
      "Epoch 80/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 23.9944 - regression_loss: 20.0884\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.9922 - regression_loss: 16.9053 - val_loss: 23.3028 - val_regression_loss: 14.8286 - lr: 1.5625e-06\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.2522 - regression_loss: 16.9037 - val_loss: 23.3022 - val_regression_loss: 14.8287 - lr: 7.8125e-07\n",
      "Epoch 82/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 21.7271 - regression_loss: 16.9029 - val_loss: 23.3019 - val_regression_loss: 14.8280 - lr: 7.8125e-07\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.8997 - regression_loss: 16.9020 - val_loss: 23.3026 - val_regression_loss: 14.8285 - lr: 7.8125e-07\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.1056 - regression_loss: 16.9018 - val_loss: 23.3014 - val_regression_loss: 14.8274 - lr: 7.8125e-07\n",
      "Epoch 85/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.9865 - regression_loss: 15.0807\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.3366 - regression_loss: 16.9009 - val_loss: 23.3004 - val_regression_loss: 14.8269 - lr: 7.8125e-07\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.0601 - regression_loss: 16.9001 - val_loss: 23.3006 - val_regression_loss: 14.8270 - lr: 3.9062e-07\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.0615 - regression_loss: 16.9004 - val_loss: 23.2996 - val_regression_loss: 14.8261 - lr: 3.9062e-07\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.9823 - regression_loss: 16.8996 - val_loss: 23.2993 - val_regression_loss: 14.8257 - lr: 3.9062e-07\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.0277 - regression_loss: 16.8993 - val_loss: 23.2994 - val_regression_loss: 14.8259 - lr: 3.9062e-07\n",
      "Epoch 90/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 21.9858 - regression_loss: 18.0800\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.0421 - regression_loss: 16.8988 - val_loss: 23.2992 - val_regression_loss: 14.8258 - lr: 3.9062e-07\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.8968 - regression_loss: 16.8987 - val_loss: 23.2988 - val_regression_loss: 14.8255 - lr: 1.9531e-07\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.2073 - regression_loss: 16.8983 - val_loss: 23.2988 - val_regression_loss: 14.8255 - lr: 1.9531e-07\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.8175 - regression_loss: 16.8984 - val_loss: 23.2984 - val_regression_loss: 14.8252 - lr: 1.9531e-07\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.5443 - regression_loss: 16.8980 - val_loss: 23.2984 - val_regression_loss: 14.8252 - lr: 1.9531e-07\n",
      "Epoch 95/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 22.5759 - regression_loss: 18.6701\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.8216 - regression_loss: 16.8979 - val_loss: 23.2986 - val_regression_loss: 14.8254 - lr: 1.9531e-07\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.0168 - regression_loss: 16.8978 - val_loss: 23.2987 - val_regression_loss: 14.8254 - lr: 9.7656e-08\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.1127 - regression_loss: 16.8976 - val_loss: 23.2986 - val_regression_loss: 14.8253 - lr: 9.7656e-08\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.1586 - regression_loss: 16.8976 - val_loss: 23.2986 - val_regression_loss: 14.8254 - lr: 9.7656e-08\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.8949 - regression_loss: 16.8976 - val_loss: 23.2987 - val_regression_loss: 14.8255 - lr: 9.7656e-08\n",
      "Epoch 100/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 22.3844 - regression_loss: 18.4785\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.7689 - regression_loss: 16.8975 - val_loss: 23.2985 - val_regression_loss: 14.8253 - lr: 9.7656e-08\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.8250 - regression_loss: 16.8973 - val_loss: 23.2984 - val_regression_loss: 14.8252 - lr: 4.8828e-08\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.2471 - regression_loss: 16.8972 - val_loss: 23.2984 - val_regression_loss: 14.8252 - lr: 4.8828e-08\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.8805 - regression_loss: 16.8972 - val_loss: 23.2984 - val_regression_loss: 14.8252 - lr: 4.8828e-08\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.9193 - regression_loss: 16.8972 - val_loss: 23.2984 - val_regression_loss: 14.8252 - lr: 4.8828e-08\n",
      "Epoch 105/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.8711 - regression_loss: 14.9653\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.8109 - regression_loss: 16.8972 - val_loss: 23.2984 - val_regression_loss: 14.8252 - lr: 4.8828e-08\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.9030 - regression_loss: 16.8971 - val_loss: 23.2984 - val_regression_loss: 14.8252 - lr: 2.4414e-08\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.9988 - regression_loss: 16.8971 - val_loss: 23.2984 - val_regression_loss: 14.8252 - lr: 2.4414e-08\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 50.2825 - regression_loss: 43.0543 - val_loss: 33.7576 - val_regression_loss: 24.5101 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 40.7976 - regression_loss: 34.0956 - val_loss: 30.8021 - val_regression_loss: 21.7706 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 37.4031 - regression_loss: 31.0806 - val_loss: 29.7549 - val_regression_loss: 20.8352 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 36.2196 - regression_loss: 30.2892 - val_loss: 28.8762 - val_regression_loss: 20.1319 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 34.9845 - regression_loss: 29.1806 - val_loss: 28.4588 - val_regression_loss: 19.7674 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 34.9440 - regression_loss: 28.5229 - val_loss: 28.4886 - val_regression_loss: 19.7390 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.9733 - regression_loss: 27.8371 - val_loss: 28.8139 - val_regression_loss: 19.9156 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 33.3757 - regression_loss: 27.5662 - val_loss: 28.9331 - val_regression_loss: 19.8842 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.8634 - regression_loss: 27.3454 - val_loss: 29.2557 - val_regression_loss: 20.0768 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.8730 - regression_loss: 27.0805 - val_loss: 29.4378 - val_regression_loss: 20.1912 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.4420 - regression_loss: 26.8661 - val_loss: 29.1417 - val_regression_loss: 19.9832 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.4223 - regression_loss: 26.6986 - val_loss: 29.1589 - val_regression_loss: 19.9767 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.1737 - regression_loss: 26.6054 - val_loss: 29.1087 - val_regression_loss: 19.9578 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.2322 - regression_loss: 26.5832 - val_loss: 29.2660 - val_regression_loss: 20.0624 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.2177 - regression_loss: 26.3619 - val_loss: 29.3516 - val_regression_loss: 20.1219 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.2534 - regression_loss: 26.2513 - val_loss: 29.3755 - val_regression_loss: 20.1542 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.9794 - regression_loss: 26.1949 - val_loss: 29.4102 - val_regression_loss: 20.1381 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.2736 - regression_loss: 26.0703 - val_loss: 29.3562 - val_regression_loss: 20.0674 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.8615 - regression_loss: 26.0646 - val_loss: 29.3378 - val_regression_loss: 20.0596 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.4488 - regression_loss: 25.9397 - val_loss: 29.2508 - val_regression_loss: 20.0142 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.6440 - regression_loss: 25.8835 - val_loss: 29.4249 - val_regression_loss: 20.1538 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.3372 - regression_loss: 25.7801 - val_loss: 29.3371 - val_regression_loss: 20.0822 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.7112 - regression_loss: 25.9545 - val_loss: 29.4501 - val_regression_loss: 20.1640 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.4044 - regression_loss: 25.7774 - val_loss: 29.6796 - val_regression_loss: 20.2888 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.2194 - regression_loss: 25.7117 - val_loss: 29.3322 - val_regression_loss: 20.0851 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.6248 - regression_loss: 25.5657 - val_loss: 29.3304 - val_regression_loss: 20.0547 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.9354 - regression_loss: 25.4992 - val_loss: 29.4190 - val_regression_loss: 20.1656 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.9040 - regression_loss: 25.5024 - val_loss: 29.3639 - val_regression_loss: 20.1372 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.8566 - regression_loss: 25.4260 - val_loss: 29.5966 - val_regression_loss: 20.2781 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.0518 - regression_loss: 25.3692 - val_loss: 29.5131 - val_regression_loss: 20.1909 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.6592 - regression_loss: 25.2545 - val_loss: 29.3887 - val_regression_loss: 20.1213 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.7147 - regression_loss: 25.2824 - val_loss: 29.4593 - val_regression_loss: 20.2039 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.4381 - regression_loss: 25.2011 - val_loss: 29.6620 - val_regression_loss: 20.3345 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.0235 - regression_loss: 25.2133 - val_loss: 29.4630 - val_regression_loss: 20.1968 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.0673 - regression_loss: 25.1540 - val_loss: 29.4937 - val_regression_loss: 20.2182 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.9751 - regression_loss: 25.2649 - val_loss: 29.4859 - val_regression_loss: 20.2351 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.0229 - regression_loss: 25.1990 - val_loss: 29.4807 - val_regression_loss: 20.2706 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 40.0046 - regression_loss: 36.1597\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.6763 - regression_loss: 24.9683 - val_loss: 29.8393 - val_regression_loss: 20.4889 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.0831 - regression_loss: 24.9099 - val_loss: 29.7736 - val_regression_loss: 20.4544 - lr: 5.0000e-05\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.4523 - regression_loss: 24.8828 - val_loss: 29.6605 - val_regression_loss: 20.3755 - lr: 5.0000e-05\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.7033 - regression_loss: 24.8279 - val_loss: 29.5027 - val_regression_loss: 20.2485 - lr: 5.0000e-05\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.5001 - regression_loss: 24.8097 - val_loss: 29.4398 - val_regression_loss: 20.2271 - lr: 5.0000e-05\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.5072 - regression_loss: 24.7774 - val_loss: 29.5046 - val_regression_loss: 20.2819 - lr: 5.0000e-05\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.6773 - regression_loss: 24.7442 - val_loss: 29.4746 - val_regression_loss: 20.2636 - lr: 5.0000e-05\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.9806 - regression_loss: 24.7408 - val_loss: 29.6642 - val_regression_loss: 20.3899 - lr: 5.0000e-05\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 24ms/step - loss: 71.9975 - regression_loss: 63.6719 - val_loss: 56.5077 - val_regression_loss: 41.8307 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 52.2967 - regression_loss: 45.4045 - val_loss: 45.6457 - val_regression_loss: 33.3258 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 41.4374 - regression_loss: 34.9602 - val_loss: 38.1594 - val_regression_loss: 27.4742 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 34.8265 - regression_loss: 28.5425 - val_loss: 35.0562 - val_regression_loss: 24.8521 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.9753 - regression_loss: 25.2209 - val_loss: 34.0195 - val_regression_loss: 23.7636 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.1815 - regression_loss: 23.7844 - val_loss: 32.3682 - val_regression_loss: 22.3221 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.9282 - regression_loss: 22.4280 - val_loss: 31.2502 - val_regression_loss: 21.2381 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.4862 - regression_loss: 21.0834 - val_loss: 30.4202 - val_regression_loss: 20.3788 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.3778 - regression_loss: 20.0418 - val_loss: 29.2249 - val_regression_loss: 19.3577 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.6756 - regression_loss: 19.4074 - val_loss: 28.3805 - val_regression_loss: 18.6196 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.3321 - regression_loss: 19.0075 - val_loss: 28.3393 - val_regression_loss: 18.4711 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.2310 - regression_loss: 18.1618 - val_loss: 27.1159 - val_regression_loss: 17.5358 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.7556 - regression_loss: 17.7028 - val_loss: 27.1428 - val_regression_loss: 17.4595 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.1646 - regression_loss: 17.3242 - val_loss: 26.6615 - val_regression_loss: 17.0714 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.8084 - regression_loss: 16.9835 - val_loss: 26.4027 - val_regression_loss: 16.8396 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.3193 - regression_loss: 16.7041 - val_loss: 26.0995 - val_regression_loss: 16.5901 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.0140 - regression_loss: 16.4277 - val_loss: 25.7929 - val_regression_loss: 16.3326 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.2965 - regression_loss: 16.2068 - val_loss: 25.5592 - val_regression_loss: 16.1495 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9052 - regression_loss: 15.9915 - val_loss: 25.6014 - val_regression_loss: 16.1361 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7961 - regression_loss: 15.7627 - val_loss: 25.4007 - val_regression_loss: 15.9738 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6854 - regression_loss: 15.6564 - val_loss: 25.0999 - val_regression_loss: 15.7466 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4489 - regression_loss: 15.5476 - val_loss: 25.0058 - val_regression_loss: 15.6735 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4572 - regression_loss: 15.3902 - val_loss: 25.4587 - val_regression_loss: 15.9594 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.0034 - regression_loss: 15.2984 - val_loss: 24.8676 - val_regression_loss: 15.5549 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.1132 - regression_loss: 15.3510 - val_loss: 25.0565 - val_regression_loss: 15.6642 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.0100 - regression_loss: 15.1031 - val_loss: 25.1307 - val_regression_loss: 15.6913 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.8336 - regression_loss: 15.0132 - val_loss: 24.6823 - val_regression_loss: 15.4192 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.7093 - regression_loss: 15.0629 - val_loss: 24.6157 - val_regression_loss: 15.3436 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.7760 - regression_loss: 14.9210 - val_loss: 24.6979 - val_regression_loss: 15.3901 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.5193 - regression_loss: 14.9564 - val_loss: 24.5125 - val_regression_loss: 15.3012 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.6251 - regression_loss: 14.7195 - val_loss: 24.8171 - val_regression_loss: 15.4641 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.3979 - regression_loss: 14.6692 - val_loss: 24.3482 - val_regression_loss: 15.1861 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.3373 - regression_loss: 14.6457 - val_loss: 24.6091 - val_regression_loss: 15.3150 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.5488 - regression_loss: 14.6066 - val_loss: 24.7405 - val_regression_loss: 15.4135 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.5707 - regression_loss: 14.6731 - val_loss: 24.3606 - val_regression_loss: 15.1746 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.5182 - regression_loss: 14.5922 - val_loss: 24.5599 - val_regression_loss: 15.2727 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.2327 - regression_loss: 14.3902 - val_loss: 24.1390 - val_regression_loss: 15.0856 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.2499 - regression_loss: 14.4122 - val_loss: 24.5902 - val_regression_loss: 15.3235 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1166 - regression_loss: 14.3092 - val_loss: 24.3907 - val_regression_loss: 15.1895 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.2046 - regression_loss: 14.3174 - val_loss: 24.2932 - val_regression_loss: 15.1040 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.9347 - regression_loss: 14.2875 - val_loss: 24.1030 - val_regression_loss: 15.0054 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.7071 - regression_loss: 14.1840 - val_loss: 24.2664 - val_regression_loss: 15.1092 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.0154 - regression_loss: 14.1907 - val_loss: 24.2239 - val_regression_loss: 15.0655 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.0627 - regression_loss: 14.0990 - val_loss: 24.2209 - val_regression_loss: 15.0862 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1825 - regression_loss: 14.1452 - val_loss: 24.0537 - val_regression_loss: 14.9659 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.8004 - regression_loss: 14.1309 - val_loss: 24.3457 - val_regression_loss: 15.1853 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 19.4148 - regression_loss: 15.6510\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.2092 - regression_loss: 14.3469 - val_loss: 24.0406 - val_regression_loss: 15.0131 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.7964 - regression_loss: 13.9907 - val_loss: 23.8453 - val_regression_loss: 14.8877 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.8185 - regression_loss: 14.0289 - val_loss: 24.1138 - val_regression_loss: 15.0097 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.8775 - regression_loss: 13.8924 - val_loss: 23.9732 - val_regression_loss: 14.9271 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.5282 - regression_loss: 13.8694 - val_loss: 23.9033 - val_regression_loss: 14.9048 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.5663 - regression_loss: 13.9074 - val_loss: 23.9885 - val_regression_loss: 14.9490 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.4982 - regression_loss: 13.8516 - val_loss: 24.0501 - val_regression_loss: 14.9903 - lr: 5.0000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.5009 - regression_loss: 13.8696 - val_loss: 23.9333 - val_regression_loss: 14.9158 - lr: 5.0000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.7058 - regression_loss: 13.8776 - val_loss: 24.1226 - val_regression_loss: 15.0192 - lr: 5.0000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.4340 - regression_loss: 13.8116 - val_loss: 23.8779 - val_regression_loss: 14.8918 - lr: 5.0000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.6467 - regression_loss: 13.8218 - val_loss: 23.9390 - val_regression_loss: 14.9339 - lr: 5.0000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.5644 - regression_loss: 13.7987 - val_loss: 24.1293 - val_regression_loss: 15.0466 - lr: 5.0000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.2134 - regression_loss: 13.7624 - val_loss: 23.9273 - val_regression_loss: 14.9323 - lr: 5.0000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.9898 - regression_loss: 13.7686 - val_loss: 23.8695 - val_regression_loss: 14.8905 - lr: 5.0000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.3274 - regression_loss: 13.7570 - val_loss: 24.0112 - val_regression_loss: 14.9620 - lr: 5.0000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.3817 - regression_loss: 13.7197 - val_loss: 23.9731 - val_regression_loss: 14.9526 - lr: 5.0000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.3894 - regression_loss: 13.7445 - val_loss: 23.8700 - val_regression_loss: 14.9202 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.4204 - regression_loss: 13.7656 - val_loss: 23.9531 - val_regression_loss: 14.9512 - lr: 5.0000e-05\n",
      "Epoch 65/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 20.7690 - regression_loss: 17.0220\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.2351 - regression_loss: 13.6885 - val_loss: 23.8028 - val_regression_loss: 14.8546 - lr: 5.0000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.4971 - regression_loss: 13.6464 - val_loss: 23.7974 - val_regression_loss: 14.8544 - lr: 2.5000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.3473 - regression_loss: 13.6227 - val_loss: 23.9200 - val_regression_loss: 14.9171 - lr: 2.5000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.2982 - regression_loss: 13.6041 - val_loss: 23.9442 - val_regression_loss: 14.9370 - lr: 2.5000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.0233 - regression_loss: 13.5960 - val_loss: 23.8660 - val_regression_loss: 14.8916 - lr: 2.5000e-05\n",
      "Epoch 70/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.8687 - regression_loss: 15.1244\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.4868 - regression_loss: 13.6139 - val_loss: 23.8366 - val_regression_loss: 14.8839 - lr: 2.5000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.1808 - regression_loss: 13.5731 - val_loss: 23.8713 - val_regression_loss: 14.9076 - lr: 1.2500e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.2335 - regression_loss: 13.5703 - val_loss: 23.8731 - val_regression_loss: 14.9057 - lr: 1.2500e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.3796 - regression_loss: 13.5666 - val_loss: 23.8398 - val_regression_loss: 14.8798 - lr: 1.2500e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.0890 - regression_loss: 13.5635 - val_loss: 23.8271 - val_regression_loss: 14.8738 - lr: 1.2500e-05\n",
      "Epoch 75/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 16.1783 - regression_loss: 12.4354\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.0513 - regression_loss: 13.5551 - val_loss: 23.8446 - val_regression_loss: 14.8855 - lr: 1.2500e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.3367 - regression_loss: 13.5442 - val_loss: 23.8414 - val_regression_loss: 14.8814 - lr: 6.2500e-06\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.3611 - regression_loss: 13.5467 - val_loss: 23.8144 - val_regression_loss: 14.8664 - lr: 6.2500e-06\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.2848 - regression_loss: 13.5443 - val_loss: 23.8533 - val_regression_loss: 14.8901 - lr: 6.2500e-06\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.3205 - regression_loss: 13.5415 - val_loss: 23.8588 - val_regression_loss: 14.8945 - lr: 6.2500e-06\n",
      "Epoch 80/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.4313 - regression_loss: 14.6891\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.2856 - regression_loss: 13.5352 - val_loss: 23.8360 - val_regression_loss: 14.8806 - lr: 6.2500e-06\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.9797 - regression_loss: 13.5330 - val_loss: 23.8167 - val_regression_loss: 14.8698 - lr: 3.1250e-06\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.2627 - regression_loss: 13.5298 - val_loss: 23.8227 - val_regression_loss: 14.8735 - lr: 3.1250e-06\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.0610 - regression_loss: 13.5291 - val_loss: 23.8315 - val_regression_loss: 14.8792 - lr: 3.1250e-06\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.7972 - regression_loss: 13.5293 - val_loss: 23.8221 - val_regression_loss: 14.8738 - lr: 3.1250e-06\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.1021 - regression_loss: 13.5286 - val_loss: 23.8247 - val_regression_loss: 14.8748 - lr: 3.1250e-06\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.2344 - regression_loss: 13.5281 - val_loss: 23.8379 - val_regression_loss: 14.8832 - lr: 3.1250e-06\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.1001 - regression_loss: 13.5281 - val_loss: 23.8292 - val_regression_loss: 14.8790 - lr: 3.1250e-06\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.1815 - regression_loss: 13.5249 - val_loss: 23.8293 - val_regression_loss: 14.8795 - lr: 3.1250e-06\n",
      "Epoch 89/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 17.3108 - regression_loss: 13.5691\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.3882 - regression_loss: 13.5228 - val_loss: 23.8382 - val_regression_loss: 14.8844 - lr: 3.1250e-06\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.2874 - regression_loss: 13.5202 - val_loss: 23.8383 - val_regression_loss: 14.8842 - lr: 1.5625e-06\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.4649 - regression_loss: 13.5201 - val_loss: 23.8371 - val_regression_loss: 14.8832 - lr: 1.5625e-06\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.4027 - regression_loss: 13.5197 - val_loss: 23.8380 - val_regression_loss: 14.8835 - lr: 1.5625e-06\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.0252 - regression_loss: 13.5196 - val_loss: 23.8336 - val_regression_loss: 14.8811 - lr: 1.5625e-06\n",
      "Epoch 94/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 19.2269 - regression_loss: 15.4854\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.1162 - regression_loss: 13.5181 - val_loss: 23.8296 - val_regression_loss: 14.8790 - lr: 1.5625e-06\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.9194 - regression_loss: 13.5171 - val_loss: 23.8277 - val_regression_loss: 14.8780 - lr: 7.8125e-07\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.9754 - regression_loss: 13.5175 - val_loss: 23.8298 - val_regression_loss: 14.8794 - lr: 7.8125e-07\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.1203 - regression_loss: 13.5169 - val_loss: 23.8274 - val_regression_loss: 14.8780 - lr: 7.8125e-07\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.2265 - regression_loss: 13.5172 - val_loss: 23.8318 - val_regression_loss: 14.8803 - lr: 7.8125e-07\n",
      "Epoch 99/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 19.3448 - regression_loss: 15.6034\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.4105 - regression_loss: 13.5163 - val_loss: 23.8300 - val_regression_loss: 14.8795 - lr: 7.8125e-07\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.3107 - regression_loss: 13.5154 - val_loss: 23.8295 - val_regression_loss: 14.8792 - lr: 3.9062e-07\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.3873 - regression_loss: 13.5155 - val_loss: 23.8294 - val_regression_loss: 14.8793 - lr: 3.9062e-07\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.1117 - regression_loss: 13.5156 - val_loss: 23.8290 - val_regression_loss: 14.8789 - lr: 3.9062e-07\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.0108 - regression_loss: 13.5154 - val_loss: 23.8299 - val_regression_loss: 14.8795 - lr: 3.9062e-07\n",
      "Epoch 104/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 [====>.........................] - ETA: 0s - loss: 17.8745 - regression_loss: 14.1331\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.9821 - regression_loss: 13.5150 - val_loss: 23.8284 - val_regression_loss: 14.8786 - lr: 3.9062e-07\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.0115 - regression_loss: 13.5148 - val_loss: 23.8281 - val_regression_loss: 14.8785 - lr: 1.9531e-07\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.1777 - regression_loss: 13.5150 - val_loss: 23.8295 - val_regression_loss: 14.8792 - lr: 1.9531e-07\n",
      "3/3 [==============================] - 0s 983us/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 23ms/step - loss: 105.3145 - regression_loss: 93.1760 - val_loss: 59.2237 - val_regression_loss: 52.4521 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 61.2039 - regression_loss: 54.3279 - val_loss: 35.4733 - val_regression_loss: 31.8106 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 42.0280 - regression_loss: 34.9092 - val_loss: 25.6904 - val_regression_loss: 22.7270 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 32.8090 - regression_loss: 26.6242 - val_loss: 21.8383 - val_regression_loss: 18.6324 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.8247 - regression_loss: 22.3097 - val_loss: 19.9887 - val_regression_loss: 16.4205 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.5346 - regression_loss: 19.3499 - val_loss: 17.6560 - val_regression_loss: 14.0658 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.2187 - regression_loss: 16.3853 - val_loss: 15.1187 - val_regression_loss: 11.7236 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.5307 - regression_loss: 13.8144 - val_loss: 13.2170 - val_regression_loss: 9.9391 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.4346 - regression_loss: 11.6958 - val_loss: 11.4004 - val_regression_loss: 8.2437 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.6642 - regression_loss: 10.0829 - val_loss: 10.1637 - val_regression_loss: 7.0157 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.3051 - regression_loss: 8.7381 - val_loss: 9.7214 - val_regression_loss: 6.5208 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1982 - regression_loss: 7.7687 - val_loss: 8.5842 - val_regression_loss: 5.4050 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4238 - regression_loss: 6.9788 - val_loss: 7.9655 - val_regression_loss: 4.7323 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.1674 - regression_loss: 6.3432 - val_loss: 7.5686 - val_regression_loss: 4.2726 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0891 - regression_loss: 5.8813 - val_loss: 7.3461 - val_regression_loss: 4.0052 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5690 - regression_loss: 5.3775 - val_loss: 6.7502 - val_regression_loss: 3.3970 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1367 - regression_loss: 4.9821 - val_loss: 6.8252 - val_regression_loss: 3.4769 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8441 - regression_loss: 4.6774 - val_loss: 6.4696 - val_regression_loss: 3.1007 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6241 - regression_loss: 4.3751 - val_loss: 6.2072 - val_regression_loss: 2.8403 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1602 - regression_loss: 4.1650 - val_loss: 6.0314 - val_regression_loss: 2.5673 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9831 - regression_loss: 3.9347 - val_loss: 6.1026 - val_regression_loss: 2.6245 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7466 - regression_loss: 3.6917 - val_loss: 5.8183 - val_regression_loss: 2.3259 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6291 - regression_loss: 3.5650 - val_loss: 5.8106 - val_regression_loss: 2.3167 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.3420 - regression_loss: 3.4009 - val_loss: 5.7340 - val_regression_loss: 2.2399 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.3720 - regression_loss: 3.3199 - val_loss: 5.5893 - val_regression_loss: 2.0255 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0426 - regression_loss: 3.1057 - val_loss: 5.7151 - val_regression_loss: 2.1613 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9813 - regression_loss: 3.0493 - val_loss: 5.5291 - val_regression_loss: 1.9181 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9368 - regression_loss: 2.9349 - val_loss: 5.5132 - val_regression_loss: 1.9186 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.7802 - regression_loss: 2.8170 - val_loss: 5.5054 - val_regression_loss: 1.9168 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6814 - regression_loss: 2.7170 - val_loss: 5.3946 - val_regression_loss: 1.8011 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5915 - regression_loss: 2.6648 - val_loss: 5.3939 - val_regression_loss: 1.7421 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5616 - regression_loss: 2.5884 - val_loss: 5.3682 - val_regression_loss: 1.7340 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4186 - regression_loss: 2.4738 - val_loss: 5.2922 - val_regression_loss: 1.6503 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2979 - regression_loss: 2.4157 - val_loss: 5.2762 - val_regression_loss: 1.6316 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2415 - regression_loss: 2.3572 - val_loss: 5.2451 - val_regression_loss: 1.5818 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.1823 - regression_loss: 2.2857 - val_loss: 5.2803 - val_regression_loss: 1.6005 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.1152 - regression_loss: 2.2115 - val_loss: 5.2426 - val_regression_loss: 1.5637 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.0165 - regression_loss: 2.1661 - val_loss: 5.1947 - val_regression_loss: 1.5209 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.0187 - regression_loss: 2.1094 - val_loss: 5.1491 - val_regression_loss: 1.4471 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9188 - regression_loss: 2.0668 - val_loss: 5.1865 - val_regression_loss: 1.5113 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8570 - regression_loss: 2.0356 - val_loss: 5.1188 - val_regression_loss: 1.4061 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.8567 - regression_loss: 1.9725 - val_loss: 5.1607 - val_regression_loss: 1.4527 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8285 - regression_loss: 1.9443 - val_loss: 5.0975 - val_regression_loss: 1.3937 - lr: 1.0000e-04\n",
      "Epoch 44/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7524 - regression_loss: 1.9069 - val_loss: 5.1368 - val_regression_loss: 1.4192 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7020 - regression_loss: 1.8826 - val_loss: 5.0642 - val_regression_loss: 1.3685 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6831 - regression_loss: 1.8244 - val_loss: 5.0996 - val_regression_loss: 1.3678 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6411 - regression_loss: 1.7946 - val_loss: 5.1188 - val_regression_loss: 1.3814 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5620 - regression_loss: 1.8040 - val_loss: 5.0428 - val_regression_loss: 1.3202 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.4978 - regression_loss: 1.6977 - val_loss: 5.1267 - val_regression_loss: 1.4454 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5948 - regression_loss: 1.7523 - val_loss: 5.0174 - val_regression_loss: 1.2896 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.4840 - regression_loss: 1.6673 - val_loss: 5.0478 - val_regression_loss: 1.3184 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.4426 - regression_loss: 1.6243 - val_loss: 5.0063 - val_regression_loss: 1.3065 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.4030 - regression_loss: 1.6122 - val_loss: 4.9824 - val_regression_loss: 1.2588 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.3671 - regression_loss: 1.5659 - val_loss: 5.0091 - val_regression_loss: 1.2562 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9786 - regression_loss: 1.5552 - val_loss: 4.9939 - val_regression_loss: 1.3026 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.3463 - regression_loss: 1.5550 - val_loss: 4.9669 - val_regression_loss: 1.2142 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.3228 - regression_loss: 1.5263 - val_loss: 5.1139 - val_regression_loss: 1.3451 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.3038 - regression_loss: 1.5355 - val_loss: 4.9565 - val_regression_loss: 1.2062 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2362 - regression_loss: 1.4873 - val_loss: 4.9762 - val_regression_loss: 1.2655 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.1648 - regression_loss: 1.5255\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1817 - regression_loss: 1.4168 - val_loss: 4.9745 - val_regression_loss: 1.1961 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2206 - regression_loss: 1.4474 - val_loss: 5.0095 - val_regression_loss: 1.2607 - lr: 5.0000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1147 - regression_loss: 1.3892 - val_loss: 4.9423 - val_regression_loss: 1.2155 - lr: 5.0000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1189 - regression_loss: 1.3725 - val_loss: 4.9611 - val_regression_loss: 1.2337 - lr: 5.0000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1414 - regression_loss: 1.3677 - val_loss: 4.9485 - val_regression_loss: 1.2253 - lr: 5.0000e-05\n",
      "Epoch 65/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.6542 - regression_loss: 1.0223\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1154 - regression_loss: 1.3566 - val_loss: 4.9413 - val_regression_loss: 1.2041 - lr: 5.0000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0697 - regression_loss: 1.3596 - val_loss: 4.9526 - val_regression_loss: 1.2066 - lr: 2.5000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0734 - regression_loss: 1.3388 - val_loss: 4.9856 - val_regression_loss: 1.2519 - lr: 2.5000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0965 - regression_loss: 1.3353 - val_loss: 4.9543 - val_regression_loss: 1.2227 - lr: 2.5000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0693 - regression_loss: 1.3313 - val_loss: 4.9391 - val_regression_loss: 1.2203 - lr: 2.5000e-05\n",
      "Epoch 70/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.8209 - regression_loss: 1.1929\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0381 - regression_loss: 1.3365 - val_loss: 4.9258 - val_regression_loss: 1.1915 - lr: 2.5000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.0850 - regression_loss: 1.3169 - val_loss: 4.9427 - val_regression_loss: 1.2099 - lr: 1.2500e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0796 - regression_loss: 1.3174 - val_loss: 4.9599 - val_regression_loss: 1.2289 - lr: 1.2500e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8153 - regression_loss: 1.3166 - val_loss: 4.9404 - val_regression_loss: 1.2135 - lr: 1.2500e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0672 - regression_loss: 1.3135 - val_loss: 4.9320 - val_regression_loss: 1.2035 - lr: 1.2500e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0581 - regression_loss: 1.3077 - val_loss: 4.9353 - val_regression_loss: 1.2081 - lr: 1.2500e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0702 - regression_loss: 1.3051 - val_loss: 4.9411 - val_regression_loss: 1.2118 - lr: 1.2500e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0424 - regression_loss: 1.3030 - val_loss: 4.9365 - val_regression_loss: 1.2063 - lr: 1.2500e-05\n",
      "Epoch 78/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.4309 - regression_loss: 0.8060\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0584 - regression_loss: 1.3006 - val_loss: 4.9337 - val_regression_loss: 1.2033 - lr: 1.2500e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0410 - regression_loss: 1.2974 - val_loss: 4.9319 - val_regression_loss: 1.2018 - lr: 6.2500e-06\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0237 - regression_loss: 1.2981 - val_loss: 4.9305 - val_regression_loss: 1.1962 - lr: 6.2500e-06\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0405 - regression_loss: 1.2960 - val_loss: 4.9298 - val_regression_loss: 1.1996 - lr: 6.2500e-06\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0356 - regression_loss: 1.2948 - val_loss: 4.9331 - val_regression_loss: 1.2051 - lr: 6.2500e-06\n",
      "Epoch 83/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.8522 - regression_loss: 1.2283\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0374 - regression_loss: 1.2945 - val_loss: 4.9343 - val_regression_loss: 1.2076 - lr: 6.2500e-06\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0419 - regression_loss: 1.2915 - val_loss: 4.9334 - val_regression_loss: 1.2050 - lr: 3.1250e-06\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0012 - regression_loss: 1.2913 - val_loss: 4.9304 - val_regression_loss: 1.2006 - lr: 3.1250e-06\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0424 - regression_loss: 1.2905 - val_loss: 4.9301 - val_regression_loss: 1.2010 - lr: 3.1250e-06\n",
      "Epoch 87/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0146 - regression_loss: 1.2895 - val_loss: 4.9289 - val_regression_loss: 1.1994 - lr: 3.1250e-06\n",
      "Epoch 88/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.8554 - regression_loss: 1.2321\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0390 - regression_loss: 1.2889 - val_loss: 4.9296 - val_regression_loss: 1.2006 - lr: 3.1250e-06\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0223 - regression_loss: 1.2881 - val_loss: 4.9301 - val_regression_loss: 1.2008 - lr: 1.5625e-06\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0189 - regression_loss: 1.2879 - val_loss: 4.9306 - val_regression_loss: 1.2019 - lr: 1.5625e-06\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9734 - regression_loss: 1.2882 - val_loss: 4.9293 - val_regression_loss: 1.1997 - lr: 1.5625e-06\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0224 - regression_loss: 1.2876 - val_loss: 4.9296 - val_regression_loss: 1.2011 - lr: 1.5625e-06\n",
      "Epoch 93/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.2439 - regression_loss: 0.6209\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0030 - regression_loss: 1.2870 - val_loss: 4.9310 - val_regression_loss: 1.2019 - lr: 1.5625e-06\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0257 - regression_loss: 1.2866 - val_loss: 4.9309 - val_regression_loss: 1.2019 - lr: 7.8125e-07\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0205 - regression_loss: 1.2868 - val_loss: 4.9300 - val_regression_loss: 1.2006 - lr: 7.8125e-07\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0191 - regression_loss: 1.2862 - val_loss: 4.9305 - val_regression_loss: 1.2012 - lr: 7.8125e-07\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0029 - regression_loss: 1.2863 - val_loss: 4.9302 - val_regression_loss: 1.2006 - lr: 7.8125e-07\n",
      "Epoch 98/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.6765 - regression_loss: 1.0536\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0195 - regression_loss: 1.2860 - val_loss: 4.9302 - val_regression_loss: 1.2009 - lr: 7.8125e-07\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0169 - regression_loss: 1.2856 - val_loss: 4.9302 - val_regression_loss: 1.2011 - lr: 3.9062e-07\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9832 - regression_loss: 1.2855 - val_loss: 4.9301 - val_regression_loss: 1.2011 - lr: 3.9062e-07\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9366 - regression_loss: 1.2856 - val_loss: 4.9300 - val_regression_loss: 1.2007 - lr: 3.9062e-07\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0117 - regression_loss: 1.2855 - val_loss: 4.9301 - val_regression_loss: 1.2009 - lr: 3.9062e-07\n",
      "Epoch 103/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.3348 - regression_loss: 1.7120\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0370 - regression_loss: 1.2854 - val_loss: 4.9300 - val_regression_loss: 1.2008 - lr: 3.9062e-07\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0153 - regression_loss: 1.2852 - val_loss: 4.9301 - val_regression_loss: 1.2009 - lr: 1.9531e-07\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0128 - regression_loss: 1.2852 - val_loss: 4.9301 - val_regression_loss: 1.2010 - lr: 1.9531e-07\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0209 - regression_loss: 1.2851 - val_loss: 4.9300 - val_regression_loss: 1.2009 - lr: 1.9531e-07\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0085 - regression_loss: 1.2851 - val_loss: 4.9299 - val_regression_loss: 1.2008 - lr: 1.9531e-07\n",
      "Epoch 108/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.4544 - regression_loss: 0.8316\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9929 - regression_loss: 1.2851 - val_loss: 4.9300 - val_regression_loss: 1.2009 - lr: 1.9531e-07\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9872 - regression_loss: 1.2850 - val_loss: 4.9300 - val_regression_loss: 1.2009 - lr: 9.7656e-08\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0036 - regression_loss: 1.2850 - val_loss: 4.9299 - val_regression_loss: 1.2008 - lr: 9.7656e-08\n",
      "3/3 [==============================] - 0s 977us/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 123.7763 - regression_loss: 112.3284 - val_loss: 53.8363 - val_regression_loss: 48.4066 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 82.9084 - regression_loss: 76.2800 - val_loss: 46.1763 - val_regression_loss: 40.5196 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 72.2698 - regression_loss: 63.5695 - val_loss: 39.6973 - val_regression_loss: 34.6345 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 62.8857 - regression_loss: 55.0548 - val_loss: 32.7872 - val_regression_loss: 28.6049 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 55.8133 - regression_loss: 47.8699 - val_loss: 27.5155 - val_regression_loss: 23.6063 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 47.6563 - regression_loss: 40.8609 - val_loss: 22.9668 - val_regression_loss: 19.1629 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 40.4960 - regression_loss: 34.5865 - val_loss: 19.2573 - val_regression_loss: 15.5402 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 36.3651 - regression_loss: 29.9511 - val_loss: 16.2624 - val_regression_loss: 12.6046 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.7997 - regression_loss: 25.0767 - val_loss: 13.7950 - val_regression_loss: 10.2901 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.3750 - regression_loss: 21.0622 - val_loss: 11.9182 - val_regression_loss: 8.4715 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.4256 - regression_loss: 17.2021 - val_loss: 10.6707 - val_regression_loss: 7.0177 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.2915 - regression_loss: 13.9196 - val_loss: 9.3730 - val_regression_loss: 5.7258 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.2518 - regression_loss: 11.2491 - val_loss: 8.6343 - val_regression_loss: 4.8731 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6772 - regression_loss: 9.2534 - val_loss: 8.2737 - val_regression_loss: 4.2910 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.7271 - regression_loss: 7.7788 - val_loss: 7.8752 - val_regression_loss: 3.8534 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6744 - regression_loss: 6.7061 - val_loss: 7.7749 - val_regression_loss: 3.5945 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.8998 - regression_loss: 5.9605 - val_loss: 7.7115 - val_regression_loss: 3.4503 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4182 - regression_loss: 5.4026 - val_loss: 7.4956 - val_regression_loss: 3.2541 - lr: 1.0000e-04\n",
      "Epoch 19/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0652 - regression_loss: 5.0905 - val_loss: 7.4147 - val_regression_loss: 3.1544 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7287 - regression_loss: 4.7365 - val_loss: 7.2667 - val_regression_loss: 3.0302 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3998 - regression_loss: 4.4916 - val_loss: 7.2399 - val_regression_loss: 2.9888 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.1799 - regression_loss: 4.2972 - val_loss: 7.0571 - val_regression_loss: 2.8737 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.0454 - regression_loss: 4.2156 - val_loss: 6.9963 - val_regression_loss: 2.8274 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9086 - regression_loss: 4.0399 - val_loss: 6.8789 - val_regression_loss: 2.7527 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6901 - regression_loss: 3.8782 - val_loss: 6.8698 - val_regression_loss: 2.7686 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6253 - regression_loss: 3.7653 - val_loss: 6.7842 - val_regression_loss: 2.7153 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.4115 - regression_loss: 3.6484 - val_loss: 6.7317 - val_regression_loss: 2.6958 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.2677 - regression_loss: 3.5642 - val_loss: 6.6149 - val_regression_loss: 2.6137 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.3092 - regression_loss: 3.5027 - val_loss: 6.6111 - val_regression_loss: 2.6303 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.1629 - regression_loss: 3.3856 - val_loss: 6.5326 - val_regression_loss: 2.5698 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0786 - regression_loss: 3.3203 - val_loss: 6.4443 - val_regression_loss: 2.5201 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9477 - regression_loss: 3.2351 - val_loss: 6.3765 - val_regression_loss: 2.4793 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9046 - regression_loss: 3.1488 - val_loss: 6.3795 - val_regression_loss: 2.5180 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8021 - regression_loss: 3.0941 - val_loss: 6.3268 - val_regression_loss: 2.4944 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6587 - regression_loss: 3.0470 - val_loss: 6.2551 - val_regression_loss: 2.4387 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6787 - regression_loss: 2.9729 - val_loss: 6.1582 - val_regression_loss: 2.3657 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6455 - regression_loss: 2.9207 - val_loss: 6.1290 - val_regression_loss: 2.3484 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5945 - regression_loss: 2.8829 - val_loss: 6.1062 - val_regression_loss: 2.3634 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.4791 - regression_loss: 2.8165 - val_loss: 6.0390 - val_regression_loss: 2.3068 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3596 - regression_loss: 2.7623 - val_loss: 5.9638 - val_regression_loss: 2.2566 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3725 - regression_loss: 2.7010 - val_loss: 5.9732 - val_regression_loss: 2.2966 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.2983 - regression_loss: 2.6630 - val_loss: 5.9964 - val_regression_loss: 2.3283 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.2539 - regression_loss: 2.6460 - val_loss: 5.8880 - val_regression_loss: 2.2466 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2043 - regression_loss: 2.6151 - val_loss: 5.8752 - val_regression_loss: 2.2396 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.2286 - regression_loss: 2.5839 - val_loss: 5.7953 - val_regression_loss: 2.1760 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.0927 - regression_loss: 2.5134 - val_loss: 5.7422 - val_regression_loss: 2.1381 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.0724 - regression_loss: 2.4769 - val_loss: 5.7103 - val_regression_loss: 2.1303 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.0680 - regression_loss: 2.4624 - val_loss: 5.7368 - val_regression_loss: 2.1632 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.9715 - regression_loss: 2.4533 - val_loss: 5.6755 - val_regression_loss: 2.1147 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.9982 - regression_loss: 2.4039 - val_loss: 5.7244 - val_regression_loss: 2.1569 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9794 - regression_loss: 2.3890 - val_loss: 5.6577 - val_regression_loss: 2.1451 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.9215 - regression_loss: 2.3453 - val_loss: 5.6332 - val_regression_loss: 2.1105 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8609 - regression_loss: 2.2872 - val_loss: 5.5263 - val_regression_loss: 2.0171 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7621 - regression_loss: 2.2383 - val_loss: 5.5678 - val_regression_loss: 2.0772 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7465 - regression_loss: 2.2295 - val_loss: 5.4944 - val_regression_loss: 2.0210 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7326 - regression_loss: 2.2198 - val_loss: 5.5176 - val_regression_loss: 2.0546 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6402 - regression_loss: 2.1342 - val_loss: 5.4517 - val_regression_loss: 1.9964 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6199 - regression_loss: 2.1122 - val_loss: 5.4035 - val_regression_loss: 1.9454 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6145 - regression_loss: 2.0977 - val_loss: 5.3971 - val_regression_loss: 1.9527 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.5876 - regression_loss: 2.0549 - val_loss: 5.3732 - val_regression_loss: 1.9375 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.5524 - regression_loss: 2.0392 - val_loss: 5.3646 - val_regression_loss: 1.9508 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.4909 - regression_loss: 2.0301 - val_loss: 5.3686 - val_regression_loss: 1.9456 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.4856 - regression_loss: 2.0194 - val_loss: 5.3267 - val_regression_loss: 1.9246 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.4735 - regression_loss: 1.9990 - val_loss: 5.2820 - val_regression_loss: 1.8600 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.3585 - regression_loss: 1.9699 - val_loss: 5.2788 - val_regression_loss: 1.8929 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.3869 - regression_loss: 1.9261 - val_loss: 5.2520 - val_regression_loss: 1.8707 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.3483 - regression_loss: 1.9184 - val_loss: 5.2686 - val_regression_loss: 1.8978 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.3972 - regression_loss: 1.9020 - val_loss: 5.2164 - val_regression_loss: 1.8407 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.3826 - regression_loss: 1.9003 - val_loss: 5.2312 - val_regression_loss: 1.8665 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.3270 - regression_loss: 1.9212 - val_loss: 5.2335 - val_regression_loss: 1.8735 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.3037 - regression_loss: 1.8417 - val_loss: 5.1919 - val_regression_loss: 1.8398 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.2604 - regression_loss: 1.8416 - val_loss: 5.1878 - val_regression_loss: 1.8285 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.3005 - regression_loss: 1.8520 - val_loss: 5.1231 - val_regression_loss: 1.7758 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.3547 - regression_loss: 1.8746 - val_loss: 5.1507 - val_regression_loss: 1.8108 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.2141 - regression_loss: 1.8198 - val_loss: 5.1819 - val_regression_loss: 1.8310 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2767 - regression_loss: 1.8104 - val_loss: 5.1762 - val_regression_loss: 1.8329 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.3428 - regression_loss: 1.9157 - val_loss: 5.3554 - val_regression_loss: 1.9946 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.3171 - regression_loss: 1.8789 - val_loss: 5.2912 - val_regression_loss: 1.9514 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.3701 - regression_loss: 1.9826 - val_loss: 5.4388 - val_regression_loss: 2.0674 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.3995 - regression_loss: 2.1144\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.3739 - regression_loss: 1.9583 - val_loss: 5.1338 - val_regression_loss: 1.8139 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.1713 - regression_loss: 1.7528 - val_loss: 5.1666 - val_regression_loss: 1.8475 - lr: 5.0000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1594 - regression_loss: 1.7698 - val_loss: 5.0229 - val_regression_loss: 1.7311 - lr: 5.0000e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0970 - regression_loss: 1.7122 - val_loss: 5.0167 - val_regression_loss: 1.7228 - lr: 5.0000e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.1122 - regression_loss: 1.7006 - val_loss: 5.0285 - val_regression_loss: 1.7378 - lr: 5.0000e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0572 - regression_loss: 1.6477 - val_loss: 5.0277 - val_regression_loss: 1.7455 - lr: 5.0000e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0256 - regression_loss: 1.6632 - val_loss: 5.0245 - val_regression_loss: 1.7440 - lr: 5.0000e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.0419 - regression_loss: 1.6528 - val_loss: 5.0187 - val_regression_loss: 1.7281 - lr: 5.0000e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9900 - regression_loss: 1.6739 - val_loss: 4.9828 - val_regression_loss: 1.6927 - lr: 5.0000e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9973 - regression_loss: 1.6315 - val_loss: 5.0315 - val_regression_loss: 1.7576 - lr: 5.0000e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0254 - regression_loss: 1.6346 - val_loss: 5.0044 - val_regression_loss: 1.7351 - lr: 5.0000e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.0025 - regression_loss: 1.6237 - val_loss: 4.9842 - val_regression_loss: 1.7060 - lr: 5.0000e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9978 - regression_loss: 1.6182 - val_loss: 4.9758 - val_regression_loss: 1.6991 - lr: 5.0000e-05\n",
      "Epoch 93/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.9846 - regression_loss: 1.7274\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9925 - regression_loss: 1.6151 - val_loss: 4.9696 - val_regression_loss: 1.6950 - lr: 5.0000e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9640 - regression_loss: 1.6032 - val_loss: 4.9819 - val_regression_loss: 1.7096 - lr: 2.5000e-05\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9478 - regression_loss: 1.6107 - val_loss: 4.9748 - val_regression_loss: 1.7081 - lr: 2.5000e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9856 - regression_loss: 1.5941 - val_loss: 4.9763 - val_regression_loss: 1.7080 - lr: 2.5000e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9841 - regression_loss: 1.5995 - val_loss: 4.9623 - val_regression_loss: 1.6916 - lr: 2.5000e-05\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9163 - regression_loss: 1.5916 - val_loss: 4.9594 - val_regression_loss: 1.6909 - lr: 2.5000e-05\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8896 - regression_loss: 1.5874 - val_loss: 4.9667 - val_regression_loss: 1.7036 - lr: 2.5000e-05\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9086 - regression_loss: 1.5828 - val_loss: 4.9548 - val_regression_loss: 1.6933 - lr: 2.5000e-05\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9170 - regression_loss: 1.5846 - val_loss: 4.9531 - val_regression_loss: 1.6908 - lr: 2.5000e-05\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8828 - regression_loss: 1.5782 - val_loss: 4.9506 - val_regression_loss: 1.6895 - lr: 2.5000e-05\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9358 - regression_loss: 1.5772 - val_loss: 4.9406 - val_regression_loss: 1.6810 - lr: 2.5000e-05\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9344 - regression_loss: 1.5749 - val_loss: 4.9452 - val_regression_loss: 1.6861 - lr: 2.5000e-05\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9698 - regression_loss: 1.5799 - val_loss: 4.9587 - val_regression_loss: 1.7009 - lr: 2.5000e-05\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9197 - regression_loss: 1.5694 - val_loss: 4.9387 - val_regression_loss: 1.6834 - lr: 2.5000e-05\n",
      "Epoch 107/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.2902 - regression_loss: 2.0480\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9457 - regression_loss: 1.5682 - val_loss: 4.9341 - val_regression_loss: 1.6741 - lr: 2.5000e-05\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9375 - regression_loss: 1.5662 - val_loss: 4.9409 - val_regression_loss: 1.6801 - lr: 1.2500e-05\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9408 - regression_loss: 1.5597 - val_loss: 4.9331 - val_regression_loss: 1.6743 - lr: 1.2500e-05\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9103 - regression_loss: 1.5598 - val_loss: 4.9268 - val_regression_loss: 1.6702 - lr: 1.2500e-05\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8930 - regression_loss: 1.5588 - val_loss: 4.9298 - val_regression_loss: 1.6740 - lr: 1.2500e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.6331 - regression_loss: 1.3938\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8857 - regression_loss: 1.5554 - val_loss: 4.9349 - val_regression_loss: 1.6806 - lr: 1.2500e-05\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9248 - regression_loss: 1.5539 - val_loss: 4.9339 - val_regression_loss: 1.6809 - lr: 6.2500e-06\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9052 - regression_loss: 1.5540 - val_loss: 4.9307 - val_regression_loss: 1.6779 - lr: 6.2500e-06\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9112 - regression_loss: 1.5527 - val_loss: 4.9272 - val_regression_loss: 1.6742 - lr: 6.2500e-06\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9182 - regression_loss: 1.5526 - val_loss: 4.9264 - val_regression_loss: 1.6737 - lr: 6.2500e-06\n",
      "Epoch 117/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.0295 - regression_loss: 1.7918\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8927 - regression_loss: 1.5521 - val_loss: 4.9277 - val_regression_loss: 1.6749 - lr: 6.2500e-06\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8938 - regression_loss: 1.5503 - val_loss: 4.9264 - val_regression_loss: 1.6733 - lr: 3.1250e-06\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8889 - regression_loss: 1.5500 - val_loss: 4.9270 - val_regression_loss: 1.6746 - lr: 3.1250e-06\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8849 - regression_loss: 1.5498 - val_loss: 4.9264 - val_regression_loss: 1.6741 - lr: 3.1250e-06\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9019 - regression_loss: 1.5491 - val_loss: 4.9267 - val_regression_loss: 1.6746 - lr: 3.1250e-06\n",
      "Epoch 122/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.0768 - regression_loss: 1.8399\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8875 - regression_loss: 1.5486 - val_loss: 4.9265 - val_regression_loss: 1.6750 - lr: 3.1250e-06\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8817 - regression_loss: 1.5480 - val_loss: 4.9260 - val_regression_loss: 1.6744 - lr: 1.5625e-06\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8894 - regression_loss: 1.5482 - val_loss: 4.9267 - val_regression_loss: 1.6751 - lr: 1.5625e-06\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8721 - regression_loss: 1.5479 - val_loss: 4.9254 - val_regression_loss: 1.6739 - lr: 1.5625e-06\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9012 - regression_loss: 1.5475 - val_loss: 4.9255 - val_regression_loss: 1.6740 - lr: 1.5625e-06\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8672 - regression_loss: 1.5473 - val_loss: 4.9250 - val_regression_loss: 1.6735 - lr: 1.5625e-06\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8917 - regression_loss: 1.5473 - val_loss: 4.9247 - val_regression_loss: 1.6733 - lr: 1.5625e-06\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8777 - regression_loss: 1.5470 - val_loss: 4.9244 - val_regression_loss: 1.6729 - lr: 1.5625e-06\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9146 - regression_loss: 1.5468 - val_loss: 4.9244 - val_regression_loss: 1.6729 - lr: 1.5625e-06\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8764 - regression_loss: 1.5468 - val_loss: 4.9252 - val_regression_loss: 1.6740 - lr: 1.5625e-06\n",
      "Epoch 132/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.0952 - regression_loss: 1.8589\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8802 - regression_loss: 1.5465 - val_loss: 4.9243 - val_regression_loss: 1.6731 - lr: 1.5625e-06\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8711 - regression_loss: 1.5463 - val_loss: 4.9246 - val_regression_loss: 1.6734 - lr: 7.8125e-07\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8457 - regression_loss: 1.5463 - val_loss: 4.9248 - val_regression_loss: 1.6738 - lr: 7.8125e-07\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9082 - regression_loss: 1.5460 - val_loss: 4.9242 - val_regression_loss: 1.6732 - lr: 7.8125e-07\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9086 - regression_loss: 1.5459 - val_loss: 4.9241 - val_regression_loss: 1.6731 - lr: 7.8125e-07\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8991 - regression_loss: 1.5459 - val_loss: 4.9237 - val_regression_loss: 1.6728 - lr: 7.8125e-07\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9057 - regression_loss: 1.5457 - val_loss: 4.9236 - val_regression_loss: 1.6727 - lr: 7.8125e-07\n",
      "Epoch 139/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.1832 - regression_loss: 1.9472\n",
      "Epoch 139: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9040 - regression_loss: 1.5457 - val_loss: 4.9234 - val_regression_loss: 1.6725 - lr: 7.8125e-07\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8855 - regression_loss: 1.5455 - val_loss: 4.9233 - val_regression_loss: 1.6724 - lr: 3.9062e-07\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9198 - regression_loss: 1.5456 - val_loss: 4.9235 - val_regression_loss: 1.6726 - lr: 3.9062e-07\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9145 - regression_loss: 1.5454 - val_loss: 4.9234 - val_regression_loss: 1.6725 - lr: 3.9062e-07\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8913 - regression_loss: 1.5454 - val_loss: 4.9231 - val_regression_loss: 1.6722 - lr: 3.9062e-07\n",
      "Epoch 144/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.9256 - regression_loss: 1.6898\n",
      "Epoch 144: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8756 - regression_loss: 1.5454 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 3.9062e-07\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8592 - regression_loss: 1.5452 - val_loss: 4.9230 - val_regression_loss: 1.6721 - lr: 1.9531e-07\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8533 - regression_loss: 1.5452 - val_loss: 4.9230 - val_regression_loss: 1.6721 - lr: 1.9531e-07\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8550 - regression_loss: 1.5452 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 1.9531e-07\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8961 - regression_loss: 1.5452 - val_loss: 4.9228 - val_regression_loss: 1.6719 - lr: 1.9531e-07\n",
      "Epoch 149/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.0567 - regression_loss: 1.8209\n",
      "Epoch 149: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8761 - regression_loss: 1.5452 - val_loss: 4.9229 - val_regression_loss: 1.6721 - lr: 1.9531e-07\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8929 - regression_loss: 1.5451 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 9.7656e-08\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8786 - regression_loss: 1.5451 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 9.7656e-08\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8777 - regression_loss: 1.5451 - val_loss: 4.9228 - val_regression_loss: 1.6720 - lr: 9.7656e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9076 - regression_loss: 1.5451 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 9.7656e-08\n",
      "Epoch 154/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.4458 - regression_loss: 2.2100\n",
      "Epoch 154: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8977 - regression_loss: 1.5451 - val_loss: 4.9229 - val_regression_loss: 1.6721 - lr: 9.7656e-08\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8822 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6721 - lr: 4.8828e-08\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9128 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 4.8828e-08\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8700 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6721 - lr: 4.8828e-08\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8994 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6721 - lr: 4.8828e-08\n",
      "Epoch 159/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.0646 - regression_loss: 1.8289\n",
      "Epoch 159: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8678 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 4.8828e-08\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8995 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 2.4414e-08\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8704 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 2.4414e-08\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8900 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 2.4414e-08\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8782 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 2.4414e-08\n",
      "Epoch 164/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.1005 - regression_loss: 1.8647\n",
      "Epoch 164: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9129 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 2.4414e-08\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8841 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 1.2207e-08\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8979 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 1.2207e-08\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8876 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 1.2207e-08\n",
      "Epoch 168/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8685 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 1.2207e-08\n",
      "Epoch 169/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.1353 - regression_loss: 1.8995\n",
      "Epoch 169: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8810 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 1.2207e-08\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8940 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 6.1035e-09\n",
      "Epoch 171/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9151 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 6.1035e-09\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8840 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 6.1035e-09\n",
      "Epoch 173/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8910 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 6.1035e-09\n",
      "Epoch 174/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.8151 - regression_loss: 1.5793\n",
      "Epoch 174: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8821 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 6.1035e-09\n",
      "Epoch 175/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8610 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 3.0518e-09\n",
      "Epoch 176/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8912 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 3.0518e-09\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8729 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 3.0518e-09\n",
      "Epoch 178/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8959 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 3.0518e-09\n",
      "Epoch 179/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.6602 - regression_loss: 1.4244\n",
      "Epoch 179: ReduceLROnPlateau reducing learning rate to 1.5258788677030566e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8469 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 3.0518e-09\n",
      "Epoch 180/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8941 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 1.5259e-09\n",
      "Epoch 181/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8940 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 1.5259e-09\n",
      "Epoch 182/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8976 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 1.5259e-09\n",
      "Epoch 183/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9004 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 1.5259e-09\n",
      "Epoch 184/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.7746 - regression_loss: 1.5388\n",
      "Epoch 184: ReduceLROnPlateau reducing learning rate to 7.629394338515283e-10.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8968 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 1.5259e-09\n",
      "Epoch 185/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9064 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 7.6294e-10\n",
      "Epoch 186/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9069 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 7.6294e-10\n",
      "Epoch 187/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9173 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 7.6294e-10\n",
      "Epoch 188/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8642 - regression_loss: 1.5450 - val_loss: 4.9229 - val_regression_loss: 1.6720 - lr: 7.6294e-10\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 24ms/step - loss: 49.0663 - regression_loss: 42.4779 - val_loss: 28.1524 - val_regression_loss: 20.1243 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 34.0582 - regression_loss: 28.5369 - val_loss: 22.0538 - val_regression_loss: 14.9173 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.4935 - regression_loss: 23.3795 - val_loss: 19.8018 - val_regression_loss: 13.2787 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.6246 - regression_loss: 19.8112 - val_loss: 18.1278 - val_regression_loss: 11.8844 - lr: 1.0000e-04\n",
      "Epoch 5/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 21.8824 - regression_loss: 17.2291 - val_loss: 16.8276 - val_regression_loss: 10.6994 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.7689 - regression_loss: 15.3221 - val_loss: 15.6485 - val_regression_loss: 9.8216 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.8578 - regression_loss: 13.7102 - val_loss: 14.9196 - val_regression_loss: 9.2168 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.0170 - regression_loss: 12.7975 - val_loss: 14.3854 - val_regression_loss: 8.7509 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.0202 - regression_loss: 11.8286 - val_loss: 13.8721 - val_regression_loss: 8.4561 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.2935 - regression_loss: 11.2281 - val_loss: 13.7453 - val_regression_loss: 8.2391 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.4792 - regression_loss: 10.7167 - val_loss: 13.4139 - val_regression_loss: 8.0454 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.3146 - regression_loss: 10.3500 - val_loss: 13.3251 - val_regression_loss: 7.9395 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.1638 - regression_loss: 10.1344 - val_loss: 13.1153 - val_regression_loss: 7.8551 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.9074 - regression_loss: 9.8994 - val_loss: 13.1464 - val_regression_loss: 7.8581 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.6798 - regression_loss: 9.7269 - val_loss: 13.0796 - val_regression_loss: 7.7927 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.3318 - regression_loss: 9.5592 - val_loss: 13.2212 - val_regression_loss: 7.8655 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.2838 - regression_loss: 9.4167 - val_loss: 13.0975 - val_regression_loss: 7.7997 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.2422 - regression_loss: 9.3231 - val_loss: 13.0473 - val_regression_loss: 7.8000 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.9902 - regression_loss: 9.2117 - val_loss: 13.0557 - val_regression_loss: 7.8013 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0463 - regression_loss: 9.1772 - val_loss: 13.0147 - val_regression_loss: 7.7764 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.7475 - regression_loss: 9.0582 - val_loss: 13.2729 - val_regression_loss: 7.8974 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.7372 - regression_loss: 8.9936 - val_loss: 12.9613 - val_regression_loss: 7.8226 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.6172 - regression_loss: 8.9768 - val_loss: 13.2873 - val_regression_loss: 7.9470 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.5516 - regression_loss: 8.9110 - val_loss: 12.9705 - val_regression_loss: 7.8054 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.0146 - regression_loss: 9.1488 - val_loss: 13.3648 - val_regression_loss: 8.0093 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.5390 - regression_loss: 8.7601 - val_loss: 13.1451 - val_regression_loss: 7.8844 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5278 - regression_loss: 8.7109 - val_loss: 12.8271 - val_regression_loss: 7.8496 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3866 - regression_loss: 8.7215 - val_loss: 13.1881 - val_regression_loss: 7.9340 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.5176 - regression_loss: 8.6693 - val_loss: 13.0409 - val_regression_loss: 7.9434 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.5947 - regression_loss: 8.6057 - val_loss: 13.0008 - val_regression_loss: 7.8779 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 12.2976 - regression_loss: 8.5346 - val_loss: 13.0004 - val_regression_loss: 7.8650 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.4261 - regression_loss: 8.4869 - val_loss: 13.1952 - val_regression_loss: 7.9620 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.1245 - regression_loss: 8.4227 - val_loss: 12.9793 - val_regression_loss: 7.8857 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.3490 - regression_loss: 8.4218 - val_loss: 13.0101 - val_regression_loss: 7.8815 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2813 - regression_loss: 8.4281 - val_loss: 12.9301 - val_regression_loss: 7.9170 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.1888 - regression_loss: 8.3605 - val_loss: 13.0150 - val_regression_loss: 7.8867 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.1024 - regression_loss: 8.3170 - val_loss: 12.8725 - val_regression_loss: 7.8141 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9635 - regression_loss: 8.2345 - val_loss: 13.0110 - val_regression_loss: 7.9032 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.9913 - regression_loss: 8.2958 - val_loss: 12.9061 - val_regression_loss: 7.9158 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.1755 - regression_loss: 8.2692 - val_loss: 13.1818 - val_regression_loss: 7.9701 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.1768 - regression_loss: 8.2900 - val_loss: 12.9009 - val_regression_loss: 7.9789 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0016 - regression_loss: 8.2221 - val_loss: 13.1757 - val_regression_loss: 7.9942 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.2744 - regression_loss: 8.0723\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.0598 - regression_loss: 8.2455 - val_loss: 12.8207 - val_regression_loss: 7.8507 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8654 - regression_loss: 8.0777 - val_loss: 12.9099 - val_regression_loss: 7.8798 - lr: 5.0000e-05\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7643 - regression_loss: 8.0635 - val_loss: 12.9454 - val_regression_loss: 7.9064 - lr: 5.0000e-05\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.9454 - regression_loss: 8.0646 - val_loss: 12.9090 - val_regression_loss: 7.8895 - lr: 5.0000e-05\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.7161 - regression_loss: 8.0306 - val_loss: 13.0054 - val_regression_loss: 7.9361 - lr: 5.0000e-05\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.7814 - regression_loss: 8.0478 - val_loss: 12.8989 - val_regression_loss: 7.9013 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4720 - regression_loss: 8.0003 - val_loss: 12.9638 - val_regression_loss: 7.9112 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.6661 - regression_loss: 8.0058 - val_loss: 12.8704 - val_regression_loss: 7.8914 - lr: 5.0000e-05\n",
      "Epoch 51/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 11.7407 - regression_loss: 7.9820 - val_loss: 12.8482 - val_regression_loss: 7.8951 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.6437 - regression_loss: 7.9854 - val_loss: 12.8744 - val_regression_loss: 7.8893 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.8092 - regression_loss: 7.9665 - val_loss: 13.0119 - val_regression_loss: 7.9354 - lr: 5.0000e-05\n",
      "Epoch 54/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.1654 - regression_loss: 8.9703\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.6188 - regression_loss: 7.9813 - val_loss: 12.8631 - val_regression_loss: 7.9160 - lr: 5.0000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7187 - regression_loss: 7.9627 - val_loss: 12.8947 - val_regression_loss: 7.8951 - lr: 2.5000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.7643 - regression_loss: 7.9616 - val_loss: 12.9582 - val_regression_loss: 7.9097 - lr: 2.5000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.5629 - regression_loss: 7.9388 - val_loss: 12.8497 - val_regression_loss: 7.9023 - lr: 2.5000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.6899 - regression_loss: 7.9308 - val_loss: 12.8425 - val_regression_loss: 7.8890 - lr: 2.5000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3866 - regression_loss: 7.9048 - val_loss: 12.8867 - val_regression_loss: 7.8910 - lr: 2.5000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6046 - regression_loss: 7.9097 - val_loss: 12.9231 - val_regression_loss: 7.9060 - lr: 2.5000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.7581 - regression_loss: 7.8970 - val_loss: 12.8447 - val_regression_loss: 7.8863 - lr: 2.5000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3729 - regression_loss: 7.8913 - val_loss: 12.8647 - val_regression_loss: 7.9038 - lr: 2.5000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.7310 - regression_loss: 7.8906 - val_loss: 12.8658 - val_regression_loss: 7.8995 - lr: 2.5000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7095 - regression_loss: 7.8801 - val_loss: 12.9116 - val_regression_loss: 7.9037 - lr: 2.5000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.7823 - regression_loss: 7.8759 - val_loss: 12.8977 - val_regression_loss: 7.9044 - lr: 2.5000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6336 - regression_loss: 7.8768 - val_loss: 12.8059 - val_regression_loss: 7.8974 - lr: 2.5000e-05\n",
      "Epoch 67/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.2271 - regression_loss: 7.0363\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.4922 - regression_loss: 7.8882 - val_loss: 12.8492 - val_regression_loss: 7.8927 - lr: 2.5000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5413 - regression_loss: 7.8596 - val_loss: 12.8919 - val_regression_loss: 7.9051 - lr: 1.2500e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.5723 - regression_loss: 7.8497 - val_loss: 12.8726 - val_regression_loss: 7.9037 - lr: 1.2500e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5568 - regression_loss: 7.8480 - val_loss: 12.8598 - val_regression_loss: 7.9126 - lr: 1.2500e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5994 - regression_loss: 7.8490 - val_loss: 12.8712 - val_regression_loss: 7.9166 - lr: 1.2500e-05\n",
      "Epoch 72/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.0077 - regression_loss: 6.8179\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.5611 - regression_loss: 7.8416 - val_loss: 12.8631 - val_regression_loss: 7.9059 - lr: 1.2500e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.6518 - regression_loss: 7.8338 - val_loss: 12.8641 - val_regression_loss: 7.9056 - lr: 6.2500e-06\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6143 - regression_loss: 7.8286 - val_loss: 12.8772 - val_regression_loss: 7.9076 - lr: 6.2500e-06\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4112 - regression_loss: 7.8309 - val_loss: 12.8891 - val_regression_loss: 7.9119 - lr: 6.2500e-06\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.4924 - regression_loss: 7.8332 - val_loss: 12.8941 - val_regression_loss: 7.9115 - lr: 6.2500e-06\n",
      "Epoch 77/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.7615 - regression_loss: 8.5722\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.5836 - regression_loss: 7.8279 - val_loss: 12.8783 - val_regression_loss: 7.9068 - lr: 6.2500e-06\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4991 - regression_loss: 7.8239 - val_loss: 12.8704 - val_regression_loss: 7.9063 - lr: 3.1250e-06\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4360 - regression_loss: 7.8234 - val_loss: 12.8651 - val_regression_loss: 7.9063 - lr: 3.1250e-06\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6445 - regression_loss: 7.8258 - val_loss: 12.8551 - val_regression_loss: 7.9050 - lr: 3.1250e-06\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4322 - regression_loss: 7.8244 - val_loss: 12.8673 - val_regression_loss: 7.9066 - lr: 3.1250e-06\n",
      "Epoch 82/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.2082 - regression_loss: 10.0192\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5068 - regression_loss: 7.8228 - val_loss: 12.8688 - val_regression_loss: 7.9083 - lr: 3.1250e-06\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5984 - regression_loss: 7.8199 - val_loss: 12.8718 - val_regression_loss: 7.9087 - lr: 1.5625e-06\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4444 - regression_loss: 7.8202 - val_loss: 12.8687 - val_regression_loss: 7.9081 - lr: 1.5625e-06\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4903 - regression_loss: 7.8197 - val_loss: 12.8678 - val_regression_loss: 7.9082 - lr: 1.5625e-06\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4575 - regression_loss: 7.8193 - val_loss: 12.8688 - val_regression_loss: 7.9073 - lr: 1.5625e-06\n",
      "Epoch 87/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.8216 - regression_loss: 8.6327\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.4191 - regression_loss: 7.8189 - val_loss: 12.8680 - val_regression_loss: 7.9077 - lr: 1.5625e-06\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.6220 - regression_loss: 7.8193 - val_loss: 12.8722 - val_regression_loss: 7.9086 - lr: 7.8125e-07\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.4719 - regression_loss: 7.8174 - val_loss: 12.8718 - val_regression_loss: 7.9089 - lr: 7.8125e-07\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.6439 - regression_loss: 7.8173 - val_loss: 12.8717 - val_regression_loss: 7.9086 - lr: 7.8125e-07\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7182 - regression_loss: 7.8169 - val_loss: 12.8716 - val_regression_loss: 7.9091 - lr: 7.8125e-07\n",
      "Epoch 92/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.4658 - regression_loss: 7.2769\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 11.4073 - regression_loss: 7.8173 - val_loss: 12.8695 - val_regression_loss: 7.9085 - lr: 7.8125e-07\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4691 - regression_loss: 7.8168 - val_loss: 12.8702 - val_regression_loss: 7.9088 - lr: 3.9062e-07\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.5717 - regression_loss: 7.8165 - val_loss: 12.8708 - val_regression_loss: 7.9088 - lr: 3.9062e-07\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.5093 - regression_loss: 7.8165 - val_loss: 12.8693 - val_regression_loss: 7.9086 - lr: 3.9062e-07\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.6353 - regression_loss: 7.8162 - val_loss: 12.8696 - val_regression_loss: 7.9087 - lr: 3.9062e-07\n",
      "Epoch 97/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.4243 - regression_loss: 9.2355\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.3991 - regression_loss: 7.8162 - val_loss: 12.8698 - val_regression_loss: 7.9088 - lr: 3.9062e-07\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.5999 - regression_loss: 7.8160 - val_loss: 12.8697 - val_regression_loss: 7.9086 - lr: 1.9531e-07\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5220 - regression_loss: 7.8160 - val_loss: 12.8691 - val_regression_loss: 7.9085 - lr: 1.9531e-07\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5545 - regression_loss: 7.8159 - val_loss: 12.8694 - val_regression_loss: 7.9086 - lr: 1.9531e-07\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.5140 - regression_loss: 7.8159 - val_loss: 12.8698 - val_regression_loss: 7.9087 - lr: 1.9531e-07\n",
      "Epoch 102/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.9312 - regression_loss: 9.7424\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.5625 - regression_loss: 7.8158 - val_loss: 12.8693 - val_regression_loss: 7.9085 - lr: 1.9531e-07\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.6197 - regression_loss: 7.8157 - val_loss: 12.8693 - val_regression_loss: 7.9085 - lr: 9.7656e-08\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.4927 - regression_loss: 7.8157 - val_loss: 12.8694 - val_regression_loss: 7.9085 - lr: 9.7656e-08\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.6101 - regression_loss: 7.8157 - val_loss: 12.8692 - val_regression_loss: 7.9085 - lr: 9.7656e-08\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.2783 - regression_loss: 7.8157 - val_loss: 12.8694 - val_regression_loss: 7.9086 - lr: 9.7656e-08\n",
      "3/3 [==============================] - 0s 903us/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 27ms/step - loss: 44.6350 - regression_loss: 39.0077 - val_loss: 27.1250 - val_regression_loss: 24.2639 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.0521 - regression_loss: 23.9875 - val_loss: 20.5357 - val_regression_loss: 18.7319 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.9367 - regression_loss: 17.6551 - val_loss: 18.0563 - val_regression_loss: 15.8697 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.1069 - regression_loss: 14.5913 - val_loss: 17.1143 - val_regression_loss: 14.5870 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.7287 - regression_loss: 12.6266 - val_loss: 15.7241 - val_regression_loss: 13.0302 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.8234 - regression_loss: 10.7556 - val_loss: 14.0633 - val_regression_loss: 11.5051 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.1449 - regression_loss: 9.3581 - val_loss: 12.5522 - val_regression_loss: 10.1019 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.0281 - regression_loss: 8.2289 - val_loss: 11.1457 - val_regression_loss: 8.4894 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.0303 - regression_loss: 7.4065 - val_loss: 10.2337 - val_regression_loss: 7.5838 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.4946 - regression_loss: 6.7141 - val_loss: 9.7397 - val_regression_loss: 7.0668 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.8454 - regression_loss: 6.2255 - val_loss: 9.4274 - val_regression_loss: 6.7705 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5708 - regression_loss: 5.9401 - val_loss: 9.1193 - val_regression_loss: 6.2305 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.0741 - regression_loss: 5.5491 - val_loss: 8.8969 - val_regression_loss: 6.0155 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.9456 - regression_loss: 5.3403 - val_loss: 8.7986 - val_regression_loss: 5.7862 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.7202 - regression_loss: 5.1989 - val_loss: 8.5976 - val_regression_loss: 5.5352 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.7253 - regression_loss: 5.1193 - val_loss: 8.6219 - val_regression_loss: 5.4681 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.4942 - regression_loss: 4.9249 - val_loss: 8.3396 - val_regression_loss: 5.1433 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3342 - regression_loss: 4.8479 - val_loss: 8.2783 - val_regression_loss: 4.9431 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.1621 - regression_loss: 4.7311 - val_loss: 8.1109 - val_regression_loss: 4.8370 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.1352 - regression_loss: 4.6358 - val_loss: 8.1019 - val_regression_loss: 4.6840 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.0601 - regression_loss: 4.5587 - val_loss: 7.9287 - val_regression_loss: 4.5095 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.0101 - regression_loss: 4.4884 - val_loss: 7.9046 - val_regression_loss: 4.3628 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.9059 - regression_loss: 4.4520 - val_loss: 7.7889 - val_regression_loss: 4.2431 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8024 - regression_loss: 4.3624 - val_loss: 7.6962 - val_regression_loss: 4.1754 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8828 - regression_loss: 4.3280 - val_loss: 7.7297 - val_regression_loss: 4.0783 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6902 - regression_loss: 4.3333 - val_loss: 7.4964 - val_regression_loss: 3.9473 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6719 - regression_loss: 4.2832 - val_loss: 7.7559 - val_regression_loss: 3.9823 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6050 - regression_loss: 4.2359 - val_loss: 7.4493 - val_regression_loss: 3.8539 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6992 - regression_loss: 4.2389 - val_loss: 7.4353 - val_regression_loss: 3.7451 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.5031 - regression_loss: 4.1163 - val_loss: 7.4015 - val_regression_loss: 3.6998 - lr: 1.0000e-04\n",
      "Epoch 31/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 7.4805 - regression_loss: 4.0949 - val_loss: 7.3452 - val_regression_loss: 3.6299 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.5048 - regression_loss: 4.1130 - val_loss: 7.2862 - val_regression_loss: 3.5878 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.4139 - regression_loss: 4.0486 - val_loss: 7.4818 - val_regression_loss: 3.6806 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.4152 - regression_loss: 4.0883 - val_loss: 7.2458 - val_regression_loss: 3.5530 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.3371 - regression_loss: 3.9588 - val_loss: 7.3194 - val_regression_loss: 3.5142 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.4275 - regression_loss: 4.0046 - val_loss: 7.1327 - val_regression_loss: 3.4419 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.4107 - regression_loss: 4.0632 - val_loss: 7.6235 - val_regression_loss: 3.6524 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.5910 - regression_loss: 4.1757 - val_loss: 7.2942 - val_regression_loss: 3.6290 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.5804 - regression_loss: 4.1477 - val_loss: 7.6693 - val_regression_loss: 3.6922 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.9537 - regression_loss: 3.8144\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.4630 - regression_loss: 4.0752 - val_loss: 7.1666 - val_regression_loss: 3.5126 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.4814 - regression_loss: 4.1037 - val_loss: 7.5241 - val_regression_loss: 3.5965 - lr: 5.0000e-05\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.3627 - regression_loss: 3.9794 - val_loss: 7.1289 - val_regression_loss: 3.3757 - lr: 5.0000e-05\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.3111 - regression_loss: 3.8786 - val_loss: 7.2149 - val_regression_loss: 3.3824 - lr: 5.0000e-05\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1970 - regression_loss: 3.8360 - val_loss: 7.1467 - val_regression_loss: 3.3464 - lr: 5.0000e-05\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1970 - regression_loss: 3.8147 - val_loss: 7.1064 - val_regression_loss: 3.3163 - lr: 5.0000e-05\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.2402 - regression_loss: 3.8165 - val_loss: 7.1343 - val_regression_loss: 3.3100 - lr: 5.0000e-05\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1205 - regression_loss: 3.8143 - val_loss: 7.0746 - val_regression_loss: 3.2918 - lr: 5.0000e-05\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.2263 - regression_loss: 3.7832 - val_loss: 7.1034 - val_regression_loss: 3.3007 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1493 - regression_loss: 3.7740 - val_loss: 7.1150 - val_regression_loss: 3.2950 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1899 - regression_loss: 3.7682 - val_loss: 7.0558 - val_regression_loss: 3.2604 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1980 - regression_loss: 3.7749 - val_loss: 7.0986 - val_regression_loss: 3.2833 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.3143 - regression_loss: 5.1849\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1912 - regression_loss: 3.7610 - val_loss: 7.0748 - val_regression_loss: 3.2665 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1121 - regression_loss: 3.7493 - val_loss: 7.0141 - val_regression_loss: 3.2395 - lr: 2.5000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1097 - regression_loss: 3.7519 - val_loss: 7.0677 - val_regression_loss: 3.2559 - lr: 2.5000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0731 - regression_loss: 3.7473 - val_loss: 7.0352 - val_regression_loss: 3.2389 - lr: 2.5000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0857 - regression_loss: 3.7272 - val_loss: 7.0812 - val_regression_loss: 3.2624 - lr: 2.5000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1126 - regression_loss: 3.7335 - val_loss: 7.0538 - val_regression_loss: 3.2441 - lr: 2.5000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0998 - regression_loss: 3.7267 - val_loss: 7.0040 - val_regression_loss: 3.2190 - lr: 2.5000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1156 - regression_loss: 3.7235 - val_loss: 7.0164 - val_regression_loss: 3.2175 - lr: 2.5000e-05\n",
      "Epoch 60/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.8696 - regression_loss: 4.7439\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1437 - regression_loss: 3.7359 - val_loss: 7.0778 - val_regression_loss: 3.2544 - lr: 2.5000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0984 - regression_loss: 3.7139 - val_loss: 7.0408 - val_regression_loss: 3.2315 - lr: 1.2500e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1088 - regression_loss: 3.7058 - val_loss: 7.0289 - val_regression_loss: 3.2248 - lr: 1.2500e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0838 - regression_loss: 3.7056 - val_loss: 7.0199 - val_regression_loss: 3.2174 - lr: 1.2500e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0754 - regression_loss: 3.7047 - val_loss: 7.0245 - val_regression_loss: 3.2177 - lr: 1.2500e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0467 - regression_loss: 3.7037 - val_loss: 7.0445 - val_regression_loss: 3.2281 - lr: 1.2500e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1241 - regression_loss: 3.7019 - val_loss: 7.0269 - val_regression_loss: 3.2223 - lr: 1.2500e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1110 - regression_loss: 3.7059 - val_loss: 7.0186 - val_regression_loss: 3.2156 - lr: 1.2500e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0745 - regression_loss: 3.6961 - val_loss: 7.0243 - val_regression_loss: 3.2166 - lr: 1.2500e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0116 - regression_loss: 3.7013 - val_loss: 7.0316 - val_regression_loss: 3.2196 - lr: 1.2500e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0918 - regression_loss: 3.6991 - val_loss: 7.0005 - val_regression_loss: 3.2060 - lr: 1.2500e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.9967 - regression_loss: 3.6944 - val_loss: 7.0185 - val_regression_loss: 3.2091 - lr: 1.2500e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0940 - regression_loss: 3.7032 - val_loss: 7.0442 - val_regression_loss: 3.2233 - lr: 1.2500e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0728 - regression_loss: 3.6972 - val_loss: 7.0027 - val_regression_loss: 3.2005 - lr: 1.2500e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0958 - regression_loss: 3.6918 - val_loss: 6.9875 - val_regression_loss: 3.1952 - lr: 1.2500e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0617 - regression_loss: 3.6876 - val_loss: 7.0187 - val_regression_loss: 3.2093 - lr: 1.2500e-05\n",
      "Epoch 76/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 [====>.........................] - ETA: 0s - loss: 7.2616 - regression_loss: 4.1395\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0381 - regression_loss: 3.6864 - val_loss: 7.0359 - val_regression_loss: 3.2197 - lr: 1.2500e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1115 - regression_loss: 3.6826 - val_loss: 7.0211 - val_regression_loss: 3.2112 - lr: 6.2500e-06\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0545 - regression_loss: 3.6783 - val_loss: 7.0113 - val_regression_loss: 3.2053 - lr: 6.2500e-06\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.9985 - regression_loss: 3.6799 - val_loss: 6.9968 - val_regression_loss: 3.1981 - lr: 6.2500e-06\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.9471 - regression_loss: 3.6787 - val_loss: 7.0071 - val_regression_loss: 3.2012 - lr: 6.2500e-06\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0663 - regression_loss: 3.6781 - val_loss: 7.0189 - val_regression_loss: 3.2078 - lr: 6.2500e-06\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0787 - regression_loss: 3.6771 - val_loss: 7.0184 - val_regression_loss: 3.2069 - lr: 6.2500e-06\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0164 - regression_loss: 3.6772 - val_loss: 6.9948 - val_regression_loss: 3.1944 - lr: 6.2500e-06\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0553 - regression_loss: 3.6769 - val_loss: 6.9874 - val_regression_loss: 3.1906 - lr: 6.2500e-06\n",
      "Epoch 85/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.6292 - regression_loss: 4.5082\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0477 - regression_loss: 3.6766 - val_loss: 7.0086 - val_regression_loss: 3.2006 - lr: 6.2500e-06\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0472 - regression_loss: 3.6722 - val_loss: 7.0026 - val_regression_loss: 3.1967 - lr: 3.1250e-06\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1119 - regression_loss: 3.6712 - val_loss: 7.0080 - val_regression_loss: 3.1993 - lr: 3.1250e-06\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0673 - regression_loss: 3.6708 - val_loss: 7.0080 - val_regression_loss: 3.1992 - lr: 3.1250e-06\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0583 - regression_loss: 3.6702 - val_loss: 7.0081 - val_regression_loss: 3.1989 - lr: 3.1250e-06\n",
      "Epoch 90/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.6402 - regression_loss: 3.5195\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0575 - regression_loss: 3.6697 - val_loss: 7.0058 - val_regression_loss: 3.1977 - lr: 3.1250e-06\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0767 - regression_loss: 3.6688 - val_loss: 7.0017 - val_regression_loss: 3.1957 - lr: 1.5625e-06\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0520 - regression_loss: 3.6680 - val_loss: 7.0038 - val_regression_loss: 3.1967 - lr: 1.5625e-06\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0378 - regression_loss: 3.6678 - val_loss: 7.0032 - val_regression_loss: 3.1962 - lr: 1.5625e-06\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0412 - regression_loss: 3.6682 - val_loss: 7.0055 - val_regression_loss: 3.1973 - lr: 1.5625e-06\n",
      "Epoch 95/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.8176 - regression_loss: 3.6971\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0580 - regression_loss: 3.6674 - val_loss: 7.0034 - val_regression_loss: 3.1961 - lr: 1.5625e-06\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0442 - regression_loss: 3.6671 - val_loss: 7.0045 - val_regression_loss: 3.1966 - lr: 7.8125e-07\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1161 - regression_loss: 3.6666 - val_loss: 7.0040 - val_regression_loss: 3.1962 - lr: 7.8125e-07\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0746 - regression_loss: 3.6665 - val_loss: 7.0043 - val_regression_loss: 3.1963 - lr: 7.8125e-07\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0545 - regression_loss: 3.6667 - val_loss: 7.0043 - val_regression_loss: 3.1961 - lr: 7.8125e-07\n",
      "Epoch 100/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.8977 - regression_loss: 4.7772\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1020 - regression_loss: 3.6664 - val_loss: 7.0042 - val_regression_loss: 3.1961 - lr: 7.8125e-07\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0203 - regression_loss: 3.6663 - val_loss: 7.0030 - val_regression_loss: 3.1955 - lr: 3.9062e-07\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0868 - regression_loss: 3.6660 - val_loss: 7.0031 - val_regression_loss: 3.1956 - lr: 3.9062e-07\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0708 - regression_loss: 3.6660 - val_loss: 7.0031 - val_regression_loss: 3.1955 - lr: 3.9062e-07\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.9953 - regression_loss: 3.6659 - val_loss: 7.0020 - val_regression_loss: 3.1949 - lr: 3.9062e-07\n",
      "Epoch 105/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.6505 - regression_loss: 3.5301\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1140 - regression_loss: 3.6658 - val_loss: 7.0019 - val_regression_loss: 3.1948 - lr: 3.9062e-07\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.9507 - regression_loss: 3.6658 - val_loss: 7.0015 - val_regression_loss: 3.1946 - lr: 1.9531e-07\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0515 - regression_loss: 3.6659 - val_loss: 7.0022 - val_regression_loss: 3.1950 - lr: 1.9531e-07\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0410 - regression_loss: 3.6657 - val_loss: 7.0019 - val_regression_loss: 3.1948 - lr: 1.9531e-07\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1157 - regression_loss: 3.6657 - val_loss: 7.0017 - val_regression_loss: 3.1947 - lr: 1.9531e-07\n",
      "Epoch 110/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.0764 - regression_loss: 3.9561\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0415 - regression_loss: 3.6656 - val_loss: 7.0020 - val_regression_loss: 3.1948 - lr: 1.9531e-07\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0544 - regression_loss: 3.6656 - val_loss: 7.0018 - val_regression_loss: 3.1947 - lr: 9.7656e-08\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.9853 - regression_loss: 3.6656 - val_loss: 7.0020 - val_regression_loss: 3.1948 - lr: 9.7656e-08\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0582 - regression_loss: 3.6655 - val_loss: 7.0020 - val_regression_loss: 3.1948 - lr: 9.7656e-08\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0402 - regression_loss: 3.6655 - val_loss: 7.0018 - val_regression_loss: 3.1947 - lr: 9.7656e-08\n",
      "Epoch 115/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.0260 - regression_loss: 3.9057\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.9899 - regression_loss: 3.6655 - val_loss: 7.0019 - val_regression_loss: 3.1947 - lr: 9.7656e-08\n",
      "Epoch 116/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0504 - regression_loss: 3.6654 - val_loss: 7.0020 - val_regression_loss: 3.1947 - lr: 4.8828e-08\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0638 - regression_loss: 3.6654 - val_loss: 7.0019 - val_regression_loss: 3.1947 - lr: 4.8828e-08\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0665 - regression_loss: 3.6654 - val_loss: 7.0018 - val_regression_loss: 3.1947 - lr: 4.8828e-08\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0669 - regression_loss: 3.6654 - val_loss: 7.0018 - val_regression_loss: 3.1947 - lr: 4.8828e-08\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.9418 - regression_loss: 3.6654 - val_loss: 7.0018 - val_regression_loss: 3.1946 - lr: 4.8828e-08\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0074 - regression_loss: 3.6654 - val_loss: 7.0018 - val_regression_loss: 3.1946 - lr: 4.8828e-08\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1117 - regression_loss: 3.6654 - val_loss: 7.0018 - val_regression_loss: 3.1946 - lr: 4.8828e-08\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0546 - regression_loss: 3.6654 - val_loss: 7.0018 - val_regression_loss: 3.1946 - lr: 4.8828e-08\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0992 - regression_loss: 3.6654 - val_loss: 7.0018 - val_regression_loss: 3.1946 - lr: 4.8828e-08\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 77.7344 - regression_loss: 68.9792 - val_loss: 72.5072 - val_regression_loss: 57.8467 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 52.1906 - regression_loss: 44.6380 - val_loss: 59.3310 - val_regression_loss: 47.4790 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 41.5530 - regression_loss: 35.0301 - val_loss: 50.3767 - val_regression_loss: 40.4768 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 34.0080 - regression_loss: 28.3769 - val_loss: 40.7496 - val_regression_loss: 32.6856 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30.0936 - regression_loss: 24.8434 - val_loss: 35.0461 - val_regression_loss: 27.9264 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.2958 - regression_loss: 21.9265 - val_loss: 32.2428 - val_regression_loss: 25.4424 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.3281 - regression_loss: 18.8614 - val_loss: 25.8111 - val_regression_loss: 19.9570 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.6721 - regression_loss: 16.2988 - val_loss: 24.4296 - val_regression_loss: 18.5765 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.3055 - regression_loss: 13.7928 - val_loss: 20.8975 - val_regression_loss: 15.4276 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.5406 - regression_loss: 11.5375 - val_loss: 18.7449 - val_regression_loss: 13.4461 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7747 - regression_loss: 9.7670 - val_loss: 16.4752 - val_regression_loss: 11.4511 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0772 - regression_loss: 8.3123 - val_loss: 15.2556 - val_regression_loss: 10.3176 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.7754 - regression_loss: 7.1266 - val_loss: 13.4160 - val_regression_loss: 8.6896 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.8209 - regression_loss: 6.1007 - val_loss: 12.2706 - val_regression_loss: 7.7108 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.6272 - regression_loss: 5.1264 - val_loss: 11.1632 - val_regression_loss: 6.7585 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.0598 - regression_loss: 4.5142 - val_loss: 10.3390 - val_regression_loss: 6.0124 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.2173 - regression_loss: 3.8253 - val_loss: 9.4789 - val_regression_loss: 5.3135 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.7330 - regression_loss: 3.3281 - val_loss: 8.9026 - val_regression_loss: 4.8443 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.1828 - regression_loss: 2.9124 - val_loss: 8.2991 - val_regression_loss: 4.3515 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.8016 - regression_loss: 2.5393 - val_loss: 7.7569 - val_regression_loss: 3.9050 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.3922 - regression_loss: 2.2507 - val_loss: 7.3516 - val_regression_loss: 3.5809 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2304 - regression_loss: 2.0240 - val_loss: 6.9755 - val_regression_loss: 3.2547 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9744 - regression_loss: 1.8274 - val_loss: 6.7775 - val_regression_loss: 3.1407 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9589 - regression_loss: 1.7169 - val_loss: 6.3908 - val_regression_loss: 2.8324 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7567 - regression_loss: 1.5436 - val_loss: 6.1831 - val_regression_loss: 2.6770 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6392 - regression_loss: 1.4469 - val_loss: 5.9318 - val_regression_loss: 2.4765 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5583 - regression_loss: 1.3531 - val_loss: 5.8150 - val_regression_loss: 2.4008 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.4689 - regression_loss: 1.2780 - val_loss: 5.6474 - val_regression_loss: 2.2450 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.4459 - regression_loss: 1.2902 - val_loss: 5.4908 - val_regression_loss: 2.1275 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3909 - regression_loss: 1.2076 - val_loss: 5.4553 - val_regression_loss: 2.0697 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3356 - regression_loss: 1.1531 - val_loss: 5.2647 - val_regression_loss: 1.9039 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2372 - regression_loss: 1.0904 - val_loss: 5.3619 - val_regression_loss: 2.0194 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0400 - regression_loss: 1.0552 - val_loss: 5.1510 - val_regression_loss: 1.8208 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.1815 - regression_loss: 1.0094 - val_loss: 5.1994 - val_regression_loss: 1.8711 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.1777 - regression_loss: 1.0134 - val_loss: 5.0287 - val_regression_loss: 1.6687 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0448 - regression_loss: 0.9293 - val_loss: 5.0485 - val_regression_loss: 1.7199 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0357 - regression_loss: 0.9052 - val_loss: 4.9223 - val_regression_loss: 1.5908 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0559 - regression_loss: 0.9015 - val_loss: 5.1097 - val_regression_loss: 1.7873 - lr: 1.0000e-04\n",
      "Epoch 39/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0864 - regression_loss: 0.9487 - val_loss: 4.9645 - val_regression_loss: 1.5882 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.1709 - regression_loss: 1.0186 - val_loss: 5.0676 - val_regression_loss: 1.7479 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9270 - regression_loss: 0.8187 - val_loss: 4.8331 - val_regression_loss: 1.4754 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9320 - regression_loss: 0.8604 - val_loss: 4.8418 - val_regression_loss: 1.5326 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8197 - regression_loss: 0.7490 - val_loss: 4.7497 - val_regression_loss: 1.4028 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8870 - regression_loss: 0.7681 - val_loss: 4.7693 - val_regression_loss: 1.4425 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8726 - regression_loss: 0.7561 - val_loss: 4.6713 - val_regression_loss: 1.3220 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8038 - regression_loss: 0.6967 - val_loss: 4.7671 - val_regression_loss: 1.4543 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7987 - regression_loss: 0.7138 - val_loss: 4.6533 - val_regression_loss: 1.3010 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7886 - regression_loss: 0.6961 - val_loss: 4.6933 - val_regression_loss: 1.3773 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7806 - regression_loss: 0.6944 - val_loss: 4.5661 - val_regression_loss: 1.2212 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7194 - regression_loss: 0.6475 - val_loss: 4.6186 - val_regression_loss: 1.3081 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7028 - regression_loss: 0.6398 - val_loss: 4.5310 - val_regression_loss: 1.2000 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7169 - regression_loss: 0.6289 - val_loss: 4.5752 - val_regression_loss: 1.2698 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6633 - regression_loss: 0.6080 - val_loss: 4.5030 - val_regression_loss: 1.1678 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6663 - regression_loss: 0.6072 - val_loss: 4.5521 - val_regression_loss: 1.2585 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6742 - regression_loss: 0.6094 - val_loss: 4.4926 - val_regression_loss: 1.1586 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7132 - regression_loss: 0.6597 - val_loss: 4.6429 - val_regression_loss: 1.3304 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7088 - regression_loss: 0.6403 - val_loss: 4.5795 - val_regression_loss: 1.1990 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.9743 - regression_loss: 0.9593\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7134 - regression_loss: 0.6590 - val_loss: 4.4989 - val_regression_loss: 1.1866 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6440 - regression_loss: 0.5756 - val_loss: 4.4390 - val_regression_loss: 1.0956 - lr: 5.0000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6204 - regression_loss: 0.5566 - val_loss: 4.3960 - val_regression_loss: 1.1072 - lr: 5.0000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5279 - regression_loss: 0.5352 - val_loss: 4.3557 - val_regression_loss: 1.0588 - lr: 5.0000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5691 - regression_loss: 0.5268 - val_loss: 4.3693 - val_regression_loss: 1.0583 - lr: 5.0000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5540 - regression_loss: 0.5169 - val_loss: 4.3669 - val_regression_loss: 1.0603 - lr: 5.0000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5585 - regression_loss: 0.5094 - val_loss: 4.3463 - val_regression_loss: 1.0406 - lr: 5.0000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5524 - regression_loss: 0.5062 - val_loss: 4.3591 - val_regression_loss: 1.0652 - lr: 5.0000e-05\n",
      "Epoch 66/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.4544 - regression_loss: 0.4513\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5582 - regression_loss: 0.5060 - val_loss: 4.3473 - val_regression_loss: 1.0473 - lr: 5.0000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4806 - regression_loss: 0.4936 - val_loss: 4.3251 - val_regression_loss: 1.0355 - lr: 2.5000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5202 - regression_loss: 0.4945 - val_loss: 4.3325 - val_regression_loss: 1.0359 - lr: 2.5000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5327 - regression_loss: 0.4901 - val_loss: 4.3279 - val_regression_loss: 1.0381 - lr: 2.5000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5039 - regression_loss: 0.4908 - val_loss: 4.3291 - val_regression_loss: 1.0272 - lr: 2.5000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5230 - regression_loss: 0.4854 - val_loss: 4.3177 - val_regression_loss: 1.0226 - lr: 2.5000e-05\n",
      "Epoch 72/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.7295 - regression_loss: 0.7313\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5255 - regression_loss: 0.4846 - val_loss: 4.3058 - val_regression_loss: 1.0186 - lr: 2.5000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5163 - regression_loss: 0.4814 - val_loss: 4.3101 - val_regression_loss: 1.0193 - lr: 1.2500e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5193 - regression_loss: 0.4785 - val_loss: 4.3085 - val_regression_loss: 1.0193 - lr: 1.2500e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5162 - regression_loss: 0.4790 - val_loss: 4.3126 - val_regression_loss: 1.0188 - lr: 1.2500e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5051 - regression_loss: 0.4770 - val_loss: 4.3091 - val_regression_loss: 1.0177 - lr: 1.2500e-05\n",
      "Epoch 77/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.7727 - regression_loss: 0.7766\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5142 - regression_loss: 0.4765 - val_loss: 4.3095 - val_regression_loss: 1.0176 - lr: 1.2500e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5072 - regression_loss: 0.4759 - val_loss: 4.3051 - val_regression_loss: 1.0163 - lr: 6.2500e-06\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5103 - regression_loss: 0.4746 - val_loss: 4.3037 - val_regression_loss: 1.0143 - lr: 6.2500e-06\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5120 - regression_loss: 0.4739 - val_loss: 4.3033 - val_regression_loss: 1.0132 - lr: 6.2500e-06\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5007 - regression_loss: 0.4736 - val_loss: 4.3057 - val_regression_loss: 1.0137 - lr: 6.2500e-06\n",
      "Epoch 82/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.5304 - regression_loss: 0.5354\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5076 - regression_loss: 0.4731 - val_loss: 4.3043 - val_regression_loss: 1.0136 - lr: 6.2500e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5106 - regression_loss: 0.4727 - val_loss: 4.3028 - val_regression_loss: 1.0129 - lr: 3.1250e-06\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5025 - regression_loss: 0.4721 - val_loss: 4.3030 - val_regression_loss: 1.0126 - lr: 3.1250e-06\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4981 - regression_loss: 0.4721 - val_loss: 4.3004 - val_regression_loss: 1.0115 - lr: 3.1250e-06\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5027 - regression_loss: 0.4714 - val_loss: 4.3012 - val_regression_loss: 1.0116 - lr: 3.1250e-06\n",
      "Epoch 87/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.5097 - regression_loss: 0.5153\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5104 - regression_loss: 0.4712 - val_loss: 4.3008 - val_regression_loss: 1.0109 - lr: 3.1250e-06\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5090 - regression_loss: 0.4710 - val_loss: 4.3001 - val_regression_loss: 1.0106 - lr: 1.5625e-06\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5002 - regression_loss: 0.4709 - val_loss: 4.3006 - val_regression_loss: 1.0106 - lr: 1.5625e-06\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5058 - regression_loss: 0.4708 - val_loss: 4.2994 - val_regression_loss: 1.0101 - lr: 1.5625e-06\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5020 - regression_loss: 0.4706 - val_loss: 4.2989 - val_regression_loss: 1.0101 - lr: 1.5625e-06\n",
      "Epoch 92/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.5002 - regression_loss: 0.5061\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4910 - regression_loss: 0.4707 - val_loss: 4.2980 - val_regression_loss: 1.0096 - lr: 1.5625e-06\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5008 - regression_loss: 0.4702 - val_loss: 4.2979 - val_regression_loss: 1.0095 - lr: 7.8125e-07\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5109 - regression_loss: 0.4702 - val_loss: 4.2982 - val_regression_loss: 1.0096 - lr: 7.8125e-07\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4962 - regression_loss: 0.4701 - val_loss: 4.2981 - val_regression_loss: 1.0093 - lr: 7.8125e-07\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5080 - regression_loss: 0.4701 - val_loss: 4.2985 - val_regression_loss: 1.0095 - lr: 7.8125e-07\n",
      "Epoch 97/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.7814 - regression_loss: 0.7875\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4978 - regression_loss: 0.4700 - val_loss: 4.2986 - val_regression_loss: 1.0096 - lr: 7.8125e-07\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5103 - regression_loss: 0.4699 - val_loss: 4.2985 - val_regression_loss: 1.0095 - lr: 3.9062e-07\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5076 - regression_loss: 0.4699 - val_loss: 4.2985 - val_regression_loss: 1.0094 - lr: 3.9062e-07\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5024 - regression_loss: 0.4698 - val_loss: 4.2982 - val_regression_loss: 1.0093 - lr: 3.9062e-07\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4952 - regression_loss: 0.4698 - val_loss: 4.2980 - val_regression_loss: 1.0091 - lr: 3.9062e-07\n",
      "Epoch 102/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.2860 - regression_loss: 0.2921\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5054 - regression_loss: 0.4697 - val_loss: 4.2980 - val_regression_loss: 1.0091 - lr: 3.9062e-07\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4885 - regression_loss: 0.4697 - val_loss: 4.2980 - val_regression_loss: 1.0091 - lr: 1.9531e-07\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5037 - regression_loss: 0.4697 - val_loss: 4.2980 - val_regression_loss: 1.0091 - lr: 1.9531e-07\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5022 - regression_loss: 0.4697 - val_loss: 4.2979 - val_regression_loss: 1.0091 - lr: 1.9531e-07\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4975 - regression_loss: 0.4697 - val_loss: 4.2979 - val_regression_loss: 1.0091 - lr: 1.9531e-07\n",
      "Epoch 107/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.5062 - regression_loss: 0.5124\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5027 - regression_loss: 0.4697 - val_loss: 4.2978 - val_regression_loss: 1.0090 - lr: 1.9531e-07\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4988 - regression_loss: 0.4696 - val_loss: 4.2978 - val_regression_loss: 1.0090 - lr: 9.7656e-08\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5102 - regression_loss: 0.4696 - val_loss: 4.2978 - val_regression_loss: 1.0090 - lr: 9.7656e-08\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4596 - regression_loss: 0.4696 - val_loss: 4.2977 - val_regression_loss: 1.0090 - lr: 9.7656e-08\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4813 - regression_loss: 0.4696 - val_loss: 4.2978 - val_regression_loss: 1.0090 - lr: 9.7656e-08\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5034 - regression_loss: 0.4696 - val_loss: 4.2977 - val_regression_loss: 1.0090 - lr: 9.7656e-08\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4895 - regression_loss: 0.4696 - val_loss: 4.2977 - val_regression_loss: 1.0090 - lr: 9.7656e-08\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4947 - regression_loss: 0.4696 - val_loss: 4.2977 - val_regression_loss: 1.0090 - lr: 9.7656e-08\n",
      "Epoch 115/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.4086 - regression_loss: 0.4148\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5058 - regression_loss: 0.4696 - val_loss: 4.2977 - val_regression_loss: 1.0090 - lr: 9.7656e-08\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4448 - regression_loss: 0.4696 - val_loss: 4.2977 - val_regression_loss: 1.0090 - lr: 4.8828e-08\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5080 - regression_loss: 0.4695 - val_loss: 4.2977 - val_regression_loss: 1.0090 - lr: 4.8828e-08\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4546 - regression_loss: 0.4695 - val_loss: 4.2977 - val_regression_loss: 1.0089 - lr: 4.8828e-08\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4991 - regression_loss: 0.4695 - val_loss: 4.2977 - val_regression_loss: 1.0090 - lr: 4.8828e-08\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4575 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 4.8828e-08\n",
      "Epoch 121/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.4243 - regression_loss: 0.4305\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4931 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 4.8828e-08\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5100 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 2.4414e-08\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4891 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 2.4414e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4919 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 2.4414e-08\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4809 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 2.4414e-08\n",
      "Epoch 126/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.4420 - regression_loss: 0.4482\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5097 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 2.4414e-08\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5054 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 1.2207e-08\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4940 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 1.2207e-08\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5077 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 1.2207e-08\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5088 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 1.2207e-08\n",
      "Epoch 131/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.4389 - regression_loss: 0.4451\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5022 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 1.2207e-08\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4961 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 6.1035e-09\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4985 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 6.1035e-09\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5068 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 6.1035e-09\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5016 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 6.1035e-09\n",
      "Epoch 136/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.4382 - regression_loss: 0.4444\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5000 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 6.1035e-09\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5013 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 3.0518e-09\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5033 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 3.0518e-09\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5037 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 3.0518e-09\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5051 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 3.0518e-09\n",
      "Epoch 141/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.6487 - regression_loss: 0.6549\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 1.5258788677030566e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5081 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 3.0518e-09\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4932 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 1.5259e-09\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4835 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 1.5259e-09\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4747 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 1.5259e-09\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5079 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 1.5259e-09\n",
      "Epoch 146/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.7596 - regression_loss: 0.7659\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 7.629394338515283e-10.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4818 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 1.5259e-09\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5021 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 7.6294e-10\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5019 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 7.6294e-10\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5064 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 7.6294e-10\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4915 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 7.6294e-10\n",
      "Epoch 151/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.4507 - regression_loss: 0.4570\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 3.8146971692576415e-10.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5019 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 7.6294e-10\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5068 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 3.8147e-10\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5036 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 3.8147e-10\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4957 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 3.8147e-10\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4885 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 3.8147e-10\n",
      "Epoch 156/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.2919 - regression_loss: 0.2981\n",
      "Epoch 156: ReduceLROnPlateau reducing learning rate to 1.9073485846288207e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4528 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 3.8147e-10\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4872 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 1.9073e-10\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4906 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 1.9073e-10\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4938 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 1.9073e-10\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4921 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 1.9073e-10\n",
      "Epoch 161/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.4145 - regression_loss: 0.4207\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 9.536742923144104e-11.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4636 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 1.9073e-10\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5033 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 9.5367e-11\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4979 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 9.5367e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4959 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 9.5367e-11\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5005 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 9.5367e-11\n",
      "Epoch 166/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.9115 - regression_loss: 0.9177\n",
      "Epoch 166: ReduceLROnPlateau reducing learning rate to 4.768371461572052e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5022 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 9.5367e-11\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4854 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 4.7684e-11\n",
      "Epoch 168/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4903 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 4.7684e-11\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4997 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 4.7684e-11\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4534 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 4.7684e-11\n",
      "Epoch 171/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.3846 - regression_loss: 0.3908\n",
      "Epoch 171: ReduceLROnPlateau reducing learning rate to 2.384185730786026e-11.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4920 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 4.7684e-11\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4553 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 2.3842e-11\n",
      "Epoch 173/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4476 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 2.3842e-11\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5106 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 2.3842e-11\n",
      "Epoch 175/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5076 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 2.3842e-11\n",
      "Epoch 176/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.6794 - regression_loss: 0.6856\n",
      "Epoch 176: ReduceLROnPlateau reducing learning rate to 1.192092865393013e-11.\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.4945 - regression_loss: 0.4695 - val_loss: 4.2976 - val_regression_loss: 1.0089 - lr: 2.3842e-11\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 135.1671 - regression_loss: 121.6745 - val_loss: 63.3004 - val_regression_loss: 49.1332 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 79.5757 - regression_loss: 70.6302 - val_loss: 44.8923 - val_regression_loss: 34.1455 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 58.1091 - regression_loss: 50.7188 - val_loss: 37.2709 - val_regression_loss: 28.0085 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 46.1566 - regression_loss: 40.2417 - val_loss: 33.6063 - val_regression_loss: 25.0496 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 40.3190 - regression_loss: 34.7846 - val_loss: 32.4727 - val_regression_loss: 24.1623 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 36.7025 - regression_loss: 31.9888 - val_loss: 31.6016 - val_regression_loss: 23.5070 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 35.8835 - regression_loss: 30.8356 - val_loss: 31.0781 - val_regression_loss: 23.0823 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 35.0973 - regression_loss: 29.6462 - val_loss: 30.6319 - val_regression_loss: 22.5903 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 34.0019 - regression_loss: 28.6747 - val_loss: 30.0275 - val_regression_loss: 22.0007 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.9225 - regression_loss: 27.9644 - val_loss: 29.2316 - val_regression_loss: 21.3489 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.7450 - regression_loss: 27.3771 - val_loss: 29.0947 - val_regression_loss: 21.0952 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.0078 - regression_loss: 26.9886 - val_loss: 28.5081 - val_regression_loss: 20.6394 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.3379 - regression_loss: 26.3473 - val_loss: 28.4319 - val_regression_loss: 20.5449 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.3761 - regression_loss: 25.9358 - val_loss: 28.1100 - val_regression_loss: 20.2865 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.8456 - regression_loss: 25.5466 - val_loss: 27.7682 - val_regression_loss: 20.0333 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.2064 - regression_loss: 25.2524 - val_loss: 27.5965 - val_regression_loss: 19.8535 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.5805 - regression_loss: 24.9480 - val_loss: 27.2679 - val_regression_loss: 19.5999 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.1518 - regression_loss: 24.6911 - val_loss: 27.1898 - val_regression_loss: 19.4684 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.0580 - regression_loss: 24.3621 - val_loss: 26.7035 - val_regression_loss: 19.1225 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.2167 - regression_loss: 24.3901 - val_loss: 26.6301 - val_regression_loss: 18.9925 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.0634 - regression_loss: 24.0710 - val_loss: 26.8738 - val_regression_loss: 19.1130 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.1719 - regression_loss: 23.8475 - val_loss: 26.3133 - val_regression_loss: 18.7519 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.4194 - regression_loss: 23.7878 - val_loss: 26.1244 - val_regression_loss: 18.5637 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.1394 - regression_loss: 23.5408 - val_loss: 26.5063 - val_regression_loss: 18.7733 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.5574 - regression_loss: 23.7204 - val_loss: 26.1142 - val_regression_loss: 18.5395 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.7209 - regression_loss: 23.7833 - val_loss: 26.4091 - val_regression_loss: 18.6928 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.4381 - regression_loss: 23.4896 - val_loss: 25.7911 - val_regression_loss: 18.2794 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.0180 - regression_loss: 23.2345 - val_loss: 26.3978 - val_regression_loss: 18.6225 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.4090 - regression_loss: 23.1383 - val_loss: 25.7976 - val_regression_loss: 18.2344 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.7477 - regression_loss: 23.0694 - val_loss: 25.9322 - val_regression_loss: 18.3163 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.5197 - regression_loss: 23.0120 - val_loss: 26.3450 - val_regression_loss: 18.5985 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.1444 - regression_loss: 22.8477 - val_loss: 25.7572 - val_regression_loss: 18.2332 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.9161 - regression_loss: 22.9059 - val_loss: 26.0899 - val_regression_loss: 18.3981 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.6655 - regression_loss: 22.9806 - val_loss: 25.8819 - val_regression_loss: 18.2506 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.2946 - regression_loss: 22.7387 - val_loss: 25.8229 - val_regression_loss: 18.2160 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.8955 - regression_loss: 22.7569 - val_loss: 26.0185 - val_regression_loss: 18.3106 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.6236 - regression_loss: 22.7281 - val_loss: 25.9124 - val_regression_loss: 18.2616 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.9212 - regression_loss: 22.5920 - val_loss: 26.1547 - val_regression_loss: 18.4077 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.6687 - regression_loss: 22.6007 - val_loss: 26.1259 - val_regression_loss: 18.4149 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.9220 - regression_loss: 22.6209 - val_loss: 25.7680 - val_regression_loss: 18.1670 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 32.0863 - regression_loss: 29.1022\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.3232 - regression_loss: 22.7016 - val_loss: 26.1391 - val_regression_loss: 18.4132 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.3983 - regression_loss: 22.5330 - val_loss: 25.8398 - val_regression_loss: 18.2466 - lr: 5.0000e-05\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.7446 - regression_loss: 22.4919 - val_loss: 26.1514 - val_regression_loss: 18.4422 - lr: 5.0000e-05\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.8153 - regression_loss: 22.5390 - val_loss: 26.5601 - val_regression_loss: 18.6794 - lr: 5.0000e-05\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.6079 - regression_loss: 22.4222 - val_loss: 25.9547 - val_regression_loss: 18.2845 - lr: 5.0000e-05\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.0790 - regression_loss: 22.3752 - val_loss: 25.8745 - val_regression_loss: 18.2452 - lr: 5.0000e-05\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.5812 - regression_loss: 22.3796 - val_loss: 26.1545 - val_regression_loss: 18.4204 - lr: 5.0000e-05\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.7515 - regression_loss: 22.4237 - val_loss: 26.1903 - val_regression_loss: 18.4298 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.2365 - regression_loss: 22.2940 - val_loss: 25.9184 - val_regression_loss: 18.2686 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.1545 - regression_loss: 22.3058 - val_loss: 25.9536 - val_regression_loss: 18.3025 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.9393 - regression_loss: 22.3071 - val_loss: 26.1476 - val_regression_loss: 18.4148 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.7688 - regression_loss: 22.2904 - val_loss: 26.1255 - val_regression_loss: 18.4312 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.8079 - regression_loss: 22.2702 - val_loss: 26.2497 - val_regression_loss: 18.4997 - lr: 5.0000e-05\n",
      "Epoch 54/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 22.0296 - regression_loss: 19.0489\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.7873 - regression_loss: 22.2241 - val_loss: 26.1435 - val_regression_loss: 18.4289 - lr: 5.0000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.6164 - regression_loss: 22.2061 - val_loss: 26.1128 - val_regression_loss: 18.4048 - lr: 2.5000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.8586 - regression_loss: 22.2315 - val_loss: 26.1008 - val_regression_loss: 18.3863 - lr: 2.5000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.5852 - regression_loss: 22.1933 - val_loss: 26.1133 - val_regression_loss: 18.3966 - lr: 2.5000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.3930 - regression_loss: 22.2163 - val_loss: 26.2641 - val_regression_loss: 18.4968 - lr: 2.5000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.1416 - regression_loss: 22.1827 - val_loss: 26.1774 - val_regression_loss: 18.4505 - lr: 2.5000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.9009 - regression_loss: 22.1768 - val_loss: 26.1564 - val_regression_loss: 18.4292 - lr: 2.5000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.4961 - regression_loss: 22.1691 - val_loss: 26.1493 - val_regression_loss: 18.4189 - lr: 2.5000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.6936 - regression_loss: 22.1677 - val_loss: 26.1656 - val_regression_loss: 18.4251 - lr: 2.5000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.7086 - regression_loss: 22.1736 - val_loss: 26.1077 - val_regression_loss: 18.3958 - lr: 2.5000e-05\n",
      "Epoch 64/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 30.9115 - regression_loss: 27.9322\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.4508 - regression_loss: 22.1521 - val_loss: 26.2372 - val_regression_loss: 18.4728 - lr: 2.5000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.6565 - regression_loss: 22.1587 - val_loss: 26.2719 - val_regression_loss: 18.4909 - lr: 1.2500e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.6686 - regression_loss: 22.1424 - val_loss: 26.1473 - val_regression_loss: 18.4135 - lr: 1.2500e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.8951 - regression_loss: 22.1191 - val_loss: 26.1448 - val_regression_loss: 18.4196 - lr: 1.2500e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.6112 - regression_loss: 22.1166 - val_loss: 26.1908 - val_regression_loss: 18.4538 - lr: 1.2500e-05\n",
      "Epoch 69/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 29.1717 - regression_loss: 26.1928\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.3776 - regression_loss: 22.1176 - val_loss: 26.1623 - val_regression_loss: 18.4328 - lr: 1.2500e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.4476 - regression_loss: 22.1043 - val_loss: 26.1715 - val_regression_loss: 18.4390 - lr: 6.2500e-06\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.5232 - regression_loss: 22.1025 - val_loss: 26.2119 - val_regression_loss: 18.4629 - lr: 6.2500e-06\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.5263 - regression_loss: 22.1006 - val_loss: 26.2181 - val_regression_loss: 18.4645 - lr: 6.2500e-06\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 80.3303 - regression_loss: 71.0353 - val_loss: 54.4372 - val_regression_loss: 43.7234 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 63.1538 - regression_loss: 57.2464 - val_loss: 44.1205 - val_regression_loss: 35.4415 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 56.8900 - regression_loss: 49.8886 - val_loss: 39.8969 - val_regression_loss: 31.5553 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 47.2040 - regression_loss: 44.1667 - val_loss: 35.1793 - val_regression_loss: 27.5803 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 45.6841 - regression_loss: 39.1016 - val_loss: 30.6122 - val_regression_loss: 23.8169 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 41.0704 - regression_loss: 35.0654 - val_loss: 27.5927 - val_regression_loss: 21.2441 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 34.8734 - regression_loss: 30.9014 - val_loss: 25.1139 - val_regression_loss: 19.0850 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.8696 - regression_loss: 27.9544 - val_loss: 23.1857 - val_regression_loss: 17.2986 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30.0195 - regression_loss: 25.3389 - val_loss: 21.6658 - val_regression_loss: 15.8295 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.5232 - regression_loss: 22.9128 - val_loss: 20.2714 - val_regression_loss: 14.4371 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.7593 - regression_loss: 21.0943 - val_loss: 19.7168 - val_regression_loss: 13.7078 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.6140 - regression_loss: 19.2190 - val_loss: 19.0956 - val_regression_loss: 12.9925 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.9196 - regression_loss: 17.6891 - val_loss: 18.5423 - val_regression_loss: 12.3990 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.1896 - regression_loss: 16.5543 - val_loss: 18.0048 - val_regression_loss: 11.8212 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.3151 - regression_loss: 15.4876 - val_loss: 17.3655 - val_regression_loss: 11.1719 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.3870 - regression_loss: 14.6413 - val_loss: 17.4759 - val_regression_loss: 11.1119 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5861 - regression_loss: 13.7675 - val_loss: 17.2759 - val_regression_loss: 10.8962 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3127 - regression_loss: 13.4620 - val_loss: 16.6760 - val_regression_loss: 10.3937 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.9597 - regression_loss: 13.1031 - val_loss: 16.9097 - val_regression_loss: 10.5466 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.4200 - regression_loss: 12.4209 - val_loss: 16.9386 - val_regression_loss: 10.5316 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.9291 - regression_loss: 12.1810 - val_loss: 16.7112 - val_regression_loss: 10.3519 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.0039 - regression_loss: 11.9605 - val_loss: 16.7218 - val_regression_loss: 10.3540 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.6992 - regression_loss: 11.7741 - val_loss: 16.8471 - val_regression_loss: 10.4285 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.2683 - regression_loss: 11.7207 - val_loss: 16.9638 - val_regression_loss: 10.5320 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.2321 - regression_loss: 11.5989 - val_loss: 16.2433 - val_regression_loss: 9.9632 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.1947 - regression_loss: 11.5091 - val_loss: 16.3210 - val_regression_loss: 10.0163 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.0044 - regression_loss: 11.3738 - val_loss: 16.8294 - val_regression_loss: 10.4152 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.1642 - regression_loss: 11.3042 - val_loss: 16.1881 - val_regression_loss: 9.9155 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.9092 - regression_loss: 11.2710 - val_loss: 16.3181 - val_regression_loss: 10.0188 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.7761 - regression_loss: 11.1575 - val_loss: 16.6205 - val_regression_loss: 10.2363 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.0307 - regression_loss: 11.2048 - val_loss: 16.2634 - val_regression_loss: 9.9399 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.8499 - regression_loss: 11.0857 - val_loss: 16.5010 - val_regression_loss: 10.1226 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.7917 - regression_loss: 11.0472 - val_loss: 15.8649 - val_regression_loss: 9.6545 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.7952 - regression_loss: 10.9978 - val_loss: 16.1053 - val_regression_loss: 9.8370 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.6933 - regression_loss: 10.9795 - val_loss: 15.8503 - val_regression_loss: 9.6643 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.6041 - regression_loss: 10.8917 - val_loss: 17.0447 - val_regression_loss: 10.5624 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.6926 - regression_loss: 10.8955 - val_loss: 15.6025 - val_regression_loss: 9.4800 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.9549 - regression_loss: 11.2041 - val_loss: 16.9388 - val_regression_loss: 10.4801 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.6634 - regression_loss: 10.9929 - val_loss: 15.9701 - val_regression_loss: 9.7293 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.5979 - regression_loss: 10.9964 - val_loss: 17.0553 - val_regression_loss: 10.5547 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.6908 - regression_loss: 10.9766 - val_loss: 15.4033 - val_regression_loss: 9.3262 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.7586 - regression_loss: 10.9238 - val_loss: 16.4899 - val_regression_loss: 10.1387 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.2834 - regression_loss: 10.7718 - val_loss: 15.7932 - val_regression_loss: 9.5828 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.6259 - regression_loss: 10.9150 - val_loss: 17.6439 - val_regression_loss: 11.0395 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.5413 - regression_loss: 10.8309 - val_loss: 15.3313 - val_regression_loss: 9.2888 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.7347 - regression_loss: 10.9491 - val_loss: 17.0280 - val_regression_loss: 10.5564 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.6126 - regression_loss: 10.9586 - val_loss: 15.5660 - val_regression_loss: 9.4155 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1586 - regression_loss: 10.5977 - val_loss: 16.5763 - val_regression_loss: 10.1590 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.2893 - regression_loss: 10.5766 - val_loss: 15.5552 - val_regression_loss: 9.4069 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1484 - regression_loss: 10.5833 - val_loss: 16.0806 - val_regression_loss: 9.8107 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1769 - regression_loss: 10.5129 - val_loss: 15.8536 - val_regression_loss: 9.6258 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.2284 - regression_loss: 10.4620 - val_loss: 15.9074 - val_regression_loss: 9.6678 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.0515 - regression_loss: 10.4225 - val_loss: 16.0496 - val_regression_loss: 9.7935 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.0267 - regression_loss: 10.4415 - val_loss: 15.8653 - val_regression_loss: 9.6552 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.4245 - regression_loss: 10.5438 - val_loss: 16.3220 - val_regression_loss: 10.0116 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.2506 - regression_loss: 10.4835 - val_loss: 15.5195 - val_regression_loss: 9.3821 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.0854 - regression_loss: 10.4334 - val_loss: 15.9595 - val_regression_loss: 9.6881 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1601 - regression_loss: 10.4399 - val_loss: 15.7340 - val_regression_loss: 9.5322 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.9709 - regression_loss: 10.3418 - val_loss: 15.5792 - val_regression_loss: 9.4532 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8851 - regression_loss: 10.3159 - val_loss: 15.9191 - val_regression_loss: 9.7082 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.9549 - regression_loss: 10.2687 - val_loss: 15.4978 - val_regression_loss: 9.3726 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.0115 - regression_loss: 10.3273 - val_loss: 16.2695 - val_regression_loss: 9.9602 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6624 - regression_loss: 10.3944 - val_loss: 15.3985 - val_regression_loss: 9.3200 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.9488 - regression_loss: 10.3290 - val_loss: 16.5518 - val_regression_loss: 10.2003 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.0822 - regression_loss: 10.3371 - val_loss: 15.3216 - val_regression_loss: 9.2587 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1105 - regression_loss: 10.3851 - val_loss: 16.1339 - val_regression_loss: 9.8587 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.9665 - regression_loss: 10.2712 - val_loss: 15.1749 - val_regression_loss: 9.1539 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.7901 - regression_loss: 9.8645\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8790 - regression_loss: 10.2146 - val_loss: 16.4000 - val_regression_loss: 10.1123 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.0582 - regression_loss: 10.2946 - val_loss: 15.4533 - val_regression_loss: 9.3559 - lr: 5.0000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8834 - regression_loss: 10.2731 - val_loss: 15.6145 - val_regression_loss: 9.4597 - lr: 5.0000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6156 - regression_loss: 10.2208 - val_loss: 16.1992 - val_regression_loss: 9.9224 - lr: 5.0000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6114 - regression_loss: 10.1718 - val_loss: 15.2305 - val_regression_loss: 9.1872 - lr: 5.0000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8163 - regression_loss: 10.2014 - val_loss: 15.7631 - val_regression_loss: 9.5924 - lr: 5.0000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7385 - regression_loss: 10.1752 - val_loss: 15.6119 - val_regression_loss: 9.4747 - lr: 5.0000e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6084 - regression_loss: 10.1124 - val_loss: 15.6661 - val_regression_loss: 9.5060 - lr: 5.0000e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8788 - regression_loss: 10.1097 - val_loss: 15.6679 - val_regression_loss: 9.5053 - lr: 5.0000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.8128 - regression_loss: 10.1028 - val_loss: 15.4675 - val_regression_loss: 9.3674 - lr: 5.0000e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6675 - regression_loss: 10.0756 - val_loss: 15.3882 - val_regression_loss: 9.3057 - lr: 5.0000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8184 - regression_loss: 10.0787 - val_loss: 15.7530 - val_regression_loss: 9.5876 - lr: 5.0000e-05\n",
      "Epoch 80/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.5378 - regression_loss: 9.6190\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6956 - regression_loss: 10.0860 - val_loss: 15.7316 - val_regression_loss: 9.5701 - lr: 5.0000e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6326 - regression_loss: 10.0349 - val_loss: 15.4848 - val_regression_loss: 9.3795 - lr: 2.5000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7039 - regression_loss: 10.0935 - val_loss: 15.5183 - val_regression_loss: 9.3981 - lr: 2.5000e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6810 - regression_loss: 10.0598 - val_loss: 15.7947 - val_regression_loss: 9.6142 - lr: 2.5000e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7510 - regression_loss: 10.0446 - val_loss: 15.5437 - val_regression_loss: 9.4246 - lr: 2.5000e-05\n",
      "Epoch 85/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 16.2188 - regression_loss: 13.3016\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7093 - regression_loss: 10.0305 - val_loss: 15.3742 - val_regression_loss: 9.2980 - lr: 2.5000e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6560 - regression_loss: 10.0245 - val_loss: 15.4732 - val_regression_loss: 9.3715 - lr: 1.2500e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.4489 - regression_loss: 10.0157 - val_loss: 15.5881 - val_regression_loss: 9.4574 - lr: 1.2500e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.5383 - regression_loss: 10.0127 - val_loss: 15.6259 - val_regression_loss: 9.4875 - lr: 1.2500e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.5593 - regression_loss: 10.0093 - val_loss: 15.5536 - val_regression_loss: 9.4315 - lr: 1.2500e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8020 - regression_loss: 10.0063 - val_loss: 15.5599 - val_regression_loss: 9.4345 - lr: 1.2500e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.3905 - regression_loss: 10.0112 - val_loss: 15.5474 - val_regression_loss: 9.4281 - lr: 1.2500e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6222 - regression_loss: 9.9960 - val_loss: 15.5609 - val_regression_loss: 9.4383 - lr: 1.2500e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6603 - regression_loss: 9.9976 - val_loss: 15.5650 - val_regression_loss: 9.4398 - lr: 1.2500e-05\n",
      "Epoch 94/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7017 - regression_loss: 9.9962 - val_loss: 15.5754 - val_regression_loss: 9.4487 - lr: 1.2500e-05\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7873 - regression_loss: 10.0067 - val_loss: 15.4721 - val_regression_loss: 9.3722 - lr: 1.2500e-05\n",
      "Epoch 96/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 14.5459 - regression_loss: 11.6303\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6646 - regression_loss: 9.9962 - val_loss: 15.5270 - val_regression_loss: 9.4140 - lr: 1.2500e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.5184 - regression_loss: 9.9875 - val_loss: 15.5468 - val_regression_loss: 9.4296 - lr: 6.2500e-06\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6177 - regression_loss: 9.9904 - val_loss: 15.4916 - val_regression_loss: 9.3876 - lr: 6.2500e-06\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.5892 - regression_loss: 9.9913 - val_loss: 15.5244 - val_regression_loss: 9.4115 - lr: 6.2500e-06\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.5119 - regression_loss: 9.9841 - val_loss: 15.5318 - val_regression_loss: 9.4171 - lr: 6.2500e-06\n",
      "Epoch 101/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 14.4375 - regression_loss: 11.5223\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.5607 - regression_loss: 9.9830 - val_loss: 15.5511 - val_regression_loss: 9.4308 - lr: 6.2500e-06\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6356 - regression_loss: 9.9792 - val_loss: 15.5506 - val_regression_loss: 9.4298 - lr: 3.1250e-06\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6632 - regression_loss: 9.9839 - val_loss: 15.5591 - val_regression_loss: 9.4367 - lr: 3.1250e-06\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.5654 - regression_loss: 9.9805 - val_loss: 15.5405 - val_regression_loss: 9.4221 - lr: 3.1250e-06\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.4524 - regression_loss: 9.9783 - val_loss: 15.5406 - val_regression_loss: 9.4225 - lr: 3.1250e-06\n",
      "Epoch 106/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.3234 - regression_loss: 10.4084\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6311 - regression_loss: 9.9891 - val_loss: 15.5721 - val_regression_loss: 9.4477 - lr: 3.1250e-06\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6975 - regression_loss: 9.9766 - val_loss: 15.5571 - val_regression_loss: 9.4360 - lr: 1.5625e-06\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 26ms/step - loss: 109.2473 - regression_loss: 98.0311 - val_loss: 45.9162 - val_regression_loss: 36.5183 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 77.1735 - regression_loss: 67.9772 - val_loss: 38.7436 - val_regression_loss: 30.0283 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 56.4373 - regression_loss: 52.4924 - val_loss: 34.7453 - val_regression_loss: 26.5079 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 50.8336 - regression_loss: 44.2853 - val_loss: 32.5910 - val_regression_loss: 24.5473 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 44.7667 - regression_loss: 38.8505 - val_loss: 31.2673 - val_regression_loss: 23.4071 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 40.7710 - regression_loss: 35.1579 - val_loss: 30.2028 - val_regression_loss: 22.4920 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 37.8652 - regression_loss: 32.6017 - val_loss: 29.0067 - val_regression_loss: 21.5854 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 35.4508 - regression_loss: 30.5267 - val_loss: 27.9923 - val_regression_loss: 20.8319 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 33.1814 - regression_loss: 28.7962 - val_loss: 27.0861 - val_regression_loss: 20.1034 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.2033 - regression_loss: 27.2070 - val_loss: 26.0800 - val_regression_loss: 19.3612 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.0207 - regression_loss: 25.9040 - val_loss: 25.1200 - val_regression_loss: 18.5863 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.9601 - regression_loss: 24.5491 - val_loss: 24.2279 - val_regression_loss: 17.8984 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.8570 - regression_loss: 23.4295 - val_loss: 23.5005 - val_regression_loss: 17.3884 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.8800 - regression_loss: 22.3196 - val_loss: 22.7503 - val_regression_loss: 16.7694 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.9334 - regression_loss: 21.4560 - val_loss: 22.0278 - val_regression_loss: 16.2319 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.2954 - regression_loss: 20.8115 - val_loss: 21.4772 - val_regression_loss: 15.8385 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.6539 - regression_loss: 19.6521 - val_loss: 21.1542 - val_regression_loss: 15.5187 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.0969 - regression_loss: 18.9032 - val_loss: 20.4936 - val_regression_loss: 15.0642 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.5926 - regression_loss: 18.3150 - val_loss: 20.0164 - val_regression_loss: 14.6731 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.0130 - regression_loss: 17.7356 - val_loss: 19.7006 - val_regression_loss: 14.4193 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.6123 - regression_loss: 17.2563 - val_loss: 19.3868 - val_regression_loss: 14.1678 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9698 - regression_loss: 16.8817 - val_loss: 19.1353 - val_regression_loss: 13.9744 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5968 - regression_loss: 16.4448 - val_loss: 18.9642 - val_regression_loss: 13.8225 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.1778 - regression_loss: 16.3561 - val_loss: 18.6148 - val_regression_loss: 13.5650 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.5654 - regression_loss: 15.9764 - val_loss: 18.4289 - val_regression_loss: 13.4170 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.4585 - regression_loss: 15.6665 - val_loss: 18.2996 - val_regression_loss: 13.3485 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.6266 - regression_loss: 15.5903 - val_loss: 18.3312 - val_regression_loss: 13.3019 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.2317 - regression_loss: 15.5301 - val_loss: 18.0812 - val_regression_loss: 13.1426 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.0984 - regression_loss: 15.2565 - val_loss: 17.8987 - val_regression_loss: 13.0486 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.0229 - regression_loss: 15.1139 - val_loss: 17.8938 - val_regression_loss: 13.0078 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.6556 - regression_loss: 14.9773 - val_loss: 17.8530 - val_regression_loss: 12.9727 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.9317 - regression_loss: 14.9049 - val_loss: 17.6427 - val_regression_loss: 12.8458 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.7946 - regression_loss: 14.8865 - val_loss: 17.6309 - val_regression_loss: 12.8259 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.5355 - regression_loss: 14.7113 - val_loss: 17.6273 - val_regression_loss: 12.8000 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.6936 - regression_loss: 14.6864 - val_loss: 17.4814 - val_regression_loss: 12.7248 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.8191 - regression_loss: 14.7973 - val_loss: 17.4850 - val_regression_loss: 12.7387 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.5996 - regression_loss: 14.6371 - val_loss: 17.4198 - val_regression_loss: 12.6807 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.2019 - regression_loss: 14.4979 - val_loss: 17.4139 - val_regression_loss: 12.6573 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.1205 - regression_loss: 14.4015 - val_loss: 17.3480 - val_regression_loss: 12.6311 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.1836 - regression_loss: 14.4246 - val_loss: 17.4710 - val_regression_loss: 12.6996 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.4446 - regression_loss: 14.3653 - val_loss: 17.3443 - val_regression_loss: 12.6596 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.2867 - regression_loss: 14.2647 - val_loss: 17.3127 - val_regression_loss: 12.5989 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.8675 - regression_loss: 14.2623 - val_loss: 17.3061 - val_regression_loss: 12.5824 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.0138 - regression_loss: 14.1882 - val_loss: 17.3224 - val_regression_loss: 12.6029 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.0353 - regression_loss: 14.1987 - val_loss: 17.3450 - val_regression_loss: 12.6329 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6677 - regression_loss: 14.1294 - val_loss: 17.3089 - val_regression_loss: 12.6143 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.8916 - regression_loss: 14.1499 - val_loss: 17.2949 - val_regression_loss: 12.6604 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5536 - regression_loss: 14.0993 - val_loss: 17.2465 - val_regression_loss: 12.5684 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.9311 - regression_loss: 14.0832 - val_loss: 17.4015 - val_regression_loss: 12.7317 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.1936 - regression_loss: 14.0709 - val_loss: 17.3450 - val_regression_loss: 12.6409 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.9985 - regression_loss: 14.1085 - val_loss: 17.2264 - val_regression_loss: 12.5741 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.9105 - regression_loss: 14.0233 - val_loss: 17.1619 - val_regression_loss: 12.5667 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 19.7973 - regression_loss: 16.9166\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.8078 - regression_loss: 13.9183 - val_loss: 17.1648 - val_regression_loss: 12.5549 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5538 - regression_loss: 13.8174 - val_loss: 17.1852 - val_regression_loss: 12.5584 - lr: 5.0000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.7157 - regression_loss: 13.8312 - val_loss: 17.1996 - val_regression_loss: 12.5506 - lr: 5.0000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.7719 - regression_loss: 13.8063 - val_loss: 17.1913 - val_regression_loss: 12.5620 - lr: 5.0000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.8042 - regression_loss: 13.8753 - val_loss: 17.1849 - val_regression_loss: 12.5444 - lr: 5.0000e-05\n",
      "Epoch 58/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.7910 - regression_loss: 12.9126\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.8129 - regression_loss: 13.8049 - val_loss: 17.2173 - val_regression_loss: 12.5966 - lr: 5.0000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.7227 - regression_loss: 13.7710 - val_loss: 17.1889 - val_regression_loss: 12.5562 - lr: 2.5000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6653 - regression_loss: 13.7427 - val_loss: 17.2015 - val_regression_loss: 12.5512 - lr: 2.5000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5197 - regression_loss: 13.7478 - val_loss: 17.1548 - val_regression_loss: 12.5353 - lr: 2.5000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6746 - regression_loss: 13.7586 - val_loss: 17.1451 - val_regression_loss: 12.5340 - lr: 2.5000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4493 - regression_loss: 13.6918 - val_loss: 17.1460 - val_regression_loss: 12.5535 - lr: 2.5000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5035 - regression_loss: 13.7221 - val_loss: 17.1686 - val_regression_loss: 12.5609 - lr: 2.5000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4242 - regression_loss: 13.7600 - val_loss: 17.1788 - val_regression_loss: 12.5495 - lr: 2.5000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5542 - regression_loss: 13.7137 - val_loss: 17.1782 - val_regression_loss: 12.5750 - lr: 2.5000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3743 - regression_loss: 13.7157 - val_loss: 17.1667 - val_regression_loss: 12.5448 - lr: 2.5000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.4137 - regression_loss: 13.6786 - val_loss: 17.1551 - val_regression_loss: 12.5364 - lr: 2.5000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6798 - regression_loss: 13.6869 - val_loss: 17.1472 - val_regression_loss: 12.5371 - lr: 2.5000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2072 - regression_loss: 13.6598 - val_loss: 17.1440 - val_regression_loss: 12.5379 - lr: 2.5000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3466 - regression_loss: 13.6765 - val_loss: 17.1415 - val_regression_loss: 12.5275 - lr: 2.5000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3678 - regression_loss: 13.6769 - val_loss: 17.1385 - val_regression_loss: 12.5312 - lr: 2.5000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.5903 - regression_loss: 13.6653 - val_loss: 17.1273 - val_regression_loss: 12.5369 - lr: 2.5000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2081 - regression_loss: 13.6745 - val_loss: 17.1352 - val_regression_loss: 12.5300 - lr: 2.5000e-05\n",
      "Epoch 75/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 16.6073 - regression_loss: 13.7325\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4111 - regression_loss: 13.6353 - val_loss: 17.1174 - val_regression_loss: 12.5223 - lr: 2.5000e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6368 - regression_loss: 13.6158 - val_loss: 17.1281 - val_regression_loss: 12.5320 - lr: 1.2500e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5394 - regression_loss: 13.6299 - val_loss: 17.1290 - val_regression_loss: 12.5285 - lr: 1.2500e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2104 - regression_loss: 13.6240 - val_loss: 17.1402 - val_regression_loss: 12.5284 - lr: 1.2500e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.5182 - regression_loss: 13.6258 - val_loss: 17.1450 - val_regression_loss: 12.5363 - lr: 1.2500e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.9718 - regression_loss: 13.6013 - val_loss: 17.1340 - val_regression_loss: 12.5326 - lr: 1.2500e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.1517 - regression_loss: 13.5984 - val_loss: 17.1260 - val_regression_loss: 12.5268 - lr: 1.2500e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6271 - regression_loss: 13.6023 - val_loss: 17.1309 - val_regression_loss: 12.5367 - lr: 1.2500e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6068 - regression_loss: 13.6038 - val_loss: 17.1289 - val_regression_loss: 12.5267 - lr: 1.2500e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.5721 - regression_loss: 13.6037 - val_loss: 17.1407 - val_regression_loss: 12.5305 - lr: 1.2500e-05\n",
      "Epoch 85/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 19.8182 - regression_loss: 16.9445\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5717 - regression_loss: 13.5892 - val_loss: 17.1283 - val_regression_loss: 12.5301 - lr: 1.2500e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4739 - regression_loss: 13.5836 - val_loss: 17.1230 - val_regression_loss: 12.5244 - lr: 6.2500e-06\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2888 - regression_loss: 13.5778 - val_loss: 17.1272 - val_regression_loss: 12.5273 - lr: 6.2500e-06\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4944 - regression_loss: 13.5761 - val_loss: 17.1311 - val_regression_loss: 12.5305 - lr: 6.2500e-06\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4179 - regression_loss: 13.5749 - val_loss: 17.1244 - val_regression_loss: 12.5273 - lr: 6.2500e-06\n",
      "Epoch 90/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.6315 - regression_loss: 15.7581\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4062 - regression_loss: 13.5819 - val_loss: 17.1294 - val_regression_loss: 12.5257 - lr: 6.2500e-06\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.0286 - regression_loss: 13.5700 - val_loss: 17.1273 - val_regression_loss: 12.5253 - lr: 3.1250e-06\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4358 - regression_loss: 13.5687 - val_loss: 17.1214 - val_regression_loss: 12.5237 - lr: 3.1250e-06\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8854 - regression_loss: 13.5695 - val_loss: 17.1227 - val_regression_loss: 12.5267 - lr: 3.1250e-06\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.1810 - regression_loss: 13.5671 - val_loss: 17.1218 - val_regression_loss: 12.5258 - lr: 3.1250e-06\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4174 - regression_loss: 13.5686 - val_loss: 17.1195 - val_regression_loss: 12.5224 - lr: 3.1250e-06\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.0939 - regression_loss: 13.5652 - val_loss: 17.1197 - val_regression_loss: 12.5228 - lr: 3.1250e-06\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2987 - regression_loss: 13.5647 - val_loss: 17.1191 - val_regression_loss: 12.5238 - lr: 3.1250e-06\n",
      "Epoch 98/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 19.0962 - regression_loss: 16.2231\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5177 - regression_loss: 13.5625 - val_loss: 17.1184 - val_regression_loss: 12.5241 - lr: 3.1250e-06\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4957 - regression_loss: 13.5610 - val_loss: 17.1199 - val_regression_loss: 12.5253 - lr: 1.5625e-06\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3854 - regression_loss: 13.5596 - val_loss: 17.1206 - val_regression_loss: 12.5252 - lr: 1.5625e-06\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5278 - regression_loss: 13.5630 - val_loss: 17.1210 - val_regression_loss: 12.5267 - lr: 1.5625e-06\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4832 - regression_loss: 13.5589 - val_loss: 17.1204 - val_regression_loss: 12.5256 - lr: 1.5625e-06\n",
      "Epoch 103/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.7255 - regression_loss: 15.8524\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5208 - regression_loss: 13.5585 - val_loss: 17.1210 - val_regression_loss: 12.5251 - lr: 1.5625e-06\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4790 - regression_loss: 13.5575 - val_loss: 17.1213 - val_regression_loss: 12.5253 - lr: 7.8125e-07\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2756 - regression_loss: 13.5581 - val_loss: 17.1215 - val_regression_loss: 12.5256 - lr: 7.8125e-07\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5781 - regression_loss: 13.5576 - val_loss: 17.1218 - val_regression_loss: 12.5251 - lr: 7.8125e-07\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3822 - regression_loss: 13.5582 - val_loss: 17.1220 - val_regression_loss: 12.5249 - lr: 7.8125e-07\n",
      "Epoch 108/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.7403 - regression_loss: 15.8673\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5073 - regression_loss: 13.5571 - val_loss: 17.1217 - val_regression_loss: 12.5250 - lr: 7.8125e-07\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2466 - regression_loss: 13.5561 - val_loss: 17.1218 - val_regression_loss: 12.5250 - lr: 3.9062e-07\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3814 - regression_loss: 13.5561 - val_loss: 17.1219 - val_regression_loss: 12.5251 - lr: 3.9062e-07\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5421 - regression_loss: 13.5559 - val_loss: 17.1216 - val_regression_loss: 12.5250 - lr: 3.9062e-07\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9701 - regression_loss: 13.5574 - val_loss: 17.1222 - val_regression_loss: 12.5250 - lr: 3.9062e-07\n",
      "Epoch 113/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.4473 - regression_loss: 12.5743\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6218 - regression_loss: 13.5557 - val_loss: 17.1219 - val_regression_loss: 12.5250 - lr: 3.9062e-07\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6692 - regression_loss: 13.5559 - val_loss: 17.1216 - val_regression_loss: 12.5251 - lr: 1.9531e-07\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8875 - regression_loss: 13.5554 - val_loss: 17.1217 - val_regression_loss: 12.5251 - lr: 1.9531e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 140.4364 - regression_loss: 127.1129 - val_loss: 110.8709 - val_regression_loss: 87.2989 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 106.4335 - regression_loss: 95.5989 - val_loss: 88.9714 - val_regression_loss: 69.6358 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 86.9241 - regression_loss: 77.3536 - val_loss: 75.0367 - val_regression_loss: 58.1622 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 72.4538 - regression_loss: 64.8070 - val_loss: 64.7508 - val_regression_loss: 49.8083 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 62.5376 - regression_loss: 56.0244 - val_loss: 56.1586 - val_regression_loss: 42.7781 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 53.8952 - regression_loss: 47.9351 - val_loss: 48.1911 - val_regression_loss: 36.4268 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 47.5879 - regression_loss: 41.8323 - val_loss: 42.4798 - val_regression_loss: 31.7039 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 42.3507 - regression_loss: 36.5855 - val_loss: 37.2006 - val_regression_loss: 27.4853 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 36.9221 - regression_loss: 31.8063 - val_loss: 32.8173 - val_regression_loss: 24.0178 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 33.4608 - regression_loss: 28.3071 - val_loss: 29.0712 - val_regression_loss: 21.0344 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.7330 - regression_loss: 24.9334 - val_loss: 25.5905 - val_regression_loss: 18.3056 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.5441 - regression_loss: 22.0962 - val_loss: 22.8711 - val_regression_loss: 16.1241 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.3150 - regression_loss: 19.6291 - val_loss: 20.5114 - val_regression_loss: 14.2817 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.4628 - regression_loss: 17.4479 - val_loss: 18.4053 - val_regression_loss: 12.6379 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.7585 - regression_loss: 15.9526 - val_loss: 16.8089 - val_regression_loss: 11.3715 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6781 - regression_loss: 14.3433 - val_loss: 15.4904 - val_regression_loss: 10.3186 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.1103 - regression_loss: 13.2804 - val_loss: 14.4192 - val_regression_loss: 9.4250 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.3142 - regression_loss: 12.3386 - val_loss: 13.5380 - val_regression_loss: 8.7690 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.7639 - regression_loss: 11.3600 - val_loss: 12.6590 - val_regression_loss: 8.0453 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.2955 - regression_loss: 10.7064 - val_loss: 12.2651 - val_regression_loss: 7.7676 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7354 - regression_loss: 10.1517 - val_loss: 11.4328 - val_regression_loss: 7.0671 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1279 - regression_loss: 9.5696 - val_loss: 11.0315 - val_regression_loss: 6.7339 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2573 - regression_loss: 9.0248 - val_loss: 10.5648 - val_regression_loss: 6.3651 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0061 - regression_loss: 8.5259 - val_loss: 10.3662 - val_regression_loss: 6.2455 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6834 - regression_loss: 8.1496 - val_loss: 9.8462 - val_regression_loss: 5.8176 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8701 - regression_loss: 7.7602 - val_loss: 9.5218 - val_regression_loss: 5.5984 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8329 - regression_loss: 7.4609 - val_loss: 9.2686 - val_regression_loss: 5.3963 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.4077 - regression_loss: 7.0771 - val_loss: 8.8761 - val_regression_loss: 5.1140 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.1674 - regression_loss: 6.8445 - val_loss: 8.7785 - val_regression_loss: 5.0561 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4729 - regression_loss: 6.5734 - val_loss: 8.4769 - val_regression_loss: 4.8025 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6152 - regression_loss: 6.3204 - val_loss: 8.3629 - val_regression_loss: 4.7183 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4261 - regression_loss: 6.1028 - val_loss: 8.0666 - val_regression_loss: 4.4872 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2432 - regression_loss: 5.9661 - val_loss: 8.0417 - val_regression_loss: 4.5064 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0127 - regression_loss: 5.8077 - val_loss: 7.8829 - val_regression_loss: 4.3713 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7596 - regression_loss: 5.5918 - val_loss: 7.6909 - val_regression_loss: 4.2106 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7560 - regression_loss: 5.4628 - val_loss: 7.6711 - val_regression_loss: 4.1851 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6597 - regression_loss: 5.3595 - val_loss: 7.5480 - val_regression_loss: 4.0692 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5716 - regression_loss: 5.2418 - val_loss: 7.5394 - val_regression_loss: 4.0745 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3729 - regression_loss: 5.1281 - val_loss: 7.4255 - val_regression_loss: 3.9862 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3060 - regression_loss: 5.1165 - val_loss: 7.4595 - val_regression_loss: 4.0126 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1683 - regression_loss: 4.9997 - val_loss: 7.3324 - val_regression_loss: 3.9000 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1265 - regression_loss: 4.9367 - val_loss: 7.3044 - val_regression_loss: 3.8860 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0429 - regression_loss: 4.8945 - val_loss: 7.1516 - val_regression_loss: 3.7483 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9705 - regression_loss: 4.7707 - val_loss: 7.2939 - val_regression_loss: 3.8638 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8289 - regression_loss: 4.6874 - val_loss: 7.1315 - val_regression_loss: 3.6995 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9139 - regression_loss: 4.7201 - val_loss: 7.1246 - val_regression_loss: 3.6866 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7217 - regression_loss: 4.6112 - val_loss: 7.1399 - val_regression_loss: 3.7187 - lr: 1.0000e-04\n",
      "Epoch 48/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7970 - regression_loss: 4.5717 - val_loss: 7.1548 - val_regression_loss: 3.7427 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6952 - regression_loss: 4.5354 - val_loss: 7.0542 - val_regression_loss: 3.6320 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5925 - regression_loss: 4.4723 - val_loss: 7.3006 - val_regression_loss: 3.8368 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6560 - regression_loss: 4.4347 - val_loss: 6.9648 - val_regression_loss: 3.5630 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.5480 - regression_loss: 4.3968 - val_loss: 7.1636 - val_regression_loss: 3.7168 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4942 - regression_loss: 4.3728 - val_loss: 6.9889 - val_regression_loss: 3.5708 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4999 - regression_loss: 4.3270 - val_loss: 7.1197 - val_regression_loss: 3.6619 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4861 - regression_loss: 4.2950 - val_loss: 7.0514 - val_regression_loss: 3.6043 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4238 - regression_loss: 4.2614 - val_loss: 7.0030 - val_regression_loss: 3.5840 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.3105 - regression_loss: 4.2311 - val_loss: 7.0166 - val_regression_loss: 3.5832 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4189 - regression_loss: 4.2243 - val_loss: 6.9201 - val_regression_loss: 3.5152 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.3439 - regression_loss: 4.2041 - val_loss: 6.9787 - val_regression_loss: 3.5547 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.2963 - regression_loss: 4.1444 - val_loss: 6.9128 - val_regression_loss: 3.4981 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.2895 - regression_loss: 4.1302 - val_loss: 6.9274 - val_regression_loss: 3.5094 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.2223 - regression_loss: 4.1157 - val_loss: 6.8955 - val_regression_loss: 3.4880 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1327 - regression_loss: 4.0864 - val_loss: 6.9237 - val_regression_loss: 3.5037 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1639 - regression_loss: 4.1068 - val_loss: 6.9384 - val_regression_loss: 3.5313 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.2123 - regression_loss: 4.0708 - val_loss: 7.0733 - val_regression_loss: 3.5991 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1617 - regression_loss: 4.0489 - val_loss: 6.8364 - val_regression_loss: 3.4248 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1883 - regression_loss: 4.0246 - val_loss: 7.1092 - val_regression_loss: 3.6549 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.5514 - regression_loss: 3.7176\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1655 - regression_loss: 4.0286 - val_loss: 6.7493 - val_regression_loss: 3.3526 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1066 - regression_loss: 4.0201 - val_loss: 6.9722 - val_regression_loss: 3.5285 - lr: 5.0000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0687 - regression_loss: 3.9505 - val_loss: 6.9410 - val_regression_loss: 3.5056 - lr: 5.0000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0634 - regression_loss: 3.9284 - val_loss: 6.7804 - val_regression_loss: 3.3861 - lr: 5.0000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0436 - regression_loss: 3.9219 - val_loss: 6.9044 - val_regression_loss: 3.4769 - lr: 5.0000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0404 - regression_loss: 3.9065 - val_loss: 6.8807 - val_regression_loss: 3.4480 - lr: 5.0000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.9596 - regression_loss: 3.8950 - val_loss: 6.8657 - val_regression_loss: 3.4382 - lr: 5.0000e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0377 - regression_loss: 3.9015 - val_loss: 6.8837 - val_regression_loss: 3.4602 - lr: 5.0000e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.9606 - regression_loss: 3.8806 - val_loss: 6.8178 - val_regression_loss: 3.4052 - lr: 5.0000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0440 - regression_loss: 3.8929 - val_loss: 6.8132 - val_regression_loss: 3.4065 - lr: 5.0000e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.9314 - regression_loss: 3.8854 - val_loss: 6.9895 - val_regression_loss: 3.5397 - lr: 5.0000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9667 - regression_loss: 3.8617 - val_loss: 6.7329 - val_regression_loss: 3.3348 - lr: 5.0000e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.9330 - regression_loss: 3.8507 - val_loss: 6.8999 - val_regression_loss: 3.4660 - lr: 5.0000e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.9836 - regression_loss: 3.8520 - val_loss: 6.8572 - val_regression_loss: 3.4311 - lr: 5.0000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9775 - regression_loss: 3.8307 - val_loss: 6.7693 - val_regression_loss: 3.3728 - lr: 5.0000e-05\n",
      "Epoch 83/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.3370 - regression_loss: 4.5097\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9626 - regression_loss: 3.8288 - val_loss: 6.8002 - val_regression_loss: 3.3872 - lr: 5.0000e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.9008 - regression_loss: 3.8083 - val_loss: 6.8313 - val_regression_loss: 3.4117 - lr: 2.5000e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9429 - regression_loss: 3.8123 - val_loss: 6.8176 - val_regression_loss: 3.4049 - lr: 2.5000e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8556 - regression_loss: 3.8023 - val_loss: 6.7496 - val_regression_loss: 3.3489 - lr: 2.5000e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8469 - regression_loss: 3.8023 - val_loss: 6.7855 - val_regression_loss: 3.3759 - lr: 2.5000e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7853 - regression_loss: 3.7905 - val_loss: 6.8663 - val_regression_loss: 3.4364 - lr: 2.5000e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.9236 - regression_loss: 3.7893 - val_loss: 6.7888 - val_regression_loss: 3.3778 - lr: 2.5000e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.8508 - regression_loss: 3.7882 - val_loss: 6.7464 - val_regression_loss: 3.3415 - lr: 2.5000e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8870 - regression_loss: 3.7804 - val_loss: 6.7956 - val_regression_loss: 3.3842 - lr: 2.5000e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8924 - regression_loss: 3.7768 - val_loss: 6.8293 - val_regression_loss: 3.4079 - lr: 2.5000e-05\n",
      "Epoch 93/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.7824 - regression_loss: 3.9575\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9063 - regression_loss: 3.7803 - val_loss: 6.7508 - val_regression_loss: 3.3455 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9130 - regression_loss: 3.7633 - val_loss: 6.7841 - val_regression_loss: 3.3718 - lr: 1.2500e-05\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8723 - regression_loss: 3.7655 - val_loss: 6.8143 - val_regression_loss: 3.3948 - lr: 1.2500e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7745 - regression_loss: 3.7595 - val_loss: 6.8020 - val_regression_loss: 3.3845 - lr: 1.2500e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8625 - regression_loss: 3.7612 - val_loss: 6.7494 - val_regression_loss: 3.3470 - lr: 1.2500e-05\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8495 - regression_loss: 3.7583 - val_loss: 6.7808 - val_regression_loss: 3.3708 - lr: 1.2500e-05\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8217 - regression_loss: 3.7528 - val_loss: 6.7850 - val_regression_loss: 3.3748 - lr: 1.2500e-05\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8584 - regression_loss: 3.7528 - val_loss: 6.7751 - val_regression_loss: 3.3662 - lr: 1.2500e-05\n",
      "Epoch 101/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.4378 - regression_loss: 3.6139\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8708 - regression_loss: 3.7516 - val_loss: 6.7425 - val_regression_loss: 3.3396 - lr: 1.2500e-05\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8612 - regression_loss: 3.7479 - val_loss: 6.7655 - val_regression_loss: 3.3561 - lr: 6.2500e-06\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7844 - regression_loss: 3.7444 - val_loss: 6.7735 - val_regression_loss: 3.3621 - lr: 6.2500e-06\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8212 - regression_loss: 3.7456 - val_loss: 6.7906 - val_regression_loss: 3.3763 - lr: 6.2500e-06\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8561 - regression_loss: 3.7425 - val_loss: 6.7756 - val_regression_loss: 3.3643 - lr: 6.2500e-06\n",
      "Epoch 106/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.0770 - regression_loss: 4.2534\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8741 - regression_loss: 3.7411 - val_loss: 6.7764 - val_regression_loss: 3.3643 - lr: 6.2500e-06\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8384 - regression_loss: 3.7390 - val_loss: 6.7742 - val_regression_loss: 3.3623 - lr: 3.1250e-06\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8597 - regression_loss: 3.7392 - val_loss: 6.7735 - val_regression_loss: 3.3622 - lr: 3.1250e-06\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7670 - regression_loss: 3.7384 - val_loss: 6.7758 - val_regression_loss: 3.3637 - lr: 3.1250e-06\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8228 - regression_loss: 3.7379 - val_loss: 6.7698 - val_regression_loss: 3.3589 - lr: 3.1250e-06\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7590 - regression_loss: 3.7385 - val_loss: 6.7684 - val_regression_loss: 3.3574 - lr: 3.1250e-06\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8640 - regression_loss: 3.7367 - val_loss: 6.7698 - val_regression_loss: 3.3590 - lr: 3.1250e-06\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8215 - regression_loss: 3.7368 - val_loss: 6.7655 - val_regression_loss: 3.3560 - lr: 3.1250e-06\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7001 - regression_loss: 3.7363 - val_loss: 6.7695 - val_regression_loss: 3.3595 - lr: 3.1250e-06\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7361 - regression_loss: 3.7355 - val_loss: 6.7659 - val_regression_loss: 3.3565 - lr: 3.1250e-06\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7685 - regression_loss: 3.7375 - val_loss: 6.7804 - val_regression_loss: 3.3681 - lr: 3.1250e-06\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8866 - regression_loss: 3.7345 - val_loss: 6.7809 - val_regression_loss: 3.3684 - lr: 3.1250e-06\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7546 - regression_loss: 3.7342 - val_loss: 6.7699 - val_regression_loss: 3.3597 - lr: 3.1250e-06\n",
      "Epoch 119/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.2212 - regression_loss: 4.3981\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8063 - regression_loss: 3.7334 - val_loss: 6.7679 - val_regression_loss: 3.3578 - lr: 3.1250e-06\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 56.2399 - regression_loss: 48.6946 - val_loss: 38.0525 - val_regression_loss: 32.5986 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 36.4349 - regression_loss: 31.0994 - val_loss: 29.5694 - val_regression_loss: 24.4262 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.4956 - regression_loss: 23.6330 - val_loss: 25.3719 - val_regression_loss: 20.9945 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.6845 - regression_loss: 19.4084 - val_loss: 22.5875 - val_regression_loss: 18.6400 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.2678 - regression_loss: 17.0393 - val_loss: 19.8311 - val_regression_loss: 15.8650 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.4689 - regression_loss: 15.6024 - val_loss: 18.5965 - val_regression_loss: 14.8258 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.2630 - regression_loss: 14.3478 - val_loss: 17.2801 - val_regression_loss: 13.5430 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2893 - regression_loss: 13.4457 - val_loss: 16.2145 - val_regression_loss: 12.4792 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.3601 - regression_loss: 12.5369 - val_loss: 15.6146 - val_regression_loss: 11.9167 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.4080 - regression_loss: 11.7284 - val_loss: 14.8424 - val_regression_loss: 10.9603 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.5748 - regression_loss: 11.0972 - val_loss: 14.5328 - val_regression_loss: 10.6901 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8903 - regression_loss: 10.5350 - val_loss: 13.7880 - val_regression_loss: 9.8597 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8303 - regression_loss: 10.1736 - val_loss: 13.8394 - val_regression_loss: 10.0202 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.3272 - regression_loss: 9.7280 - val_loss: 13.0766 - val_regression_loss: 9.2571 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0392 - regression_loss: 9.4326 - val_loss: 12.7451 - val_regression_loss: 8.8688 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6186 - regression_loss: 9.0478 - val_loss: 12.7931 - val_regression_loss: 8.9131 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2650 - regression_loss: 8.7748 - val_loss: 12.5804 - val_regression_loss: 8.6789 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8379 - regression_loss: 8.5531 - val_loss: 12.0526 - val_regression_loss: 8.1375 - lr: 1.0000e-04\n",
      "Epoch 19/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6595 - regression_loss: 8.3323 - val_loss: 12.0721 - val_regression_loss: 8.2153 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5750 - regression_loss: 8.1333 - val_loss: 11.8379 - val_regression_loss: 7.8637 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2792 - regression_loss: 7.8735 - val_loss: 11.7812 - val_regression_loss: 7.8531 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1843 - regression_loss: 7.7813 - val_loss: 11.5362 - val_regression_loss: 7.4998 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8090 - regression_loss: 7.5723 - val_loss: 11.5851 - val_regression_loss: 7.5996 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7875 - regression_loss: 7.4824 - val_loss: 11.4254 - val_regression_loss: 7.4456 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7190 - regression_loss: 7.3651 - val_loss: 11.1936 - val_regression_loss: 7.1550 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.5099 - regression_loss: 7.3492 - val_loss: 11.5779 - val_regression_loss: 7.6042 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7460 - regression_loss: 7.4144 - val_loss: 10.9227 - val_regression_loss: 6.8325 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6339 - regression_loss: 7.2632 - val_loss: 11.8360 - val_regression_loss: 7.7940 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6293 - regression_loss: 7.2741 - val_loss: 10.9704 - val_regression_loss: 6.7550 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.5436 - regression_loss: 7.2152 - val_loss: 11.6551 - val_regression_loss: 7.6486 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2974 - regression_loss: 7.0032 - val_loss: 10.7644 - val_regression_loss: 6.6450 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2485 - regression_loss: 6.9380 - val_loss: 11.5014 - val_regression_loss: 7.4567 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3989 - regression_loss: 6.9734 - val_loss: 10.8931 - val_regression_loss: 6.7190 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2186 - regression_loss: 6.8698 - val_loss: 11.2747 - val_regression_loss: 7.2121 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9769 - regression_loss: 6.7220 - val_loss: 10.7260 - val_regression_loss: 6.5778 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0450 - regression_loss: 6.7437 - val_loss: 11.7161 - val_regression_loss: 7.6741 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.1177 - regression_loss: 6.8377 - val_loss: 10.8388 - val_regression_loss: 6.5874 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.1190 - regression_loss: 6.8983 - val_loss: 11.4837 - val_regression_loss: 7.4943 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0317 - regression_loss: 6.7254 - val_loss: 10.6803 - val_regression_loss: 6.5591 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.2784 - regression_loss: 6.4891\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0111 - regression_loss: 6.7721 - val_loss: 11.3847 - val_regression_loss: 7.3092 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7909 - regression_loss: 6.5435 - val_loss: 10.8050 - val_regression_loss: 6.7185 - lr: 5.0000e-05\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5976 - regression_loss: 6.4722 - val_loss: 10.7252 - val_regression_loss: 6.6177 - lr: 5.0000e-05\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6724 - regression_loss: 6.4951 - val_loss: 11.0765 - val_regression_loss: 6.9953 - lr: 5.0000e-05\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7402 - regression_loss: 6.4562 - val_loss: 10.9077 - val_regression_loss: 6.7972 - lr: 5.0000e-05\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6229 - regression_loss: 6.4401 - val_loss: 10.8718 - val_regression_loss: 6.7828 - lr: 5.0000e-05\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6987 - regression_loss: 6.4504 - val_loss: 10.7081 - val_regression_loss: 6.6252 - lr: 5.0000e-05\n",
      "Epoch 47/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.9204 - regression_loss: 6.1349\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7990 - regression_loss: 6.4192 - val_loss: 11.0156 - val_regression_loss: 6.9376 - lr: 5.0000e-05\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5928 - regression_loss: 6.4140 - val_loss: 10.7952 - val_regression_loss: 6.6917 - lr: 2.5000e-05\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4655 - regression_loss: 6.3768 - val_loss: 10.7731 - val_regression_loss: 6.6807 - lr: 2.5000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6521 - regression_loss: 6.3741 - val_loss: 10.8495 - val_regression_loss: 6.7468 - lr: 2.5000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5815 - regression_loss: 6.3563 - val_loss: 10.8979 - val_regression_loss: 6.8080 - lr: 2.5000e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5911 - regression_loss: 6.3560 - val_loss: 10.8506 - val_regression_loss: 6.7654 - lr: 2.5000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5938 - regression_loss: 6.3449 - val_loss: 10.8101 - val_regression_loss: 6.7252 - lr: 2.5000e-05\n",
      "Epoch 54/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.9831 - regression_loss: 7.1996\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5286 - regression_loss: 6.3431 - val_loss: 10.8189 - val_regression_loss: 6.7166 - lr: 2.5000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5286 - regression_loss: 6.3281 - val_loss: 10.8033 - val_regression_loss: 6.7036 - lr: 1.2500e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4235 - regression_loss: 6.3252 - val_loss: 10.8207 - val_regression_loss: 6.7265 - lr: 1.2500e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4938 - regression_loss: 6.3232 - val_loss: 10.8161 - val_regression_loss: 6.7258 - lr: 1.2500e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4981 - regression_loss: 6.3165 - val_loss: 10.8400 - val_regression_loss: 6.7537 - lr: 1.2500e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5062 - regression_loss: 6.3245 - val_loss: 10.8283 - val_regression_loss: 6.7349 - lr: 1.2500e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5963 - regression_loss: 6.3186 - val_loss: 10.8434 - val_regression_loss: 6.7649 - lr: 1.2500e-05\n",
      "Epoch 61/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.1132 - regression_loss: 6.3307\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5376 - regression_loss: 6.3095 - val_loss: 10.8068 - val_regression_loss: 6.7255 - lr: 1.2500e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3971 - regression_loss: 6.3032 - val_loss: 10.7964 - val_regression_loss: 6.7133 - lr: 6.2500e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5108 - regression_loss: 6.3040 - val_loss: 10.7909 - val_regression_loss: 6.7052 - lr: 6.2500e-06\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6125 - regression_loss: 6.3006 - val_loss: 10.8064 - val_regression_loss: 6.7219 - lr: 6.2500e-06\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.6170 - regression_loss: 6.3016 - val_loss: 10.7926 - val_regression_loss: 6.7068 - lr: 6.2500e-06\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4398 - regression_loss: 6.3021 - val_loss: 10.7815 - val_regression_loss: 6.6908 - lr: 6.2500e-06\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3600 - regression_loss: 6.2988 - val_loss: 10.8104 - val_regression_loss: 6.7299 - lr: 6.2500e-06\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5686 - regression_loss: 6.2977 - val_loss: 10.8176 - val_regression_loss: 6.7385 - lr: 6.2500e-06\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4792 - regression_loss: 6.2941 - val_loss: 10.8115 - val_regression_loss: 6.7316 - lr: 6.2500e-06\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5291 - regression_loss: 6.2900 - val_loss: 10.8043 - val_regression_loss: 6.7178 - lr: 6.2500e-06\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5155 - regression_loss: 6.2898 - val_loss: 10.8100 - val_regression_loss: 6.7201 - lr: 6.2500e-06\n",
      "Epoch 72/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.1456 - regression_loss: 7.3638\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4918 - regression_loss: 6.2880 - val_loss: 10.7944 - val_regression_loss: 6.7066 - lr: 6.2500e-06\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4665 - regression_loss: 6.2856 - val_loss: 10.7918 - val_regression_loss: 6.7014 - lr: 3.1250e-06\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5792 - regression_loss: 6.2854 - val_loss: 10.7928 - val_regression_loss: 6.7021 - lr: 3.1250e-06\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5167 - regression_loss: 6.2852 - val_loss: 10.8075 - val_regression_loss: 6.7150 - lr: 3.1250e-06\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5075 - regression_loss: 6.2832 - val_loss: 10.7958 - val_regression_loss: 6.7046 - lr: 3.1250e-06\n",
      "Epoch 77/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.8248 - regression_loss: 6.0432\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5354 - regression_loss: 6.2817 - val_loss: 10.8020 - val_regression_loss: 6.7115 - lr: 3.1250e-06\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5311 - regression_loss: 6.2806 - val_loss: 10.8018 - val_regression_loss: 6.7120 - lr: 1.5625e-06\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5287 - regression_loss: 6.2801 - val_loss: 10.8025 - val_regression_loss: 6.7126 - lr: 1.5625e-06\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 24ms/step - loss: 60.8340 - regression_loss: 55.1054 - val_loss: 49.2059 - val_regression_loss: 37.1927 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 44.4941 - regression_loss: 38.8430 - val_loss: 38.9078 - val_regression_loss: 28.9777 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 36.0253 - regression_loss: 30.6125 - val_loss: 32.7871 - val_regression_loss: 23.9974 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.3033 - regression_loss: 24.7679 - val_loss: 28.6746 - val_regression_loss: 20.6393 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.5728 - regression_loss: 21.1961 - val_loss: 25.6563 - val_regression_loss: 18.0386 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.5691 - regression_loss: 18.4224 - val_loss: 23.8986 - val_regression_loss: 16.5218 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.4853 - regression_loss: 16.7614 - val_loss: 22.4928 - val_regression_loss: 15.3667 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.5504 - regression_loss: 15.5240 - val_loss: 21.1375 - val_regression_loss: 14.2710 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.1795 - regression_loss: 14.3154 - val_loss: 19.7906 - val_regression_loss: 13.2347 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.0019 - regression_loss: 13.1138 - val_loss: 18.5342 - val_regression_loss: 12.2967 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.0779 - regression_loss: 12.3213 - val_loss: 17.4913 - val_regression_loss: 11.5009 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.1565 - regression_loss: 11.4953 - val_loss: 16.4422 - val_regression_loss: 10.6802 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.1232 - regression_loss: 10.7003 - val_loss: 15.6175 - val_regression_loss: 10.0471 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.5348 - regression_loss: 10.0327 - val_loss: 15.0488 - val_regression_loss: 9.5931 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.0630 - regression_loss: 9.4337 - val_loss: 14.1795 - val_regression_loss: 8.9483 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.3368 - regression_loss: 8.8493 - val_loss: 13.7093 - val_regression_loss: 8.6018 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.8100 - regression_loss: 8.6802 - val_loss: 12.9516 - val_regression_loss: 8.0755 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.3670 - regression_loss: 8.0331 - val_loss: 12.4665 - val_regression_loss: 7.7094 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.9610 - regression_loss: 7.6632 - val_loss: 12.1883 - val_regression_loss: 7.4719 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.5009 - regression_loss: 7.3417 - val_loss: 11.6598 - val_regression_loss: 7.1088 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.1789 - regression_loss: 6.9195 - val_loss: 11.4926 - val_regression_loss: 6.9979 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.1809 - regression_loss: 6.8253 - val_loss: 11.0566 - val_regression_loss: 6.7335 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.8383 - regression_loss: 6.5964 - val_loss: 10.7651 - val_regression_loss: 6.5022 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5363 - regression_loss: 6.3453 - val_loss: 10.6809 - val_regression_loss: 6.4363 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5708 - regression_loss: 6.2866 - val_loss: 10.3251 - val_regression_loss: 6.2042 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3455 - regression_loss: 6.1437 - val_loss: 10.2675 - val_regression_loss: 6.1639 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1667 - regression_loss: 5.9866 - val_loss: 10.0978 - val_regression_loss: 6.0342 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.0949 - regression_loss: 5.9358 - val_loss: 10.0436 - val_regression_loss: 6.0009 - lr: 1.0000e-04\n",
      "Epoch 29/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 9.0414 - regression_loss: 5.8153 - val_loss: 9.8755 - val_regression_loss: 5.8969 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.8223 - regression_loss: 5.7201 - val_loss: 9.7985 - val_regression_loss: 5.8498 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6787 - regression_loss: 5.6248 - val_loss: 9.6601 - val_regression_loss: 5.7349 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.6190 - regression_loss: 5.5976 - val_loss: 9.6516 - val_regression_loss: 5.7268 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.6077 - regression_loss: 5.5125 - val_loss: 9.5247 - val_regression_loss: 5.6341 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5691 - regression_loss: 5.4835 - val_loss: 9.5197 - val_regression_loss: 5.6500 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.6439 - regression_loss: 5.4675 - val_loss: 9.3772 - val_regression_loss: 5.5226 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5094 - regression_loss: 5.3859 - val_loss: 9.3427 - val_regression_loss: 5.4954 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.6222 - regression_loss: 5.4157 - val_loss: 9.3049 - val_regression_loss: 5.4982 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4813 - regression_loss: 5.4059 - val_loss: 9.2381 - val_regression_loss: 5.4408 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2704 - regression_loss: 5.2869 - val_loss: 9.1820 - val_regression_loss: 5.3854 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3594 - regression_loss: 5.2657 - val_loss: 9.1715 - val_regression_loss: 5.3795 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2881 - regression_loss: 5.2149 - val_loss: 9.1004 - val_regression_loss: 5.3227 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2575 - regression_loss: 5.2096 - val_loss: 9.0666 - val_regression_loss: 5.2809 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3235 - regression_loss: 5.2091 - val_loss: 9.1016 - val_regression_loss: 5.3460 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2955 - regression_loss: 5.1554 - val_loss: 9.0419 - val_regression_loss: 5.2818 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3741 - regression_loss: 5.1930 - val_loss: 8.8987 - val_regression_loss: 5.1581 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2918 - regression_loss: 5.1429 - val_loss: 8.9804 - val_regression_loss: 5.2472 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2206 - regression_loss: 5.1434 - val_loss: 8.9885 - val_regression_loss: 5.2858 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.1362 - regression_loss: 5.0810 - val_loss: 8.9021 - val_regression_loss: 5.1822 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1778 - regression_loss: 5.0443 - val_loss: 8.8396 - val_regression_loss: 5.0930 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.0625 - regression_loss: 5.0555 - val_loss: 8.8650 - val_regression_loss: 5.1577 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.1550 - regression_loss: 5.1091 - val_loss: 9.0984 - val_regression_loss: 5.3519 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1672 - regression_loss: 5.1624 - val_loss: 8.9370 - val_regression_loss: 5.1986 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0857 - regression_loss: 5.1197 - val_loss: 9.0338 - val_regression_loss: 5.2999 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2272 - regression_loss: 5.1669 - val_loss: 8.9269 - val_regression_loss: 5.2154 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.1237 - regression_loss: 6.3905\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.1321 - regression_loss: 4.9804 - val_loss: 8.9465 - val_regression_loss: 5.2251 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0492 - regression_loss: 5.0131 - val_loss: 8.7558 - val_regression_loss: 5.0810 - lr: 5.0000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.1149 - regression_loss: 4.9983 - val_loss: 8.6963 - val_regression_loss: 5.0177 - lr: 5.0000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.0142 - regression_loss: 4.9019 - val_loss: 8.7363 - val_regression_loss: 5.0478 - lr: 5.0000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8952 - regression_loss: 4.8884 - val_loss: 8.6776 - val_regression_loss: 5.0045 - lr: 5.0000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8510 - regression_loss: 4.8548 - val_loss: 8.6658 - val_regression_loss: 4.9981 - lr: 5.0000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.0029 - regression_loss: 4.8682 - val_loss: 8.6636 - val_regression_loss: 5.0070 - lr: 5.0000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.9778 - regression_loss: 4.8822 - val_loss: 8.6401 - val_regression_loss: 4.9851 - lr: 5.0000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.9733 - regression_loss: 4.8780 - val_loss: 8.6875 - val_regression_loss: 5.0192 - lr: 5.0000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8602 - regression_loss: 4.8314 - val_loss: 8.6473 - val_regression_loss: 4.9854 - lr: 5.0000e-05\n",
      "Epoch 65/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.7045 - regression_loss: 5.9774\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8562 - regression_loss: 4.8652 - val_loss: 8.6578 - val_regression_loss: 4.9977 - lr: 5.0000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.9083 - regression_loss: 4.8135 - val_loss: 8.6339 - val_regression_loss: 4.9774 - lr: 2.5000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8648 - regression_loss: 4.8109 - val_loss: 8.6143 - val_regression_loss: 4.9633 - lr: 2.5000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.9137 - regression_loss: 4.8096 - val_loss: 8.6032 - val_regression_loss: 4.9530 - lr: 2.5000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8866 - regression_loss: 4.8001 - val_loss: 8.5894 - val_regression_loss: 4.9409 - lr: 2.5000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8415 - regression_loss: 4.8050 - val_loss: 8.5870 - val_regression_loss: 4.9389 - lr: 2.5000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8718 - regression_loss: 4.7962 - val_loss: 8.6009 - val_regression_loss: 4.9540 - lr: 2.5000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7494 - regression_loss: 4.7992 - val_loss: 8.5957 - val_regression_loss: 4.9473 - lr: 2.5000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8682 - regression_loss: 4.7967 - val_loss: 8.5819 - val_regression_loss: 4.9359 - lr: 2.5000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8574 - regression_loss: 4.7942 - val_loss: 8.5824 - val_regression_loss: 4.9375 - lr: 2.5000e-05\n",
      "Epoch 75/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8503 - regression_loss: 4.7830 - val_loss: 8.5743 - val_regression_loss: 4.9298 - lr: 2.5000e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7443 - regression_loss: 4.7810 - val_loss: 8.5772 - val_regression_loss: 4.9336 - lr: 2.5000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8438 - regression_loss: 4.7763 - val_loss: 8.5985 - val_regression_loss: 4.9579 - lr: 2.5000e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7650 - regression_loss: 4.7976 - val_loss: 8.5872 - val_regression_loss: 4.9489 - lr: 2.5000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8342 - regression_loss: 4.7850 - val_loss: 8.5901 - val_regression_loss: 4.9458 - lr: 2.5000e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8657 - regression_loss: 4.7958 - val_loss: 8.5478 - val_regression_loss: 4.9090 - lr: 2.5000e-05\n",
      "Epoch 81/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.7321 - regression_loss: 5.0100\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8650 - regression_loss: 4.7916 - val_loss: 8.5771 - val_regression_loss: 4.9390 - lr: 2.5000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.9125 - regression_loss: 4.7629 - val_loss: 8.5610 - val_regression_loss: 4.9215 - lr: 1.2500e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6252 - regression_loss: 4.7582 - val_loss: 8.5541 - val_regression_loss: 4.9172 - lr: 1.2500e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7511 - regression_loss: 4.7509 - val_loss: 8.5501 - val_regression_loss: 4.9136 - lr: 1.2500e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8884 - regression_loss: 4.7508 - val_loss: 8.5550 - val_regression_loss: 4.9194 - lr: 1.2500e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7728 - regression_loss: 4.7489 - val_loss: 8.5524 - val_regression_loss: 4.9176 - lr: 1.2500e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8060 - regression_loss: 4.7526 - val_loss: 8.5546 - val_regression_loss: 4.9200 - lr: 1.2500e-05\n",
      "Epoch 88/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.0201 - regression_loss: 4.2991\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8497 - regression_loss: 4.7471 - val_loss: 8.5475 - val_regression_loss: 4.9133 - lr: 1.2500e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7853 - regression_loss: 4.7458 - val_loss: 8.5392 - val_regression_loss: 4.9059 - lr: 6.2500e-06\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7715 - regression_loss: 4.7440 - val_loss: 8.5412 - val_regression_loss: 4.9075 - lr: 6.2500e-06\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8122 - regression_loss: 4.7411 - val_loss: 8.5401 - val_regression_loss: 4.9064 - lr: 6.2500e-06\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8578 - regression_loss: 4.7426 - val_loss: 8.5377 - val_regression_loss: 4.9039 - lr: 6.2500e-06\n",
      "Epoch 93/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.8128 - regression_loss: 5.0923\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7410 - regression_loss: 4.7386 - val_loss: 8.5358 - val_regression_loss: 4.9029 - lr: 6.2500e-06\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8322 - regression_loss: 4.7373 - val_loss: 8.5379 - val_regression_loss: 4.9048 - lr: 3.1250e-06\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8192 - regression_loss: 4.7375 - val_loss: 8.5380 - val_regression_loss: 4.9051 - lr: 3.1250e-06\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8580 - regression_loss: 4.7364 - val_loss: 8.5373 - val_regression_loss: 4.9052 - lr: 3.1250e-06\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8044 - regression_loss: 4.7364 - val_loss: 8.5388 - val_regression_loss: 4.9069 - lr: 3.1250e-06\n",
      "Epoch 98/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.8818 - regression_loss: 6.1616\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7883 - regression_loss: 4.7362 - val_loss: 8.5386 - val_regression_loss: 4.9074 - lr: 3.1250e-06\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7485 - regression_loss: 4.7357 - val_loss: 8.5377 - val_regression_loss: 4.9066 - lr: 1.5625e-06\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6837 - regression_loss: 4.7343 - val_loss: 8.5377 - val_regression_loss: 4.9066 - lr: 1.5625e-06\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8742 - regression_loss: 4.7345 - val_loss: 8.5371 - val_regression_loss: 4.9060 - lr: 1.5625e-06\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7667 - regression_loss: 4.7339 - val_loss: 8.5373 - val_regression_loss: 4.9062 - lr: 1.5625e-06\n",
      "Epoch 103/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.9107 - regression_loss: 5.1905\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7559 - regression_loss: 4.7341 - val_loss: 8.5366 - val_regression_loss: 4.9055 - lr: 1.5625e-06\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7480 - regression_loss: 4.7342 - val_loss: 8.5371 - val_regression_loss: 4.9060 - lr: 7.8125e-07\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7779 - regression_loss: 4.7331 - val_loss: 8.5365 - val_regression_loss: 4.9054 - lr: 7.8125e-07\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8721 - regression_loss: 4.7331 - val_loss: 8.5363 - val_regression_loss: 4.9052 - lr: 7.8125e-07\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8119 - regression_loss: 4.7331 - val_loss: 8.5362 - val_regression_loss: 4.9053 - lr: 7.8125e-07\n",
      "Epoch 108/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.4371 - regression_loss: 5.7171\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8077 - regression_loss: 4.7330 - val_loss: 8.5363 - val_regression_loss: 4.9054 - lr: 7.8125e-07\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7987 - regression_loss: 4.7327 - val_loss: 8.5361 - val_regression_loss: 4.9052 - lr: 3.9062e-07\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7371 - regression_loss: 4.7327 - val_loss: 8.5362 - val_regression_loss: 4.9053 - lr: 3.9062e-07\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6827 - regression_loss: 4.7327 - val_loss: 8.5360 - val_regression_loss: 4.9052 - lr: 3.9062e-07\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7675 - regression_loss: 4.7326 - val_loss: 8.5361 - val_regression_loss: 4.9052 - lr: 3.9062e-07\n",
      "Epoch 113/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.5954 - regression_loss: 5.8754\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7649 - regression_loss: 4.7325 - val_loss: 8.5360 - val_regression_loss: 4.9052 - lr: 3.9062e-07\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7609 - regression_loss: 4.7323 - val_loss: 8.5360 - val_regression_loss: 4.9052 - lr: 1.9531e-07\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8004 - regression_loss: 4.7323 - val_loss: 8.5360 - val_regression_loss: 4.9051 - lr: 1.9531e-07\n",
      "Epoch 116/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7846 - regression_loss: 4.7324 - val_loss: 8.5358 - val_regression_loss: 4.9050 - lr: 1.9531e-07\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7516 - regression_loss: 4.7323 - val_loss: 8.5359 - val_regression_loss: 4.9051 - lr: 1.9531e-07\n",
      "Epoch 118/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.9304 - regression_loss: 3.2104\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6851 - regression_loss: 4.7323 - val_loss: 8.5358 - val_regression_loss: 4.9050 - lr: 1.9531e-07\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6875 - regression_loss: 4.7322 - val_loss: 8.5358 - val_regression_loss: 4.9050 - lr: 9.7656e-08\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8272 - regression_loss: 4.7322 - val_loss: 8.5358 - val_regression_loss: 4.9050 - lr: 9.7656e-08\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8216 - regression_loss: 4.7322 - val_loss: 8.5358 - val_regression_loss: 4.9049 - lr: 9.7656e-08\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7289 - regression_loss: 4.7322 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 9.7656e-08\n",
      "Epoch 123/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.4849 - regression_loss: 5.7649\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8042 - regression_loss: 4.7321 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 9.7656e-08\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7860 - regression_loss: 4.7321 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 4.8828e-08\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7896 - regression_loss: 4.7321 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 4.8828e-08\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7834 - regression_loss: 4.7321 - val_loss: 8.5358 - val_regression_loss: 4.9050 - lr: 4.8828e-08\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7523 - regression_loss: 4.7321 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 4.8828e-08\n",
      "Epoch 128/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.8665 - regression_loss: 5.1466\n",
      "Epoch 128: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8007 - regression_loss: 4.7321 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 4.8828e-08\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8651 - regression_loss: 4.7321 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 2.4414e-08\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7619 - regression_loss: 4.7321 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 2.4414e-08\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8318 - regression_loss: 4.7321 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 2.4414e-08\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7800 - regression_loss: 4.7321 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 2.4414e-08\n",
      "Epoch 133/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.5734 - regression_loss: 4.8534\n",
      "Epoch 133: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6696 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 2.4414e-08\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7810 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 1.2207e-08\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8236 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 1.2207e-08\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7960 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 1.2207e-08\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8191 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 1.2207e-08\n",
      "Epoch 138/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.4554 - regression_loss: 5.7354\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8280 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 1.2207e-08\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8215 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 6.1035e-09\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7434 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 6.1035e-09\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7673 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 6.1035e-09\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6831 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 6.1035e-09\n",
      "Epoch 143/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.1157 - regression_loss: 5.3957\n",
      "Epoch 143: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8190 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 6.1035e-09\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8778 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 3.0518e-09\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7043 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 3.0518e-09\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7409 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 3.0518e-09\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8272 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 3.0518e-09\n",
      "Epoch 148/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.9554 - regression_loss: 5.2354\n",
      "Epoch 148: ReduceLROnPlateau reducing learning rate to 1.5258788677030566e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6656 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 3.0518e-09\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6898 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 1.5259e-09\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8071 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 1.5259e-09\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7983 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 1.5259e-09\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7993 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 1.5259e-09\n",
      "Epoch 153/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.5893 - regression_loss: 3.8693\n",
      "Epoch 153: ReduceLROnPlateau reducing learning rate to 7.629394338515283e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7851 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 1.5259e-09\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8037 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 7.6294e-10\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6625 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 7.6294e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7603 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 7.6294e-10\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6826 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 7.6294e-10\n",
      "Epoch 158/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.3699 - regression_loss: 6.6499\n",
      "Epoch 158: ReduceLROnPlateau reducing learning rate to 3.8146971692576415e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7544 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 7.6294e-10\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7086 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 3.8147e-10\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7513 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 3.8147e-10\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8062 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 3.8147e-10\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7887 - regression_loss: 4.7320 - val_loss: 8.5357 - val_regression_loss: 4.9049 - lr: 3.8147e-10\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 84.7412 - regression_loss: 76.0493 - val_loss: 44.8128 - val_regression_loss: 34.5508 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 52.6136 - regression_loss: 46.7841 - val_loss: 37.8786 - val_regression_loss: 28.9094 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 41.7458 - regression_loss: 36.6801 - val_loss: 37.2992 - val_regression_loss: 28.3570 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 37.6965 - regression_loss: 32.5170 - val_loss: 36.4182 - val_regression_loss: 27.5155 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 34.7257 - regression_loss: 30.1602 - val_loss: 35.0504 - val_regression_loss: 26.3056 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.6474 - regression_loss: 28.0453 - val_loss: 34.2799 - val_regression_loss: 25.0675 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.2949 - regression_loss: 26.7190 - val_loss: 32.7594 - val_regression_loss: 23.9480 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.5786 - regression_loss: 25.8072 - val_loss: 31.8970 - val_regression_loss: 23.2353 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.0908 - regression_loss: 24.6442 - val_loss: 31.5803 - val_regression_loss: 22.7872 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.8360 - regression_loss: 23.9514 - val_loss: 30.8512 - val_regression_loss: 22.3787 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.6119 - regression_loss: 23.3145 - val_loss: 30.4975 - val_regression_loss: 22.2715 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.9386 - regression_loss: 22.5061 - val_loss: 30.3962 - val_regression_loss: 21.9850 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.7670 - regression_loss: 22.0062 - val_loss: 29.8525 - val_regression_loss: 21.6610 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.6681 - regression_loss: 21.4378 - val_loss: 29.2319 - val_regression_loss: 21.3030 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.0776 - regression_loss: 20.9026 - val_loss: 28.7577 - val_regression_loss: 20.9345 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.3820 - regression_loss: 20.4050 - val_loss: 28.1457 - val_regression_loss: 20.6116 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.4874 - regression_loss: 20.2609 - val_loss: 27.8860 - val_regression_loss: 20.3005 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.6438 - regression_loss: 19.4922 - val_loss: 27.3630 - val_regression_loss: 20.0641 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.7609 - regression_loss: 19.1701 - val_loss: 27.1919 - val_regression_loss: 19.9217 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.7317 - regression_loss: 18.6389 - val_loss: 27.1242 - val_regression_loss: 19.7945 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.1640 - regression_loss: 18.4740 - val_loss: 26.5956 - val_regression_loss: 19.5465 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.3195 - regression_loss: 18.1509 - val_loss: 26.2773 - val_regression_loss: 19.3463 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.5182 - regression_loss: 17.7457 - val_loss: 26.1845 - val_regression_loss: 19.1790 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.2507 - regression_loss: 17.6138 - val_loss: 25.6768 - val_regression_loss: 18.9041 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.2600 - regression_loss: 17.2612 - val_loss: 25.3126 - val_regression_loss: 18.7151 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.3482 - regression_loss: 17.1081 - val_loss: 25.1936 - val_regression_loss: 18.5215 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8962 - regression_loss: 16.9433 - val_loss: 24.9509 - val_regression_loss: 18.4377 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.3913 - regression_loss: 16.7425 - val_loss: 24.8625 - val_regression_loss: 18.3413 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.1314 - regression_loss: 16.5548 - val_loss: 24.8435 - val_regression_loss: 18.3088 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5615 - regression_loss: 16.5264 - val_loss: 24.6511 - val_regression_loss: 18.2282 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.1974 - regression_loss: 16.3123 - val_loss: 24.4663 - val_regression_loss: 17.9600 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.2324 - regression_loss: 16.1491 - val_loss: 24.0138 - val_regression_loss: 17.7625 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.8273 - regression_loss: 16.1096 - val_loss: 24.1958 - val_regression_loss: 17.7690 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.1630 - regression_loss: 16.1331 - val_loss: 24.0277 - val_regression_loss: 17.7405 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.7625 - regression_loss: 15.8619 - val_loss: 24.0356 - val_regression_loss: 17.6936 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.5616 - regression_loss: 15.7534 - val_loss: 23.7670 - val_regression_loss: 17.5177 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.6039 - regression_loss: 15.7484 - val_loss: 23.6743 - val_regression_loss: 17.4027 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.2742 - regression_loss: 15.6779 - val_loss: 23.6090 - val_regression_loss: 17.3798 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1414 - regression_loss: 15.5168 - val_loss: 23.6196 - val_regression_loss: 17.4265 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.9524 - regression_loss: 15.3908 - val_loss: 23.6127 - val_regression_loss: 17.2957 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1015 - regression_loss: 15.4730 - val_loss: 23.2882 - val_regression_loss: 17.0947 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1620 - regression_loss: 15.3332 - val_loss: 23.3291 - val_regression_loss: 17.1103 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.7538 - regression_loss: 15.2405 - val_loss: 23.4402 - val_regression_loss: 17.2633 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.0132 - regression_loss: 15.1982 - val_loss: 23.5326 - val_regression_loss: 17.2265 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.6562 - regression_loss: 15.0578 - val_loss: 23.1476 - val_regression_loss: 17.0144 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.9554 - regression_loss: 15.0632 - val_loss: 23.2053 - val_regression_loss: 17.0524 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.5259 - regression_loss: 15.0452 - val_loss: 23.2243 - val_regression_loss: 16.9648 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.7781 - regression_loss: 14.9590 - val_loss: 23.3367 - val_regression_loss: 17.1761 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.7586 - regression_loss: 14.8971 - val_loss: 23.0921 - val_regression_loss: 16.8970 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.4711 - regression_loss: 14.6926 - val_loss: 23.0363 - val_regression_loss: 16.9708 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.4416 - regression_loss: 14.7092 - val_loss: 23.1678 - val_regression_loss: 16.9752 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.3861 - regression_loss: 14.6887 - val_loss: 23.1940 - val_regression_loss: 16.9813 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.6515 - regression_loss: 14.7438 - val_loss: 23.2420 - val_regression_loss: 16.9967 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.1669 - regression_loss: 14.5221 - val_loss: 23.1797 - val_regression_loss: 16.9893 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.4032 - regression_loss: 14.5714 - val_loss: 23.3232 - val_regression_loss: 17.0786 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.0687 - regression_loss: 14.4380 - val_loss: 23.0026 - val_regression_loss: 16.8897 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.0092 - regression_loss: 14.4590 - val_loss: 22.8210 - val_regression_loss: 16.7016 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.0159 - regression_loss: 14.2740 - val_loss: 23.3601 - val_regression_loss: 17.0413 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.0492 - regression_loss: 14.3338 - val_loss: 23.4362 - val_regression_loss: 17.2304 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.0936 - regression_loss: 14.2651 - val_loss: 23.3320 - val_regression_loss: 17.0311 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.9616 - regression_loss: 14.3775 - val_loss: 23.2632 - val_regression_loss: 17.2017 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.1596 - regression_loss: 14.4228 - val_loss: 23.2431 - val_regression_loss: 16.9480 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.2144 - regression_loss: 14.5606 - val_loss: 23.4848 - val_regression_loss: 17.2970 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.3646 - regression_loss: 14.7318 - val_loss: 23.2843 - val_regression_loss: 16.9966 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.4292 - regression_loss: 14.7602 - val_loss: 23.0330 - val_regression_loss: 16.9343 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 17.3880 - regression_loss: 14.7027\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.1608 - regression_loss: 14.3753 - val_loss: 23.0839 - val_regression_loss: 16.9439 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4258 - regression_loss: 13.8872 - val_loss: 23.2102 - val_regression_loss: 16.9279 - lr: 5.0000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5239 - regression_loss: 14.0141 - val_loss: 23.1898 - val_regression_loss: 16.9591 - lr: 5.0000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.5722 - regression_loss: 13.9070 - val_loss: 23.1615 - val_regression_loss: 16.9698 - lr: 5.0000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.4490 - regression_loss: 13.8542 - val_loss: 23.0202 - val_regression_loss: 16.8734 - lr: 5.0000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5143 - regression_loss: 13.8607 - val_loss: 22.9635 - val_regression_loss: 16.8093 - lr: 5.0000e-05\n",
      "Epoch 72/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.1799 - regression_loss: 12.4967\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.5662 - regression_loss: 13.8323 - val_loss: 23.0316 - val_regression_loss: 16.8759 - lr: 5.0000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.5752 - regression_loss: 13.8143 - val_loss: 23.0448 - val_regression_loss: 16.8737 - lr: 2.5000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4185 - regression_loss: 13.7978 - val_loss: 23.0490 - val_regression_loss: 16.8845 - lr: 2.5000e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.2234 - regression_loss: 13.7912 - val_loss: 23.0335 - val_regression_loss: 16.8785 - lr: 2.5000e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4564 - regression_loss: 13.7798 - val_loss: 23.0312 - val_regression_loss: 16.8643 - lr: 2.5000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2640 - regression_loss: 13.7933 - val_loss: 23.0755 - val_regression_loss: 16.8963 - lr: 2.5000e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6242 - regression_loss: 13.7677 - val_loss: 23.0429 - val_regression_loss: 16.8769 - lr: 2.5000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5873 - regression_loss: 13.7618 - val_loss: 23.0473 - val_regression_loss: 16.8805 - lr: 2.5000e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.0937 - regression_loss: 13.7511 - val_loss: 23.0866 - val_regression_loss: 16.9121 - lr: 2.5000e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4102 - regression_loss: 13.7507 - val_loss: 23.0600 - val_regression_loss: 16.8975 - lr: 2.5000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.1816 - regression_loss: 13.7302 - val_loss: 23.0518 - val_regression_loss: 16.8970 - lr: 2.5000e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.0352 - regression_loss: 13.7384 - val_loss: 23.0397 - val_regression_loss: 16.8671 - lr: 2.5000e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2913 - regression_loss: 13.7099 - val_loss: 23.0246 - val_regression_loss: 16.8665 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.0984 - regression_loss: 13.7010 - val_loss: 23.0250 - val_regression_loss: 16.8720 - lr: 2.5000e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.0868 - regression_loss: 13.6963 - val_loss: 23.0588 - val_regression_loss: 16.8964 - lr: 2.5000e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 17.3882 - regression_loss: 13.6973 - val_loss: 23.0404 - val_regression_loss: 16.8850 - lr: 2.5000e-05\n",
      "Epoch 88/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 21.6993 - regression_loss: 19.0184\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.4956 - regression_loss: 13.7407 - val_loss: 23.1105 - val_regression_loss: 16.9178 - lr: 2.5000e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.0876 - regression_loss: 13.6717 - val_loss: 23.0952 - val_regression_loss: 16.9249 - lr: 1.2500e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2540 - regression_loss: 13.6633 - val_loss: 23.0932 - val_regression_loss: 16.9311 - lr: 1.2500e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.3981 - regression_loss: 13.6644 - val_loss: 23.0909 - val_regression_loss: 16.9236 - lr: 1.2500e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.0134 - regression_loss: 13.6624 - val_loss: 23.0599 - val_regression_loss: 16.9028 - lr: 1.2500e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.2384 - regression_loss: 13.6552 - val_loss: 23.0685 - val_regression_loss: 16.8919 - lr: 1.2500e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.3082 - regression_loss: 13.6502 - val_loss: 23.0763 - val_regression_loss: 16.9056 - lr: 1.2500e-05\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.2368 - regression_loss: 13.6479 - val_loss: 23.0891 - val_regression_loss: 16.9186 - lr: 1.2500e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.3825 - regression_loss: 13.6489 - val_loss: 23.0490 - val_regression_loss: 16.8817 - lr: 1.2500e-05\n",
      "Epoch 97/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 20.1554 - regression_loss: 17.4752\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.6363 - regression_loss: 13.6563 - val_loss: 23.0461 - val_regression_loss: 16.8952 - lr: 1.2500e-05\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 101.7557 - regression_loss: 90.7165 - val_loss: 53.3923 - val_regression_loss: 44.1266 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 66.8252 - regression_loss: 59.3859 - val_loss: 36.5454 - val_regression_loss: 30.7442 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 46.6269 - regression_loss: 40.8595 - val_loss: 28.0767 - val_regression_loss: 23.1323 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 35.4992 - regression_loss: 30.2501 - val_loss: 23.8578 - val_regression_loss: 19.1890 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.4845 - regression_loss: 24.9331 - val_loss: 21.7495 - val_regression_loss: 17.3047 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.1088 - regression_loss: 21.4793 - val_loss: 20.3320 - val_regression_loss: 16.0319 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.2417 - regression_loss: 19.0939 - val_loss: 18.6173 - val_regression_loss: 14.4097 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.1457 - regression_loss: 17.2055 - val_loss: 17.1725 - val_regression_loss: 13.1715 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.4609 - regression_loss: 15.4677 - val_loss: 15.5097 - val_regression_loss: 11.7175 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.3978 - regression_loss: 13.9946 - val_loss: 14.2065 - val_regression_loss: 10.5868 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.5210 - regression_loss: 12.7951 - val_loss: 13.3600 - val_regression_loss: 9.8794 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.1259 - regression_loss: 11.7710 - val_loss: 12.4679 - val_regression_loss: 9.1409 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.4625 - regression_loss: 10.7748 - val_loss: 11.7043 - val_regression_loss: 8.4174 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.4499 - regression_loss: 10.1014 - val_loss: 11.3020 - val_regression_loss: 8.0847 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.8466 - regression_loss: 9.4746 - val_loss: 10.7935 - val_regression_loss: 7.6230 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.4176 - regression_loss: 8.8957 - val_loss: 10.2401 - val_regression_loss: 7.1430 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.4979 - regression_loss: 8.4108 - val_loss: 9.7970 - val_regression_loss: 6.7799 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.9807 - regression_loss: 7.9093 - val_loss: 9.4301 - val_regression_loss: 6.4078 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.7529 - regression_loss: 7.5262 - val_loss: 9.2128 - val_regression_loss: 6.1876 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0481 - regression_loss: 7.1620 - val_loss: 8.8191 - val_regression_loss: 5.8447 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.0473 - regression_loss: 6.8794 - val_loss: 8.6022 - val_regression_loss: 5.6437 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.9225 - regression_loss: 6.6079 - val_loss: 8.4471 - val_regression_loss: 5.4833 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5132 - regression_loss: 6.3246 - val_loss: 8.1976 - val_regression_loss: 5.2503 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.1563 - regression_loss: 6.1606 - val_loss: 8.1583 - val_regression_loss: 5.2064 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.1707 - regression_loss: 5.9699 - val_loss: 7.7883 - val_regression_loss: 4.8573 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.7927 - regression_loss: 5.7044 - val_loss: 7.7504 - val_regression_loss: 4.7973 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.5794 - regression_loss: 5.5210 - val_loss: 7.5183 - val_regression_loss: 4.6014 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.4536 - regression_loss: 5.3888 - val_loss: 7.4648 - val_regression_loss: 4.5388 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2953 - regression_loss: 5.2915 - val_loss: 7.2606 - val_regression_loss: 4.3506 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.1739 - regression_loss: 5.0924 - val_loss: 7.1750 - val_regression_loss: 4.2505 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0831 - regression_loss: 4.9416 - val_loss: 7.1461 - val_regression_loss: 4.2210 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8233 - regression_loss: 4.7915 - val_loss: 6.9899 - val_regression_loss: 4.0748 - lr: 1.0000e-04\n",
      "Epoch 33/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 7.5516 - regression_loss: 4.6357 - val_loss: 6.9613 - val_regression_loss: 4.0300 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7221 - regression_loss: 4.5810 - val_loss: 6.7981 - val_regression_loss: 3.8820 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4833 - regression_loss: 4.4298 - val_loss: 6.7773 - val_regression_loss: 3.8392 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.2756 - regression_loss: 4.3202 - val_loss: 6.6740 - val_regression_loss: 3.7467 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.3212 - regression_loss: 4.2746 - val_loss: 6.5629 - val_regression_loss: 3.6422 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0965 - regression_loss: 4.1416 - val_loss: 6.5978 - val_regression_loss: 3.6574 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.8592 - regression_loss: 4.0497 - val_loss: 6.4612 - val_regression_loss: 3.5456 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0257 - regression_loss: 4.0064 - val_loss: 6.4434 - val_regression_loss: 3.5087 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.8424 - regression_loss: 3.8983 - val_loss: 6.2933 - val_regression_loss: 3.3667 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8381 - regression_loss: 3.8955 - val_loss: 6.4951 - val_regression_loss: 3.5323 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.7494 - regression_loss: 3.7496 - val_loss: 6.2378 - val_regression_loss: 3.3303 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5880 - regression_loss: 3.7063 - val_loss: 6.2262 - val_regression_loss: 3.3019 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.3052 - regression_loss: 3.6065 - val_loss: 6.1119 - val_regression_loss: 3.1660 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4683 - regression_loss: 3.5166 - val_loss: 6.0367 - val_regression_loss: 3.1043 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.4181 - regression_loss: 3.4457 - val_loss: 6.0556 - val_regression_loss: 3.1238 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2012 - regression_loss: 3.4479 - val_loss: 5.9199 - val_regression_loss: 3.0059 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3590 - regression_loss: 3.5312 - val_loss: 6.2971 - val_regression_loss: 3.2967 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.3739 - regression_loss: 3.4525 - val_loss: 5.9047 - val_regression_loss: 2.9813 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.3226 - regression_loss: 3.3693 - val_loss: 6.1662 - val_regression_loss: 3.1982 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.2438 - regression_loss: 3.2881 - val_loss: 5.7445 - val_regression_loss: 2.8118 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.0610 - regression_loss: 3.1401 - val_loss: 5.9950 - val_regression_loss: 3.0561 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.9506 - regression_loss: 3.1086 - val_loss: 5.6696 - val_regression_loss: 2.7484 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.8848 - regression_loss: 3.0238 - val_loss: 5.9057 - val_regression_loss: 2.9563 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7630 - regression_loss: 2.9246 - val_loss: 5.6304 - val_regression_loss: 2.6947 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7600 - regression_loss: 2.9354 - val_loss: 5.7982 - val_regression_loss: 2.8283 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7462 - regression_loss: 2.9027 - val_loss: 5.6458 - val_regression_loss: 2.7102 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6462 - regression_loss: 2.8153 - val_loss: 5.5991 - val_regression_loss: 2.6825 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6387 - regression_loss: 2.7465 - val_loss: 5.5205 - val_regression_loss: 2.5879 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.5862 - regression_loss: 2.7180 - val_loss: 5.5055 - val_regression_loss: 2.5607 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5011 - regression_loss: 2.6636 - val_loss: 5.6426 - val_regression_loss: 2.6984 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.4225 - regression_loss: 2.6328 - val_loss: 5.4800 - val_regression_loss: 2.5556 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.4213 - regression_loss: 2.5664 - val_loss: 5.3809 - val_regression_loss: 2.4509 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.3975 - regression_loss: 2.5442 - val_loss: 5.3321 - val_regression_loss: 2.4046 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.3326 - regression_loss: 2.5173 - val_loss: 5.4214 - val_regression_loss: 2.4706 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.3682 - regression_loss: 2.4917 - val_loss: 5.3448 - val_regression_loss: 2.4140 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.3070 - regression_loss: 2.5080 - val_loss: 5.9332 - val_regression_loss: 2.9326 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.4092 - regression_loss: 2.5728 - val_loss: 5.1868 - val_regression_loss: 2.2507 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.2574 - regression_loss: 2.4272 - val_loss: 5.5789 - val_regression_loss: 2.6095 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.1553 - regression_loss: 2.3629 - val_loss: 5.2654 - val_regression_loss: 2.3336 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2619 - regression_loss: 2.4766 - val_loss: 5.8934 - val_regression_loss: 2.8452 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1775 - regression_loss: 2.3355 - val_loss: 5.3042 - val_regression_loss: 2.3438 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.3378 - regression_loss: 2.4805 - val_loss: 5.9529 - val_regression_loss: 2.9080 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2327 - regression_loss: 2.3749 - val_loss: 5.2249 - val_regression_loss: 2.2601 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.3096 - regression_loss: 2.6550\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.1620 - regression_loss: 2.3982 - val_loss: 6.2787 - val_regression_loss: 3.1934 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.3105 - regression_loss: 2.4778 - val_loss: 5.0671 - val_regression_loss: 2.1269 - lr: 5.0000e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.2459 - regression_loss: 2.4304 - val_loss: 5.2416 - val_regression_loss: 2.2982 - lr: 5.0000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0564 - regression_loss: 2.2716 - val_loss: 5.3066 - val_regression_loss: 2.3588 - lr: 5.0000e-05\n",
      "Epoch 80/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9454 - regression_loss: 2.1205 - val_loss: 5.0820 - val_regression_loss: 2.1588 - lr: 5.0000e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9097 - regression_loss: 2.1529 - val_loss: 5.2643 - val_regression_loss: 2.3181 - lr: 5.0000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9354 - regression_loss: 2.1149 - val_loss: 5.1083 - val_regression_loss: 2.1645 - lr: 5.0000e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9088 - regression_loss: 2.0870 - val_loss: 5.1311 - val_regression_loss: 2.1899 - lr: 5.0000e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9121 - regression_loss: 2.0793 - val_loss: 5.0905 - val_regression_loss: 2.1600 - lr: 5.0000e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8069 - regression_loss: 2.0826 - val_loss: 5.1242 - val_regression_loss: 2.1953 - lr: 5.0000e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.7882 - regression_loss: 2.0394 - val_loss: 5.0237 - val_regression_loss: 2.0884 - lr: 5.0000e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8498 - regression_loss: 2.0469 - val_loss: 5.1229 - val_regression_loss: 2.1765 - lr: 5.0000e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7937 - regression_loss: 2.0233 - val_loss: 5.0725 - val_regression_loss: 2.1331 - lr: 5.0000e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.7726 - regression_loss: 2.0079 - val_loss: 5.1064 - val_regression_loss: 2.1696 - lr: 5.0000e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8586 - regression_loss: 2.0122 - val_loss: 5.0781 - val_regression_loss: 2.1483 - lr: 5.0000e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.7888 - regression_loss: 1.9849 - val_loss: 4.9939 - val_regression_loss: 2.0575 - lr: 5.0000e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.7707 - regression_loss: 1.9947 - val_loss: 5.0512 - val_regression_loss: 2.1027 - lr: 5.0000e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6621 - regression_loss: 1.9621 - val_loss: 5.0996 - val_regression_loss: 2.1559 - lr: 5.0000e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.7205 - regression_loss: 1.9605 - val_loss: 5.0266 - val_regression_loss: 2.1010 - lr: 5.0000e-05\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7373 - regression_loss: 1.9618 - val_loss: 5.0094 - val_regression_loss: 2.0753 - lr: 5.0000e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.7829 - regression_loss: 1.9456 - val_loss: 5.0087 - val_regression_loss: 2.0692 - lr: 5.0000e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.7298 - regression_loss: 1.9469 - val_loss: 5.0384 - val_regression_loss: 2.1103 - lr: 5.0000e-05\n",
      "Epoch 98/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.4535 - regression_loss: 1.8055\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6746 - regression_loss: 1.9186 - val_loss: 5.0553 - val_regression_loss: 2.1118 - lr: 5.0000e-05\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.7078 - regression_loss: 1.9139 - val_loss: 4.9810 - val_regression_loss: 2.0394 - lr: 2.5000e-05\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6374 - regression_loss: 1.8909 - val_loss: 5.0173 - val_regression_loss: 2.0803 - lr: 2.5000e-05\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6015 - regression_loss: 1.9005 - val_loss: 5.0641 - val_regression_loss: 2.1259 - lr: 2.5000e-05\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6748 - regression_loss: 1.8871 - val_loss: 4.9713 - val_regression_loss: 2.0412 - lr: 2.5000e-05\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6599 - regression_loss: 1.8909 - val_loss: 5.0061 - val_regression_loss: 2.0664 - lr: 2.5000e-05\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6455 - regression_loss: 1.8797 - val_loss: 5.0143 - val_regression_loss: 2.0754 - lr: 2.5000e-05\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6635 - regression_loss: 1.8857 - val_loss: 4.9436 - val_regression_loss: 2.0144 - lr: 2.5000e-05\n",
      "Epoch 106/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.3138 - regression_loss: 1.6673\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6650 - regression_loss: 1.8653 - val_loss: 5.0304 - val_regression_loss: 2.0930 - lr: 2.5000e-05\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.5826 - regression_loss: 1.8611 - val_loss: 5.0342 - val_regression_loss: 2.0939 - lr: 1.2500e-05\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6298 - regression_loss: 1.8544 - val_loss: 4.9901 - val_regression_loss: 2.0543 - lr: 1.2500e-05\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.5977 - regression_loss: 1.8491 - val_loss: 4.9731 - val_regression_loss: 2.0397 - lr: 1.2500e-05\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6075 - regression_loss: 1.8464 - val_loss: 4.9662 - val_regression_loss: 2.0321 - lr: 1.2500e-05\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6142 - regression_loss: 1.8460 - val_loss: 4.9629 - val_regression_loss: 2.0253 - lr: 1.2500e-05\n",
      "Epoch 112/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.9503 - regression_loss: 2.3043\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6271 - regression_loss: 1.8446 - val_loss: 4.9729 - val_regression_loss: 2.0370 - lr: 1.2500e-05\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6005 - regression_loss: 1.8426 - val_loss: 5.0002 - val_regression_loss: 2.0628 - lr: 6.2500e-06\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6323 - regression_loss: 1.8378 - val_loss: 4.9805 - val_regression_loss: 2.0452 - lr: 6.2500e-06\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6432 - regression_loss: 1.8353 - val_loss: 4.9818 - val_regression_loss: 2.0468 - lr: 6.2500e-06\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6169 - regression_loss: 1.8333 - val_loss: 4.9672 - val_regression_loss: 2.0335 - lr: 6.2500e-06\n",
      "Epoch 117/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.2344 - regression_loss: 2.5888\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6308 - regression_loss: 1.8370 - val_loss: 4.9543 - val_regression_loss: 2.0193 - lr: 6.2500e-06\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6158 - regression_loss: 1.8305 - val_loss: 4.9586 - val_regression_loss: 2.0234 - lr: 3.1250e-06\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.5464 - regression_loss: 1.8316 - val_loss: 4.9723 - val_regression_loss: 2.0363 - lr: 3.1250e-06\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6084 - regression_loss: 1.8290 - val_loss: 4.9750 - val_regression_loss: 2.0384 - lr: 3.1250e-06\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6073 - regression_loss: 1.8285 - val_loss: 4.9734 - val_regression_loss: 2.0373 - lr: 3.1250e-06\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6339 - regression_loss: 1.8290 - val_loss: 4.9658 - val_regression_loss: 2.0304 - lr: 3.1250e-06\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6103 - regression_loss: 1.8269 - val_loss: 4.9652 - val_regression_loss: 2.0297 - lr: 3.1250e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.7004 - regression_loss: 2.0549\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6090 - regression_loss: 1.8265 - val_loss: 4.9696 - val_regression_loss: 2.0345 - lr: 3.1250e-06\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6257 - regression_loss: 1.8264 - val_loss: 4.9729 - val_regression_loss: 2.0374 - lr: 1.5625e-06\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6113 - regression_loss: 1.8260 - val_loss: 4.9675 - val_regression_loss: 2.0318 - lr: 1.5625e-06\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6314 - regression_loss: 1.8249 - val_loss: 4.9658 - val_regression_loss: 2.0306 - lr: 1.5625e-06\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.5851 - regression_loss: 1.8250 - val_loss: 4.9643 - val_regression_loss: 2.0297 - lr: 1.5625e-06\n",
      "Epoch 129/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.5938 - regression_loss: 1.9484\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.5898 - regression_loss: 1.8241 - val_loss: 4.9634 - val_regression_loss: 2.0286 - lr: 1.5625e-06\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6232 - regression_loss: 1.8236 - val_loss: 4.9643 - val_regression_loss: 2.0294 - lr: 7.8125e-07\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6304 - regression_loss: 1.8235 - val_loss: 4.9654 - val_regression_loss: 2.0305 - lr: 7.8125e-07\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.5650 - regression_loss: 1.8234 - val_loss: 4.9652 - val_regression_loss: 2.0300 - lr: 7.8125e-07\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.5594 - regression_loss: 1.8234 - val_loss: 4.9664 - val_regression_loss: 2.0311 - lr: 7.8125e-07\n",
      "Epoch 134/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.7026 - regression_loss: 2.0572\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6008 - regression_loss: 1.8230 - val_loss: 4.9658 - val_regression_loss: 2.0306 - lr: 7.8125e-07\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5138 - regression_loss: 1.8228 - val_loss: 4.9655 - val_regression_loss: 2.0303 - lr: 3.9062e-07\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6234 - regression_loss: 1.8228 - val_loss: 4.9645 - val_regression_loss: 2.0294 - lr: 3.9062e-07\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5773 - regression_loss: 1.8228 - val_loss: 4.9657 - val_regression_loss: 2.0306 - lr: 3.9062e-07\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6210 - regression_loss: 1.8225 - val_loss: 4.9660 - val_regression_loss: 2.0308 - lr: 3.9062e-07\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.5975 - regression_loss: 1.8225 - val_loss: 4.9654 - val_regression_loss: 2.0302 - lr: 3.9062e-07\n",
      "Epoch 140/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.7854 - regression_loss: 2.1400\n",
      "Epoch 140: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6003 - regression_loss: 1.8223 - val_loss: 4.9652 - val_regression_loss: 2.0300 - lr: 3.9062e-07\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6224 - regression_loss: 1.8222 - val_loss: 4.9650 - val_regression_loss: 2.0298 - lr: 1.9531e-07\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.5636 - regression_loss: 1.8222 - val_loss: 4.9652 - val_regression_loss: 2.0300 - lr: 1.9531e-07\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6106 - regression_loss: 1.8221 - val_loss: 4.9653 - val_regression_loss: 2.0301 - lr: 1.9531e-07\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5642 - regression_loss: 1.8221 - val_loss: 4.9650 - val_regression_loss: 2.0298 - lr: 1.9531e-07\n",
      "Epoch 145/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.1124 - regression_loss: 1.4671\n",
      "Epoch 145: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.5754 - regression_loss: 1.8220 - val_loss: 4.9652 - val_regression_loss: 2.0300 - lr: 1.9531e-07\n",
      "3/3 [==============================] - 0s 947us/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 26ms/step - loss: 115.9456 - regression_loss: 105.0346 - val_loss: 64.0498 - val_regression_loss: 49.6098 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 70.7144 - regression_loss: 62.0181 - val_loss: 46.5360 - val_regression_loss: 35.2244 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 47.5892 - regression_loss: 42.7687 - val_loss: 39.0286 - val_regression_loss: 29.1161 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 41.6230 - regression_loss: 36.3586 - val_loss: 36.2776 - val_regression_loss: 26.7279 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 37.3860 - regression_loss: 32.1648 - val_loss: 34.5029 - val_regression_loss: 25.0397 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 33.8157 - regression_loss: 29.3620 - val_loss: 32.6211 - val_regression_loss: 23.3496 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31.5554 - regression_loss: 26.8522 - val_loss: 29.9594 - val_regression_loss: 21.2510 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30.1926 - regression_loss: 25.5024 - val_loss: 28.3574 - val_regression_loss: 20.0345 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.9324 - regression_loss: 23.8555 - val_loss: 28.1330 - val_regression_loss: 19.7701 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.8593 - regression_loss: 22.7862 - val_loss: 27.3443 - val_regression_loss: 19.1766 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.3782 - regression_loss: 21.8730 - val_loss: 26.6582 - val_regression_loss: 18.6542 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.7807 - regression_loss: 21.2393 - val_loss: 26.6334 - val_regression_loss: 18.5758 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.7171 - regression_loss: 20.6356 - val_loss: 26.1159 - val_regression_loss: 18.1759 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.1947 - regression_loss: 19.9610 - val_loss: 25.8622 - val_regression_loss: 17.9850 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.9851 - regression_loss: 19.7755 - val_loss: 25.5193 - val_regression_loss: 17.7612 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.0240 - regression_loss: 19.0974 - val_loss: 25.5311 - val_regression_loss: 17.7338 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.7377 - regression_loss: 18.7979 - val_loss: 25.2060 - val_regression_loss: 17.5295 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.5931 - regression_loss: 18.4460 - val_loss: 24.7234 - val_regression_loss: 17.2381 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.6733 - regression_loss: 18.1762 - val_loss: 24.6331 - val_regression_loss: 17.1771 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.6201 - regression_loss: 17.9173 - val_loss: 24.4301 - val_regression_loss: 17.0464 - lr: 1.0000e-04\n",
      "Epoch 21/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 21.5088 - regression_loss: 17.6812 - val_loss: 24.3418 - val_regression_loss: 16.9847 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.0766 - regression_loss: 17.4562 - val_loss: 24.3292 - val_regression_loss: 16.9708 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.2148 - regression_loss: 17.3457 - val_loss: 23.8891 - val_regression_loss: 16.7097 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.0574 - regression_loss: 17.0962 - val_loss: 24.0627 - val_regression_loss: 16.8281 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.7181 - regression_loss: 16.9392 - val_loss: 24.0368 - val_regression_loss: 16.8206 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.8183 - regression_loss: 16.8021 - val_loss: 23.7865 - val_regression_loss: 16.6667 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.5150 - regression_loss: 16.6765 - val_loss: 23.6415 - val_regression_loss: 16.5673 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.1910 - regression_loss: 16.5596 - val_loss: 23.5179 - val_regression_loss: 16.4948 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.2682 - regression_loss: 16.5133 - val_loss: 23.6533 - val_regression_loss: 16.5731 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.3027 - regression_loss: 16.3297 - val_loss: 23.2692 - val_regression_loss: 16.3550 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.1506 - regression_loss: 16.3793 - val_loss: 23.4065 - val_regression_loss: 16.4323 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.0120 - regression_loss: 16.1504 - val_loss: 23.6086 - val_regression_loss: 16.5636 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.6486 - regression_loss: 16.0346 - val_loss: 23.0648 - val_regression_loss: 16.2247 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.6154 - regression_loss: 15.9904 - val_loss: 23.1759 - val_regression_loss: 16.3049 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.7701 - regression_loss: 15.9269 - val_loss: 23.0232 - val_regression_loss: 16.2043 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.8091 - regression_loss: 15.8341 - val_loss: 23.0638 - val_regression_loss: 16.1971 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.4438 - regression_loss: 15.6648 - val_loss: 23.1949 - val_regression_loss: 16.3016 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.2619 - regression_loss: 15.6510 - val_loss: 22.8824 - val_regression_loss: 16.1197 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1500 - regression_loss: 15.5643 - val_loss: 22.7699 - val_regression_loss: 16.0108 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.0267 - regression_loss: 15.5308 - val_loss: 22.9934 - val_regression_loss: 16.1620 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.0608 - regression_loss: 15.4521 - val_loss: 22.6847 - val_regression_loss: 15.9630 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.8541 - regression_loss: 15.3409 - val_loss: 22.8229 - val_regression_loss: 16.0603 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.6013 - regression_loss: 15.3454 - val_loss: 22.8202 - val_regression_loss: 16.0445 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.6375 - regression_loss: 15.2034 - val_loss: 22.6951 - val_regression_loss: 15.9513 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.9162 - regression_loss: 15.2848 - val_loss: 22.3223 - val_regression_loss: 15.7122 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.9009 - regression_loss: 15.2375 - val_loss: 22.6311 - val_regression_loss: 15.9106 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.7553 - regression_loss: 15.1365 - val_loss: 22.3833 - val_regression_loss: 15.7484 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.8401 - regression_loss: 16.2098\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.7464 - regression_loss: 15.0721 - val_loss: 22.8508 - val_regression_loss: 16.0747 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.9584 - regression_loss: 14.9780 - val_loss: 22.4080 - val_regression_loss: 15.7738 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.6507 - regression_loss: 14.8969 - val_loss: 22.2503 - val_regression_loss: 15.6598 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.5865 - regression_loss: 14.9369 - val_loss: 22.3346 - val_regression_loss: 15.7072 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.5679 - regression_loss: 14.8337 - val_loss: 22.6173 - val_regression_loss: 15.8870 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.4184 - regression_loss: 14.8229 - val_loss: 22.3737 - val_regression_loss: 15.7279 - lr: 5.0000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.6124 - regression_loss: 14.7664 - val_loss: 22.2945 - val_regression_loss: 15.6837 - lr: 5.0000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.4802 - regression_loss: 14.8257 - val_loss: 22.2809 - val_regression_loss: 15.6715 - lr: 5.0000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.0835 - regression_loss: 14.7298 - val_loss: 22.3410 - val_regression_loss: 15.6911 - lr: 5.0000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.1595 - regression_loss: 14.7706 - val_loss: 22.2739 - val_regression_loss: 15.6515 - lr: 5.0000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.3237 - regression_loss: 14.6838 - val_loss: 22.0753 - val_regression_loss: 15.5133 - lr: 5.0000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.3050 - regression_loss: 14.6525 - val_loss: 22.1331 - val_regression_loss: 15.5530 - lr: 5.0000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.9135 - regression_loss: 14.6432 - val_loss: 22.2990 - val_regression_loss: 15.6630 - lr: 5.0000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.3769 - regression_loss: 14.6048 - val_loss: 22.0766 - val_regression_loss: 15.5144 - lr: 5.0000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.2371 - regression_loss: 14.6466 - val_loss: 22.0837 - val_regression_loss: 15.5191 - lr: 5.0000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.2865 - regression_loss: 14.6510 - val_loss: 22.3453 - val_regression_loss: 15.6913 - lr: 5.0000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.0885 - regression_loss: 14.5361 - val_loss: 22.0457 - val_regression_loss: 15.4856 - lr: 5.0000e-05\n",
      "Epoch 65/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 16.7064 - regression_loss: 14.0800\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.1489 - regression_loss: 14.5764 - val_loss: 21.9751 - val_regression_loss: 15.4353 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.1568 - regression_loss: 14.4627 - val_loss: 22.1136 - val_regression_loss: 15.5273 - lr: 2.5000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.0758 - regression_loss: 14.4393 - val_loss: 22.1463 - val_regression_loss: 15.5525 - lr: 2.5000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.0090 - regression_loss: 14.4434 - val_loss: 22.1221 - val_regression_loss: 15.5338 - lr: 2.5000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.1088 - regression_loss: 14.4304 - val_loss: 21.9952 - val_regression_loss: 15.4425 - lr: 2.5000e-05\n",
      "Epoch 70/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.9258 - regression_loss: 13.3000\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.0389 - regression_loss: 14.4104 - val_loss: 22.0090 - val_regression_loss: 15.4548 - lr: 2.5000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.1046 - regression_loss: 14.4030 - val_loss: 21.9731 - val_regression_loss: 15.4331 - lr: 1.2500e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.6363 - regression_loss: 14.3899 - val_loss: 22.0246 - val_regression_loss: 15.4673 - lr: 1.2500e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.1550 - regression_loss: 14.3917 - val_loss: 22.0351 - val_regression_loss: 15.4711 - lr: 1.2500e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.8858 - regression_loss: 14.3768 - val_loss: 21.9956 - val_regression_loss: 15.4404 - lr: 1.2500e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.2749 - regression_loss: 14.3665 - val_loss: 21.9961 - val_regression_loss: 15.4410 - lr: 1.2500e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.7412 - regression_loss: 14.3589 - val_loss: 21.9944 - val_regression_loss: 15.4384 - lr: 1.2500e-05\n",
      "Epoch 77/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 16.6375 - regression_loss: 14.0121\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.8738 - regression_loss: 14.3563 - val_loss: 21.9724 - val_regression_loss: 15.4232 - lr: 1.2500e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.6245 - regression_loss: 14.3501 - val_loss: 21.9667 - val_regression_loss: 15.4190 - lr: 6.2500e-06\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.9496 - regression_loss: 14.3466 - val_loss: 21.9975 - val_regression_loss: 15.4394 - lr: 6.2500e-06\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.1165 - regression_loss: 14.3406 - val_loss: 21.9991 - val_regression_loss: 15.4415 - lr: 6.2500e-06\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.7925 - regression_loss: 14.3411 - val_loss: 22.0148 - val_regression_loss: 15.4531 - lr: 6.2500e-06\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.1435 - regression_loss: 14.3378 - val_loss: 22.0121 - val_regression_loss: 15.4517 - lr: 6.2500e-06\n",
      "Epoch 83/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 19.9140 - regression_loss: 17.2890\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.2223 - regression_loss: 14.3332 - val_loss: 21.9912 - val_regression_loss: 15.4367 - lr: 6.2500e-06\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.9344 - regression_loss: 14.3277 - val_loss: 21.9750 - val_regression_loss: 15.4249 - lr: 3.1250e-06\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.0810 - regression_loss: 14.3260 - val_loss: 21.9748 - val_regression_loss: 15.4240 - lr: 3.1250e-06\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.9651 - regression_loss: 14.3242 - val_loss: 21.9764 - val_regression_loss: 15.4249 - lr: 3.1250e-06\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.8839 - regression_loss: 14.3252 - val_loss: 21.9635 - val_regression_loss: 15.4168 - lr: 3.1250e-06\n",
      "Epoch 88/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 19.6897 - regression_loss: 17.0647\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.9938 - regression_loss: 14.3291 - val_loss: 21.9817 - val_regression_loss: 15.4296 - lr: 3.1250e-06\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.8763 - regression_loss: 14.3196 - val_loss: 21.9788 - val_regression_loss: 15.4273 - lr: 1.5625e-06\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.2517 - regression_loss: 14.3180 - val_loss: 21.9761 - val_regression_loss: 15.4252 - lr: 1.5625e-06\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6321 - regression_loss: 14.3181 - val_loss: 21.9753 - val_regression_loss: 15.4247 - lr: 1.5625e-06\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.9663 - regression_loss: 14.3170 - val_loss: 21.9718 - val_regression_loss: 15.4221 - lr: 1.5625e-06\n",
      "Epoch 93/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 16.1906 - regression_loss: 13.5657\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.9394 - regression_loss: 14.3166 - val_loss: 21.9758 - val_regression_loss: 15.4249 - lr: 1.5625e-06\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.2340 - regression_loss: 14.3156 - val_loss: 21.9762 - val_regression_loss: 15.4249 - lr: 7.8125e-07\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.7339 - regression_loss: 14.3145 - val_loss: 21.9751 - val_regression_loss: 15.4241 - lr: 7.8125e-07\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.1911 - regression_loss: 14.3142 - val_loss: 21.9745 - val_regression_loss: 15.4237 - lr: 7.8125e-07\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.9577 - regression_loss: 14.3139 - val_loss: 21.9750 - val_regression_loss: 15.4239 - lr: 7.8125e-07\n",
      "Epoch 98/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 14.2142 - regression_loss: 11.5893\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.8792 - regression_loss: 14.3139 - val_loss: 21.9713 - val_regression_loss: 15.4215 - lr: 7.8125e-07\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.9238 - regression_loss: 14.3130 - val_loss: 21.9702 - val_regression_loss: 15.4208 - lr: 3.9062e-07\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.6037 - regression_loss: 14.3128 - val_loss: 21.9706 - val_regression_loss: 15.4210 - lr: 3.9062e-07\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.0267 - regression_loss: 14.3126 - val_loss: 21.9710 - val_regression_loss: 15.4212 - lr: 3.9062e-07\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.8565 - regression_loss: 14.3124 - val_loss: 21.9707 - val_regression_loss: 15.4208 - lr: 3.9062e-07\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.7736 - regression_loss: 14.3124 - val_loss: 21.9714 - val_regression_loss: 15.4215 - lr: 3.9062e-07\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.8866 - regression_loss: 14.3122 - val_loss: 21.9699 - val_regression_loss: 15.4204 - lr: 3.9062e-07\n",
      "Epoch 105/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 20.0489 - regression_loss: 17.4239\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.0472 - regression_loss: 14.3127 - val_loss: 21.9686 - val_regression_loss: 15.4195 - lr: 3.9062e-07\n",
      "Epoch 106/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 18.0771 - regression_loss: 14.3116 - val_loss: 21.9694 - val_regression_loss: 15.4200 - lr: 1.9531e-07\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.9115 - regression_loss: 14.3118 - val_loss: 21.9706 - val_regression_loss: 15.4208 - lr: 1.9531e-07\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.7703 - regression_loss: 14.3113 - val_loss: 21.9704 - val_regression_loss: 15.4207 - lr: 1.9531e-07\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.7206 - regression_loss: 14.3113 - val_loss: 21.9696 - val_regression_loss: 15.4202 - lr: 1.9531e-07\n",
      "Epoch 110/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.0335 - regression_loss: 15.4086\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.0327 - regression_loss: 14.3111 - val_loss: 21.9694 - val_regression_loss: 15.4200 - lr: 1.9531e-07\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.8843 - regression_loss: 14.3110 - val_loss: 21.9691 - val_regression_loss: 15.4198 - lr: 9.7656e-08\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.0946 - regression_loss: 14.3109 - val_loss: 21.9692 - val_regression_loss: 15.4198 - lr: 9.7656e-08\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.8573 - regression_loss: 14.3108 - val_loss: 21.9692 - val_regression_loss: 15.4198 - lr: 9.7656e-08\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.8262 - regression_loss: 14.3108 - val_loss: 21.9693 - val_regression_loss: 15.4199 - lr: 9.7656e-08\n",
      "Epoch 115/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 20.4385 - regression_loss: 17.8136\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.9219 - regression_loss: 14.3107 - val_loss: 21.9696 - val_regression_loss: 15.4201 - lr: 9.7656e-08\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.7356 - regression_loss: 14.3106 - val_loss: 21.9695 - val_regression_loss: 15.4200 - lr: 4.8828e-08\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.9319 - regression_loss: 14.3106 - val_loss: 21.9695 - val_regression_loss: 15.4200 - lr: 4.8828e-08\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.0225 - regression_loss: 14.3106 - val_loss: 21.9694 - val_regression_loss: 15.4200 - lr: 4.8828e-08\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.7299 - regression_loss: 14.3106 - val_loss: 21.9696 - val_regression_loss: 15.4201 - lr: 4.8828e-08\n",
      "Epoch 120/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 19.4756 - regression_loss: 16.8507\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.7841 - regression_loss: 14.3105 - val_loss: 21.9695 - val_regression_loss: 15.4200 - lr: 4.8828e-08\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.8962 - regression_loss: 14.3105 - val_loss: 21.9695 - val_regression_loss: 15.4200 - lr: 2.4414e-08\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.9349 - regression_loss: 14.3105 - val_loss: 21.9694 - val_regression_loss: 15.4199 - lr: 2.4414e-08\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.0894 - regression_loss: 14.3105 - val_loss: 21.9694 - val_regression_loss: 15.4200 - lr: 2.4414e-08\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.9429 - regression_loss: 14.3105 - val_loss: 21.9694 - val_regression_loss: 15.4199 - lr: 2.4414e-08\n",
      "Epoch 125/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 20.2634 - regression_loss: 17.6384\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.0300 - regression_loss: 14.3105 - val_loss: 21.9694 - val_regression_loss: 15.4199 - lr: 2.4414e-08\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.8531 - regression_loss: 14.3104 - val_loss: 21.9694 - val_regression_loss: 15.4199 - lr: 1.2207e-08\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.9170 - regression_loss: 14.3104 - val_loss: 21.9694 - val_regression_loss: 15.4199 - lr: 1.2207e-08\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 27ms/step - loss: 109.1430 - regression_loss: 98.3229 - val_loss: 76.1728 - val_regression_loss: 54.0566 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 85.3468 - regression_loss: 76.2997 - val_loss: 59.6142 - val_regression_loss: 42.5952 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 72.8028 - regression_loss: 66.7373 - val_loss: 50.5612 - val_regression_loss: 36.4357 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 65.6838 - regression_loss: 60.7751 - val_loss: 45.3869 - val_regression_loss: 32.9402 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 63.4654 - regression_loss: 57.1809 - val_loss: 42.5187 - val_regression_loss: 31.0358 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 61.1157 - regression_loss: 54.1542 - val_loss: 39.7408 - val_regression_loss: 29.0622 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 59.1924 - regression_loss: 52.0130 - val_loss: 38.1495 - val_regression_loss: 27.9599 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 54.5038 - regression_loss: 48.8336 - val_loss: 34.7021 - val_regression_loss: 25.3527 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 51.7554 - regression_loss: 46.3061 - val_loss: 33.0660 - val_regression_loss: 24.1664 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 49.6433 - regression_loss: 43.5067 - val_loss: 31.8225 - val_regression_loss: 23.3271 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 45.8259 - regression_loss: 40.5780 - val_loss: 28.1799 - val_regression_loss: 20.5237 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 44.0214 - regression_loss: 37.9483 - val_loss: 26.5196 - val_regression_loss: 19.3446 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 40.3659 - regression_loss: 34.9476 - val_loss: 24.2816 - val_regression_loss: 17.6239 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 36.7348 - regression_loss: 31.9058 - val_loss: 21.5268 - val_regression_loss: 15.4563 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 33.4009 - regression_loss: 29.2221 - val_loss: 20.6128 - val_regression_loss: 14.7399 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.5021 - regression_loss: 27.1991 - val_loss: 18.4917 - val_regression_loss: 13.1183 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.3960 - regression_loss: 24.5700 - val_loss: 16.7244 - val_regression_loss: 11.7343 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.8490 - regression_loss: 22.7718 - val_loss: 16.6660 - val_regression_loss: 11.6638 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.6041 - regression_loss: 21.3090 - val_loss: 15.0410 - val_regression_loss: 10.3441 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.3229 - regression_loss: 20.3477 - val_loss: 14.0326 - val_regression_loss: 9.5123 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.0058 - regression_loss: 19.1915 - val_loss: 14.1373 - val_regression_loss: 9.5525 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.4575 - regression_loss: 18.2015 - val_loss: 13.6356 - val_regression_loss: 9.1331 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 20.6475 - regression_loss: 17.5167 - val_loss: 12.9990 - val_regression_loss: 8.6492 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.1922 - regression_loss: 17.2112 - val_loss: 12.8917 - val_regression_loss: 8.5477 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.6085 - regression_loss: 16.6867 - val_loss: 13.1341 - val_regression_loss: 8.7132 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.6772 - regression_loss: 16.1783 - val_loss: 12.6372 - val_regression_loss: 8.3318 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.7178 - regression_loss: 15.8436 - val_loss: 12.8035 - val_regression_loss: 8.4609 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.3148 - regression_loss: 15.5139 - val_loss: 12.4233 - val_regression_loss: 8.1423 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.9219 - regression_loss: 15.3413 - val_loss: 12.3864 - val_regression_loss: 8.0990 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.8277 - regression_loss: 15.1481 - val_loss: 12.5457 - val_regression_loss: 8.2224 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.3129 - regression_loss: 14.8243 - val_loss: 12.3701 - val_regression_loss: 8.1076 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.3545 - regression_loss: 14.7260 - val_loss: 12.6055 - val_regression_loss: 8.2782 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.8875 - regression_loss: 14.5089 - val_loss: 12.0959 - val_regression_loss: 7.8687 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.0914 - regression_loss: 14.3181 - val_loss: 12.4796 - val_regression_loss: 8.1862 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.7049 - regression_loss: 14.1495 - val_loss: 12.0647 - val_regression_loss: 7.8776 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.7431 - regression_loss: 14.1097 - val_loss: 12.2165 - val_regression_loss: 7.9787 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.5195 - regression_loss: 13.8907 - val_loss: 11.7215 - val_regression_loss: 7.5786 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2352 - regression_loss: 13.8751 - val_loss: 12.2340 - val_regression_loss: 7.9919 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.0904 - regression_loss: 13.5831 - val_loss: 11.5496 - val_regression_loss: 7.4822 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2426 - regression_loss: 13.5602 - val_loss: 12.0569 - val_regression_loss: 7.8565 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7552 - regression_loss: 13.2586 - val_loss: 11.4674 - val_regression_loss: 7.3805 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8122 - regression_loss: 13.2972 - val_loss: 11.9469 - val_regression_loss: 7.7430 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.3063 - regression_loss: 13.2082 - val_loss: 11.3394 - val_regression_loss: 7.2739 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.5127 - regression_loss: 13.0125 - val_loss: 11.7788 - val_regression_loss: 7.6183 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 16.5489 - regression_loss: 12.9634 - val_loss: 11.4542 - val_regression_loss: 7.3746 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.0970 - regression_loss: 12.6711 - val_loss: 11.3530 - val_regression_loss: 7.2943 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.8718 - regression_loss: 12.5554 - val_loss: 11.3279 - val_regression_loss: 7.2891 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.8219 - regression_loss: 12.3810 - val_loss: 11.3684 - val_regression_loss: 7.2875 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.7579 - regression_loss: 12.2670 - val_loss: 11.1177 - val_regression_loss: 7.1024 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5553 - regression_loss: 12.2158 - val_loss: 11.4996 - val_regression_loss: 7.3804 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.9231 - regression_loss: 12.2627 - val_loss: 10.7824 - val_regression_loss: 6.8258 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.8648 - regression_loss: 11.8800 - val_loss: 11.0640 - val_regression_loss: 7.0584 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.1449 - regression_loss: 11.7857 - val_loss: 11.3165 - val_regression_loss: 7.2809 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.9263 - regression_loss: 11.6863 - val_loss: 10.6564 - val_regression_loss: 6.7421 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.1023 - regression_loss: 11.6951 - val_loss: 11.6484 - val_regression_loss: 7.4854 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.0672 - regression_loss: 11.5669 - val_loss: 10.4448 - val_regression_loss: 6.5519 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.4507 - regression_loss: 11.3266 - val_loss: 10.4317 - val_regression_loss: 6.5531 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.6038 - regression_loss: 11.2048 - val_loss: 10.9188 - val_regression_loss: 6.9343 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1197 - regression_loss: 11.0003 - val_loss: 10.1754 - val_regression_loss: 6.3805 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.3834 - regression_loss: 10.9618 - val_loss: 10.6005 - val_regression_loss: 6.6733 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.2269 - regression_loss: 10.6743 - val_loss: 9.9297 - val_regression_loss: 6.1969 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1169 - regression_loss: 10.6186 - val_loss: 10.9438 - val_regression_loss: 6.9304 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6964 - regression_loss: 10.5112 - val_loss: 9.8978 - val_regression_loss: 6.2103 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7473 - regression_loss: 10.4620 - val_loss: 10.8560 - val_regression_loss: 6.8427 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.6805 - regression_loss: 10.2583 - val_loss: 9.5958 - val_regression_loss: 5.9783 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8043 - regression_loss: 10.4788 - val_loss: 10.9937 - val_regression_loss: 6.9677 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.3319 - regression_loss: 10.1771 - val_loss: 9.4656 - val_regression_loss: 5.9065 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.3889 - regression_loss: 9.9454 - val_loss: 10.9952 - val_regression_loss: 6.9628 - lr: 1.0000e-04\n",
      "Epoch 69/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 13.4425 - regression_loss: 10.1116 - val_loss: 9.3796 - val_regression_loss: 5.8182 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1294 - regression_loss: 9.9221 - val_loss: 9.8640 - val_regression_loss: 6.0989 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7187 - regression_loss: 9.6534 - val_loss: 9.6483 - val_regression_loss: 5.9396 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5357 - regression_loss: 9.2556 - val_loss: 9.3554 - val_regression_loss: 5.7497 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5417 - regression_loss: 9.2341 - val_loss: 9.8451 - val_regression_loss: 6.1138 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2340 - regression_loss: 9.0765 - val_loss: 9.1652 - val_regression_loss: 5.6388 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3351 - regression_loss: 9.1091 - val_loss: 9.9644 - val_regression_loss: 6.1930 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3427 - regression_loss: 9.0721 - val_loss: 8.9197 - val_regression_loss: 5.4902 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2176 - regression_loss: 9.0241 - val_loss: 10.0981 - val_regression_loss: 6.2663 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9846 - regression_loss: 8.8924 - val_loss: 8.8401 - val_regression_loss: 5.4623 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0133 - regression_loss: 8.8091 - val_loss: 10.1575 - val_regression_loss: 6.3194 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8466 - regression_loss: 8.6578 - val_loss: 8.8309 - val_regression_loss: 5.4415 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7862 - regression_loss: 8.6410 - val_loss: 9.9465 - val_regression_loss: 6.1632 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5965 - regression_loss: 8.4240 - val_loss: 8.8198 - val_regression_loss: 5.4673 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6419 - regression_loss: 8.4190 - val_loss: 9.9690 - val_regression_loss: 6.1832 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6317 - regression_loss: 8.4095 - val_loss: 8.7756 - val_regression_loss: 5.4226 - lr: 1.0000e-04\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2538 - regression_loss: 8.2295 - val_loss: 10.0717 - val_regression_loss: 6.2598 - lr: 1.0000e-04\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5822 - regression_loss: 8.4013 - val_loss: 8.8381 - val_regression_loss: 5.5321 - lr: 1.0000e-04\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7129 - regression_loss: 8.6033 - val_loss: 10.2172 - val_regression_loss: 6.3468 - lr: 1.0000e-04\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2213 - regression_loss: 8.0407 - val_loss: 8.7792 - val_regression_loss: 5.4498 - lr: 1.0000e-04\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.5816 - regression_loss: 8.3517 - val_loss: 10.2716 - val_regression_loss: 6.4334 - lr: 1.0000e-04\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5521 - regression_loss: 8.2907 - val_loss: 8.8223 - val_regression_loss: 5.5178 - lr: 1.0000e-04\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3311 - regression_loss: 8.2192 - val_loss: 10.5180 - val_regression_loss: 6.5884 - lr: 1.0000e-04\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5270 - regression_loss: 8.2624 - val_loss: 8.8078 - val_regression_loss: 5.4808 - lr: 1.0000e-04\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9622 - regression_loss: 7.8926 - val_loss: 9.7827 - val_regression_loss: 6.0735 - lr: 1.0000e-04\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7600 - regression_loss: 7.8461 - val_loss: 8.9245 - val_regression_loss: 5.5234 - lr: 1.0000e-04\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8521 - regression_loss: 7.7286 - val_loss: 9.3477 - val_regression_loss: 5.7805 - lr: 1.0000e-04\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8143 - regression_loss: 7.6957 - val_loss: 9.0425 - val_regression_loss: 5.5993 - lr: 1.0000e-04\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9717 - regression_loss: 7.7166 - val_loss: 9.4635 - val_regression_loss: 5.8768 - lr: 1.0000e-04\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7789 - regression_loss: 7.7134 - val_loss: 8.9119 - val_regression_loss: 5.5203 - lr: 1.0000e-04\n",
      "Epoch 99/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.2653 - regression_loss: 8.6864\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.7750 - regression_loss: 7.6693 - val_loss: 9.5174 - val_regression_loss: 5.8972 - lr: 1.0000e-04\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7950 - regression_loss: 7.6244 - val_loss: 9.0653 - val_regression_loss: 5.6252 - lr: 5.0000e-05\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.7763 - regression_loss: 7.5939 - val_loss: 9.1243 - val_regression_loss: 5.6618 - lr: 5.0000e-05\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6942 - regression_loss: 7.5468 - val_loss: 9.1617 - val_regression_loss: 5.6849 - lr: 5.0000e-05\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.6503 - regression_loss: 7.5714 - val_loss: 9.2322 - val_regression_loss: 5.7187 - lr: 5.0000e-05\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.6096 - regression_loss: 7.5715 - val_loss: 9.1071 - val_regression_loss: 5.6500 - lr: 5.0000e-05\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.7781 - regression_loss: 7.5356 - val_loss: 9.3347 - val_regression_loss: 5.8016 - lr: 5.0000e-05\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.4573 - regression_loss: 7.5337 - val_loss: 9.1361 - val_regression_loss: 5.6722 - lr: 5.0000e-05\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.5570 - regression_loss: 7.5799 - val_loss: 9.1220 - val_regression_loss: 5.6668 - lr: 5.0000e-05\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.6440 - regression_loss: 7.5887 - val_loss: 9.3456 - val_regression_loss: 5.8090 - lr: 5.0000e-05\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.6501 - regression_loss: 7.4844 - val_loss: 9.0787 - val_regression_loss: 5.6463 - lr: 5.0000e-05\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.6153 - regression_loss: 7.5439 - val_loss: 9.2206 - val_regression_loss: 5.7260 - lr: 5.0000e-05\n",
      "Epoch 111/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.5337 - regression_loss: 8.9583\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.4972 - regression_loss: 7.4733 - val_loss: 9.1911 - val_regression_loss: 5.7066 - lr: 5.0000e-05\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3691 - regression_loss: 7.4640 - val_loss: 9.1654 - val_regression_loss: 5.6924 - lr: 2.5000e-05\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.5459 - regression_loss: 7.4739 - val_loss: 9.2370 - val_regression_loss: 5.7394 - lr: 2.5000e-05\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6342 - regression_loss: 7.4585 - val_loss: 9.2821 - val_regression_loss: 5.7691 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.5900 - regression_loss: 7.4689 - val_loss: 9.1843 - val_regression_loss: 5.7103 - lr: 2.5000e-05\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.5349 - regression_loss: 7.4494 - val_loss: 9.2525 - val_regression_loss: 5.7519 - lr: 2.5000e-05\n",
      "Epoch 117/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.5815 - regression_loss: 7.0071\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.4805 - regression_loss: 7.4529 - val_loss: 9.2975 - val_regression_loss: 5.7826 - lr: 2.5000e-05\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.5490 - regression_loss: 7.4401 - val_loss: 9.2190 - val_regression_loss: 5.7321 - lr: 1.2500e-05\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.4889 - regression_loss: 7.4331 - val_loss: 9.1796 - val_regression_loss: 5.7090 - lr: 1.2500e-05\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.3382 - regression_loss: 7.4343 - val_loss: 9.2183 - val_regression_loss: 5.7338 - lr: 1.2500e-05\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.5409 - regression_loss: 7.4332 - val_loss: 9.2160 - val_regression_loss: 5.7335 - lr: 1.2500e-05\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.4639 - regression_loss: 7.4306 - val_loss: 9.2205 - val_regression_loss: 5.7358 - lr: 1.2500e-05\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.5204 - regression_loss: 7.4277 - val_loss: 9.2323 - val_regression_loss: 5.7432 - lr: 1.2500e-05\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.5828 - regression_loss: 7.4312 - val_loss: 9.2602 - val_regression_loss: 5.7598 - lr: 1.2500e-05\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 26ms/step - loss: 114.7033 - regression_loss: 106.0078 - val_loss: 101.3488 - val_regression_loss: 74.3774 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 85.2335 - regression_loss: 75.8373 - val_loss: 81.9605 - val_regression_loss: 59.6646 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 64.2397 - regression_loss: 58.7224 - val_loss: 70.9281 - val_regression_loss: 51.1873 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 56.2680 - regression_loss: 48.9285 - val_loss: 60.1587 - val_regression_loss: 43.0252 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 45.8868 - regression_loss: 39.9145 - val_loss: 50.2805 - val_regression_loss: 35.5497 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 38.6229 - regression_loss: 33.0536 - val_loss: 42.8961 - val_regression_loss: 29.8057 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.5942 - regression_loss: 27.4976 - val_loss: 36.5563 - val_regression_loss: 24.9399 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.3943 - regression_loss: 22.8048 - val_loss: 30.8873 - val_regression_loss: 20.7117 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.7029 - regression_loss: 19.6009 - val_loss: 27.1510 - val_regression_loss: 17.9348 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.6366 - regression_loss: 17.4306 - val_loss: 24.4962 - val_regression_loss: 15.9718 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.9078 - regression_loss: 15.5852 - val_loss: 23.1568 - val_regression_loss: 14.9223 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.9983 - regression_loss: 14.2768 - val_loss: 21.2600 - val_regression_loss: 13.5341 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.8680 - regression_loss: 13.0942 - val_loss: 20.4515 - val_regression_loss: 12.8757 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.4293 - regression_loss: 11.9398 - val_loss: 19.0595 - val_regression_loss: 11.8780 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.5281 - regression_loss: 11.0524 - val_loss: 18.1494 - val_regression_loss: 11.2050 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.5329 - regression_loss: 10.3482 - val_loss: 17.2159 - val_regression_loss: 10.5406 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6901 - regression_loss: 9.5107 - val_loss: 16.1457 - val_regression_loss: 9.8312 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8784 - regression_loss: 8.9185 - val_loss: 16.0184 - val_regression_loss: 9.6791 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.8343 - regression_loss: 8.4327 - val_loss: 14.7465 - val_regression_loss: 8.8376 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.2569 - regression_loss: 8.0205 - val_loss: 14.2817 - val_regression_loss: 8.4888 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7753 - regression_loss: 7.6662 - val_loss: 13.5597 - val_regression_loss: 8.0181 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.8125 - regression_loss: 7.3455 - val_loss: 13.4180 - val_regression_loss: 7.8996 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.9791 - regression_loss: 6.9987 - val_loss: 12.6745 - val_regression_loss: 7.4412 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.7265 - regression_loss: 6.7872 - val_loss: 12.5448 - val_regression_loss: 7.3040 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.6709 - regression_loss: 6.6171 - val_loss: 11.9295 - val_regression_loss: 6.9219 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4106 - regression_loss: 6.3197 - val_loss: 11.8600 - val_regression_loss: 6.8432 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.1394 - regression_loss: 6.1531 - val_loss: 11.4646 - val_regression_loss: 6.5836 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.7970 - regression_loss: 5.8963 - val_loss: 11.3803 - val_regression_loss: 6.5097 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.8447 - regression_loss: 5.8182 - val_loss: 10.9429 - val_regression_loss: 6.2464 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.5099 - regression_loss: 5.6464 - val_loss: 10.8546 - val_regression_loss: 6.1614 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.4273 - regression_loss: 5.4328 - val_loss: 10.4161 - val_regression_loss: 5.8967 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.1898 - regression_loss: 5.2617 - val_loss: 10.3114 - val_regression_loss: 5.8040 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.9689 - regression_loss: 5.1343 - val_loss: 10.0840 - val_regression_loss: 5.6810 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.9386 - regression_loss: 5.0463 - val_loss: 9.9178 - val_regression_loss: 5.5553 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7430 - regression_loss: 4.8598 - val_loss: 9.7429 - val_regression_loss: 5.4374 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7669 - regression_loss: 4.7737 - val_loss: 9.5115 - val_regression_loss: 5.3020 - lr: 1.0000e-04\n",
      "Epoch 37/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5093 - regression_loss: 4.6725 - val_loss: 9.4328 - val_regression_loss: 5.2326 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.4588 - regression_loss: 4.5560 - val_loss: 9.2807 - val_regression_loss: 5.1385 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.3277 - regression_loss: 4.4856 - val_loss: 9.1500 - val_regression_loss: 5.0603 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.3560 - regression_loss: 4.4063 - val_loss: 8.9497 - val_regression_loss: 4.9379 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.3195 - regression_loss: 4.3432 - val_loss: 8.8695 - val_regression_loss: 4.8779 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0808 - regression_loss: 4.2004 - val_loss: 8.8439 - val_regression_loss: 4.8490 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6.9874 - regression_loss: 4.1148 - val_loss: 8.7110 - val_regression_loss: 4.7795 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.8933 - regression_loss: 4.0759 - val_loss: 8.5786 - val_regression_loss: 4.7137 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.9199 - regression_loss: 4.0328 - val_loss: 8.5717 - val_regression_loss: 4.6909 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.8358 - regression_loss: 3.9497 - val_loss: 8.4361 - val_regression_loss: 4.5975 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.6884 - regression_loss: 3.8817 - val_loss: 8.3062 - val_regression_loss: 4.5269 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.7100 - regression_loss: 3.8271 - val_loss: 8.3410 - val_regression_loss: 4.5283 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4908 - regression_loss: 3.7563 - val_loss: 8.1760 - val_regression_loss: 4.4901 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6422 - regression_loss: 3.7605 - val_loss: 8.2482 - val_regression_loss: 4.4803 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5393 - regression_loss: 3.7088 - val_loss: 7.9541 - val_regression_loss: 4.3131 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.3327 - regression_loss: 3.5418 - val_loss: 8.1645 - val_regression_loss: 4.4078 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.2770 - regression_loss: 3.4799 - val_loss: 7.8654 - val_regression_loss: 4.2812 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.1065 - regression_loss: 3.4409 - val_loss: 7.8574 - val_regression_loss: 4.2178 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6.2546 - regression_loss: 3.4499 - val_loss: 7.7505 - val_regression_loss: 4.2263 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2234 - regression_loss: 3.4190 - val_loss: 8.0652 - val_regression_loss: 4.3599 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.1693 - regression_loss: 3.4201 - val_loss: 7.6035 - val_regression_loss: 4.1184 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.1393 - regression_loss: 3.3323 - val_loss: 7.6560 - val_regression_loss: 4.0865 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9705 - regression_loss: 3.2049 - val_loss: 7.4751 - val_regression_loss: 4.0137 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8688 - regression_loss: 3.1453 - val_loss: 7.5304 - val_regression_loss: 4.0233 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9209 - regression_loss: 3.1081 - val_loss: 7.3850 - val_regression_loss: 3.9390 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.8414 - regression_loss: 3.0599 - val_loss: 7.3399 - val_regression_loss: 3.9041 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7802 - regression_loss: 3.0328 - val_loss: 7.3765 - val_regression_loss: 3.9171 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7292 - regression_loss: 3.0015 - val_loss: 7.2615 - val_regression_loss: 3.8557 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7208 - regression_loss: 2.9550 - val_loss: 7.2093 - val_regression_loss: 3.8083 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6963 - regression_loss: 2.9661 - val_loss: 7.2276 - val_regression_loss: 3.8204 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6923 - regression_loss: 2.9398 - val_loss: 7.1789 - val_regression_loss: 3.8089 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7019 - regression_loss: 2.9312 - val_loss: 7.3131 - val_regression_loss: 3.8583 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6148 - regression_loss: 2.8631 - val_loss: 7.0091 - val_regression_loss: 3.7397 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5860 - regression_loss: 2.8706 - val_loss: 7.1284 - val_regression_loss: 3.7503 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5395 - regression_loss: 2.8334 - val_loss: 7.0726 - val_regression_loss: 3.7568 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.4563 - regression_loss: 2.8049 - val_loss: 7.0806 - val_regression_loss: 3.7061 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.4006 - regression_loss: 2.7115 - val_loss: 6.8676 - val_regression_loss: 3.6404 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.4386 - regression_loss: 2.7505 - val_loss: 7.0784 - val_regression_loss: 3.7150 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5001 - regression_loss: 2.7651 - val_loss: 6.8661 - val_regression_loss: 3.6288 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.4625 - regression_loss: 2.7055 - val_loss: 7.1072 - val_regression_loss: 3.7219 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.4621 - regression_loss: 2.6978 - val_loss: 6.7400 - val_regression_loss: 3.5181 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2957 - regression_loss: 2.6012 - val_loss: 6.8484 - val_regression_loss: 3.5508 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.3234 - regression_loss: 2.6233 - val_loss: 6.6503 - val_regression_loss: 3.4370 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1680 - regression_loss: 2.5384 - val_loss: 6.7744 - val_regression_loss: 3.4844 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2312 - regression_loss: 2.5578 - val_loss: 6.6076 - val_regression_loss: 3.4054 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1718 - regression_loss: 2.5146 - val_loss: 6.5558 - val_regression_loss: 3.3703 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2041 - regression_loss: 2.4834 - val_loss: 6.6033 - val_regression_loss: 3.3878 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1385 - regression_loss: 2.4837 - val_loss: 6.6055 - val_regression_loss: 3.3809 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2105 - regression_loss: 2.5289 - val_loss: 6.4650 - val_regression_loss: 3.3157 - lr: 1.0000e-04\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1973 - regression_loss: 2.4857 - val_loss: 6.5209 - val_regression_loss: 3.3442 - lr: 1.0000e-04\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1457 - regression_loss: 2.4463 - val_loss: 6.4923 - val_regression_loss: 3.3128 - lr: 1.0000e-04\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0832 - regression_loss: 2.4383 - val_loss: 6.4371 - val_regression_loss: 3.2859 - lr: 1.0000e-04\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0481 - regression_loss: 2.3898 - val_loss: 6.4301 - val_regression_loss: 3.2804 - lr: 1.0000e-04\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0705 - regression_loss: 2.3743 - val_loss: 6.3891 - val_regression_loss: 3.2366 - lr: 1.0000e-04\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0408 - regression_loss: 2.3709 - val_loss: 6.4300 - val_regression_loss: 3.2590 - lr: 1.0000e-04\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0261 - regression_loss: 2.3642 - val_loss: 6.2807 - val_regression_loss: 3.1886 - lr: 1.0000e-04\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0586 - regression_loss: 2.3498 - val_loss: 6.3316 - val_regression_loss: 3.2008 - lr: 1.0000e-04\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0397 - regression_loss: 2.3271 - val_loss: 6.3272 - val_regression_loss: 3.2043 - lr: 1.0000e-04\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0203 - regression_loss: 2.3152 - val_loss: 6.2617 - val_regression_loss: 3.1762 - lr: 1.0000e-04\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0409 - regression_loss: 2.3383 - val_loss: 6.3635 - val_regression_loss: 3.2158 - lr: 1.0000e-04\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0140 - regression_loss: 2.3463 - val_loss: 6.2497 - val_regression_loss: 3.1888 - lr: 1.0000e-04\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9834 - regression_loss: 2.3176 - val_loss: 6.5646 - val_regression_loss: 3.3767 - lr: 1.0000e-04\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0968 - regression_loss: 2.4471 - val_loss: 6.2885 - val_regression_loss: 3.2192 - lr: 1.0000e-04\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9530 - regression_loss: 2.2827 - val_loss: 6.5687 - val_regression_loss: 3.3823 - lr: 1.0000e-04\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1044 - regression_loss: 2.4267 - val_loss: 6.1686 - val_regression_loss: 3.1364 - lr: 1.0000e-04\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0071 - regression_loss: 2.3280 - val_loss: 6.3377 - val_regression_loss: 3.1927 - lr: 1.0000e-04\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9171 - regression_loss: 2.2699 - val_loss: 6.0859 - val_regression_loss: 3.0602 - lr: 1.0000e-04\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8557 - regression_loss: 2.2176 - val_loss: 6.1589 - val_regression_loss: 3.0552 - lr: 1.0000e-04\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8832 - regression_loss: 2.2255 - val_loss: 6.0342 - val_regression_loss: 2.9985 - lr: 1.0000e-04\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8538 - regression_loss: 2.1848 - val_loss: 6.0755 - val_regression_loss: 3.0258 - lr: 1.0000e-04\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8584 - regression_loss: 2.1931 - val_loss: 5.9842 - val_regression_loss: 2.9795 - lr: 1.0000e-04\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8596 - regression_loss: 2.2220 - val_loss: 6.1208 - val_regression_loss: 3.0488 - lr: 1.0000e-04\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8809 - regression_loss: 2.2280 - val_loss: 5.9707 - val_regression_loss: 2.9596 - lr: 1.0000e-04\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8169 - regression_loss: 2.1568 - val_loss: 6.0318 - val_regression_loss: 2.9803 - lr: 1.0000e-04\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8586 - regression_loss: 2.2179 - val_loss: 6.0780 - val_regression_loss: 3.0247 - lr: 1.0000e-04\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8164 - regression_loss: 2.1781 - val_loss: 5.9185 - val_regression_loss: 2.9199 - lr: 1.0000e-04\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8297 - regression_loss: 2.1820 - val_loss: 6.0437 - val_regression_loss: 2.9818 - lr: 1.0000e-04\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7717 - regression_loss: 2.1388 - val_loss: 5.9047 - val_regression_loss: 2.9275 - lr: 1.0000e-04\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7856 - regression_loss: 2.1463 - val_loss: 6.0994 - val_regression_loss: 3.0135 - lr: 1.0000e-04\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7863 - regression_loss: 2.1333 - val_loss: 5.8767 - val_regression_loss: 2.8975 - lr: 1.0000e-04\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7843 - regression_loss: 2.1282 - val_loss: 6.0734 - val_regression_loss: 3.0054 - lr: 1.0000e-04\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8263 - regression_loss: 2.1511 - val_loss: 5.8173 - val_regression_loss: 2.8413 - lr: 1.0000e-04\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6916 - regression_loss: 2.1054 - val_loss: 5.9076 - val_regression_loss: 2.8855 - lr: 1.0000e-04\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7411 - regression_loss: 2.1220 - val_loss: 5.8431 - val_regression_loss: 2.8617 - lr: 1.0000e-04\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7111 - regression_loss: 2.0884 - val_loss: 5.7752 - val_regression_loss: 2.8230 - lr: 1.0000e-04\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7814 - regression_loss: 2.1594 - val_loss: 5.9223 - val_regression_loss: 2.8940 - lr: 1.0000e-04\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8046 - regression_loss: 2.1688 - val_loss: 5.7766 - val_regression_loss: 2.8175 - lr: 1.0000e-04\n",
      "Epoch 124/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.3539 - regression_loss: 1.8661\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8278 - regression_loss: 2.1612 - val_loss: 5.8430 - val_regression_loss: 2.8564 - lr: 1.0000e-04\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6957 - regression_loss: 2.0729 - val_loss: 5.7685 - val_regression_loss: 2.8049 - lr: 5.0000e-05\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7271 - regression_loss: 2.0920 - val_loss: 5.7409 - val_regression_loss: 2.7886 - lr: 5.0000e-05\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6921 - regression_loss: 2.0783 - val_loss: 5.7803 - val_regression_loss: 2.8081 - lr: 5.0000e-05\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7015 - regression_loss: 2.0798 - val_loss: 5.7228 - val_regression_loss: 2.7763 - lr: 5.0000e-05\n",
      "Epoch 129/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.6100 - regression_loss: 2.1249\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7572 - regression_loss: 2.0893 - val_loss: 5.7783 - val_regression_loss: 2.8041 - lr: 5.0000e-05\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6899 - regression_loss: 2.0215 - val_loss: 5.7014 - val_regression_loss: 2.7676 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6603 - regression_loss: 2.0439 - val_loss: 5.7039 - val_regression_loss: 2.7619 - lr: 2.5000e-05\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7076 - regression_loss: 2.0453 - val_loss: 5.8028 - val_regression_loss: 2.8209 - lr: 2.5000e-05\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6226 - regression_loss: 2.0249 - val_loss: 5.7058 - val_regression_loss: 2.7612 - lr: 2.5000e-05\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6597 - regression_loss: 2.0369 - val_loss: 5.6914 - val_regression_loss: 2.7527 - lr: 2.5000e-05\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6642 - regression_loss: 2.0263 - val_loss: 5.7607 - val_regression_loss: 2.7874 - lr: 2.5000e-05\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6516 - regression_loss: 2.0254 - val_loss: 5.7081 - val_regression_loss: 2.7600 - lr: 2.5000e-05\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6438 - regression_loss: 2.0148 - val_loss: 5.7047 - val_regression_loss: 2.7592 - lr: 2.5000e-05\n",
      "Epoch 138/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.9279 - regression_loss: 2.4450\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6273 - regression_loss: 2.0129 - val_loss: 5.7137 - val_regression_loss: 2.7621 - lr: 2.5000e-05\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6555 - regression_loss: 2.0129 - val_loss: 5.7172 - val_regression_loss: 2.7650 - lr: 1.2500e-05\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6348 - regression_loss: 2.0078 - val_loss: 5.7033 - val_regression_loss: 2.7568 - lr: 1.2500e-05\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6017 - regression_loss: 2.0068 - val_loss: 5.6910 - val_regression_loss: 2.7469 - lr: 1.2500e-05\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6044 - regression_loss: 2.0078 - val_loss: 5.7019 - val_regression_loss: 2.7532 - lr: 1.2500e-05\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6211 - regression_loss: 2.0107 - val_loss: 5.7071 - val_regression_loss: 2.7571 - lr: 1.2500e-05\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5891 - regression_loss: 2.0047 - val_loss: 5.6881 - val_regression_loss: 2.7456 - lr: 1.2500e-05\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6528 - regression_loss: 2.0065 - val_loss: 5.6903 - val_regression_loss: 2.7464 - lr: 1.2500e-05\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5927 - regression_loss: 2.0080 - val_loss: 5.7045 - val_regression_loss: 2.7523 - lr: 1.2500e-05\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6457 - regression_loss: 2.0052 - val_loss: 5.6917 - val_regression_loss: 2.7428 - lr: 1.2500e-05\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5914 - regression_loss: 2.0031 - val_loss: 5.6891 - val_regression_loss: 2.7424 - lr: 1.2500e-05\n",
      "Epoch 149/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.9633 - regression_loss: 2.4818\n",
      "Epoch 149: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6241 - regression_loss: 2.0051 - val_loss: 5.6808 - val_regression_loss: 2.7397 - lr: 1.2500e-05\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6303 - regression_loss: 2.0010 - val_loss: 5.6873 - val_regression_loss: 2.7437 - lr: 6.2500e-06\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6308 - regression_loss: 2.0012 - val_loss: 5.6952 - val_regression_loss: 2.7476 - lr: 6.2500e-06\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6306 - regression_loss: 1.9999 - val_loss: 5.6919 - val_regression_loss: 2.7457 - lr: 6.2500e-06\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6002 - regression_loss: 1.9994 - val_loss: 5.6896 - val_regression_loss: 2.7457 - lr: 6.2500e-06\n",
      "Epoch 154/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.6207 - regression_loss: 2.1395\n",
      "Epoch 154: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6364 - regression_loss: 2.0001 - val_loss: 5.6909 - val_regression_loss: 2.7466 - lr: 6.2500e-06\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6043 - regression_loss: 1.9983 - val_loss: 5.6906 - val_regression_loss: 2.7465 - lr: 3.1250e-06\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6072 - regression_loss: 1.9979 - val_loss: 5.6891 - val_regression_loss: 2.7451 - lr: 3.1250e-06\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5918 - regression_loss: 1.9986 - val_loss: 5.6839 - val_regression_loss: 2.7417 - lr: 3.1250e-06\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6137 - regression_loss: 1.9976 - val_loss: 5.6851 - val_regression_loss: 2.7417 - lr: 3.1250e-06\n",
      "Epoch 159/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.7255 - regression_loss: 2.2445\n",
      "Epoch 159: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6117 - regression_loss: 1.9973 - val_loss: 5.6843 - val_regression_loss: 2.7409 - lr: 3.1250e-06\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6330 - regression_loss: 1.9970 - val_loss: 5.6835 - val_regression_loss: 2.7404 - lr: 1.5625e-06\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6281 - regression_loss: 1.9974 - val_loss: 5.6859 - val_regression_loss: 2.7413 - lr: 1.5625e-06\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5747 - regression_loss: 1.9967 - val_loss: 5.6857 - val_regression_loss: 2.7414 - lr: 1.5625e-06\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6330 - regression_loss: 1.9968 - val_loss: 5.6852 - val_regression_loss: 2.7413 - lr: 1.5625e-06\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5847 - regression_loss: 1.9966 - val_loss: 5.6839 - val_regression_loss: 2.7407 - lr: 1.5625e-06\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6084 - regression_loss: 1.9963 - val_loss: 5.6837 - val_regression_loss: 2.7406 - lr: 1.5625e-06\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6116 - regression_loss: 1.9967 - val_loss: 5.6829 - val_regression_loss: 2.7399 - lr: 1.5625e-06\n",
      "Epoch 167/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.3467 - regression_loss: 1.8659\n",
      "Epoch 167: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6487 - regression_loss: 1.9962 - val_loss: 5.6839 - val_regression_loss: 2.7406 - lr: 1.5625e-06\n",
      "Epoch 168/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6362 - regression_loss: 1.9960 - val_loss: 5.6839 - val_regression_loss: 2.7404 - lr: 7.8125e-07\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6249 - regression_loss: 1.9960 - val_loss: 5.6836 - val_regression_loss: 2.7401 - lr: 7.8125e-07\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6023 - regression_loss: 1.9961 - val_loss: 5.6838 - val_regression_loss: 2.7401 - lr: 7.8125e-07\n",
      "Epoch 171/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6222 - regression_loss: 1.9959 - val_loss: 5.6841 - val_regression_loss: 2.7403 - lr: 7.8125e-07\n",
      "Epoch 172/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.9171 - regression_loss: 2.4363\n",
      "Epoch 172: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5794 - regression_loss: 1.9962 - val_loss: 5.6851 - val_regression_loss: 2.7408 - lr: 7.8125e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.5959 - regression_loss: 1.9958 - val_loss: 5.6846 - val_regression_loss: 2.7406 - lr: 3.9062e-07\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6373 - regression_loss: 1.9958 - val_loss: 5.6842 - val_regression_loss: 2.7404 - lr: 3.9062e-07\n",
      "Epoch 175/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5784 - regression_loss: 1.9957 - val_loss: 5.6843 - val_regression_loss: 2.7405 - lr: 3.9062e-07\n",
      "Epoch 176/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5637 - regression_loss: 1.9957 - val_loss: 5.6840 - val_regression_loss: 2.7403 - lr: 3.9062e-07\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5895 - regression_loss: 1.9958 - val_loss: 5.6842 - val_regression_loss: 2.7404 - lr: 3.9062e-07\n",
      "Epoch 178/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5941 - regression_loss: 1.9958 - val_loss: 5.6835 - val_regression_loss: 2.7400 - lr: 3.9062e-07\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6126 - regression_loss: 1.9956 - val_loss: 5.6835 - val_regression_loss: 2.7400 - lr: 3.9062e-07\n",
      "Epoch 180/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6240 - regression_loss: 1.9956 - val_loss: 5.6834 - val_regression_loss: 2.7398 - lr: 3.9062e-07\n",
      "Epoch 181/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.7934 - regression_loss: 2.3126\n",
      "Epoch 181: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6001 - regression_loss: 1.9957 - val_loss: 5.6831 - val_regression_loss: 2.7397 - lr: 3.9062e-07\n",
      "Epoch 182/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5983 - regression_loss: 1.9955 - val_loss: 5.6832 - val_regression_loss: 2.7398 - lr: 1.9531e-07\n",
      "Epoch 183/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6350 - regression_loss: 1.9955 - val_loss: 5.6830 - val_regression_loss: 2.7397 - lr: 1.9531e-07\n",
      "Epoch 184/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5899 - regression_loss: 1.9955 - val_loss: 5.6831 - val_regression_loss: 2.7397 - lr: 1.9531e-07\n",
      "Epoch 185/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6275 - regression_loss: 1.9955 - val_loss: 5.6828 - val_regression_loss: 2.7396 - lr: 1.9531e-07\n",
      "Epoch 186/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.5955 - regression_loss: 2.1148\n",
      "Epoch 186: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6410 - regression_loss: 1.9955 - val_loss: 5.6830 - val_regression_loss: 2.7397 - lr: 1.9531e-07\n",
      "Epoch 187/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6213 - regression_loss: 1.9955 - val_loss: 5.6829 - val_regression_loss: 2.7397 - lr: 9.7656e-08\n",
      "Epoch 188/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6257 - regression_loss: 1.9954 - val_loss: 5.6830 - val_regression_loss: 2.7397 - lr: 9.7656e-08\n",
      "Epoch 189/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6295 - regression_loss: 1.9954 - val_loss: 5.6829 - val_regression_loss: 2.7396 - lr: 9.7656e-08\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 107.1227 - regression_loss: 96.5997 - val_loss: 66.3591 - val_regression_loss: 51.9177 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 67.6466 - regression_loss: 63.3687 - val_loss: 51.7892 - val_regression_loss: 40.5085 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 57.7338 - regression_loss: 51.5124 - val_loss: 46.7431 - val_regression_loss: 36.4871 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 48.3824 - regression_loss: 44.5947 - val_loss: 41.6638 - val_regression_loss: 32.1531 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 45.2002 - regression_loss: 38.9981 - val_loss: 37.0012 - val_regression_loss: 28.0828 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 39.8236 - regression_loss: 34.3487 - val_loss: 33.7174 - val_regression_loss: 25.1198 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 34.4272 - regression_loss: 31.0135 - val_loss: 31.4350 - val_regression_loss: 23.0881 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 33.1481 - regression_loss: 28.6366 - val_loss: 29.3133 - val_regression_loss: 21.3001 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.2740 - regression_loss: 26.4032 - val_loss: 27.6785 - val_regression_loss: 19.9601 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.1934 - regression_loss: 24.7472 - val_loss: 26.5121 - val_regression_loss: 19.0347 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.8170 - regression_loss: 23.2712 - val_loss: 24.6017 - val_regression_loss: 17.6261 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.9659 - regression_loss: 21.5026 - val_loss: 24.1685 - val_regression_loss: 17.2353 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.5235 - regression_loss: 20.3269 - val_loss: 22.9220 - val_regression_loss: 16.2796 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.0195 - regression_loss: 19.1318 - val_loss: 21.8573 - val_regression_loss: 15.4327 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.4947 - regression_loss: 18.0242 - val_loss: 21.4783 - val_regression_loss: 15.0945 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9248 - regression_loss: 17.0831 - val_loss: 20.3855 - val_regression_loss: 14.2799 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.0406 - regression_loss: 16.2931 - val_loss: 19.7391 - val_regression_loss: 13.8073 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.5534 - regression_loss: 15.3504 - val_loss: 19.5656 - val_regression_loss: 13.6623 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.3119 - regression_loss: 14.8074 - val_loss: 18.6889 - val_regression_loss: 12.9771 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.7323 - regression_loss: 14.2139 - val_loss: 18.3570 - val_regression_loss: 12.7355 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.0968 - regression_loss: 13.5890 - val_loss: 17.8704 - val_regression_loss: 12.3749 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.6010 - regression_loss: 13.1147 - val_loss: 17.3723 - val_regression_loss: 11.9892 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.8116 - regression_loss: 12.5704 - val_loss: 16.9976 - val_regression_loss: 11.6874 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5401 - regression_loss: 12.1243 - val_loss: 16.5510 - val_regression_loss: 11.3318 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.0198 - regression_loss: 11.7193 - val_loss: 16.1896 - val_regression_loss: 11.0480 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.8689 - regression_loss: 11.3936 - val_loss: 15.9767 - val_regression_loss: 10.8752 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.4801 - regression_loss: 11.1282 - val_loss: 15.5953 - val_regression_loss: 10.5534 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.0598 - regression_loss: 10.7216 - val_loss: 15.2670 - val_regression_loss: 10.2941 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.2797 - regression_loss: 10.3660 - val_loss: 15.1061 - val_regression_loss: 10.1658 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1063 - regression_loss: 10.1354 - val_loss: 14.8048 - val_regression_loss: 9.9023 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8980 - regression_loss: 9.8882 - val_loss: 14.5077 - val_regression_loss: 9.6548 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5709 - regression_loss: 9.6557 - val_loss: 14.2716 - val_regression_loss: 9.4625 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6743 - regression_loss: 9.6738 - val_loss: 14.0600 - val_regression_loss: 9.2891 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2573 - regression_loss: 9.3808 - val_loss: 13.7958 - val_regression_loss: 9.0385 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0925 - regression_loss: 9.0787 - val_loss: 13.9573 - val_regression_loss: 9.2187 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1288 - regression_loss: 9.0113 - val_loss: 13.3951 - val_regression_loss: 8.6742 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6316 - regression_loss: 8.8335 - val_loss: 13.4208 - val_regression_loss: 8.7215 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6367 - regression_loss: 8.6829 - val_loss: 13.1914 - val_regression_loss: 8.5072 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7035 - regression_loss: 8.5146 - val_loss: 13.1113 - val_regression_loss: 8.4625 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5127 - regression_loss: 8.4171 - val_loss: 12.7889 - val_regression_loss: 8.1469 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3838 - regression_loss: 8.4097 - val_loss: 12.8184 - val_regression_loss: 8.2139 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0398 - regression_loss: 8.1413 - val_loss: 12.5695 - val_regression_loss: 7.9596 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1109 - regression_loss: 8.0916 - val_loss: 12.4914 - val_regression_loss: 7.8896 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0069 - regression_loss: 7.9702 - val_loss: 12.4186 - val_regression_loss: 7.8395 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9066 - regression_loss: 7.8919 - val_loss: 12.2511 - val_regression_loss: 7.6744 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8183 - regression_loss: 7.8241 - val_loss: 12.2413 - val_regression_loss: 7.6909 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7025 - regression_loss: 7.7978 - val_loss: 12.0690 - val_regression_loss: 7.4954 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7451 - regression_loss: 7.8389 - val_loss: 12.0474 - val_regression_loss: 7.5341 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8359 - regression_loss: 7.8518 - val_loss: 11.8207 - val_regression_loss: 7.3018 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6601 - regression_loss: 7.5981 - val_loss: 11.7317 - val_regression_loss: 7.2260 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3349 - regression_loss: 7.4737 - val_loss: 11.7204 - val_regression_loss: 7.2319 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3451 - regression_loss: 7.4283 - val_loss: 11.5661 - val_regression_loss: 7.1085 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.5057 - regression_loss: 7.3961 - val_loss: 11.4767 - val_regression_loss: 7.0331 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2135 - regression_loss: 7.3266 - val_loss: 11.4311 - val_regression_loss: 6.9867 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.1291 - regression_loss: 7.3091 - val_loss: 11.4160 - val_regression_loss: 6.9813 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2327 - regression_loss: 7.2336 - val_loss: 11.2281 - val_regression_loss: 6.8027 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3433 - regression_loss: 7.2799 - val_loss: 11.2069 - val_regression_loss: 6.8239 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.1666 - regression_loss: 7.1553 - val_loss: 11.0985 - val_regression_loss: 6.7101 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8598 - regression_loss: 7.1508 - val_loss: 11.0703 - val_regression_loss: 6.6844 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9849 - regression_loss: 7.1175 - val_loss: 11.0261 - val_regression_loss: 6.6502 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8875 - regression_loss: 7.0315 - val_loss: 10.9033 - val_regression_loss: 6.5683 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9036 - regression_loss: 7.0271 - val_loss: 10.8710 - val_regression_loss: 6.5419 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0003 - regression_loss: 6.9678 - val_loss: 10.8516 - val_regression_loss: 6.5512 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.8168 - regression_loss: 7.3622\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8936 - regression_loss: 6.9414 - val_loss: 10.7051 - val_regression_loss: 6.4416 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7852 - regression_loss: 6.8556 - val_loss: 10.6496 - val_regression_loss: 6.3902 - lr: 5.0000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7158 - regression_loss: 6.8469 - val_loss: 10.6025 - val_regression_loss: 6.3484 - lr: 5.0000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8055 - regression_loss: 6.8554 - val_loss: 10.5994 - val_regression_loss: 6.3513 - lr: 5.0000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7958 - regression_loss: 6.8343 - val_loss: 10.5712 - val_regression_loss: 6.3281 - lr: 5.0000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.7154 - regression_loss: 6.8491 - val_loss: 10.5373 - val_regression_loss: 6.2833 - lr: 5.0000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.8066 - regression_loss: 6.8288 - val_loss: 10.6059 - val_regression_loss: 6.3694 - lr: 5.0000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.7171 - regression_loss: 6.7674 - val_loss: 10.4960 - val_regression_loss: 6.2548 - lr: 5.0000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.6064 - regression_loss: 6.7679 - val_loss: 10.4550 - val_regression_loss: 6.2395 - lr: 5.0000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.6126 - regression_loss: 6.7299 - val_loss: 10.4691 - val_regression_loss: 6.2656 - lr: 5.0000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.6888 - regression_loss: 6.7229 - val_loss: 10.3841 - val_regression_loss: 6.1830 - lr: 5.0000e-05\n",
      "Epoch 75/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 9.7328 - regression_loss: 6.7473 - val_loss: 10.3447 - val_regression_loss: 6.1592 - lr: 5.0000e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5732 - regression_loss: 6.7087 - val_loss: 10.3574 - val_regression_loss: 6.1814 - lr: 5.0000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5128 - regression_loss: 6.6871 - val_loss: 10.2738 - val_regression_loss: 6.1008 - lr: 5.0000e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3801 - regression_loss: 6.6657 - val_loss: 10.2833 - val_regression_loss: 6.1048 - lr: 5.0000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5628 - regression_loss: 6.6403 - val_loss: 10.2561 - val_regression_loss: 6.0965 - lr: 5.0000e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5581 - regression_loss: 6.6625 - val_loss: 10.2041 - val_regression_loss: 6.0599 - lr: 5.0000e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5849 - regression_loss: 6.6212 - val_loss: 10.1699 - val_regression_loss: 6.0457 - lr: 5.0000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4498 - regression_loss: 6.6374 - val_loss: 10.1451 - val_regression_loss: 6.0076 - lr: 5.0000e-05\n",
      "Epoch 83/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.9438 - regression_loss: 7.4950\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4043 - regression_loss: 6.5984 - val_loss: 10.1255 - val_regression_loss: 6.0106 - lr: 5.0000e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4639 - regression_loss: 6.5815 - val_loss: 10.1194 - val_regression_loss: 6.0053 - lr: 2.5000e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4663 - regression_loss: 6.5759 - val_loss: 10.0805 - val_regression_loss: 5.9700 - lr: 2.5000e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.6636 - regression_loss: 6.5677 - val_loss: 10.0736 - val_regression_loss: 5.9641 - lr: 2.5000e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5363 - regression_loss: 6.5647 - val_loss: 10.0793 - val_regression_loss: 5.9748 - lr: 2.5000e-05\n",
      "Epoch 88/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.6881 - regression_loss: 7.2402\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4819 - regression_loss: 6.5494 - val_loss: 10.0553 - val_regression_loss: 5.9537 - lr: 2.5000e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5327 - regression_loss: 6.5404 - val_loss: 10.0449 - val_regression_loss: 5.9415 - lr: 1.2500e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4778 - regression_loss: 6.5379 - val_loss: 10.0439 - val_regression_loss: 5.9438 - lr: 1.2500e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3208 - regression_loss: 6.5318 - val_loss: 10.0377 - val_regression_loss: 5.9403 - lr: 1.2500e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5103 - regression_loss: 6.5316 - val_loss: 10.0250 - val_regression_loss: 5.9319 - lr: 1.2500e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5372 - regression_loss: 6.5322 - val_loss: 10.0173 - val_regression_loss: 5.9245 - lr: 1.2500e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4549 - regression_loss: 6.5230 - val_loss: 10.0126 - val_regression_loss: 5.9242 - lr: 1.2500e-05\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4706 - regression_loss: 6.5240 - val_loss: 9.9988 - val_regression_loss: 5.9151 - lr: 1.2500e-05\n",
      "Epoch 96/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.5716 - regression_loss: 7.1244\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3585 - regression_loss: 6.5237 - val_loss: 10.0056 - val_regression_loss: 5.9244 - lr: 1.2500e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4065 - regression_loss: 6.5146 - val_loss: 9.9996 - val_regression_loss: 5.9166 - lr: 6.2500e-06\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5625 - regression_loss: 6.5118 - val_loss: 9.9938 - val_regression_loss: 5.9098 - lr: 6.2500e-06\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3340 - regression_loss: 6.5099 - val_loss: 9.9878 - val_regression_loss: 5.9034 - lr: 6.2500e-06\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5312 - regression_loss: 6.5086 - val_loss: 9.9816 - val_regression_loss: 5.9001 - lr: 6.2500e-06\n",
      "Epoch 101/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.4272 - regression_loss: 8.9802\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4691 - regression_loss: 6.5071 - val_loss: 9.9786 - val_regression_loss: 5.8983 - lr: 6.2500e-06\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3287 - regression_loss: 6.5051 - val_loss: 9.9755 - val_regression_loss: 5.8960 - lr: 3.1250e-06\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4846 - regression_loss: 6.5037 - val_loss: 9.9758 - val_regression_loss: 5.8977 - lr: 3.1250e-06\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4182 - regression_loss: 6.5016 - val_loss: 9.9754 - val_regression_loss: 5.8981 - lr: 3.1250e-06\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3864 - regression_loss: 6.5015 - val_loss: 9.9730 - val_regression_loss: 5.8963 - lr: 3.1250e-06\n",
      "Epoch 106/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.7296 - regression_loss: 7.2828\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5787 - regression_loss: 6.5003 - val_loss: 9.9707 - val_regression_loss: 5.8946 - lr: 3.1250e-06\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3655 - regression_loss: 6.4993 - val_loss: 9.9703 - val_regression_loss: 5.8944 - lr: 1.5625e-06\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4464 - regression_loss: 6.5001 - val_loss: 9.9711 - val_regression_loss: 5.8953 - lr: 1.5625e-06\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3998 - regression_loss: 6.5001 - val_loss: 9.9704 - val_regression_loss: 5.8954 - lr: 1.5625e-06\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4787 - regression_loss: 6.4990 - val_loss: 9.9667 - val_regression_loss: 5.8917 - lr: 1.5625e-06\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.2333 - regression_loss: 6.4977 - val_loss: 9.9649 - val_regression_loss: 5.8899 - lr: 1.5625e-06\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3635 - regression_loss: 6.4970 - val_loss: 9.9643 - val_regression_loss: 5.8894 - lr: 1.5625e-06\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4613 - regression_loss: 6.4965 - val_loss: 9.9635 - val_regression_loss: 5.8888 - lr: 1.5625e-06\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.2988 - regression_loss: 6.4964 - val_loss: 9.9621 - val_regression_loss: 5.8881 - lr: 1.5625e-06\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3865 - regression_loss: 6.4971 - val_loss: 9.9609 - val_regression_loss: 5.8865 - lr: 1.5625e-06\n",
      "Epoch 116/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.3790 - regression_loss: 7.9323\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3674 - regression_loss: 6.4956 - val_loss: 9.9611 - val_regression_loss: 5.8867 - lr: 1.5625e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3471 - regression_loss: 6.4952 - val_loss: 9.9609 - val_regression_loss: 5.8870 - lr: 7.8125e-07\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4065 - regression_loss: 6.4949 - val_loss: 9.9606 - val_regression_loss: 5.8870 - lr: 7.8125e-07\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4266 - regression_loss: 6.4945 - val_loss: 9.9600 - val_regression_loss: 5.8864 - lr: 7.8125e-07\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3583 - regression_loss: 6.4943 - val_loss: 9.9593 - val_regression_loss: 5.8859 - lr: 7.8125e-07\n",
      "Epoch 121/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.5725 - regression_loss: 8.1259\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4071 - regression_loss: 6.4939 - val_loss: 9.9587 - val_regression_loss: 5.8854 - lr: 7.8125e-07\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2967 - regression_loss: 6.4936 - val_loss: 9.9584 - val_regression_loss: 5.8852 - lr: 3.9062e-07\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3400 - regression_loss: 6.4936 - val_loss: 9.9583 - val_regression_loss: 5.8851 - lr: 3.9062e-07\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4144 - regression_loss: 6.4934 - val_loss: 9.9583 - val_regression_loss: 5.8852 - lr: 3.9062e-07\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4134 - regression_loss: 6.4932 - val_loss: 9.9581 - val_regression_loss: 5.8852 - lr: 3.9062e-07\n",
      "Epoch 126/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.1811 - regression_loss: 6.7345\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.2581 - regression_loss: 6.4931 - val_loss: 9.9578 - val_regression_loss: 5.8850 - lr: 3.9062e-07\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3612 - regression_loss: 6.4930 - val_loss: 9.9575 - val_regression_loss: 5.8848 - lr: 1.9531e-07\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.2092 - regression_loss: 6.4929 - val_loss: 9.9575 - val_regression_loss: 5.8847 - lr: 1.9531e-07\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.2811 - regression_loss: 6.4929 - val_loss: 9.9574 - val_regression_loss: 5.8848 - lr: 1.9531e-07\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4502 - regression_loss: 6.4928 - val_loss: 9.9572 - val_regression_loss: 5.8846 - lr: 1.9531e-07\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2860 - regression_loss: 6.4928 - val_loss: 9.9571 - val_regression_loss: 5.8844 - lr: 1.9531e-07\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2922 - regression_loss: 6.4929 - val_loss: 9.9573 - val_regression_loss: 5.8847 - lr: 1.9531e-07\n",
      "Epoch 133/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.2156 - regression_loss: 6.7690\n",
      "Epoch 133: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4446 - regression_loss: 6.4927 - val_loss: 9.9573 - val_regression_loss: 5.8847 - lr: 1.9531e-07\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4297 - regression_loss: 6.4926 - val_loss: 9.9571 - val_regression_loss: 5.8846 - lr: 9.7656e-08\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3828 - regression_loss: 6.4926 - val_loss: 9.9571 - val_regression_loss: 5.8846 - lr: 9.7656e-08\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3788 - regression_loss: 6.4924 - val_loss: 9.9571 - val_regression_loss: 5.8846 - lr: 9.7656e-08\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4578 - regression_loss: 6.4924 - val_loss: 9.9570 - val_regression_loss: 5.8845 - lr: 9.7656e-08\n",
      "Epoch 138/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.2982 - regression_loss: 6.8515\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3469 - regression_loss: 6.4924 - val_loss: 9.9570 - val_regression_loss: 5.8845 - lr: 9.7656e-08\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3806 - regression_loss: 6.4923 - val_loss: 9.9569 - val_regression_loss: 5.8845 - lr: 4.8828e-08\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3694 - regression_loss: 6.4924 - val_loss: 9.9569 - val_regression_loss: 5.8844 - lr: 4.8828e-08\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3890 - regression_loss: 6.4923 - val_loss: 9.9568 - val_regression_loss: 5.8844 - lr: 4.8828e-08\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4249 - regression_loss: 6.4923 - val_loss: 9.9568 - val_regression_loss: 5.8844 - lr: 4.8828e-08\n",
      "Epoch 143/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.8780 - regression_loss: 7.4313\n",
      "Epoch 143: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4359 - regression_loss: 6.4923 - val_loss: 9.9568 - val_regression_loss: 5.8843 - lr: 4.8828e-08\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3367 - regression_loss: 6.4923 - val_loss: 9.9567 - val_regression_loss: 5.8843 - lr: 2.4414e-08\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4584 - regression_loss: 6.4922 - val_loss: 9.9567 - val_regression_loss: 5.8843 - lr: 2.4414e-08\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3451 - regression_loss: 6.4922 - val_loss: 9.9567 - val_regression_loss: 5.8843 - lr: 2.4414e-08\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4101 - regression_loss: 6.4922 - val_loss: 9.9567 - val_regression_loss: 5.8843 - lr: 2.4414e-08\n",
      "Epoch 148/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.0593 - regression_loss: 7.6127\n",
      "Epoch 148: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3615 - regression_loss: 6.4922 - val_loss: 9.9567 - val_regression_loss: 5.8843 - lr: 2.4414e-08\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3940 - regression_loss: 6.4922 - val_loss: 9.9567 - val_regression_loss: 5.8843 - lr: 1.2207e-08\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4926 - regression_loss: 6.4922 - val_loss: 9.9567 - val_regression_loss: 5.8843 - lr: 1.2207e-08\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3505 - regression_loss: 6.4922 - val_loss: 9.9567 - val_regression_loss: 5.8843 - lr: 1.2207e-08\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3310 - regression_loss: 6.4922 - val_loss: 9.9567 - val_regression_loss: 5.8843 - lr: 1.2207e-08\n",
      "Epoch 153/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.3245 - regression_loss: 5.8779\n",
      "Epoch 153: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3964 - regression_loss: 6.4922 - val_loss: 9.9567 - val_regression_loss: 5.8842 - lr: 1.2207e-08\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4105 - regression_loss: 6.4922 - val_loss: 9.9567 - val_regression_loss: 5.8842 - lr: 6.1035e-09\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4475 - regression_loss: 6.4922 - val_loss: 9.9567 - val_regression_loss: 5.8842 - lr: 6.1035e-09\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3851 - regression_loss: 6.4922 - val_loss: 9.9567 - val_regression_loss: 5.8842 - lr: 6.1035e-09\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.1782 - regression_loss: 6.4922 - val_loss: 9.9567 - val_regression_loss: 5.8842 - lr: 6.1035e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4808 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 6.1035e-09\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3786 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 6.1035e-09\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3784 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 6.1035e-09\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3749 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 6.1035e-09\n",
      "Epoch 162/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.1742 - regression_loss: 6.7275\n",
      "Epoch 162: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4355 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 6.1035e-09\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3266 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 3.0518e-09\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4024 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 3.0518e-09\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3624 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 3.0518e-09\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2587 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 3.0518e-09\n",
      "Epoch 167/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.9247 - regression_loss: 7.4781\n",
      "Epoch 167: ReduceLROnPlateau reducing learning rate to 1.5258788677030566e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4278 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 3.0518e-09\n",
      "Epoch 168/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3638 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 1.5259e-09\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4797 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 1.5259e-09\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4162 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 1.5259e-09\n",
      "Epoch 171/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4128 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 1.5259e-09\n",
      "Epoch 172/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.6858 - regression_loss: 8.2392\n",
      "Epoch 172: ReduceLROnPlateau reducing learning rate to 7.629394338515283e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4274 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 1.5259e-09\n",
      "Epoch 173/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4136 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 7.6294e-10\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5022 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 7.6294e-10\n",
      "Epoch 175/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3765 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 7.6294e-10\n",
      "Epoch 176/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4599 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 7.6294e-10\n",
      "Epoch 177/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.2384 - regression_loss: 6.7918\n",
      "Epoch 177: ReduceLROnPlateau reducing learning rate to 3.8146971692576415e-10.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5211 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 7.6294e-10\n",
      "Epoch 178/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4006 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 3.8147e-10\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3859 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 3.8147e-10\n",
      "Epoch 180/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3796 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 3.8147e-10\n",
      "Epoch 181/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4555 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 3.8147e-10\n",
      "Epoch 182/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.5723 - regression_loss: 6.1257\n",
      "Epoch 182: ReduceLROnPlateau reducing learning rate to 1.9073485846288207e-10.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4168 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 3.8147e-10\n",
      "Epoch 183/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3156 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 1.9073e-10\n",
      "Epoch 184/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3050 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 1.9073e-10\n",
      "Epoch 185/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4552 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 1.9073e-10\n",
      "Epoch 186/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4158 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 1.9073e-10\n",
      "Epoch 187/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.4086 - regression_loss: 6.9619\n",
      "Epoch 187: ReduceLROnPlateau reducing learning rate to 9.536742923144104e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3371 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 1.9073e-10\n",
      "Epoch 188/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4447 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 9.5367e-11\n",
      "Epoch 189/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3866 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 9.5367e-11\n",
      "Epoch 190/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3177 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 9.5367e-11\n",
      "Epoch 191/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4283 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 9.5367e-11\n",
      "Epoch 192/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.4549 - regression_loss: 8.0083\n",
      "Epoch 192: ReduceLROnPlateau reducing learning rate to 4.768371461572052e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4937 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 9.5367e-11\n",
      "Epoch 193/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3389 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 4.7684e-11\n",
      "Epoch 194/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3165 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 4.7684e-11\n",
      "Epoch 195/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.2849 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 4.7684e-11\n",
      "Epoch 196/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3994 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 4.7684e-11\n",
      "Epoch 197/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.0410 - regression_loss: 6.5944\n",
      "Epoch 197: ReduceLROnPlateau reducing learning rate to 2.384185730786026e-11.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3369 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 4.7684e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4128 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 2.3842e-11\n",
      "Epoch 199/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4281 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 2.3842e-11\n",
      "Epoch 200/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3884 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 2.3842e-11\n",
      "Epoch 201/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5003 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 2.3842e-11\n",
      "Epoch 202/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.0295 - regression_loss: 7.5828\n",
      "Epoch 202: ReduceLROnPlateau reducing learning rate to 1.192092865393013e-11.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4389 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 2.3842e-11\n",
      "Epoch 203/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3261 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 1.1921e-11\n",
      "Epoch 204/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4722 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 1.1921e-11\n",
      "Epoch 205/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3467 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 1.1921e-11\n",
      "Epoch 206/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4018 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 1.1921e-11\n",
      "Epoch 207/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.3296 - regression_loss: 8.8829\n",
      "Epoch 207: ReduceLROnPlateau reducing learning rate to 5.960464326965065e-12.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4525 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 1.1921e-11\n",
      "Epoch 208/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4477 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 5.9605e-12\n",
      "Epoch 209/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4678 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 5.9605e-12\n",
      "Epoch 210/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2531 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 5.9605e-12\n",
      "Epoch 211/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4683 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 5.9605e-12\n",
      "Epoch 212/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.9524 - regression_loss: 7.5058\n",
      "Epoch 212: ReduceLROnPlateau reducing learning rate to 2.9802321634825324e-12.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3640 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 5.9605e-12\n",
      "Epoch 213/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2306 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 2.9802e-12\n",
      "Epoch 214/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5191 - regression_loss: 6.4922 - val_loss: 9.9566 - val_regression_loss: 5.8842 - lr: 2.9802e-12\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 26ms/step - loss: 79.3719 - regression_loss: 71.8468 - val_loss: 46.4232 - val_regression_loss: 35.9588 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 55.3876 - regression_loss: 51.1238 - val_loss: 32.4143 - val_regression_loss: 23.9270 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 44.0636 - regression_loss: 39.8153 - val_loss: 28.2699 - val_regression_loss: 20.1537 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 36.4584 - regression_loss: 32.4739 - val_loss: 29.6213 - val_regression_loss: 20.6580 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.8447 - regression_loss: 27.8802 - val_loss: 26.6992 - val_regression_loss: 18.2204 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.9650 - regression_loss: 23.6882 - val_loss: 24.1623 - val_regression_loss: 16.2498 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.2748 - regression_loss: 21.1924 - val_loss: 24.5496 - val_regression_loss: 16.4513 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.8546 - regression_loss: 18.6796 - val_loss: 23.0880 - val_regression_loss: 15.3028 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.2017 - regression_loss: 17.1343 - val_loss: 21.4091 - val_regression_loss: 14.0378 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.4224 - regression_loss: 15.2638 - val_loss: 21.1124 - val_regression_loss: 13.8005 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6640 - regression_loss: 13.9611 - val_loss: 19.6829 - val_regression_loss: 12.7175 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.9671 - regression_loss: 12.5930 - val_loss: 18.4782 - val_regression_loss: 11.8169 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.0654 - regression_loss: 11.4469 - val_loss: 17.3933 - val_regression_loss: 11.0274 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7736 - regression_loss: 10.3544 - val_loss: 16.3824 - val_regression_loss: 10.2957 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.7118 - regression_loss: 9.3968 - val_loss: 15.2379 - val_regression_loss: 9.4506 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3091 - regression_loss: 8.4572 - val_loss: 14.2183 - val_regression_loss: 8.6986 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8988 - regression_loss: 7.7585 - val_loss: 13.8096 - val_regression_loss: 8.4395 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0141 - regression_loss: 6.9838 - val_loss: 12.7856 - val_regression_loss: 7.6644 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3236 - regression_loss: 6.3757 - val_loss: 12.4638 - val_regression_loss: 7.4752 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5748 - regression_loss: 5.8201 - val_loss: 11.6955 - val_regression_loss: 6.8972 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6888 - regression_loss: 5.3762 - val_loss: 11.2444 - val_regression_loss: 6.5723 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6301 - regression_loss: 4.9134 - val_loss: 10.8479 - val_regression_loss: 6.2874 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9840 - regression_loss: 4.5853 - val_loss: 10.4348 - val_regression_loss: 5.9619 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.1707 - regression_loss: 4.3181 - val_loss: 10.2434 - val_regression_loss: 5.8298 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8463 - regression_loss: 4.0369 - val_loss: 9.8566 - val_regression_loss: 5.5324 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5488 - regression_loss: 3.8025 - val_loss: 9.6403 - val_regression_loss: 5.3665 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2903 - regression_loss: 3.6034 - val_loss: 9.5766 - val_regression_loss: 5.3323 - lr: 1.0000e-04\n",
      "Epoch 28/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 6.0819 - regression_loss: 3.4463 - val_loss: 9.3167 - val_regression_loss: 5.1119 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.0258 - regression_loss: 3.2792 - val_loss: 8.9712 - val_regression_loss: 4.8421 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8144 - regression_loss: 3.1245 - val_loss: 8.7479 - val_regression_loss: 4.6671 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.4920 - regression_loss: 2.9875 - val_loss: 8.7507 - val_regression_loss: 4.6848 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6158 - regression_loss: 2.9203 - val_loss: 8.2218 - val_regression_loss: 4.2580 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.3307 - regression_loss: 2.7457 - val_loss: 8.1903 - val_regression_loss: 4.2492 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.2436 - regression_loss: 2.6310 - val_loss: 7.9157 - val_regression_loss: 4.0277 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.2614 - regression_loss: 2.5509 - val_loss: 7.8823 - val_regression_loss: 4.0205 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.1222 - regression_loss: 2.4903 - val_loss: 7.5736 - val_regression_loss: 3.7556 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.0358 - regression_loss: 2.3878 - val_loss: 7.5198 - val_regression_loss: 3.7414 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8542 - regression_loss: 2.2886 - val_loss: 7.2601 - val_regression_loss: 3.5407 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8309 - regression_loss: 2.2055 - val_loss: 7.2257 - val_regression_loss: 3.5283 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7156 - regression_loss: 2.1384 - val_loss: 7.0114 - val_regression_loss: 3.3582 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6804 - regression_loss: 2.0692 - val_loss: 6.8126 - val_regression_loss: 3.2091 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6151 - regression_loss: 2.0076 - val_loss: 6.7687 - val_regression_loss: 3.1901 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5653 - regression_loss: 1.9480 - val_loss: 6.4811 - val_regression_loss: 2.9512 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.5195 - regression_loss: 1.9201 - val_loss: 6.5749 - val_regression_loss: 3.0535 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.2860 - regression_loss: 1.8451 - val_loss: 6.3303 - val_regression_loss: 2.8516 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3573 - regression_loss: 1.8142 - val_loss: 6.2072 - val_regression_loss: 2.7712 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3029 - regression_loss: 1.7600 - val_loss: 6.1560 - val_regression_loss: 2.7442 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3336 - regression_loss: 1.7491 - val_loss: 6.0860 - val_regression_loss: 2.6854 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3101 - regression_loss: 1.7430 - val_loss: 6.0783 - val_regression_loss: 2.7000 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2675 - regression_loss: 1.6779 - val_loss: 5.7988 - val_regression_loss: 2.4651 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2571 - regression_loss: 1.7028 - val_loss: 5.8170 - val_regression_loss: 2.5003 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0640 - regression_loss: 1.5225 - val_loss: 5.5714 - val_regression_loss: 2.2985 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0979 - regression_loss: 1.5405 - val_loss: 5.5550 - val_regression_loss: 2.3090 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9165 - regression_loss: 1.4740 - val_loss: 5.4785 - val_regression_loss: 2.2406 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0035 - regression_loss: 1.4557 - val_loss: 5.4695 - val_regression_loss: 2.2483 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9484 - regression_loss: 1.4056 - val_loss: 5.3377 - val_regression_loss: 2.1418 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9247 - regression_loss: 1.3781 - val_loss: 5.3362 - val_regression_loss: 2.1596 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8478 - regression_loss: 1.3599 - val_loss: 5.2032 - val_regression_loss: 2.0618 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8438 - regression_loss: 1.3343 - val_loss: 5.1525 - val_regression_loss: 2.0247 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.8430 - regression_loss: 1.2936 - val_loss: 5.1123 - val_regression_loss: 1.9929 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7593 - regression_loss: 1.3011 - val_loss: 5.0401 - val_regression_loss: 1.9313 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7915 - regression_loss: 1.2697 - val_loss: 5.0594 - val_regression_loss: 1.9654 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6956 - regression_loss: 1.2341 - val_loss: 4.9505 - val_regression_loss: 1.8702 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7504 - regression_loss: 1.2174 - val_loss: 4.9420 - val_regression_loss: 1.8752 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6837 - regression_loss: 1.1904 - val_loss: 4.8367 - val_regression_loss: 1.7884 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6453 - regression_loss: 1.1496 - val_loss: 4.7785 - val_regression_loss: 1.7413 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6187 - regression_loss: 1.1304 - val_loss: 4.7127 - val_regression_loss: 1.7002 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6420 - regression_loss: 1.1274 - val_loss: 4.7174 - val_regression_loss: 1.7157 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6325 - regression_loss: 1.1438 - val_loss: 4.5861 - val_regression_loss: 1.5865 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6114 - regression_loss: 1.0879 - val_loss: 4.6817 - val_regression_loss: 1.6910 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5435 - regression_loss: 1.0661 - val_loss: 4.5095 - val_regression_loss: 1.5408 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5573 - regression_loss: 1.0611 - val_loss: 4.5497 - val_regression_loss: 1.5960 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5532 - regression_loss: 1.0484 - val_loss: 4.4770 - val_regression_loss: 1.5333 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5350 - regression_loss: 1.0498 - val_loss: 4.4149 - val_regression_loss: 1.4588 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4490 - regression_loss: 0.9905 - val_loss: 4.5828 - val_regression_loss: 1.6412 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5513 - regression_loss: 1.0533 - val_loss: 4.3712 - val_regression_loss: 1.4247 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5374 - regression_loss: 1.0822 - val_loss: 4.7222 - val_regression_loss: 1.7708 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4711 - regression_loss: 1.0155 - val_loss: 4.3446 - val_regression_loss: 1.4024 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5262 - regression_loss: 1.0232 - val_loss: 4.4896 - val_regression_loss: 1.5709 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4346 - regression_loss: 0.9915 - val_loss: 4.2138 - val_regression_loss: 1.3209 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4537 - regression_loss: 0.9805 - val_loss: 4.2193 - val_regression_loss: 1.3417 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3990 - regression_loss: 0.9191 - val_loss: 4.2839 - val_regression_loss: 1.3948 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3531 - regression_loss: 0.9090 - val_loss: 4.1950 - val_regression_loss: 1.3244 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3214 - regression_loss: 0.8862 - val_loss: 4.1381 - val_regression_loss: 1.2718 - lr: 1.0000e-04\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3575 - regression_loss: 0.8786 - val_loss: 4.1249 - val_regression_loss: 1.2725 - lr: 1.0000e-04\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3546 - regression_loss: 0.8653 - val_loss: 4.0510 - val_regression_loss: 1.2039 - lr: 1.0000e-04\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3450 - regression_loss: 0.8660 - val_loss: 4.2430 - val_regression_loss: 1.3981 - lr: 1.0000e-04\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3486 - regression_loss: 0.8662 - val_loss: 4.0133 - val_regression_loss: 1.1745 - lr: 1.0000e-04\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3039 - regression_loss: 0.8524 - val_loss: 4.1568 - val_regression_loss: 1.3270 - lr: 1.0000e-04\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3190 - regression_loss: 0.8508 - val_loss: 3.9681 - val_regression_loss: 1.1400 - lr: 1.0000e-04\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3011 - regression_loss: 0.8524 - val_loss: 4.0249 - val_regression_loss: 1.2021 - lr: 1.0000e-04\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2636 - regression_loss: 0.8149 - val_loss: 3.9766 - val_regression_loss: 1.1588 - lr: 1.0000e-04\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2407 - regression_loss: 0.8519 - val_loss: 4.0335 - val_regression_loss: 1.2197 - lr: 1.0000e-04\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.2766 - regression_loss: 0.8060 - val_loss: 3.9183 - val_regression_loss: 1.1322 - lr: 1.0000e-04\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.2697 - regression_loss: 0.8178 - val_loss: 3.8924 - val_regression_loss: 1.0855 - lr: 1.0000e-04\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.2823 - regression_loss: 0.8228 - val_loss: 3.9703 - val_regression_loss: 1.1691 - lr: 1.0000e-04\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2432 - regression_loss: 0.7811 - val_loss: 3.8745 - val_regression_loss: 1.1017 - lr: 1.0000e-04\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1952 - regression_loss: 0.7747 - val_loss: 3.9415 - val_regression_loss: 1.1549 - lr: 1.0000e-04\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1955 - regression_loss: 0.7565 - val_loss: 3.8295 - val_regression_loss: 1.0480 - lr: 1.0000e-04\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2357 - regression_loss: 0.7702 - val_loss: 3.8787 - val_regression_loss: 1.0995 - lr: 1.0000e-04\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1717 - regression_loss: 0.7454 - val_loss: 3.7973 - val_regression_loss: 1.0280 - lr: 1.0000e-04\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1673 - regression_loss: 0.7280 - val_loss: 3.8240 - val_regression_loss: 1.0649 - lr: 1.0000e-04\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1653 - regression_loss: 0.7136 - val_loss: 3.8205 - val_regression_loss: 1.0437 - lr: 1.0000e-04\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.2088 - regression_loss: 0.7666 - val_loss: 3.7962 - val_regression_loss: 1.0130 - lr: 1.0000e-04\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.2236 - regression_loss: 0.7688 - val_loss: 3.8472 - val_regression_loss: 1.0806 - lr: 1.0000e-04\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1697 - regression_loss: 0.7171 - val_loss: 3.7367 - val_regression_loss: 0.9910 - lr: 1.0000e-04\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1333 - regression_loss: 0.6942 - val_loss: 3.7228 - val_regression_loss: 0.9829 - lr: 1.0000e-04\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1272 - regression_loss: 0.6930 - val_loss: 3.7149 - val_regression_loss: 0.9577 - lr: 1.0000e-04\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1349 - regression_loss: 0.6979 - val_loss: 3.7915 - val_regression_loss: 1.0510 - lr: 1.0000e-04\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1134 - regression_loss: 0.6731 - val_loss: 3.6932 - val_regression_loss: 0.9519 - lr: 1.0000e-04\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1095 - regression_loss: 0.6746 - val_loss: 3.6975 - val_regression_loss: 0.9630 - lr: 1.0000e-04\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1089 - regression_loss: 0.6689 - val_loss: 3.6921 - val_regression_loss: 0.9681 - lr: 1.0000e-04\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1033 - regression_loss: 0.6602 - val_loss: 3.6839 - val_regression_loss: 0.9542 - lr: 1.0000e-04\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0758 - regression_loss: 0.6429 - val_loss: 3.6438 - val_regression_loss: 0.9212 - lr: 1.0000e-04\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0806 - regression_loss: 0.6695 - val_loss: 3.6634 - val_regression_loss: 0.9465 - lr: 1.0000e-04\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1147 - regression_loss: 0.6851 - val_loss: 3.7296 - val_regression_loss: 1.0074 - lr: 1.0000e-04\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0974 - regression_loss: 0.6781 - val_loss: 3.6021 - val_regression_loss: 0.8849 - lr: 1.0000e-04\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1216 - regression_loss: 0.6936 - val_loss: 3.5947 - val_regression_loss: 0.8764 - lr: 1.0000e-04\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.0466 - regression_loss: 0.6391 - val_loss: 3.6624 - val_regression_loss: 0.9642 - lr: 1.0000e-04\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0691 - regression_loss: 0.6377 - val_loss: 3.5625 - val_regression_loss: 0.8617 - lr: 1.0000e-04\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0446 - regression_loss: 0.6137 - val_loss: 3.5963 - val_regression_loss: 0.8990 - lr: 1.0000e-04\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.0354 - regression_loss: 0.6203 - val_loss: 3.5485 - val_regression_loss: 0.8511 - lr: 1.0000e-04\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.0278 - regression_loss: 0.6042 - val_loss: 3.6208 - val_regression_loss: 0.9297 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.0592 - regression_loss: 0.6249 - val_loss: 3.5476 - val_regression_loss: 0.8665 - lr: 1.0000e-04\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.0348 - regression_loss: 0.6214 - val_loss: 3.5730 - val_regression_loss: 0.8543 - lr: 1.0000e-04\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0919 - regression_loss: 0.6638 - val_loss: 3.8305 - val_regression_loss: 1.1147 - lr: 1.0000e-04\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1193 - regression_loss: 0.7037 - val_loss: 3.5781 - val_regression_loss: 0.8827 - lr: 1.0000e-04\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.0198 - regression_loss: 0.6063 - val_loss: 3.6321 - val_regression_loss: 0.9319 - lr: 1.0000e-04\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.0141 - regression_loss: 0.6037 - val_loss: 3.5207 - val_regression_loss: 0.8397 - lr: 1.0000e-04\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9982 - regression_loss: 0.5826 - val_loss: 3.5043 - val_regression_loss: 0.8211 - lr: 1.0000e-04\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9697 - regression_loss: 0.5727 - val_loss: 3.5598 - val_regression_loss: 0.8817 - lr: 1.0000e-04\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9882 - regression_loss: 0.5760 - val_loss: 3.4881 - val_regression_loss: 0.8148 - lr: 1.0000e-04\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9818 - regression_loss: 0.5677 - val_loss: 3.4650 - val_regression_loss: 0.7939 - lr: 1.0000e-04\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0016 - regression_loss: 0.5950 - val_loss: 3.5748 - val_regression_loss: 0.9110 - lr: 1.0000e-04\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.0328 - regression_loss: 0.6238 - val_loss: 3.4747 - val_regression_loss: 0.8031 - lr: 1.0000e-04\n",
      "Epoch 136/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.7807 - regression_loss: 0.4138\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0114 - regression_loss: 0.5997 - val_loss: 3.4712 - val_regression_loss: 0.7984 - lr: 1.0000e-04\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9950 - regression_loss: 0.5885 - val_loss: 3.5111 - val_regression_loss: 0.8559 - lr: 5.0000e-05\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9946 - regression_loss: 0.5820 - val_loss: 3.4532 - val_regression_loss: 0.7834 - lr: 5.0000e-05\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9674 - regression_loss: 0.5556 - val_loss: 3.5025 - val_regression_loss: 0.8486 - lr: 5.0000e-05\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9922 - regression_loss: 0.5834 - val_loss: 3.4573 - val_regression_loss: 0.7939 - lr: 5.0000e-05\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9577 - regression_loss: 0.5537 - val_loss: 3.5007 - val_regression_loss: 0.8396 - lr: 5.0000e-05\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9542 - regression_loss: 0.5484 - val_loss: 3.4547 - val_regression_loss: 0.7892 - lr: 5.0000e-05\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9426 - regression_loss: 0.5375 - val_loss: 3.4686 - val_regression_loss: 0.8208 - lr: 5.0000e-05\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9509 - regression_loss: 0.5369 - val_loss: 3.4337 - val_regression_loss: 0.7774 - lr: 5.0000e-05\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9235 - regression_loss: 0.5380 - val_loss: 3.4701 - val_regression_loss: 0.8214 - lr: 5.0000e-05\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9314 - regression_loss: 0.5300 - val_loss: 3.4263 - val_regression_loss: 0.7738 - lr: 5.0000e-05\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9640 - regression_loss: 0.5478 - val_loss: 3.4590 - val_regression_loss: 0.8096 - lr: 5.0000e-05\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9523 - regression_loss: 0.5474 - val_loss: 3.4304 - val_regression_loss: 0.7787 - lr: 5.0000e-05\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9442 - regression_loss: 0.5380 - val_loss: 3.4322 - val_regression_loss: 0.7862 - lr: 5.0000e-05\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9120 - regression_loss: 0.5251 - val_loss: 3.4258 - val_regression_loss: 0.7806 - lr: 5.0000e-05\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9069 - regression_loss: 0.5186 - val_loss: 3.4194 - val_regression_loss: 0.7724 - lr: 5.0000e-05\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9354 - regression_loss: 0.5269 - val_loss: 3.4166 - val_regression_loss: 0.7751 - lr: 5.0000e-05\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9130 - regression_loss: 0.5172 - val_loss: 3.4177 - val_regression_loss: 0.7723 - lr: 5.0000e-05\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9153 - regression_loss: 0.5134 - val_loss: 3.4456 - val_regression_loss: 0.8065 - lr: 5.0000e-05\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9065 - regression_loss: 0.5152 - val_loss: 3.4193 - val_regression_loss: 0.7707 - lr: 5.0000e-05\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9136 - regression_loss: 0.5230 - val_loss: 3.4436 - val_regression_loss: 0.8004 - lr: 5.0000e-05\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9096 - regression_loss: 0.5216 - val_loss: 3.4001 - val_regression_loss: 0.7575 - lr: 5.0000e-05\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9009 - regression_loss: 0.5098 - val_loss: 3.4158 - val_regression_loss: 0.7844 - lr: 5.0000e-05\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9058 - regression_loss: 0.5120 - val_loss: 3.4008 - val_regression_loss: 0.7636 - lr: 5.0000e-05\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9001 - regression_loss: 0.5061 - val_loss: 3.4215 - val_regression_loss: 0.7840 - lr: 5.0000e-05\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9113 - regression_loss: 0.5096 - val_loss: 3.4081 - val_regression_loss: 0.7763 - lr: 5.0000e-05\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9063 - regression_loss: 0.5079 - val_loss: 3.4036 - val_regression_loss: 0.7723 - lr: 5.0000e-05\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9023 - regression_loss: 0.5096 - val_loss: 3.3972 - val_regression_loss: 0.7705 - lr: 5.0000e-05\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9037 - regression_loss: 0.5129 - val_loss: 3.3886 - val_regression_loss: 0.7569 - lr: 5.0000e-05\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8792 - regression_loss: 0.5025 - val_loss: 3.3999 - val_regression_loss: 0.7651 - lr: 5.0000e-05\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8857 - regression_loss: 0.4948 - val_loss: 3.3950 - val_regression_loss: 0.7666 - lr: 5.0000e-05\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8805 - regression_loss: 0.4960 - val_loss: 3.3958 - val_regression_loss: 0.7694 - lr: 5.0000e-05\n",
      "Epoch 168/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8898 - regression_loss: 0.4917 - val_loss: 3.3900 - val_regression_loss: 0.7594 - lr: 5.0000e-05\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8883 - regression_loss: 0.4971 - val_loss: 3.3988 - val_regression_loss: 0.7728 - lr: 5.0000e-05\n",
      "Epoch 170/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.9399 - regression_loss: 0.5884\n",
      "Epoch 170: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8882 - regression_loss: 0.4935 - val_loss: 3.3779 - val_regression_loss: 0.7541 - lr: 5.0000e-05\n",
      "Epoch 171/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8711 - regression_loss: 0.4899 - val_loss: 3.3887 - val_regression_loss: 0.7645 - lr: 2.5000e-05\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8639 - regression_loss: 0.4850 - val_loss: 3.3847 - val_regression_loss: 0.7582 - lr: 2.5000e-05\n",
      "Epoch 173/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8665 - regression_loss: 0.4865 - val_loss: 3.3873 - val_regression_loss: 0.7597 - lr: 2.5000e-05\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8650 - regression_loss: 0.4852 - val_loss: 3.3863 - val_regression_loss: 0.7642 - lr: 2.5000e-05\n",
      "Epoch 175/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8673 - regression_loss: 0.4832 - val_loss: 3.3753 - val_regression_loss: 0.7534 - lr: 2.5000e-05\n",
      "Epoch 176/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8577 - regression_loss: 0.4834 - val_loss: 3.3839 - val_regression_loss: 0.7612 - lr: 2.5000e-05\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8617 - regression_loss: 0.4814 - val_loss: 3.3800 - val_regression_loss: 0.7551 - lr: 2.5000e-05\n",
      "Epoch 178/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8690 - regression_loss: 0.4883 - val_loss: 3.3783 - val_regression_loss: 0.7543 - lr: 2.5000e-05\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8740 - regression_loss: 0.4892 - val_loss: 3.3873 - val_regression_loss: 0.7683 - lr: 2.5000e-05\n",
      "Epoch 180/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8539 - regression_loss: 0.4796 - val_loss: 3.3682 - val_regression_loss: 0.7462 - lr: 2.5000e-05\n",
      "Epoch 181/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8682 - regression_loss: 0.4830 - val_loss: 3.3722 - val_regression_loss: 0.7523 - lr: 2.5000e-05\n",
      "Epoch 182/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8660 - regression_loss: 0.4831 - val_loss: 3.3783 - val_regression_loss: 0.7612 - lr: 2.5000e-05\n",
      "Epoch 183/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8594 - regression_loss: 0.4811 - val_loss: 3.3683 - val_regression_loss: 0.7462 - lr: 2.5000e-05\n",
      "Epoch 184/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8739 - regression_loss: 0.4893 - val_loss: 3.3784 - val_regression_loss: 0.7619 - lr: 2.5000e-05\n",
      "Epoch 185/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8535 - regression_loss: 0.4809 - val_loss: 3.3704 - val_regression_loss: 0.7494 - lr: 2.5000e-05\n",
      "Epoch 186/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8560 - regression_loss: 0.4789 - val_loss: 3.3748 - val_regression_loss: 0.7565 - lr: 2.5000e-05\n",
      "Epoch 187/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8738 - regression_loss: 0.4836 - val_loss: 3.3654 - val_regression_loss: 0.7523 - lr: 2.5000e-05\n",
      "Epoch 188/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8582 - regression_loss: 0.4770 - val_loss: 3.3655 - val_regression_loss: 0.7471 - lr: 2.5000e-05\n",
      "Epoch 189/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8607 - regression_loss: 0.4751 - val_loss: 3.3789 - val_regression_loss: 0.7610 - lr: 2.5000e-05\n",
      "Epoch 190/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.0668 - regression_loss: 0.7203\n",
      "Epoch 190: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8601 - regression_loss: 0.4771 - val_loss: 3.3655 - val_regression_loss: 0.7494 - lr: 2.5000e-05\n",
      "Epoch 191/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8564 - regression_loss: 0.4718 - val_loss: 3.3646 - val_regression_loss: 0.7483 - lr: 1.2500e-05\n",
      "Epoch 192/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8532 - regression_loss: 0.4728 - val_loss: 3.3658 - val_regression_loss: 0.7515 - lr: 1.2500e-05\n",
      "Epoch 193/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8634 - regression_loss: 0.4753 - val_loss: 3.3686 - val_regression_loss: 0.7560 - lr: 1.2500e-05\n",
      "Epoch 194/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8561 - regression_loss: 0.4707 - val_loss: 3.3604 - val_regression_loss: 0.7459 - lr: 1.2500e-05\n",
      "Epoch 195/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8367 - regression_loss: 0.4725 - val_loss: 3.3628 - val_regression_loss: 0.7479 - lr: 1.2500e-05\n",
      "Epoch 196/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8364 - regression_loss: 0.4700 - val_loss: 3.3605 - val_regression_loss: 0.7473 - lr: 1.2500e-05\n",
      "Epoch 197/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8493 - regression_loss: 0.4688 - val_loss: 3.3631 - val_regression_loss: 0.7501 - lr: 1.2500e-05\n",
      "Epoch 198/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8518 - regression_loss: 0.4703 - val_loss: 3.3646 - val_regression_loss: 0.7521 - lr: 1.2500e-05\n",
      "Epoch 199/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8470 - regression_loss: 0.4696 - val_loss: 3.3613 - val_regression_loss: 0.7475 - lr: 1.2500e-05\n",
      "Epoch 200/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8470 - regression_loss: 0.4691 - val_loss: 3.3627 - val_regression_loss: 0.7498 - lr: 1.2500e-05\n",
      "Epoch 201/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.9183 - regression_loss: 0.5734\n",
      "Epoch 201: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8374 - regression_loss: 0.4681 - val_loss: 3.3628 - val_regression_loss: 0.7492 - lr: 1.2500e-05\n",
      "Epoch 202/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8419 - regression_loss: 0.4685 - val_loss: 3.3600 - val_regression_loss: 0.7467 - lr: 6.2500e-06\n",
      "Epoch 203/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8523 - regression_loss: 0.4675 - val_loss: 3.3626 - val_regression_loss: 0.7501 - lr: 6.2500e-06\n",
      "Epoch 204/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8454 - regression_loss: 0.4676 - val_loss: 3.3620 - val_regression_loss: 0.7498 - lr: 6.2500e-06\n",
      "Epoch 205/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8518 - regression_loss: 0.4675 - val_loss: 3.3615 - val_regression_loss: 0.7498 - lr: 6.2500e-06\n",
      "Epoch 206/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.8211 - regression_loss: 0.4766\n",
      "Epoch 206: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8436 - regression_loss: 0.4670 - val_loss: 3.3618 - val_regression_loss: 0.7497 - lr: 6.2500e-06\n",
      "Epoch 207/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8458 - regression_loss: 0.4669 - val_loss: 3.3619 - val_regression_loss: 0.7501 - lr: 3.1250e-06\n",
      "Epoch 208/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8477 - regression_loss: 0.4663 - val_loss: 3.3602 - val_regression_loss: 0.7479 - lr: 3.1250e-06\n",
      "Epoch 209/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8433 - regression_loss: 0.4663 - val_loss: 3.3595 - val_regression_loss: 0.7471 - lr: 3.1250e-06\n",
      "Epoch 210/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8421 - regression_loss: 0.4663 - val_loss: 3.3599 - val_regression_loss: 0.7473 - lr: 3.1250e-06\n",
      "Epoch 211/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.7705 - regression_loss: 0.4262\n",
      "Epoch 211: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8455 - regression_loss: 0.4658 - val_loss: 3.3604 - val_regression_loss: 0.7481 - lr: 3.1250e-06\n",
      "Epoch 212/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8385 - regression_loss: 0.4657 - val_loss: 3.3610 - val_regression_loss: 0.7491 - lr: 1.5625e-06\n",
      "Epoch 213/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8450 - regression_loss: 0.4655 - val_loss: 3.3606 - val_regression_loss: 0.7488 - lr: 1.5625e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8388 - regression_loss: 0.4655 - val_loss: 3.3605 - val_regression_loss: 0.7489 - lr: 1.5625e-06\n",
      "Epoch 215/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8368 - regression_loss: 0.4656 - val_loss: 3.3608 - val_regression_loss: 0.7492 - lr: 1.5625e-06\n",
      "Epoch 216/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.9384 - regression_loss: 0.5942\n",
      "Epoch 216: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8450 - regression_loss: 0.4654 - val_loss: 3.3606 - val_regression_loss: 0.7491 - lr: 1.5625e-06\n",
      "Epoch 217/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8287 - regression_loss: 0.4653 - val_loss: 3.3604 - val_regression_loss: 0.7489 - lr: 7.8125e-07\n",
      "Epoch 218/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8443 - regression_loss: 0.4653 - val_loss: 3.3601 - val_regression_loss: 0.7486 - lr: 7.8125e-07\n",
      "Epoch 219/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8461 - regression_loss: 0.4653 - val_loss: 3.3599 - val_regression_loss: 0.7484 - lr: 7.8125e-07\n",
      "Epoch 220/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8459 - regression_loss: 0.4652 - val_loss: 3.3597 - val_regression_loss: 0.7481 - lr: 7.8125e-07\n",
      "Epoch 221/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8418 - regression_loss: 0.4652 - val_loss: 3.3597 - val_regression_loss: 0.7481 - lr: 7.8125e-07\n",
      "Epoch 222/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.8830 - regression_loss: 0.5389\n",
      "Epoch 222: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8524 - regression_loss: 0.4652 - val_loss: 3.3597 - val_regression_loss: 0.7482 - lr: 7.8125e-07\n",
      "Epoch 223/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8461 - regression_loss: 0.4651 - val_loss: 3.3598 - val_regression_loss: 0.7482 - lr: 3.9062e-07\n",
      "Epoch 224/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8365 - regression_loss: 0.4651 - val_loss: 3.3598 - val_regression_loss: 0.7482 - lr: 3.9062e-07\n",
      "Epoch 225/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8371 - regression_loss: 0.4651 - val_loss: 3.3598 - val_regression_loss: 0.7482 - lr: 3.9062e-07\n",
      "Epoch 226/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8455 - regression_loss: 0.4651 - val_loss: 3.3599 - val_regression_loss: 0.7484 - lr: 3.9062e-07\n",
      "Epoch 227/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.8142 - regression_loss: 0.4701\n",
      "Epoch 227: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8449 - regression_loss: 0.4651 - val_loss: 3.3600 - val_regression_loss: 0.7485 - lr: 3.9062e-07\n",
      "Epoch 228/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8277 - regression_loss: 0.4650 - val_loss: 3.3599 - val_regression_loss: 0.7484 - lr: 1.9531e-07\n",
      "Epoch 229/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8439 - regression_loss: 0.4650 - val_loss: 3.3599 - val_regression_loss: 0.7484 - lr: 1.9531e-07\n",
      "Epoch 230/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8401 - regression_loss: 0.4650 - val_loss: 3.3599 - val_regression_loss: 0.7484 - lr: 1.9531e-07\n",
      "Epoch 231/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8335 - regression_loss: 0.4650 - val_loss: 3.3599 - val_regression_loss: 0.7484 - lr: 1.9531e-07\n",
      "Epoch 232/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8507 - regression_loss: 0.4650 - val_loss: 3.3599 - val_regression_loss: 0.7484 - lr: 1.9531e-07\n",
      "Epoch 233/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.7668 - regression_loss: 0.4227\n",
      "Epoch 233: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8332 - regression_loss: 0.4650 - val_loss: 3.3598 - val_regression_loss: 0.7484 - lr: 1.9531e-07\n",
      "Epoch 234/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8338 - regression_loss: 0.4650 - val_loss: 3.3598 - val_regression_loss: 0.7483 - lr: 9.7656e-08\n",
      "Epoch 235/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8387 - regression_loss: 0.4650 - val_loss: 3.3598 - val_regression_loss: 0.7483 - lr: 9.7656e-08\n",
      "Epoch 236/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8429 - regression_loss: 0.4650 - val_loss: 3.3598 - val_regression_loss: 0.7483 - lr: 9.7656e-08\n",
      "Epoch 237/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8346 - regression_loss: 0.4650 - val_loss: 3.3598 - val_regression_loss: 0.7483 - lr: 9.7656e-08\n",
      "Epoch 238/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.7571 - regression_loss: 0.4130\n",
      "Epoch 238: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8339 - regression_loss: 0.4650 - val_loss: 3.3598 - val_regression_loss: 0.7484 - lr: 9.7656e-08\n",
      "Epoch 239/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8391 - regression_loss: 0.4649 - val_loss: 3.3598 - val_regression_loss: 0.7484 - lr: 4.8828e-08\n",
      "Epoch 240/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8457 - regression_loss: 0.4650 - val_loss: 3.3598 - val_regression_loss: 0.7484 - lr: 4.8828e-08\n",
      "Epoch 241/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8457 - regression_loss: 0.4649 - val_loss: 3.3598 - val_regression_loss: 0.7484 - lr: 4.8828e-08\n",
      "Epoch 242/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8474 - regression_loss: 0.4649 - val_loss: 3.3598 - val_regression_loss: 0.7484 - lr: 4.8828e-08\n",
      "Epoch 243/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.6826 - regression_loss: 0.3385\n",
      "Epoch 243: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8372 - regression_loss: 0.4649 - val_loss: 3.3598 - val_regression_loss: 0.7484 - lr: 4.8828e-08\n",
      "Epoch 244/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8475 - regression_loss: 0.4649 - val_loss: 3.3598 - val_regression_loss: 0.7484 - lr: 2.4414e-08\n",
      "Epoch 245/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8326 - regression_loss: 0.4649 - val_loss: 3.3598 - val_regression_loss: 0.7484 - lr: 2.4414e-08\n",
      "Epoch 246/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8472 - regression_loss: 0.4649 - val_loss: 3.3598 - val_regression_loss: 0.7484 - lr: 2.4414e-08\n",
      "Epoch 247/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8447 - regression_loss: 0.4649 - val_loss: 3.3598 - val_regression_loss: 0.7484 - lr: 2.4414e-08\n",
      "Epoch 248/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.8677 - regression_loss: 0.5236\n",
      "Epoch 248: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8301 - regression_loss: 0.4649 - val_loss: 3.3598 - val_regression_loss: 0.7484 - lr: 2.4414e-08\n",
      "Epoch 249/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8467 - regression_loss: 0.4649 - val_loss: 3.3598 - val_regression_loss: 0.7484 - lr: 1.2207e-08\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 149.8955 - regression_loss: 135.9315 - val_loss: 82.2913 - val_regression_loss: 66.7032 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 82.7460 - regression_loss: 74.8024 - val_loss: 53.9452 - val_regression_loss: 44.2314 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 65.7769 - regression_loss: 59.2579 - val_loss: 45.8631 - val_regression_loss: 37.8470 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 62.5134 - regression_loss: 55.2464 - val_loss: 42.5728 - val_regression_loss: 35.3305 - lr: 1.0000e-04\n",
      "Epoch 5/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 57.0676 - regression_loss: 51.0676 - val_loss: 40.5953 - val_regression_loss: 33.7939 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 53.4715 - regression_loss: 47.3268 - val_loss: 39.7030 - val_regression_loss: 33.0263 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 49.6623 - regression_loss: 44.2656 - val_loss: 39.8080 - val_regression_loss: 33.1360 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 47.9189 - regression_loss: 42.1213 - val_loss: 38.8782 - val_regression_loss: 32.3121 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 45.7770 - regression_loss: 40.2798 - val_loss: 38.2680 - val_regression_loss: 31.9598 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 43.8357 - regression_loss: 38.7050 - val_loss: 36.9734 - val_regression_loss: 30.9506 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 42.2014 - regression_loss: 37.1521 - val_loss: 35.9283 - val_regression_loss: 30.2339 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 41.4322 - regression_loss: 35.8861 - val_loss: 34.9444 - val_regression_loss: 29.4688 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 39.7747 - regression_loss: 34.6236 - val_loss: 33.8206 - val_regression_loss: 28.4481 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 38.3371 - regression_loss: 33.3524 - val_loss: 32.8450 - val_regression_loss: 27.6193 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 35.1738 - regression_loss: 32.2271 - val_loss: 32.0360 - val_regression_loss: 26.9649 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 35.4723 - regression_loss: 30.9843 - val_loss: 30.8568 - val_regression_loss: 25.8209 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 34.7517 - regression_loss: 29.9263 - val_loss: 29.9130 - val_regression_loss: 24.9774 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 33.6599 - regression_loss: 29.0297 - val_loss: 29.2355 - val_regression_loss: 24.4500 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 32.3086 - regression_loss: 27.8373 - val_loss: 27.6681 - val_regression_loss: 22.9543 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30.4952 - regression_loss: 26.9347 - val_loss: 27.3166 - val_regression_loss: 22.7766 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30.6967 - regression_loss: 26.1824 - val_loss: 26.0047 - val_regression_loss: 21.5306 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.4395 - regression_loss: 25.1056 - val_loss: 25.1184 - val_regression_loss: 20.6527 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.2760 - regression_loss: 24.4219 - val_loss: 24.9319 - val_regression_loss: 20.5990 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.4558 - regression_loss: 23.8249 - val_loss: 23.6846 - val_regression_loss: 19.4257 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.4899 - regression_loss: 23.0459 - val_loss: 23.2142 - val_regression_loss: 19.0660 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.0907 - regression_loss: 22.5343 - val_loss: 22.3123 - val_regression_loss: 18.2102 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.4232 - regression_loss: 21.8396 - val_loss: 22.0042 - val_regression_loss: 17.9360 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.8077 - regression_loss: 21.4688 - val_loss: 21.2254 - val_regression_loss: 17.2495 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.8587 - regression_loss: 20.9386 - val_loss: 20.7280 - val_regression_loss: 16.7926 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.6783 - regression_loss: 20.5996 - val_loss: 20.3818 - val_regression_loss: 16.4808 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.5295 - regression_loss: 20.1890 - val_loss: 19.9040 - val_regression_loss: 16.0352 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.8913 - regression_loss: 19.8197 - val_loss: 19.5505 - val_regression_loss: 15.6146 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.4222 - regression_loss: 19.6140 - val_loss: 19.2671 - val_regression_loss: 15.3515 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.0792 - regression_loss: 19.4320 - val_loss: 19.1921 - val_regression_loss: 15.3298 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.8204 - regression_loss: 19.1854 - val_loss: 18.5820 - val_regression_loss: 14.7824 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.6884 - regression_loss: 18.9554 - val_loss: 18.5084 - val_regression_loss: 14.7123 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.1040 - regression_loss: 18.4874 - val_loss: 18.2180 - val_regression_loss: 14.3941 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.7202 - regression_loss: 18.1922 - val_loss: 17.8452 - val_regression_loss: 14.0713 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.3081 - regression_loss: 18.0210 - val_loss: 17.5911 - val_regression_loss: 13.8584 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.4570 - regression_loss: 17.7523 - val_loss: 17.8078 - val_regression_loss: 14.0001 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.1648 - regression_loss: 17.7134 - val_loss: 17.3998 - val_regression_loss: 13.5951 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.8573 - regression_loss: 17.4175 - val_loss: 17.2947 - val_regression_loss: 13.5140 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.0407 - regression_loss: 17.3522 - val_loss: 17.1465 - val_regression_loss: 13.3758 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.7799 - regression_loss: 17.2633 - val_loss: 16.9234 - val_regression_loss: 13.1457 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.7219 - regression_loss: 17.2806 - val_loss: 16.9005 - val_regression_loss: 13.1404 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.1680 - regression_loss: 16.8198 - val_loss: 16.6333 - val_regression_loss: 12.8853 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.1916 - regression_loss: 16.6461 - val_loss: 16.4400 - val_regression_loss: 12.6710 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.9752 - regression_loss: 16.5825 - val_loss: 16.4707 - val_regression_loss: 12.7254 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.1622 - regression_loss: 16.3753 - val_loss: 16.3961 - val_regression_loss: 12.5628 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.0511 - regression_loss: 16.4301 - val_loss: 16.5993 - val_regression_loss: 12.7482 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.4209 - regression_loss: 16.3220 - val_loss: 16.0548 - val_regression_loss: 12.3457 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.7725 - regression_loss: 16.1246 - val_loss: 16.8147 - val_regression_loss: 13.0205 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.7414 - regression_loss: 16.4061 - val_loss: 15.9669 - val_regression_loss: 12.1343 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.2634 - regression_loss: 15.8107 - val_loss: 16.7775 - val_regression_loss: 12.8563 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.2846 - regression_loss: 15.7562 - val_loss: 15.8109 - val_regression_loss: 11.9755 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.4483 - regression_loss: 15.9239 - val_loss: 16.1220 - val_regression_loss: 12.3493 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.2342 - regression_loss: 15.7002 - val_loss: 15.5651 - val_regression_loss: 11.8437 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.9381 - regression_loss: 15.6419 - val_loss: 15.9822 - val_regression_loss: 12.1315 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.0203 - regression_loss: 15.4277 - val_loss: 15.8483 - val_regression_loss: 11.9493 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.7021 - regression_loss: 15.1838 - val_loss: 15.5787 - val_regression_loss: 11.8081 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.5035 - regression_loss: 15.1053 - val_loss: 15.8564 - val_regression_loss: 12.0063 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.3703 - regression_loss: 14.9526 - val_loss: 15.6596 - val_regression_loss: 11.7391 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.2162 - regression_loss: 14.8664 - val_loss: 15.5859 - val_regression_loss: 11.7067 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.0036 - regression_loss: 14.7630 - val_loss: 15.4433 - val_regression_loss: 11.7002 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.9973 - regression_loss: 14.6763 - val_loss: 15.2982 - val_regression_loss: 11.4966 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.0678 - regression_loss: 14.6590 - val_loss: 15.6782 - val_regression_loss: 11.7341 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.8092 - regression_loss: 14.5098 - val_loss: 15.4380 - val_regression_loss: 11.4798 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.8089 - regression_loss: 14.4803 - val_loss: 15.7127 - val_regression_loss: 11.7809 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.8160 - regression_loss: 14.3638 - val_loss: 15.2812 - val_regression_loss: 11.4341 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.5336 - regression_loss: 14.3091 - val_loss: 15.5910 - val_regression_loss: 11.6525 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.4460 - regression_loss: 14.2308 - val_loss: 15.3750 - val_regression_loss: 11.4128 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.6692 - regression_loss: 14.1111 - val_loss: 15.2937 - val_regression_loss: 11.3657 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.4347 - regression_loss: 14.0809 - val_loss: 15.4696 - val_regression_loss: 11.4941 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.5582 - regression_loss: 13.9522 - val_loss: 15.4062 - val_regression_loss: 11.3872 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.2348 - regression_loss: 13.8927 - val_loss: 15.2767 - val_regression_loss: 11.3335 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.9344 - regression_loss: 13.8502 - val_loss: 15.8041 - val_regression_loss: 11.7219 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.3770 - regression_loss: 13.9034 - val_loss: 15.2152 - val_regression_loss: 11.2659 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.9260 - regression_loss: 13.7032 - val_loss: 15.5346 - val_regression_loss: 11.5045 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.9923 - regression_loss: 13.7007 - val_loss: 15.3998 - val_regression_loss: 11.3922 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.8854 - regression_loss: 13.5855 - val_loss: 15.4194 - val_regression_loss: 11.3862 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.8987 - regression_loss: 13.5443 - val_loss: 15.5810 - val_regression_loss: 11.4243 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.6802 - regression_loss: 13.4471 - val_loss: 15.4552 - val_regression_loss: 11.3986 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 16.6471 - regression_loss: 13.3460 - val_loss: 15.2847 - val_regression_loss: 11.3131 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.4808 - regression_loss: 13.3883 - val_loss: 15.3269 - val_regression_loss: 11.3039 - lr: 1.0000e-04\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.5538 - regression_loss: 13.3563 - val_loss: 15.9002 - val_regression_loss: 11.6138 - lr: 1.0000e-04\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.4987 - regression_loss: 13.3271 - val_loss: 15.4668 - val_regression_loss: 11.3594 - lr: 1.0000e-04\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.1533 - regression_loss: 13.1002 - val_loss: 15.5056 - val_regression_loss: 11.4227 - lr: 1.0000e-04\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.3229 - regression_loss: 13.1010 - val_loss: 15.3911 - val_regression_loss: 11.3108 - lr: 1.0000e-04\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.9964 - regression_loss: 13.0918 - val_loss: 15.6353 - val_regression_loss: 11.4731 - lr: 1.0000e-04\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.3698 - regression_loss: 13.0349 - val_loss: 15.7691 - val_regression_loss: 11.5233 - lr: 1.0000e-04\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.3623 - regression_loss: 13.0152 - val_loss: 15.5305 - val_regression_loss: 11.3891 - lr: 1.0000e-04\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.1458 - regression_loss: 12.9430 - val_loss: 15.5808 - val_regression_loss: 11.4018 - lr: 1.0000e-04\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.9978 - regression_loss: 12.9615 - val_loss: 15.5804 - val_regression_loss: 11.3847 - lr: 1.0000e-04\n",
      "Epoch 94/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 16.7762 - regression_loss: 14.4518\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.3579 - regression_loss: 12.9695 - val_loss: 15.7156 - val_regression_loss: 11.5062 - lr: 1.0000e-04\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.1583 - regression_loss: 12.8372 - val_loss: 15.8467 - val_regression_loss: 11.5995 - lr: 5.0000e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.8301 - regression_loss: 12.7146 - val_loss: 15.6577 - val_regression_loss: 11.4717 - lr: 5.0000e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.8675 - regression_loss: 12.7730 - val_loss: 15.6285 - val_regression_loss: 11.4333 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.9748 - regression_loss: 12.7809 - val_loss: 15.7245 - val_regression_loss: 11.5104 - lr: 5.0000e-05\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.0662 - regression_loss: 12.8074 - val_loss: 15.6802 - val_regression_loss: 11.4527 - lr: 5.0000e-05\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.0284 - regression_loss: 12.6558 - val_loss: 15.9573 - val_regression_loss: 11.6245 - lr: 5.0000e-05\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.6003 - regression_loss: 12.6946 - val_loss: 15.7606 - val_regression_loss: 11.4831 - lr: 5.0000e-05\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.0811 - regression_loss: 12.7169 - val_loss: 15.6370 - val_regression_loss: 11.4034 - lr: 5.0000e-05\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.7359 - regression_loss: 12.6728 - val_loss: 15.8152 - val_regression_loss: 11.5555 - lr: 5.0000e-05\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.9106 - regression_loss: 12.6508 - val_loss: 15.6483 - val_regression_loss: 11.4316 - lr: 5.0000e-05\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.7224 - regression_loss: 12.6252 - val_loss: 15.8912 - val_regression_loss: 11.5991 - lr: 5.0000e-05\n",
      "Epoch 106/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.9468 - regression_loss: 11.6240\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.6976 - regression_loss: 12.5763 - val_loss: 15.8578 - val_regression_loss: 11.5567 - lr: 5.0000e-05\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.5878 - regression_loss: 12.5324 - val_loss: 15.8251 - val_regression_loss: 11.5355 - lr: 2.5000e-05\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.8320 - regression_loss: 12.5397 - val_loss: 15.7903 - val_regression_loss: 11.5076 - lr: 2.5000e-05\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.7424 - regression_loss: 12.5140 - val_loss: 15.8192 - val_regression_loss: 11.5341 - lr: 2.5000e-05\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.6004 - regression_loss: 12.5622 - val_loss: 15.9329 - val_regression_loss: 11.6244 - lr: 2.5000e-05\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.7005 - regression_loss: 12.4921 - val_loss: 15.8080 - val_regression_loss: 11.5279 - lr: 2.5000e-05\n",
      "Epoch 112/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.9494 - regression_loss: 13.6271\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.6456 - regression_loss: 12.5026 - val_loss: 15.7592 - val_regression_loss: 11.4915 - lr: 2.5000e-05\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.5882 - regression_loss: 12.4793 - val_loss: 15.8034 - val_regression_loss: 11.5185 - lr: 1.2500e-05\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.7167 - regression_loss: 12.4767 - val_loss: 15.8728 - val_regression_loss: 11.5707 - lr: 1.2500e-05\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.6012 - regression_loss: 12.4765 - val_loss: 15.8816 - val_regression_loss: 11.5797 - lr: 1.2500e-05\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.7440 - regression_loss: 12.4639 - val_loss: 15.8601 - val_regression_loss: 11.5541 - lr: 1.2500e-05\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.5684 - regression_loss: 12.4676 - val_loss: 15.7805 - val_regression_loss: 11.5034 - lr: 1.2500e-05\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 27ms/step - loss: 119.4945 - regression_loss: 107.7286 - val_loss: 75.1741 - val_regression_loss: 61.5809 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 71.2391 - regression_loss: 64.2078 - val_loss: 53.3665 - val_regression_loss: 42.9750 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 46.8957 - regression_loss: 41.0877 - val_loss: 44.9834 - val_regression_loss: 35.4825 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 36.2078 - regression_loss: 31.1250 - val_loss: 42.3755 - val_regression_loss: 32.7928 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30.6967 - regression_loss: 26.4250 - val_loss: 40.4854 - val_regression_loss: 31.0302 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.1762 - regression_loss: 23.1880 - val_loss: 38.7537 - val_regression_loss: 29.5215 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.3497 - regression_loss: 21.2802 - val_loss: 37.1990 - val_regression_loss: 28.2250 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.6214 - regression_loss: 19.4364 - val_loss: 35.5816 - val_regression_loss: 26.9895 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.1745 - regression_loss: 18.1410 - val_loss: 34.1695 - val_regression_loss: 25.8818 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.0789 - regression_loss: 17.3296 - val_loss: 32.2047 - val_regression_loss: 24.2350 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.3826 - regression_loss: 15.8548 - val_loss: 30.6095 - val_regression_loss: 22.9058 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.5490 - regression_loss: 14.8846 - val_loss: 28.8549 - val_regression_loss: 21.4148 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.1996 - regression_loss: 13.9705 - val_loss: 27.0591 - val_regression_loss: 19.8638 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.0224 - regression_loss: 13.0048 - val_loss: 25.3544 - val_regression_loss: 18.3920 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.3713 - regression_loss: 12.1401 - val_loss: 23.6723 - val_regression_loss: 16.9853 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.4441 - regression_loss: 11.3025 - val_loss: 22.3774 - val_regression_loss: 15.9082 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.8038 - regression_loss: 10.4488 - val_loss: 20.7933 - val_regression_loss: 14.5759 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.9860 - regression_loss: 9.7110 - val_loss: 19.0337 - val_regression_loss: 13.1188 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.8863 - regression_loss: 8.8962 - val_loss: 17.5393 - val_regression_loss: 11.8606 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2435 - regression_loss: 8.1594 - val_loss: 16.2912 - val_regression_loss: 10.8628 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.4018 - regression_loss: 7.3457 - val_loss: 14.9826 - val_regression_loss: 9.8092 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4829 - regression_loss: 6.6501 - val_loss: 13.7861 - val_regression_loss: 8.8131 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5776 - regression_loss: 6.0138 - val_loss: 12.4411 - val_regression_loss: 7.7700 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2458 - regression_loss: 5.5857 - val_loss: 11.4239 - val_regression_loss: 6.9362 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6271 - regression_loss: 4.9411 - val_loss: 10.5732 - val_regression_loss: 6.2497 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0588 - regression_loss: 4.3679 - val_loss: 9.4859 - val_regression_loss: 5.4557 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.1381 - regression_loss: 4.0042 - val_loss: 9.1589 - val_regression_loss: 5.1541 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.1817 - regression_loss: 3.5913 - val_loss: 8.0080 - val_regression_loss: 4.3454 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.4997 - regression_loss: 3.0811 - val_loss: 7.7179 - val_regression_loss: 4.0667 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2131 - regression_loss: 2.9225 - val_loss: 7.1332 - val_regression_loss: 3.6713 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1784 - regression_loss: 2.6416 - val_loss: 6.7660 - val_regression_loss: 3.3624 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.5750 - regression_loss: 2.3842 - val_loss: 6.2706 - val_regression_loss: 3.0054 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8009 - regression_loss: 2.2779 - val_loss: 5.9991 - val_regression_loss: 2.7945 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6759 - regression_loss: 2.1864 - val_loss: 5.8949 - val_regression_loss: 2.7172 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.4387 - regression_loss: 1.9361 - val_loss: 5.5370 - val_regression_loss: 2.4491 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.2658 - regression_loss: 1.7992 - val_loss: 5.3413 - val_regression_loss: 2.2995 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.1248 - regression_loss: 1.6751 - val_loss: 5.3299 - val_regression_loss: 2.2810 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0276 - regression_loss: 1.5811 - val_loss: 5.1647 - val_regression_loss: 2.1548 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9024 - regression_loss: 1.4907 - val_loss: 4.8649 - val_regression_loss: 1.9378 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8526 - regression_loss: 1.4513 - val_loss: 4.8135 - val_regression_loss: 1.8953 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7667 - regression_loss: 1.3478 - val_loss: 4.7869 - val_regression_loss: 1.8706 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7137 - regression_loss: 1.2957 - val_loss: 4.6613 - val_regression_loss: 1.7779 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6339 - regression_loss: 1.2413 - val_loss: 4.6060 - val_regression_loss: 1.7325 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6147 - regression_loss: 1.1820 - val_loss: 4.5182 - val_regression_loss: 1.6667 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5153 - regression_loss: 1.1283 - val_loss: 4.4040 - val_regression_loss: 1.5824 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5291 - regression_loss: 1.1130 - val_loss: 4.3510 - val_regression_loss: 1.5426 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4729 - regression_loss: 1.0750 - val_loss: 4.4136 - val_regression_loss: 1.5934 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4508 - regression_loss: 1.0435 - val_loss: 4.1773 - val_regression_loss: 1.4141 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3687 - regression_loss: 0.9859 - val_loss: 4.1946 - val_regression_loss: 1.4229 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3450 - regression_loss: 0.9603 - val_loss: 4.1951 - val_regression_loss: 1.4221 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2608 - regression_loss: 0.9210 - val_loss: 4.0585 - val_regression_loss: 1.3188 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3036 - regression_loss: 0.9287 - val_loss: 4.2448 - val_regression_loss: 1.4688 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.2526 - regression_loss: 0.8888 - val_loss: 4.0233 - val_regression_loss: 1.2966 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2366 - regression_loss: 0.8628 - val_loss: 4.1804 - val_regression_loss: 1.4238 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.2910 - regression_loss: 0.9014 - val_loss: 3.9269 - val_regression_loss: 1.2225 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.2249 - regression_loss: 0.8407 - val_loss: 3.9396 - val_regression_loss: 1.2277 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1288 - regression_loss: 0.8029 - val_loss: 3.8176 - val_regression_loss: 1.1448 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1381 - regression_loss: 0.7749 - val_loss: 3.8180 - val_regression_loss: 1.1410 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1267 - regression_loss: 0.7551 - val_loss: 3.9795 - val_regression_loss: 1.2689 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.0997 - regression_loss: 0.7415 - val_loss: 3.7053 - val_regression_loss: 1.0561 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.0732 - regression_loss: 0.7060 - val_loss: 3.7870 - val_regression_loss: 1.1201 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0195 - regression_loss: 0.6905 - val_loss: 3.8057 - val_regression_loss: 1.1329 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.0363 - regression_loss: 0.6858 - val_loss: 3.7126 - val_regression_loss: 1.0658 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0177 - regression_loss: 0.6560 - val_loss: 3.7339 - val_regression_loss: 1.0816 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9937 - regression_loss: 0.6504 - val_loss: 3.6453 - val_regression_loss: 1.0149 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9817 - regression_loss: 0.6329 - val_loss: 3.6470 - val_regression_loss: 1.0171 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9876 - regression_loss: 0.6343 - val_loss: 3.7696 - val_regression_loss: 1.1135 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9758 - regression_loss: 0.6169 - val_loss: 3.6441 - val_regression_loss: 1.0118 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8889 - regression_loss: 0.5955 - val_loss: 3.6415 - val_regression_loss: 1.0125 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9350 - regression_loss: 0.5889 - val_loss: 3.6281 - val_regression_loss: 1.0031 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9310 - regression_loss: 0.5856 - val_loss: 3.5593 - val_regression_loss: 0.9506 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9252 - regression_loss: 0.5796 - val_loss: 3.6036 - val_regression_loss: 0.9916 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9212 - regression_loss: 0.5797 - val_loss: 3.6129 - val_regression_loss: 0.9965 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.9096 - regression_loss: 0.6103\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8986 - regression_loss: 0.5484 - val_loss: 3.5469 - val_regression_loss: 0.9415 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8631 - regression_loss: 0.5282 - val_loss: 3.5613 - val_regression_loss: 0.9559 - lr: 5.0000e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8729 - regression_loss: 0.5297 - val_loss: 3.5177 - val_regression_loss: 0.9214 - lr: 5.0000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8582 - regression_loss: 0.5162 - val_loss: 3.4977 - val_regression_loss: 0.9084 - lr: 5.0000e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8533 - regression_loss: 0.5180 - val_loss: 3.4845 - val_regression_loss: 0.8973 - lr: 5.0000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8537 - regression_loss: 0.5158 - val_loss: 3.5200 - val_regression_loss: 0.9258 - lr: 5.0000e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8524 - regression_loss: 0.5078 - val_loss: 3.5644 - val_regression_loss: 0.9607 - lr: 5.0000e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8049 - regression_loss: 0.4938 - val_loss: 3.4617 - val_regression_loss: 0.8800 - lr: 5.0000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8128 - regression_loss: 0.4949 - val_loss: 3.4560 - val_regression_loss: 0.8780 - lr: 5.0000e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8324 - regression_loss: 0.4908 - val_loss: 3.5281 - val_regression_loss: 0.9343 - lr: 5.0000e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.7962 - regression_loss: 0.4851 - val_loss: 3.4549 - val_regression_loss: 0.8773 - lr: 5.0000e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8307 - regression_loss: 0.4913 - val_loss: 3.4579 - val_regression_loss: 0.8809 - lr: 5.0000e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8186 - regression_loss: 0.4806 - val_loss: 3.4208 - val_regression_loss: 0.8529 - lr: 5.0000e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8080 - regression_loss: 0.4714 - val_loss: 3.5074 - val_regression_loss: 0.9221 - lr: 5.0000e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.7957 - regression_loss: 0.4774 - val_loss: 3.4499 - val_regression_loss: 0.8757 - lr: 5.0000e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8067 - regression_loss: 0.4696 - val_loss: 3.4383 - val_regression_loss: 0.8684 - lr: 5.0000e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8101 - regression_loss: 0.4712 - val_loss: 3.4258 - val_regression_loss: 0.8568 - lr: 5.0000e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.7890 - regression_loss: 0.4630 - val_loss: 3.4298 - val_regression_loss: 0.8589 - lr: 5.0000e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7886 - regression_loss: 0.4513 - val_loss: 3.4976 - val_regression_loss: 0.9163 - lr: 5.0000e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.7769 - regression_loss: 0.4486 - val_loss: 3.4095 - val_regression_loss: 0.8459 - lr: 5.0000e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7957 - regression_loss: 0.4607 - val_loss: 3.4134 - val_regression_loss: 0.8516 - lr: 5.0000e-05\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7795 - regression_loss: 0.4519 - val_loss: 3.4733 - val_regression_loss: 0.8988 - lr: 5.0000e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.7750 - regression_loss: 0.4460 - val_loss: 3.4276 - val_regression_loss: 0.8628 - lr: 5.0000e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.7652 - regression_loss: 0.4330 - val_loss: 3.4202 - val_regression_loss: 0.8574 - lr: 5.0000e-05\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.7539 - regression_loss: 0.4306 - val_loss: 3.3897 - val_regression_loss: 0.8311 - lr: 5.0000e-05\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.7529 - regression_loss: 0.4344 - val_loss: 3.4169 - val_regression_loss: 0.8534 - lr: 5.0000e-05\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7541 - regression_loss: 0.4293 - val_loss: 3.4006 - val_regression_loss: 0.8413 - lr: 5.0000e-05\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.7482 - regression_loss: 0.4306 - val_loss: 3.3488 - val_regression_loss: 0.8032 - lr: 5.0000e-05\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.7444 - regression_loss: 0.4222 - val_loss: 3.4034 - val_regression_loss: 0.8470 - lr: 5.0000e-05\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.7183 - regression_loss: 0.4155 - val_loss: 3.3829 - val_regression_loss: 0.8301 - lr: 5.0000e-05\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.7289 - regression_loss: 0.4205 - val_loss: 3.3637 - val_regression_loss: 0.8160 - lr: 5.0000e-05\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7414 - regression_loss: 0.4216 - val_loss: 3.3457 - val_regression_loss: 0.8028 - lr: 5.0000e-05\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7324 - regression_loss: 0.4117 - val_loss: 3.4292 - val_regression_loss: 0.8694 - lr: 5.0000e-05\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7238 - regression_loss: 0.4054 - val_loss: 3.3183 - val_regression_loss: 0.7806 - lr: 5.0000e-05\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7149 - regression_loss: 0.4075 - val_loss: 3.4321 - val_regression_loss: 0.8722 - lr: 5.0000e-05\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.7282 - regression_loss: 0.4034 - val_loss: 3.3004 - val_regression_loss: 0.7667 - lr: 5.0000e-05\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.7070 - regression_loss: 0.3948 - val_loss: 3.4391 - val_regression_loss: 0.8794 - lr: 5.0000e-05\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.7268 - regression_loss: 0.4031 - val_loss: 3.3471 - val_regression_loss: 0.8043 - lr: 5.0000e-05\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.7094 - regression_loss: 0.3886 - val_loss: 3.3360 - val_regression_loss: 0.7980 - lr: 5.0000e-05\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6902 - regression_loss: 0.3851 - val_loss: 3.3406 - val_regression_loss: 0.8019 - lr: 5.0000e-05\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6971 - regression_loss: 0.3808 - val_loss: 3.3542 - val_regression_loss: 0.8130 - lr: 5.0000e-05\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6835 - regression_loss: 0.3771 - val_loss: 3.3545 - val_regression_loss: 0.8145 - lr: 5.0000e-05\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6782 - regression_loss: 0.3804 - val_loss: 3.3267 - val_regression_loss: 0.7938 - lr: 5.0000e-05\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6842 - regression_loss: 0.3770 - val_loss: 3.3815 - val_regression_loss: 0.8348 - lr: 5.0000e-05\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6900 - regression_loss: 0.3732 - val_loss: 3.3189 - val_regression_loss: 0.7853 - lr: 5.0000e-05\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6790 - regression_loss: 0.3691 - val_loss: 3.2827 - val_regression_loss: 0.7579 - lr: 5.0000e-05\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6813 - regression_loss: 0.3661 - val_loss: 3.3166 - val_regression_loss: 0.7863 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.5289 - regression_loss: 0.2425\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6785 - regression_loss: 0.3632 - val_loss: 3.3689 - val_regression_loss: 0.8281 - lr: 5.0000e-05\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6687 - regression_loss: 0.3653 - val_loss: 3.3144 - val_regression_loss: 0.7849 - lr: 2.5000e-05\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6771 - regression_loss: 0.3572 - val_loss: 3.3334 - val_regression_loss: 0.8005 - lr: 2.5000e-05\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6689 - regression_loss: 0.3553 - val_loss: 3.2963 - val_regression_loss: 0.7707 - lr: 2.5000e-05\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6735 - regression_loss: 0.3549 - val_loss: 3.3059 - val_regression_loss: 0.7781 - lr: 2.5000e-05\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6601 - regression_loss: 0.3533 - val_loss: 3.2899 - val_regression_loss: 0.7669 - lr: 2.5000e-05\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6608 - regression_loss: 0.3519 - val_loss: 3.3305 - val_regression_loss: 0.7994 - lr: 2.5000e-05\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6656 - regression_loss: 0.3520 - val_loss: 3.3155 - val_regression_loss: 0.7875 - lr: 2.5000e-05\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6716 - regression_loss: 0.3573 - val_loss: 3.2636 - val_regression_loss: 0.7457 - lr: 2.5000e-05\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6422 - regression_loss: 0.3607 - val_loss: 3.3814 - val_regression_loss: 0.8412 - lr: 2.5000e-05\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6589 - regression_loss: 0.3466 - val_loss: 3.2646 - val_regression_loss: 0.7460 - lr: 2.5000e-05\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6678 - regression_loss: 0.3517 - val_loss: 3.2862 - val_regression_loss: 0.7644 - lr: 2.5000e-05\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6533 - regression_loss: 0.3470 - val_loss: 3.3335 - val_regression_loss: 0.8034 - lr: 2.5000e-05\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6526 - regression_loss: 0.3417 - val_loss: 3.2706 - val_regression_loss: 0.7516 - lr: 2.5000e-05\n",
      "Epoch 135/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.7050 - regression_loss: 0.4210\n",
      "Epoch 135: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6487 - regression_loss: 0.3451 - val_loss: 3.2964 - val_regression_loss: 0.7738 - lr: 2.5000e-05\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6424 - regression_loss: 0.3409 - val_loss: 3.2980 - val_regression_loss: 0.7753 - lr: 1.2500e-05\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6499 - regression_loss: 0.3398 - val_loss: 3.2781 - val_regression_loss: 0.7591 - lr: 1.2500e-05\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6595 - regression_loss: 0.3441 - val_loss: 3.3306 - val_regression_loss: 0.8013 - lr: 1.2500e-05\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6543 - regression_loss: 0.3396 - val_loss: 3.2849 - val_regression_loss: 0.7645 - lr: 1.2500e-05\n",
      "Epoch 140/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.7046 - regression_loss: 0.4211\n",
      "Epoch 140: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6489 - regression_loss: 0.3375 - val_loss: 3.2869 - val_regression_loss: 0.7666 - lr: 1.2500e-05\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6471 - regression_loss: 0.3365 - val_loss: 3.2922 - val_regression_loss: 0.7712 - lr: 6.2500e-06\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6446 - regression_loss: 0.3359 - val_loss: 3.2826 - val_regression_loss: 0.7635 - lr: 6.2500e-06\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6416 - regression_loss: 0.3354 - val_loss: 3.2779 - val_regression_loss: 0.7596 - lr: 6.2500e-06\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6445 - regression_loss: 0.3363 - val_loss: 3.2724 - val_regression_loss: 0.7552 - lr: 6.2500e-06\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6401 - regression_loss: 0.3348 - val_loss: 3.2881 - val_regression_loss: 0.7678 - lr: 6.2500e-06\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6409 - regression_loss: 0.3348 - val_loss: 3.2968 - val_regression_loss: 0.7751 - lr: 6.2500e-06\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6463 - regression_loss: 0.3350 - val_loss: 3.3027 - val_regression_loss: 0.7797 - lr: 6.2500e-06\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6426 - regression_loss: 0.3339 - val_loss: 3.2900 - val_regression_loss: 0.7695 - lr: 6.2500e-06\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6252 - regression_loss: 0.3345 - val_loss: 3.2983 - val_regression_loss: 0.7762 - lr: 6.2500e-06\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6462 - regression_loss: 0.3337 - val_loss: 3.2769 - val_regression_loss: 0.7591 - lr: 6.2500e-06\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6418 - regression_loss: 0.3329 - val_loss: 3.2804 - val_regression_loss: 0.7621 - lr: 6.2500e-06\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6390 - regression_loss: 0.3328 - val_loss: 3.2801 - val_regression_loss: 0.7620 - lr: 6.2500e-06\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6237 - regression_loss: 0.3325 - val_loss: 3.2866 - val_regression_loss: 0.7671 - lr: 6.2500e-06\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6313 - regression_loss: 0.3330 - val_loss: 3.2749 - val_regression_loss: 0.7578 - lr: 6.2500e-06\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6415 - regression_loss: 0.3333 - val_loss: 3.2915 - val_regression_loss: 0.7714 - lr: 6.2500e-06\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6431 - regression_loss: 0.3317 - val_loss: 3.2905 - val_regression_loss: 0.7704 - lr: 6.2500e-06\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6393 - regression_loss: 0.3320 - val_loss: 3.2952 - val_regression_loss: 0.7741 - lr: 6.2500e-06\n",
      "Epoch 158/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.6509 - regression_loss: 0.3684\n",
      "Epoch 158: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6306 - regression_loss: 0.3309 - val_loss: 3.2799 - val_regression_loss: 0.7619 - lr: 6.2500e-06\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6217 - regression_loss: 0.3307 - val_loss: 3.2853 - val_regression_loss: 0.7663 - lr: 3.1250e-06\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6373 - regression_loss: 0.3302 - val_loss: 3.2802 - val_regression_loss: 0.7623 - lr: 3.1250e-06\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6294 - regression_loss: 0.3303 - val_loss: 3.2849 - val_regression_loss: 0.7662 - lr: 3.1250e-06\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6366 - regression_loss: 0.3306 - val_loss: 3.2754 - val_regression_loss: 0.7587 - lr: 3.1250e-06\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6383 - regression_loss: 0.3299 - val_loss: 3.2756 - val_regression_loss: 0.7589 - lr: 3.1250e-06\n",
      "Epoch 164/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.6822 - regression_loss: 0.3998\n",
      "Epoch 164: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6343 - regression_loss: 0.3295 - val_loss: 3.2790 - val_regression_loss: 0.7617 - lr: 3.1250e-06\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6311 - regression_loss: 0.3297 - val_loss: 3.2856 - val_regression_loss: 0.7671 - lr: 1.5625e-06\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6354 - regression_loss: 0.3292 - val_loss: 3.2822 - val_regression_loss: 0.7643 - lr: 1.5625e-06\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6389 - regression_loss: 0.3292 - val_loss: 3.2811 - val_regression_loss: 0.7634 - lr: 1.5625e-06\n",
      "Epoch 168/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6436 - regression_loss: 0.3292 - val_loss: 3.2797 - val_regression_loss: 0.7623 - lr: 1.5625e-06\n",
      "Epoch 169/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.6336 - regression_loss: 0.3513\n",
      "Epoch 169: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6324 - regression_loss: 0.3289 - val_loss: 3.2820 - val_regression_loss: 0.7642 - lr: 1.5625e-06\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 28ms/step - loss: 116.4430 - regression_loss: 107.0512 - val_loss: 73.6536 - val_regression_loss: 63.9686 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 78.1117 - regression_loss: 69.6296 - val_loss: 61.6761 - val_regression_loss: 52.7509 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 65.3264 - regression_loss: 58.8860 - val_loss: 54.5271 - val_regression_loss: 46.2232 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 54.5743 - regression_loss: 49.6138 - val_loss: 46.8824 - val_regression_loss: 39.6308 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 44.8698 - regression_loss: 41.2911 - val_loss: 39.8595 - val_regression_loss: 33.6604 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 39.9499 - regression_loss: 34.6877 - val_loss: 35.5672 - val_regression_loss: 29.9207 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 34.9916 - regression_loss: 30.0319 - val_loss: 31.7672 - val_regression_loss: 26.4745 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30.7324 - regression_loss: 26.1995 - val_loss: 29.1765 - val_regression_loss: 23.9881 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.4050 - regression_loss: 23.1425 - val_loss: 27.4728 - val_regression_loss: 22.2715 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.9231 - regression_loss: 21.0026 - val_loss: 26.0466 - val_regression_loss: 20.8272 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.1517 - regression_loss: 19.3817 - val_loss: 25.0597 - val_regression_loss: 19.8306 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.4462 - regression_loss: 18.0013 - val_loss: 24.1896 - val_regression_loss: 18.9786 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.3153 - regression_loss: 16.8508 - val_loss: 23.1468 - val_regression_loss: 18.0078 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.5468 - regression_loss: 15.9856 - val_loss: 22.5504 - val_regression_loss: 17.4207 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.2416 - regression_loss: 14.7927 - val_loss: 21.7789 - val_regression_loss: 16.7316 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.4270 - regression_loss: 14.0664 - val_loss: 20.6640 - val_regression_loss: 15.7755 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.6552 - regression_loss: 13.3868 - val_loss: 20.2842 - val_regression_loss: 15.4185 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.1172 - regression_loss: 12.7791 - val_loss: 19.5762 - val_regression_loss: 14.7769 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.7982 - regression_loss: 12.4204 - val_loss: 19.2329 - val_regression_loss: 14.4425 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.1290 - regression_loss: 11.7882 - val_loss: 18.6468 - val_regression_loss: 13.9132 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.5680 - regression_loss: 11.4352 - val_loss: 18.2234 - val_regression_loss: 13.5232 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.6812 - regression_loss: 11.0303 - val_loss: 17.8995 - val_regression_loss: 13.2115 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.9389 - regression_loss: 10.7220 - val_loss: 17.7216 - val_regression_loss: 13.0366 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1266 - regression_loss: 10.5121 - val_loss: 17.4120 - val_regression_loss: 12.7592 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.2650 - regression_loss: 10.2853 - val_loss: 17.2213 - val_regression_loss: 12.5628 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1201 - regression_loss: 10.1065 - val_loss: 16.6117 - val_regression_loss: 12.0228 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.9046 - regression_loss: 9.8695 - val_loss: 16.5478 - val_regression_loss: 11.9454 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.7000 - regression_loss: 9.6679 - val_loss: 16.2361 - val_regression_loss: 11.6665 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.4556 - regression_loss: 9.5610 - val_loss: 16.2924 - val_regression_loss: 11.6966 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.1404 - regression_loss: 9.3959 - val_loss: 15.7713 - val_regression_loss: 11.2874 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.2790 - regression_loss: 9.2634 - val_loss: 15.9433 - val_regression_loss: 11.4071 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.1588 - regression_loss: 9.0902 - val_loss: 15.4766 - val_regression_loss: 11.0144 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.7536 - regression_loss: 9.0335 - val_loss: 15.4025 - val_regression_loss: 10.9240 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6429 - regression_loss: 8.8275 - val_loss: 15.3498 - val_regression_loss: 10.8529 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.6691 - regression_loss: 8.7474 - val_loss: 15.3550 - val_regression_loss: 10.8315 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6276 - regression_loss: 8.6443 - val_loss: 14.9727 - val_regression_loss: 10.5105 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.6192 - regression_loss: 8.5847 - val_loss: 14.9898 - val_regression_loss: 10.5158 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1899 - regression_loss: 8.5256 - val_loss: 14.6915 - val_regression_loss: 10.2823 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.2715 - regression_loss: 8.4057 - val_loss: 14.9399 - val_regression_loss: 10.4495 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.2423 - regression_loss: 8.3173 - val_loss: 14.5251 - val_regression_loss: 10.1148 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.0692 - regression_loss: 8.3133 - val_loss: 15.1300 - val_regression_loss: 10.5561 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.1801 - regression_loss: 8.5420 - val_loss: 14.2859 - val_regression_loss: 9.8998 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.0290 - regression_loss: 8.1161 - val_loss: 14.8157 - val_regression_loss: 10.3008 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.0775 - regression_loss: 8.1598 - val_loss: 14.2553 - val_regression_loss: 9.8321 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.6589 - regression_loss: 7.9650 - val_loss: 14.1779 - val_regression_loss: 9.7681 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.8000 - regression_loss: 7.8575 - val_loss: 14.3095 - val_regression_loss: 9.8541 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.7228 - regression_loss: 7.8453 - val_loss: 13.9324 - val_regression_loss: 9.5531 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.5641 - regression_loss: 7.6886 - val_loss: 14.1074 - val_regression_loss: 9.6683 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3823 - regression_loss: 7.6484 - val_loss: 13.7576 - val_regression_loss: 9.3915 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3441 - regression_loss: 7.5924 - val_loss: 14.6900 - val_regression_loss: 10.1022 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.6704 - regression_loss: 7.7718 - val_loss: 13.7127 - val_regression_loss: 9.3317 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7657 - regression_loss: 7.8495 - val_loss: 14.2007 - val_regression_loss: 9.6858 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.1035 - regression_loss: 7.4694 - val_loss: 13.6075 - val_regression_loss: 9.2330 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.1917 - regression_loss: 7.4157 - val_loss: 14.6939 - val_regression_loss: 10.0722 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.4243 - regression_loss: 7.5945 - val_loss: 13.4989 - val_regression_loss: 9.1152 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.0336 - regression_loss: 7.2917 - val_loss: 14.6407 - val_regression_loss: 10.0101 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.2123 - regression_loss: 7.4648 - val_loss: 13.3370 - val_regression_loss: 8.9767 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.0356 - regression_loss: 7.2826 - val_loss: 14.2616 - val_regression_loss: 9.6811 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.2205 - regression_loss: 7.4543 - val_loss: 13.3055 - val_regression_loss: 8.9254 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.0152 - regression_loss: 7.1836 - val_loss: 13.7291 - val_regression_loss: 9.2271 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.7101 - regression_loss: 6.9502 - val_loss: 13.0548 - val_regression_loss: 8.6906 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.6732 - regression_loss: 7.0420 - val_loss: 13.5736 - val_regression_loss: 9.0857 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.6523 - regression_loss: 6.9429 - val_loss: 12.9665 - val_regression_loss: 8.6034 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3938 - regression_loss: 6.8319 - val_loss: 13.1549 - val_regression_loss: 8.7430 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5664 - regression_loss: 6.8319 - val_loss: 12.8773 - val_regression_loss: 8.5079 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5464 - regression_loss: 6.7439 - val_loss: 13.0128 - val_regression_loss: 8.6243 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3894 - regression_loss: 6.6763 - val_loss: 12.8003 - val_regression_loss: 8.4548 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5421 - regression_loss: 6.6858 - val_loss: 13.0594 - val_regression_loss: 8.6398 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4783 - regression_loss: 6.6958 - val_loss: 12.7435 - val_regression_loss: 8.3787 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3141 - regression_loss: 6.5662 - val_loss: 12.9531 - val_regression_loss: 8.5497 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.2564 - regression_loss: 6.5248 - val_loss: 12.6179 - val_regression_loss: 8.2829 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2608 - regression_loss: 6.4561 - val_loss: 12.6455 - val_regression_loss: 8.2831 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.1507 - regression_loss: 6.4461 - val_loss: 12.5114 - val_regression_loss: 8.1619 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.1332 - regression_loss: 6.4256 - val_loss: 12.6740 - val_regression_loss: 8.2970 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.0733 - regression_loss: 6.3863 - val_loss: 12.4052 - val_regression_loss: 8.0845 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1062 - regression_loss: 6.3637 - val_loss: 12.3743 - val_regression_loss: 8.0424 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1328 - regression_loss: 6.3350 - val_loss: 12.4429 - val_regression_loss: 8.0919 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8979 - regression_loss: 6.2651 - val_loss: 12.3305 - val_regression_loss: 7.9953 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.9230 - regression_loss: 6.3079 - val_loss: 12.2376 - val_regression_loss: 7.9274 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.0033 - regression_loss: 6.2781 - val_loss: 12.4837 - val_regression_loss: 8.1381 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.0688 - regression_loss: 6.3439 - val_loss: 12.1801 - val_regression_loss: 7.8710 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0019 - regression_loss: 6.2672 - val_loss: 12.9282 - val_regression_loss: 8.4889 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.9018 - regression_loss: 6.6442\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.9823 - regression_loss: 6.3384 - val_loss: 12.2287 - val_regression_loss: 7.9109 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.1532 - regression_loss: 6.3698 - val_loss: 12.2582 - val_regression_loss: 7.9414 - lr: 5.0000e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.0461 - regression_loss: 6.3200 - val_loss: 12.1583 - val_regression_loss: 7.8481 - lr: 5.0000e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8403 - regression_loss: 6.1616 - val_loss: 12.0316 - val_regression_loss: 7.7358 - lr: 5.0000e-05\n",
      "Epoch 87/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 8.5726 - regression_loss: 6.0237 - val_loss: 12.3184 - val_regression_loss: 7.9727 - lr: 5.0000e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7766 - regression_loss: 6.0581 - val_loss: 12.0219 - val_regression_loss: 7.7296 - lr: 5.0000e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7612 - regression_loss: 6.0084 - val_loss: 11.9427 - val_regression_loss: 7.6634 - lr: 5.0000e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.5345 - regression_loss: 5.9348 - val_loss: 12.0714 - val_regression_loss: 7.7726 - lr: 5.0000e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.6236 - regression_loss: 5.9625 - val_loss: 12.0096 - val_regression_loss: 7.7180 - lr: 5.0000e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.6867 - regression_loss: 5.9274 - val_loss: 11.9469 - val_regression_loss: 7.6618 - lr: 5.0000e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.6098 - regression_loss: 5.9140 - val_loss: 12.0425 - val_regression_loss: 7.7422 - lr: 5.0000e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5026 - regression_loss: 5.9196 - val_loss: 11.9541 - val_regression_loss: 7.6705 - lr: 5.0000e-05\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.4301 - regression_loss: 5.8690 - val_loss: 11.8719 - val_regression_loss: 7.5993 - lr: 5.0000e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5848 - regression_loss: 5.8847 - val_loss: 11.8584 - val_regression_loss: 7.5906 - lr: 5.0000e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.5709 - regression_loss: 5.8734 - val_loss: 11.8841 - val_regression_loss: 7.6095 - lr: 5.0000e-05\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.5769 - regression_loss: 5.9110 - val_loss: 11.8211 - val_regression_loss: 7.5508 - lr: 5.0000e-05\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.6015 - regression_loss: 5.8434 - val_loss: 11.8943 - val_regression_loss: 7.6163 - lr: 5.0000e-05\n",
      "Epoch 100/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.2060 - regression_loss: 7.9518\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.5513 - regression_loss: 5.8272 - val_loss: 11.8067 - val_regression_loss: 7.5408 - lr: 5.0000e-05\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3318 - regression_loss: 5.8082 - val_loss: 11.8331 - val_regression_loss: 7.5629 - lr: 2.5000e-05\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.4908 - regression_loss: 5.7950 - val_loss: 11.8514 - val_regression_loss: 7.5788 - lr: 2.5000e-05\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.4762 - regression_loss: 5.7914 - val_loss: 11.7759 - val_regression_loss: 7.5140 - lr: 2.5000e-05\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.4750 - regression_loss: 5.7739 - val_loss: 11.7860 - val_regression_loss: 7.5233 - lr: 2.5000e-05\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.4185 - regression_loss: 5.7653 - val_loss: 11.7899 - val_regression_loss: 7.5231 - lr: 2.5000e-05\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2661 - regression_loss: 5.7774 - val_loss: 11.8084 - val_regression_loss: 7.5407 - lr: 2.5000e-05\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3894 - regression_loss: 5.7656 - val_loss: 11.7544 - val_regression_loss: 7.4926 - lr: 2.5000e-05\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.4867 - regression_loss: 5.7498 - val_loss: 11.7246 - val_regression_loss: 7.4693 - lr: 2.5000e-05\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3719 - regression_loss: 5.7607 - val_loss: 11.7695 - val_regression_loss: 7.5066 - lr: 2.5000e-05\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2620 - regression_loss: 5.7413 - val_loss: 11.7437 - val_regression_loss: 7.4840 - lr: 2.5000e-05\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4301 - regression_loss: 5.7333 - val_loss: 11.7369 - val_regression_loss: 7.4786 - lr: 2.5000e-05\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3218 - regression_loss: 5.7469 - val_loss: 11.7006 - val_regression_loss: 7.4497 - lr: 2.5000e-05\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2930 - regression_loss: 5.7221 - val_loss: 11.7252 - val_regression_loss: 7.4700 - lr: 2.5000e-05\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4832 - regression_loss: 5.7360 - val_loss: 11.7427 - val_regression_loss: 7.4828 - lr: 2.5000e-05\n",
      "Epoch 115/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.2547 - regression_loss: 7.0022\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3526 - regression_loss: 5.7127 - val_loss: 11.7196 - val_regression_loss: 7.4640 - lr: 2.5000e-05\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2403 - regression_loss: 5.6955 - val_loss: 11.7105 - val_regression_loss: 7.4552 - lr: 1.2500e-05\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3709 - regression_loss: 5.6937 - val_loss: 11.6987 - val_regression_loss: 7.4454 - lr: 1.2500e-05\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3849 - regression_loss: 5.6911 - val_loss: 11.6669 - val_regression_loss: 7.4188 - lr: 1.2500e-05\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3606 - regression_loss: 5.6882 - val_loss: 11.6749 - val_regression_loss: 7.4250 - lr: 1.2500e-05\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3131 - regression_loss: 5.6934 - val_loss: 11.6796 - val_regression_loss: 7.4290 - lr: 1.2500e-05\n",
      "Epoch 121/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.9482 - regression_loss: 5.6961\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2531 - regression_loss: 5.6819 - val_loss: 11.6672 - val_regression_loss: 7.4184 - lr: 1.2500e-05\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3609 - regression_loss: 5.6802 - val_loss: 11.6869 - val_regression_loss: 7.4349 - lr: 6.2500e-06\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3222 - regression_loss: 5.6734 - val_loss: 11.6783 - val_regression_loss: 7.4275 - lr: 6.2500e-06\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3562 - regression_loss: 5.6736 - val_loss: 11.6681 - val_regression_loss: 7.4190 - lr: 6.2500e-06\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3090 - regression_loss: 5.6708 - val_loss: 11.6639 - val_regression_loss: 7.4156 - lr: 6.2500e-06\n",
      "Epoch 126/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.1535 - regression_loss: 5.9015\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3551 - regression_loss: 5.6706 - val_loss: 11.6719 - val_regression_loss: 7.4222 - lr: 6.2500e-06\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3765 - regression_loss: 5.6672 - val_loss: 11.6745 - val_regression_loss: 7.4244 - lr: 3.1250e-06\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1836 - regression_loss: 5.6670 - val_loss: 11.6707 - val_regression_loss: 7.4213 - lr: 3.1250e-06\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3796 - regression_loss: 5.6659 - val_loss: 11.6678 - val_regression_loss: 7.4188 - lr: 3.1250e-06\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3373 - regression_loss: 5.6655 - val_loss: 11.6703 - val_regression_loss: 7.4208 - lr: 3.1250e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2886 - regression_loss: 5.6649 - val_loss: 11.6656 - val_regression_loss: 7.4170 - lr: 3.1250e-06\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2879 - regression_loss: 5.6636 - val_loss: 11.6618 - val_regression_loss: 7.4137 - lr: 3.1250e-06\n",
      "Epoch 133/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.6398 - regression_loss: 7.3880\n",
      "Epoch 133: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.4060 - regression_loss: 5.6632 - val_loss: 11.6594 - val_regression_loss: 7.4115 - lr: 3.1250e-06\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2708 - regression_loss: 5.6611 - val_loss: 11.6607 - val_regression_loss: 7.4125 - lr: 1.5625e-06\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3656 - regression_loss: 5.6606 - val_loss: 11.6610 - val_regression_loss: 7.4128 - lr: 1.5625e-06\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2768 - regression_loss: 5.6605 - val_loss: 11.6606 - val_regression_loss: 7.4125 - lr: 1.5625e-06\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2411 - regression_loss: 5.6612 - val_loss: 11.6637 - val_regression_loss: 7.4151 - lr: 1.5625e-06\n",
      "Epoch 138/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.1968 - regression_loss: 6.9450\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2928 - regression_loss: 5.6600 - val_loss: 11.6601 - val_regression_loss: 7.4121 - lr: 1.5625e-06\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2255 - regression_loss: 5.6589 - val_loss: 11.6585 - val_regression_loss: 7.4108 - lr: 7.8125e-07\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2032 - regression_loss: 5.6588 - val_loss: 11.6586 - val_regression_loss: 7.4108 - lr: 7.8125e-07\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2268 - regression_loss: 5.6597 - val_loss: 11.6600 - val_regression_loss: 7.4120 - lr: 7.8125e-07\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2790 - regression_loss: 5.6584 - val_loss: 11.6591 - val_regression_loss: 7.4112 - lr: 7.8125e-07\n",
      "Epoch 143/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.2141 - regression_loss: 6.9622\n",
      "Epoch 143: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3577 - regression_loss: 5.6585 - val_loss: 11.6597 - val_regression_loss: 7.4117 - lr: 7.8125e-07\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2182 - regression_loss: 5.6579 - val_loss: 11.6598 - val_regression_loss: 7.4117 - lr: 3.9062e-07\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2706 - regression_loss: 5.6578 - val_loss: 11.6599 - val_regression_loss: 7.4117 - lr: 3.9062e-07\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.4052 - regression_loss: 5.6578 - val_loss: 11.6588 - val_regression_loss: 7.4108 - lr: 3.9062e-07\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3352 - regression_loss: 5.6579 - val_loss: 11.6575 - val_regression_loss: 7.4098 - lr: 3.9062e-07\n",
      "Epoch 148/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.5081 - regression_loss: 7.2563\n",
      "Epoch 148: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2706 - regression_loss: 5.6582 - val_loss: 11.6588 - val_regression_loss: 7.4108 - lr: 3.9062e-07\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3309 - regression_loss: 5.6575 - val_loss: 11.6583 - val_regression_loss: 7.4104 - lr: 1.9531e-07\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3327 - regression_loss: 5.6572 - val_loss: 11.6583 - val_regression_loss: 7.4104 - lr: 1.9531e-07\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2136 - regression_loss: 5.6571 - val_loss: 11.6584 - val_regression_loss: 7.4104 - lr: 1.9531e-07\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2570 - regression_loss: 5.6572 - val_loss: 11.6588 - val_regression_loss: 7.4108 - lr: 1.9531e-07\n",
      "Epoch 153/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.4759 - regression_loss: 6.2241\n",
      "Epoch 153: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3542 - regression_loss: 5.6572 - val_loss: 11.6590 - val_regression_loss: 7.4109 - lr: 1.9531e-07\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3655 - regression_loss: 5.6570 - val_loss: 11.6591 - val_regression_loss: 7.4110 - lr: 9.7656e-08\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1798 - regression_loss: 5.6570 - val_loss: 11.6589 - val_regression_loss: 7.4109 - lr: 9.7656e-08\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2616 - regression_loss: 5.6569 - val_loss: 11.6586 - val_regression_loss: 7.4107 - lr: 9.7656e-08\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2889 - regression_loss: 5.6569 - val_loss: 11.6587 - val_regression_loss: 7.4107 - lr: 9.7656e-08\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3346 - regression_loss: 5.6570 - val_loss: 11.6584 - val_regression_loss: 7.4105 - lr: 9.7656e-08\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3490 - regression_loss: 5.6569 - val_loss: 11.6583 - val_regression_loss: 7.4104 - lr: 9.7656e-08\n",
      "Epoch 160/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.8331 - regression_loss: 5.5813\n",
      "Epoch 160: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2820 - regression_loss: 5.6568 - val_loss: 11.6582 - val_regression_loss: 7.4103 - lr: 9.7656e-08\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2761 - regression_loss: 5.6568 - val_loss: 11.6582 - val_regression_loss: 7.4103 - lr: 4.8828e-08\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2371 - regression_loss: 5.6568 - val_loss: 11.6584 - val_regression_loss: 7.4104 - lr: 4.8828e-08\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3641 - regression_loss: 5.6567 - val_loss: 11.6583 - val_regression_loss: 7.4103 - lr: 4.8828e-08\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3667 - regression_loss: 5.6568 - val_loss: 11.6582 - val_regression_loss: 7.4103 - lr: 4.8828e-08\n",
      "Epoch 165/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.2374 - regression_loss: 5.9856\n",
      "Epoch 165: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.4016 - regression_loss: 5.6567 - val_loss: 11.6582 - val_regression_loss: 7.4103 - lr: 4.8828e-08\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2471 - regression_loss: 5.6567 - val_loss: 11.6582 - val_regression_loss: 7.4103 - lr: 2.4414e-08\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3311 - regression_loss: 5.6567 - val_loss: 11.6582 - val_regression_loss: 7.4103 - lr: 2.4414e-08\n",
      "Epoch 168/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2792 - regression_loss: 5.6567 - val_loss: 11.6582 - val_regression_loss: 7.4103 - lr: 2.4414e-08\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2764 - regression_loss: 5.6567 - val_loss: 11.6582 - val_regression_loss: 7.4103 - lr: 2.4414e-08\n",
      "Epoch 170/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.0473 - regression_loss: 4.7955\n",
      "Epoch 170: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2194 - regression_loss: 5.6567 - val_loss: 11.6582 - val_regression_loss: 7.4103 - lr: 2.4414e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3295 - regression_loss: 5.6567 - val_loss: 11.6582 - val_regression_loss: 7.4103 - lr: 1.2207e-08\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2851 - regression_loss: 5.6567 - val_loss: 11.6582 - val_regression_loss: 7.4103 - lr: 1.2207e-08\n",
      "Epoch 173/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3000 - regression_loss: 5.6567 - val_loss: 11.6582 - val_regression_loss: 7.4103 - lr: 1.2207e-08\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2485 - regression_loss: 5.6567 - val_loss: 11.6582 - val_regression_loss: 7.4102 - lr: 1.2207e-08\n",
      "Epoch 175/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.6650 - regression_loss: 5.4132\n",
      "Epoch 175: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3226 - regression_loss: 5.6566 - val_loss: 11.6582 - val_regression_loss: 7.4102 - lr: 1.2207e-08\n",
      "Epoch 176/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2238 - regression_loss: 5.6566 - val_loss: 11.6582 - val_regression_loss: 7.4102 - lr: 6.1035e-09\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2932 - regression_loss: 5.6566 - val_loss: 11.6582 - val_regression_loss: 7.4102 - lr: 6.1035e-09\n",
      "Epoch 178/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3542 - regression_loss: 5.6566 - val_loss: 11.6582 - val_regression_loss: 7.4102 - lr: 6.1035e-09\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3230 - regression_loss: 5.6566 - val_loss: 11.6582 - val_regression_loss: 7.4102 - lr: 6.1035e-09\n",
      "Epoch 180/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.4976 - regression_loss: 4.2458\n",
      "Epoch 180: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2734 - regression_loss: 5.6566 - val_loss: 11.6582 - val_regression_loss: 7.4102 - lr: 6.1035e-09\n",
      "Epoch 181/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2287 - regression_loss: 5.6566 - val_loss: 11.6582 - val_regression_loss: 7.4102 - lr: 3.0518e-09\n",
      "Epoch 182/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2381 - regression_loss: 5.6566 - val_loss: 11.6582 - val_regression_loss: 7.4102 - lr: 3.0518e-09\n",
      "Epoch 183/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2522 - regression_loss: 5.6566 - val_loss: 11.6582 - val_regression_loss: 7.4102 - lr: 3.0518e-09\n",
      "Epoch 184/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3391 - regression_loss: 5.6566 - val_loss: 11.6582 - val_regression_loss: 7.4102 - lr: 3.0518e-09\n",
      "Epoch 185/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.2571 - regression_loss: 6.0053\n",
      "Epoch 185: ReduceLROnPlateau reducing learning rate to 1.5258788677030566e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2404 - regression_loss: 5.6566 - val_loss: 11.6582 - val_regression_loss: 7.4102 - lr: 3.0518e-09\n",
      "Epoch 186/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3586 - regression_loss: 5.6566 - val_loss: 11.6582 - val_regression_loss: 7.4102 - lr: 1.5259e-09\n",
      "Epoch 187/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3624 - regression_loss: 5.6566 - val_loss: 11.6582 - val_regression_loss: 7.4102 - lr: 1.5259e-09\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 27ms/step - loss: 96.5687 - regression_loss: 88.7583 - val_loss: 60.5661 - val_regression_loss: 46.1343 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 67.6930 - regression_loss: 59.8738 - val_loss: 43.8608 - val_regression_loss: 33.5678 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 51.0354 - regression_loss: 45.5022 - val_loss: 34.7633 - val_regression_loss: 26.2174 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 42.1750 - regression_loss: 36.6594 - val_loss: 28.4611 - val_regression_loss: 21.1381 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 35.9386 - regression_loss: 31.2517 - val_loss: 24.8430 - val_regression_loss: 18.2035 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 32.1662 - regression_loss: 27.7209 - val_loss: 23.1567 - val_regression_loss: 16.8038 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.7904 - regression_loss: 25.4034 - val_loss: 21.6463 - val_regression_loss: 15.5695 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.8443 - regression_loss: 22.9168 - val_loss: 20.5896 - val_regression_loss: 14.7563 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.3073 - regression_loss: 21.1562 - val_loss: 19.7410 - val_regression_loss: 14.1036 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.2702 - regression_loss: 19.6437 - val_loss: 18.7711 - val_regression_loss: 13.3445 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.9501 - regression_loss: 18.2987 - val_loss: 18.1013 - val_regression_loss: 12.7717 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.6998 - regression_loss: 16.8501 - val_loss: 17.5464 - val_regression_loss: 12.3012 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.2192 - regression_loss: 15.8708 - val_loss: 16.9103 - val_regression_loss: 11.7706 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.2902 - regression_loss: 14.9232 - val_loss: 16.4236 - val_regression_loss: 11.3396 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.9977 - regression_loss: 13.9666 - val_loss: 15.8250 - val_regression_loss: 10.7922 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.5623 - regression_loss: 13.2368 - val_loss: 15.3364 - val_regression_loss: 10.3786 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5108 - regression_loss: 12.4662 - val_loss: 14.8704 - val_regression_loss: 9.9739 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.1251 - regression_loss: 11.8986 - val_loss: 14.3060 - val_regression_loss: 9.5154 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.0910 - regression_loss: 11.2823 - val_loss: 13.8573 - val_regression_loss: 9.1319 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.0282 - regression_loss: 10.8317 - val_loss: 13.4637 - val_regression_loss: 8.8129 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.5429 - regression_loss: 10.3944 - val_loss: 12.9598 - val_regression_loss: 8.4052 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.9255 - regression_loss: 9.9958 - val_loss: 12.7387 - val_regression_loss: 8.2424 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6944 - regression_loss: 9.7583 - val_loss: 12.3621 - val_regression_loss: 7.9369 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4415 - regression_loss: 9.5106 - val_loss: 12.0546 - val_regression_loss: 7.7136 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1648 - regression_loss: 9.2399 - val_loss: 11.6987 - val_regression_loss: 7.4442 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.8549 - regression_loss: 8.9188 - val_loss: 11.7001 - val_regression_loss: 7.4398 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4534 - regression_loss: 8.7567 - val_loss: 11.3126 - val_regression_loss: 7.1977 - lr: 1.0000e-04\n",
      "Epoch 28/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3556 - regression_loss: 8.5382 - val_loss: 11.1079 - val_regression_loss: 7.0057 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0745 - regression_loss: 8.4097 - val_loss: 10.8863 - val_regression_loss: 6.8621 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.1521 - regression_loss: 8.3134 - val_loss: 10.9773 - val_regression_loss: 6.9494 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.0034 - regression_loss: 8.2162 - val_loss: 10.8313 - val_regression_loss: 6.8447 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8927 - regression_loss: 8.0824 - val_loss: 10.4821 - val_regression_loss: 6.5761 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.6661 - regression_loss: 7.9637 - val_loss: 10.6013 - val_regression_loss: 6.6907 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.6887 - regression_loss: 7.8369 - val_loss: 10.5099 - val_regression_loss: 6.6184 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.4848 - regression_loss: 7.7572 - val_loss: 10.3133 - val_regression_loss: 6.4785 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.4040 - regression_loss: 7.7184 - val_loss: 10.5245 - val_regression_loss: 6.6377 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3638 - regression_loss: 7.6245 - val_loss: 10.3180 - val_regression_loss: 6.5346 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3458 - regression_loss: 7.5538 - val_loss: 10.2727 - val_regression_loss: 6.4298 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2269 - regression_loss: 7.5559 - val_loss: 10.2290 - val_regression_loss: 6.4608 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.1341 - regression_loss: 7.4084 - val_loss: 10.1652 - val_regression_loss: 6.4072 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2220 - regression_loss: 7.3768 - val_loss: 10.1787 - val_regression_loss: 6.3944 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.1130 - regression_loss: 7.3287 - val_loss: 10.0182 - val_regression_loss: 6.2940 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9550 - regression_loss: 7.2282 - val_loss: 10.0563 - val_regression_loss: 6.3234 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9360 - regression_loss: 7.2116 - val_loss: 10.0986 - val_regression_loss: 6.3806 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7471 - regression_loss: 7.1864 - val_loss: 9.8971 - val_regression_loss: 6.2259 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8150 - regression_loss: 7.1786 - val_loss: 10.1493 - val_regression_loss: 6.3896 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8695 - regression_loss: 7.1172 - val_loss: 9.9652 - val_regression_loss: 6.3192 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7435 - regression_loss: 7.0537 - val_loss: 9.9385 - val_regression_loss: 6.2187 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7495 - regression_loss: 7.0364 - val_loss: 10.0112 - val_regression_loss: 6.3666 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7814 - regression_loss: 7.1227 - val_loss: 9.9477 - val_regression_loss: 6.2610 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5287 - regression_loss: 6.8896 - val_loss: 9.8473 - val_regression_loss: 6.2105 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7411 - regression_loss: 6.9546 - val_loss: 9.8481 - val_regression_loss: 6.2469 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5531 - regression_loss: 6.8662 - val_loss: 9.7520 - val_regression_loss: 6.1259 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4970 - regression_loss: 6.7530 - val_loss: 9.9668 - val_regression_loss: 6.3030 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4779 - regression_loss: 6.7589 - val_loss: 9.8152 - val_regression_loss: 6.1932 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3684 - regression_loss: 6.7715 - val_loss: 9.9556 - val_regression_loss: 6.2360 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5842 - regression_loss: 6.8299 - val_loss: 9.8290 - val_regression_loss: 6.2942 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4273 - regression_loss: 6.7465 - val_loss: 10.2604 - val_regression_loss: 6.4861 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6337 - regression_loss: 6.7946 - val_loss: 9.6794 - val_regression_loss: 6.1880 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3526 - regression_loss: 6.7111 - val_loss: 10.0555 - val_regression_loss: 6.3167 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5452 - regression_loss: 6.7734 - val_loss: 9.8599 - val_regression_loss: 6.3293 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3355 - regression_loss: 6.6526 - val_loss: 9.9057 - val_regression_loss: 6.2150 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1330 - regression_loss: 6.6226 - val_loss: 9.6577 - val_regression_loss: 6.1356 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1499 - regression_loss: 6.5894 - val_loss: 10.3880 - val_regression_loss: 6.5875 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4011 - regression_loss: 6.6897 - val_loss: 9.7065 - val_regression_loss: 6.1754 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4734 - regression_loss: 6.6653 - val_loss: 9.5929 - val_regression_loss: 6.0499 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.2350 - regression_loss: 6.5778 - val_loss: 9.8995 - val_regression_loss: 6.2343 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.7890 - regression_loss: 7.5688\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3575 - regression_loss: 6.6082 - val_loss: 9.7989 - val_regression_loss: 6.2946 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.2589 - regression_loss: 6.5572 - val_loss: 9.7073 - val_regression_loss: 6.1268 - lr: 5.0000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1872 - regression_loss: 6.4744 - val_loss: 9.7063 - val_regression_loss: 6.1228 - lr: 5.0000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0075 - regression_loss: 6.3607 - val_loss: 9.6640 - val_regression_loss: 6.1405 - lr: 5.0000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0776 - regression_loss: 6.3509 - val_loss: 9.7928 - val_regression_loss: 6.2025 - lr: 5.0000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9888 - regression_loss: 6.3452 - val_loss: 9.7151 - val_regression_loss: 6.1407 - lr: 5.0000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0489 - regression_loss: 6.3517 - val_loss: 9.7193 - val_regression_loss: 6.1747 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0086 - regression_loss: 6.3524 - val_loss: 9.6931 - val_regression_loss: 6.1212 - lr: 5.0000e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9254 - regression_loss: 6.3077 - val_loss: 9.7016 - val_regression_loss: 6.1344 - lr: 5.0000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.9293 - regression_loss: 6.3139 - val_loss: 9.7358 - val_regression_loss: 6.1670 - lr: 5.0000e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9292 - regression_loss: 6.2813 - val_loss: 9.7530 - val_regression_loss: 6.1988 - lr: 5.0000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0785 - regression_loss: 6.2858 - val_loss: 9.6948 - val_regression_loss: 6.1267 - lr: 5.0000e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9182 - regression_loss: 6.2731 - val_loss: 9.6779 - val_regression_loss: 6.1292 - lr: 5.0000e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8803 - regression_loss: 6.2492 - val_loss: 9.8325 - val_regression_loss: 6.2267 - lr: 5.0000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8180 - regression_loss: 6.3124 - val_loss: 9.7276 - val_regression_loss: 6.1868 - lr: 5.0000e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1297 - regression_loss: 6.3871 - val_loss: 9.6882 - val_regression_loss: 6.1574 - lr: 5.0000e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8006 - regression_loss: 6.2440 - val_loss: 9.8738 - val_regression_loss: 6.2393 - lr: 5.0000e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7644 - regression_loss: 6.2302 - val_loss: 9.6844 - val_regression_loss: 6.1644 - lr: 5.0000e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9283 - regression_loss: 6.2506 - val_loss: 9.7668 - val_regression_loss: 6.1734 - lr: 5.0000e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9069 - regression_loss: 6.2475 - val_loss: 9.7081 - val_regression_loss: 6.1699 - lr: 5.0000e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8421 - regression_loss: 6.1872 - val_loss: 9.7433 - val_regression_loss: 6.1797 - lr: 5.0000e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7951 - regression_loss: 6.2077 - val_loss: 9.7034 - val_regression_loss: 6.1509 - lr: 5.0000e-05\n",
      "Epoch 90/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.9982 - regression_loss: 7.7832\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8971 - regression_loss: 6.1820 - val_loss: 9.7447 - val_regression_loss: 6.1741 - lr: 5.0000e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8569 - regression_loss: 6.1606 - val_loss: 9.7298 - val_regression_loss: 6.1761 - lr: 2.5000e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7595 - regression_loss: 6.1714 - val_loss: 9.7094 - val_regression_loss: 6.1749 - lr: 2.5000e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8839 - regression_loss: 6.1529 - val_loss: 9.7370 - val_regression_loss: 6.1682 - lr: 2.5000e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7960 - regression_loss: 6.1649 - val_loss: 9.7599 - val_regression_loss: 6.1781 - lr: 2.5000e-05\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8485 - regression_loss: 6.1395 - val_loss: 9.6939 - val_regression_loss: 6.1617 - lr: 2.5000e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6959 - regression_loss: 6.1433 - val_loss: 9.7246 - val_regression_loss: 6.1818 - lr: 2.5000e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7743 - regression_loss: 6.1706 - val_loss: 9.8474 - val_regression_loss: 6.2394 - lr: 2.5000e-05\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7217 - regression_loss: 6.1489 - val_loss: 9.6867 - val_regression_loss: 6.1564 - lr: 2.5000e-05\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7818 - regression_loss: 6.1279 - val_loss: 9.7082 - val_regression_loss: 6.1575 - lr: 2.5000e-05\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8235 - regression_loss: 6.1345 - val_loss: 9.7636 - val_regression_loss: 6.1817 - lr: 2.5000e-05\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6862 - regression_loss: 6.1177 - val_loss: 9.7077 - val_regression_loss: 6.1662 - lr: 2.5000e-05\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.8002 - regression_loss: 6.1221 - val_loss: 9.7229 - val_regression_loss: 6.1895 - lr: 2.5000e-05\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.6820 - regression_loss: 6.1188 - val_loss: 9.7971 - val_regression_loss: 6.2123 - lr: 2.5000e-05\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6688 - regression_loss: 6.1133 - val_loss: 9.7337 - val_regression_loss: 6.1797 - lr: 2.5000e-05\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6921 - regression_loss: 6.0992 - val_loss: 9.7296 - val_regression_loss: 6.1758 - lr: 2.5000e-05\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6895 - regression_loss: 6.1114 - val_loss: 9.6834 - val_regression_loss: 6.1292 - lr: 2.5000e-05\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 58.8928 - regression_loss: 51.6628 - val_loss: 46.8087 - val_regression_loss: 35.6782 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 45.7013 - regression_loss: 40.7661 - val_loss: 41.2998 - val_regression_loss: 31.3176 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 41.3069 - regression_loss: 35.8100 - val_loss: 35.9564 - val_regression_loss: 27.2947 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 36.3487 - regression_loss: 31.8682 - val_loss: 32.4858 - val_regression_loss: 24.6576 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 33.1779 - regression_loss: 29.0865 - val_loss: 30.3846 - val_regression_loss: 23.0886 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.1472 - regression_loss: 26.9825 - val_loss: 28.3998 - val_regression_loss: 21.3898 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.3251 - regression_loss: 25.5833 - val_loss: 27.0975 - val_regression_loss: 20.3998 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.9205 - regression_loss: 24.2357 - val_loss: 25.9192 - val_regression_loss: 19.4754 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.7922 - regression_loss: 22.7210 - val_loss: 24.5854 - val_regression_loss: 18.2698 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.7372 - regression_loss: 21.9247 - val_loss: 23.6618 - val_regression_loss: 17.6473 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.7896 - regression_loss: 20.3323 - val_loss: 22.6967 - val_regression_loss: 16.6781 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.0328 - regression_loss: 19.1015 - val_loss: 22.2853 - val_regression_loss: 16.6308 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.9569 - regression_loss: 18.4011 - val_loss: 20.5303 - val_regression_loss: 14.9592 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5106 - regression_loss: 17.0996 - val_loss: 19.8046 - val_regression_loss: 14.4751 - lr: 1.0000e-04\n",
      "Epoch 15/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 19.5873 - regression_loss: 16.2755 - val_loss: 18.6438 - val_regression_loss: 13.4967 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.0369 - regression_loss: 15.6676 - val_loss: 17.9975 - val_regression_loss: 13.0438 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.1880 - regression_loss: 15.0087 - val_loss: 17.3255 - val_regression_loss: 12.4573 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4826 - regression_loss: 14.2019 - val_loss: 16.6676 - val_regression_loss: 11.8062 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5589 - regression_loss: 13.3411 - val_loss: 16.3944 - val_regression_loss: 11.7427 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.3180 - regression_loss: 13.0352 - val_loss: 15.3728 - val_regression_loss: 10.8252 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5969 - regression_loss: 12.4327 - val_loss: 14.9413 - val_regression_loss: 10.4425 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.9583 - regression_loss: 11.9266 - val_loss: 14.5433 - val_regression_loss: 10.1276 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.5852 - regression_loss: 11.6104 - val_loss: 14.2473 - val_regression_loss: 9.9145 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.9844 - regression_loss: 11.2595 - val_loss: 13.8059 - val_regression_loss: 9.4848 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8945 - regression_loss: 10.8702 - val_loss: 13.7051 - val_regression_loss: 9.4981 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8210 - regression_loss: 10.7032 - val_loss: 13.3738 - val_regression_loss: 9.1480 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.5834 - regression_loss: 10.4583 - val_loss: 12.9840 - val_regression_loss: 8.8679 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1501 - regression_loss: 10.1119 - val_loss: 12.9221 - val_regression_loss: 8.7973 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8133 - regression_loss: 9.9111 - val_loss: 12.9000 - val_regression_loss: 8.7680 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2809 - regression_loss: 9.7323 - val_loss: 12.4454 - val_regression_loss: 8.4795 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2022 - regression_loss: 9.5243 - val_loss: 12.3724 - val_regression_loss: 8.3531 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2799 - regression_loss: 9.3998 - val_loss: 12.3875 - val_regression_loss: 8.4237 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4086 - regression_loss: 9.4639 - val_loss: 12.3426 - val_regression_loss: 8.4289 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0397 - regression_loss: 9.1696 - val_loss: 12.1561 - val_regression_loss: 8.1263 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0175 - regression_loss: 9.0421 - val_loss: 12.1061 - val_regression_loss: 8.2147 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8038 - regression_loss: 8.9566 - val_loss: 11.8775 - val_regression_loss: 7.9719 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3568 - regression_loss: 8.7891 - val_loss: 11.7914 - val_regression_loss: 7.8879 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3001 - regression_loss: 8.6835 - val_loss: 11.8105 - val_regression_loss: 7.8546 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5654 - regression_loss: 8.6410 - val_loss: 11.7959 - val_regression_loss: 8.0266 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4025 - regression_loss: 8.5653 - val_loss: 11.6627 - val_regression_loss: 7.7200 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2871 - regression_loss: 8.4439 - val_loss: 11.5462 - val_regression_loss: 7.7124 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2325 - regression_loss: 8.3474 - val_loss: 11.5167 - val_regression_loss: 7.6181 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1114 - regression_loss: 8.2886 - val_loss: 11.4960 - val_regression_loss: 7.7178 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0106 - regression_loss: 8.1803 - val_loss: 11.3192 - val_regression_loss: 7.5013 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8871 - regression_loss: 8.0551 - val_loss: 11.2655 - val_regression_loss: 7.4844 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8784 - regression_loss: 8.0811 - val_loss: 11.5531 - val_regression_loss: 7.5696 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9108 - regression_loss: 8.0722 - val_loss: 11.1464 - val_regression_loss: 7.4474 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6645 - regression_loss: 7.9862 - val_loss: 11.2254 - val_regression_loss: 7.3663 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7965 - regression_loss: 8.0666 - val_loss: 11.0789 - val_regression_loss: 7.2846 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6984 - regression_loss: 7.9526 - val_loss: 11.3446 - val_regression_loss: 7.7353 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0861 - regression_loss: 8.4011 - val_loss: 12.2135 - val_regression_loss: 7.9790 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0132 - regression_loss: 8.5401 - val_loss: 11.3676 - val_regression_loss: 7.8461 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3718 - regression_loss: 7.7381 - val_loss: 11.6350 - val_regression_loss: 7.5709 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6874 - regression_loss: 7.9003 - val_loss: 11.0057 - val_regression_loss: 7.5055 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6452 - regression_loss: 8.1201 - val_loss: 11.0629 - val_regression_loss: 7.1934 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.1920 - regression_loss: 7.6854 - val_loss: 10.7901 - val_regression_loss: 7.1231 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3200 - regression_loss: 7.8293 - val_loss: 10.7517 - val_regression_loss: 7.1989 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3082 - regression_loss: 7.6258 - val_loss: 11.0259 - val_regression_loss: 7.1742 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3262 - regression_loss: 7.7053 - val_loss: 10.8876 - val_regression_loss: 7.3380 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2062 - regression_loss: 7.4129 - val_loss: 11.8179 - val_regression_loss: 7.6732 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.6685 - regression_loss: 8.4838\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.5565 - regression_loss: 7.7568 - val_loss: 10.7554 - val_regression_loss: 7.2441 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0823 - regression_loss: 7.4303 - val_loss: 10.9123 - val_regression_loss: 7.0775 - lr: 5.0000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2085 - regression_loss: 7.4536 - val_loss: 10.5289 - val_regression_loss: 6.9627 - lr: 5.0000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0235 - regression_loss: 7.3298 - val_loss: 10.4769 - val_regression_loss: 6.9119 - lr: 5.0000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0264 - regression_loss: 7.2450 - val_loss: 10.7164 - val_regression_loss: 6.9567 - lr: 5.0000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0675 - regression_loss: 7.2835 - val_loss: 10.4557 - val_regression_loss: 6.8684 - lr: 5.0000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9198 - regression_loss: 7.1567 - val_loss: 10.5196 - val_regression_loss: 6.8583 - lr: 5.0000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7999 - regression_loss: 7.1178 - val_loss: 10.4050 - val_regression_loss: 6.8414 - lr: 5.0000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6938 - regression_loss: 7.1355 - val_loss: 10.4724 - val_regression_loss: 6.8532 - lr: 5.0000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7686 - regression_loss: 7.0963 - val_loss: 10.5284 - val_regression_loss: 6.8455 - lr: 5.0000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6355 - regression_loss: 7.0933 - val_loss: 10.4733 - val_regression_loss: 6.8541 - lr: 5.0000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5721 - regression_loss: 7.0964 - val_loss: 10.4228 - val_regression_loss: 6.8162 - lr: 5.0000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7951 - regression_loss: 7.0414 - val_loss: 10.4369 - val_regression_loss: 6.8045 - lr: 5.0000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6073 - regression_loss: 7.0293 - val_loss: 10.3245 - val_regression_loss: 6.7951 - lr: 5.0000e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7819 - regression_loss: 7.0691 - val_loss: 10.4225 - val_regression_loss: 6.7876 - lr: 5.0000e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5473 - regression_loss: 7.0173 - val_loss: 10.4493 - val_regression_loss: 6.8067 - lr: 5.0000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7018 - regression_loss: 7.0444 - val_loss: 10.3274 - val_regression_loss: 6.7757 - lr: 5.0000e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6201 - regression_loss: 7.0256 - val_loss: 10.3960 - val_regression_loss: 6.7691 - lr: 5.0000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6545 - regression_loss: 6.9690 - val_loss: 10.3082 - val_regression_loss: 6.7486 - lr: 5.0000e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6323 - regression_loss: 6.9558 - val_loss: 10.3931 - val_regression_loss: 6.7668 - lr: 5.0000e-05\n",
      "Epoch 81/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.0778 - regression_loss: 6.8982\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6702 - regression_loss: 6.9976 - val_loss: 10.3121 - val_regression_loss: 6.7328 - lr: 5.0000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5890 - regression_loss: 6.8936 - val_loss: 10.2578 - val_regression_loss: 6.7298 - lr: 2.5000e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4623 - regression_loss: 6.9068 - val_loss: 10.2861 - val_regression_loss: 6.7226 - lr: 2.5000e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5801 - regression_loss: 6.8906 - val_loss: 10.3340 - val_regression_loss: 6.7289 - lr: 2.5000e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4432 - regression_loss: 6.8907 - val_loss: 10.3177 - val_regression_loss: 6.7310 - lr: 2.5000e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5628 - regression_loss: 6.8795 - val_loss: 10.2652 - val_regression_loss: 6.7164 - lr: 2.5000e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6387 - regression_loss: 6.8870 - val_loss: 10.2929 - val_regression_loss: 6.7085 - lr: 2.5000e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6150 - regression_loss: 6.8711 - val_loss: 10.2507 - val_regression_loss: 6.7022 - lr: 2.5000e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4479 - regression_loss: 6.8674 - val_loss: 10.2371 - val_regression_loss: 6.6847 - lr: 2.5000e-05\n",
      "Epoch 90/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.2213 - regression_loss: 7.0430\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5083 - regression_loss: 6.8557 - val_loss: 10.2053 - val_regression_loss: 6.6819 - lr: 2.5000e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5208 - regression_loss: 6.8545 - val_loss: 10.2148 - val_regression_loss: 6.6785 - lr: 1.2500e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6081 - regression_loss: 6.8461 - val_loss: 10.2992 - val_regression_loss: 6.6996 - lr: 1.2500e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4482 - regression_loss: 6.8461 - val_loss: 10.2757 - val_regression_loss: 6.6965 - lr: 1.2500e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5390 - regression_loss: 6.8312 - val_loss: 10.2591 - val_regression_loss: 6.6977 - lr: 1.2500e-05\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3525 - regression_loss: 6.8209 - val_loss: 10.2654 - val_regression_loss: 6.6961 - lr: 1.2500e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5032 - regression_loss: 6.8261 - val_loss: 10.2311 - val_regression_loss: 6.6827 - lr: 1.2500e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3750 - regression_loss: 6.8204 - val_loss: 10.2333 - val_regression_loss: 6.6796 - lr: 1.2500e-05\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5250 - regression_loss: 6.8153 - val_loss: 10.2592 - val_regression_loss: 6.6810 - lr: 1.2500e-05\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3325 - regression_loss: 6.8165 - val_loss: 10.2518 - val_regression_loss: 6.6782 - lr: 1.2500e-05\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3377 - regression_loss: 6.8314 - val_loss: 10.2013 - val_regression_loss: 6.6747 - lr: 1.2500e-05\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4931 - regression_loss: 6.8212 - val_loss: 10.2351 - val_regression_loss: 6.6701 - lr: 1.2500e-05\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.2756 - regression_loss: 6.8039 - val_loss: 10.2205 - val_regression_loss: 6.6709 - lr: 1.2500e-05\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5330 - regression_loss: 6.8039 - val_loss: 10.2325 - val_regression_loss: 6.6683 - lr: 1.2500e-05\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4008 - regression_loss: 6.7939 - val_loss: 10.2027 - val_regression_loss: 6.6599 - lr: 1.2500e-05\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3466 - regression_loss: 6.8042 - val_loss: 10.1995 - val_regression_loss: 6.6612 - lr: 1.2500e-05\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5399 - regression_loss: 6.7915 - val_loss: 10.2213 - val_regression_loss: 6.6607 - lr: 1.2500e-05\n",
      "Epoch 107/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.7402 - regression_loss: 7.5630\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2813 - regression_loss: 6.7950 - val_loss: 10.2112 - val_regression_loss: 6.6605 - lr: 1.2500e-05\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4616 - regression_loss: 6.7818 - val_loss: 10.2155 - val_regression_loss: 6.6621 - lr: 6.2500e-06\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4547 - regression_loss: 6.7748 - val_loss: 10.2121 - val_regression_loss: 6.6585 - lr: 6.2500e-06\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4766 - regression_loss: 6.7742 - val_loss: 10.2034 - val_regression_loss: 6.6558 - lr: 6.2500e-06\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4229 - regression_loss: 6.7773 - val_loss: 10.2111 - val_regression_loss: 6.6563 - lr: 6.2500e-06\n",
      "Epoch 112/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.8254 - regression_loss: 6.6484\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4476 - regression_loss: 6.7747 - val_loss: 10.2009 - val_regression_loss: 6.6556 - lr: 6.2500e-06\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4386 - regression_loss: 6.7676 - val_loss: 10.1912 - val_regression_loss: 6.6523 - lr: 3.1250e-06\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5110 - regression_loss: 6.7664 - val_loss: 10.1949 - val_regression_loss: 6.6528 - lr: 3.1250e-06\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5114 - regression_loss: 6.7665 - val_loss: 10.1932 - val_regression_loss: 6.6519 - lr: 3.1250e-06\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3948 - regression_loss: 6.7659 - val_loss: 10.1965 - val_regression_loss: 6.6512 - lr: 3.1250e-06\n",
      "Epoch 117/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.3408 - regression_loss: 7.1639\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3749 - regression_loss: 6.7643 - val_loss: 10.1994 - val_regression_loss: 6.6503 - lr: 3.1250e-06\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4037 - regression_loss: 6.7631 - val_loss: 10.1975 - val_regression_loss: 6.6492 - lr: 1.5625e-06\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5296 - regression_loss: 6.7623 - val_loss: 10.1964 - val_regression_loss: 6.6486 - lr: 1.5625e-06\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2532 - regression_loss: 6.7632 - val_loss: 10.2009 - val_regression_loss: 6.6498 - lr: 1.5625e-06\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2334 - regression_loss: 6.7618 - val_loss: 10.1994 - val_regression_loss: 6.6496 - lr: 1.5625e-06\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4019 - regression_loss: 6.7606 - val_loss: 10.1938 - val_regression_loss: 6.6481 - lr: 1.5625e-06\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4453 - regression_loss: 6.7600 - val_loss: 10.1903 - val_regression_loss: 6.6471 - lr: 1.5625e-06\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3070 - regression_loss: 6.7602 - val_loss: 10.1894 - val_regression_loss: 6.6477 - lr: 1.5625e-06\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5185 - regression_loss: 6.7598 - val_loss: 10.1911 - val_regression_loss: 6.6474 - lr: 1.5625e-06\n",
      "Epoch 126/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.0397 - regression_loss: 8.8628\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3503 - regression_loss: 6.7595 - val_loss: 10.1944 - val_regression_loss: 6.6482 - lr: 1.5625e-06\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2855 - regression_loss: 6.7585 - val_loss: 10.1922 - val_regression_loss: 6.6477 - lr: 7.8125e-07\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3541 - regression_loss: 6.7582 - val_loss: 10.1904 - val_regression_loss: 6.6472 - lr: 7.8125e-07\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4336 - regression_loss: 6.7579 - val_loss: 10.1908 - val_regression_loss: 6.6474 - lr: 7.8125e-07\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3553 - regression_loss: 6.7578 - val_loss: 10.1924 - val_regression_loss: 6.6477 - lr: 7.8125e-07\n",
      "Epoch 131/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.9031 - regression_loss: 7.7263\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4411 - regression_loss: 6.7576 - val_loss: 10.1915 - val_regression_loss: 6.6477 - lr: 7.8125e-07\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3676 - regression_loss: 6.7570 - val_loss: 10.1916 - val_regression_loss: 6.6476 - lr: 3.9062e-07\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3770 - regression_loss: 6.7573 - val_loss: 10.1906 - val_regression_loss: 6.6473 - lr: 3.9062e-07\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4288 - regression_loss: 6.7566 - val_loss: 10.1909 - val_regression_loss: 6.6473 - lr: 3.9062e-07\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4604 - regression_loss: 6.7565 - val_loss: 10.1914 - val_regression_loss: 6.6473 - lr: 3.9062e-07\n",
      "Epoch 136/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.1967 - regression_loss: 5.0199\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4660 - regression_loss: 6.7566 - val_loss: 10.1917 - val_regression_loss: 6.6472 - lr: 3.9062e-07\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4897 - regression_loss: 6.7563 - val_loss: 10.1921 - val_regression_loss: 6.6472 - lr: 1.9531e-07\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4000 - regression_loss: 6.7562 - val_loss: 10.1923 - val_regression_loss: 6.6472 - lr: 1.9531e-07\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3706 - regression_loss: 6.7562 - val_loss: 10.1919 - val_regression_loss: 6.6471 - lr: 1.9531e-07\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4333 - regression_loss: 6.7561 - val_loss: 10.1916 - val_regression_loss: 6.6470 - lr: 1.9531e-07\n",
      "Epoch 141/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.0694 - regression_loss: 6.8926\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4855 - regression_loss: 6.7561 - val_loss: 10.1912 - val_regression_loss: 6.6469 - lr: 1.9531e-07\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4095 - regression_loss: 6.7560 - val_loss: 10.1911 - val_regression_loss: 6.6469 - lr: 9.7656e-08\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4240 - regression_loss: 6.7559 - val_loss: 10.1913 - val_regression_loss: 6.6469 - lr: 9.7656e-08\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3394 - regression_loss: 6.7561 - val_loss: 10.1910 - val_regression_loss: 6.6468 - lr: 9.7656e-08\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4336 - regression_loss: 6.7558 - val_loss: 10.1910 - val_regression_loss: 6.6468 - lr: 9.7656e-08\n",
      "Epoch 146/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.5100 - regression_loss: 7.3332\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2757 - regression_loss: 6.7558 - val_loss: 10.1912 - val_regression_loss: 6.6469 - lr: 9.7656e-08\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4673 - regression_loss: 6.7559 - val_loss: 10.1915 - val_regression_loss: 6.6469 - lr: 4.8828e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4646 - regression_loss: 6.7558 - val_loss: 10.1913 - val_regression_loss: 6.6469 - lr: 4.8828e-08\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4571 - regression_loss: 6.7557 - val_loss: 10.1912 - val_regression_loss: 6.6468 - lr: 4.8828e-08\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3413 - regression_loss: 6.7557 - val_loss: 10.1912 - val_regression_loss: 6.6468 - lr: 4.8828e-08\n",
      "Epoch 151/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.3001 - regression_loss: 7.1234\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2844 - regression_loss: 6.7557 - val_loss: 10.1912 - val_regression_loss: 6.6468 - lr: 4.8828e-08\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3684 - regression_loss: 6.7557 - val_loss: 10.1912 - val_regression_loss: 6.6468 - lr: 2.4414e-08\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3256 - regression_loss: 6.7557 - val_loss: 10.1912 - val_regression_loss: 6.6468 - lr: 2.4414e-08\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9.3340 - regression_loss: 6.7556 - val_loss: 10.1912 - val_regression_loss: 6.6468 - lr: 2.4414e-08\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4113 - regression_loss: 6.7556 - val_loss: 10.1912 - val_regression_loss: 6.6468 - lr: 2.4414e-08\n",
      "Epoch 156/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.5137 - regression_loss: 10.3369\n",
      "Epoch 156: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4840 - regression_loss: 6.7556 - val_loss: 10.1912 - val_regression_loss: 6.6468 - lr: 2.4414e-08\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3493 - regression_loss: 6.7556 - val_loss: 10.1912 - val_regression_loss: 6.6468 - lr: 1.2207e-08\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4968 - regression_loss: 6.7556 - val_loss: 10.1912 - val_regression_loss: 6.6468 - lr: 1.2207e-08\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3809 - regression_loss: 6.7556 - val_loss: 10.1912 - val_regression_loss: 6.6468 - lr: 1.2207e-08\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4530 - regression_loss: 6.7556 - val_loss: 10.1912 - val_regression_loss: 6.6468 - lr: 1.2207e-08\n",
      "Epoch 161/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.8860 - regression_loss: 8.7092\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4749 - regression_loss: 6.7556 - val_loss: 10.1912 - val_regression_loss: 6.6468 - lr: 1.2207e-08\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4446 - regression_loss: 6.7556 - val_loss: 10.1912 - val_regression_loss: 6.6468 - lr: 6.1035e-09\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4685 - regression_loss: 6.7556 - val_loss: 10.1912 - val_regression_loss: 6.6468 - lr: 6.1035e-09\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3393 - regression_loss: 6.7556 - val_loss: 10.1912 - val_regression_loss: 6.6468 - lr: 6.1035e-09\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 26ms/step - loss: 84.2827 - regression_loss: 76.5239 - val_loss: 43.0661 - val_regression_loss: 32.7020 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 52.0427 - regression_loss: 45.8988 - val_loss: 34.1665 - val_regression_loss: 26.2495 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 48.8244 - regression_loss: 43.0614 - val_loss: 32.8056 - val_regression_loss: 25.1446 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 45.2334 - regression_loss: 39.8844 - val_loss: 30.0887 - val_regression_loss: 22.6380 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 39.9362 - regression_loss: 35.2068 - val_loss: 28.6093 - val_regression_loss: 21.2395 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 37.9669 - regression_loss: 33.3395 - val_loss: 28.3689 - val_regression_loss: 20.8748 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 36.2756 - regression_loss: 32.0436 - val_loss: 27.8887 - val_regression_loss: 20.4984 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 34.9873 - regression_loss: 30.6260 - val_loss: 27.1959 - val_regression_loss: 20.0363 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 33.9628 - regression_loss: 29.4965 - val_loss: 26.9564 - val_regression_loss: 19.8508 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 33.2042 - regression_loss: 28.6352 - val_loss: 26.2606 - val_regression_loss: 19.3483 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31.7391 - regression_loss: 28.0075 - val_loss: 26.0386 - val_regression_loss: 19.1137 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31.4132 - regression_loss: 27.1942 - val_loss: 26.0281 - val_regression_loss: 19.0469 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30.8140 - regression_loss: 26.6977 - val_loss: 25.6680 - val_regression_loss: 18.7541 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.7147 - regression_loss: 26.3550 - val_loss: 25.6669 - val_regression_loss: 18.7497 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.6237 - regression_loss: 25.7401 - val_loss: 26.0268 - val_regression_loss: 18.9374 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.6483 - regression_loss: 25.5976 - val_loss: 25.4483 - val_regression_loss: 18.5181 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.2175 - regression_loss: 25.1214 - val_loss: 25.5530 - val_regression_loss: 18.5869 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.6867 - regression_loss: 24.8599 - val_loss: 25.5371 - val_regression_loss: 18.5207 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.0122 - regression_loss: 25.1012 - val_loss: 25.2474 - val_regression_loss: 18.3165 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.5879 - regression_loss: 24.3643 - val_loss: 25.8312 - val_regression_loss: 18.7120 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.8183 - regression_loss: 23.9118 - val_loss: 25.1804 - val_regression_loss: 18.2561 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.7506 - regression_loss: 23.8216 - val_loss: 25.3471 - val_regression_loss: 18.2752 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.2822 - regression_loss: 23.6424 - val_loss: 25.0064 - val_regression_loss: 18.0239 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.1936 - regression_loss: 23.4964 - val_loss: 25.1914 - val_regression_loss: 18.1314 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.0986 - regression_loss: 23.0423 - val_loss: 25.1286 - val_regression_loss: 18.1040 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.5828 - regression_loss: 22.8548 - val_loss: 25.0670 - val_regression_loss: 18.0674 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.4049 - regression_loss: 22.6775 - val_loss: 25.0411 - val_regression_loss: 18.0304 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.4008 - regression_loss: 22.5070 - val_loss: 24.9280 - val_regression_loss: 17.9066 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.4798 - regression_loss: 22.5864 - val_loss: 25.0773 - val_regression_loss: 17.9763 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.4002 - regression_loss: 22.2140 - val_loss: 25.0667 - val_regression_loss: 18.0012 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.4889 - regression_loss: 22.1421 - val_loss: 25.0406 - val_regression_loss: 17.9765 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.4316 - regression_loss: 22.0564 - val_loss: 25.0237 - val_regression_loss: 17.9584 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.6873 - regression_loss: 21.8424 - val_loss: 24.8413 - val_regression_loss: 17.7897 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.6054 - regression_loss: 21.7429 - val_loss: 25.0362 - val_regression_loss: 17.9450 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.2861 - regression_loss: 21.6705 - val_loss: 24.7578 - val_regression_loss: 17.7694 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.2683 - regression_loss: 21.5153 - val_loss: 25.0799 - val_regression_loss: 17.9611 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.3316 - regression_loss: 21.4765 - val_loss: 24.9171 - val_regression_loss: 17.8305 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.3064 - regression_loss: 21.5161 - val_loss: 24.6738 - val_regression_loss: 17.6583 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.0845 - regression_loss: 21.3584 - val_loss: 24.7820 - val_regression_loss: 17.7360 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.5592 - regression_loss: 21.1608 - val_loss: 24.6935 - val_regression_loss: 17.6746 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.5883 - regression_loss: 21.0529 - val_loss: 24.7779 - val_regression_loss: 17.7058 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.8065 - regression_loss: 20.9379 - val_loss: 24.6869 - val_regression_loss: 17.6181 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.5733 - regression_loss: 21.0205 - val_loss: 24.6323 - val_regression_loss: 17.6292 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.4823 - regression_loss: 20.8170 - val_loss: 24.5714 - val_regression_loss: 17.6119 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.3388 - regression_loss: 20.7712 - val_loss: 24.7226 - val_regression_loss: 17.6946 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.1439 - regression_loss: 20.6629 - val_loss: 24.6107 - val_regression_loss: 17.5593 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.2274 - regression_loss: 20.5628 - val_loss: 24.5581 - val_regression_loss: 17.5212 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.0815 - regression_loss: 20.4341 - val_loss: 24.2506 - val_regression_loss: 17.3510 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.2217 - regression_loss: 20.5153 - val_loss: 24.9180 - val_regression_loss: 17.8451 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.9619 - regression_loss: 20.4352 - val_loss: 24.3792 - val_regression_loss: 17.4678 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.3808 - regression_loss: 20.5494 - val_loss: 25.0585 - val_regression_loss: 17.9278 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.9948 - regression_loss: 20.2006 - val_loss: 24.4248 - val_regression_loss: 17.4142 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.6795 - regression_loss: 20.1902 - val_loss: 24.6673 - val_regression_loss: 17.6448 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.9490 - regression_loss: 20.1647 - val_loss: 24.2727 - val_regression_loss: 17.3152 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.4504 - regression_loss: 20.0739 - val_loss: 24.3116 - val_regression_loss: 17.3357 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.9212 - regression_loss: 20.1993 - val_loss: 24.3068 - val_regression_loss: 17.3373 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.1216 - regression_loss: 20.5489 - val_loss: 25.0916 - val_regression_loss: 17.9656 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.1389 - regression_loss: 20.0085 - val_loss: 24.0582 - val_regression_loss: 17.1693 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.6055 - regression_loss: 20.0000 - val_loss: 24.6005 - val_regression_loss: 17.6086 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.6081 - regression_loss: 19.7895 - val_loss: 24.2248 - val_regression_loss: 17.2691 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.1919 - regression_loss: 19.7421 - val_loss: 24.4989 - val_regression_loss: 17.5029 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.4521 - regression_loss: 19.8639 - val_loss: 23.8820 - val_regression_loss: 17.0086 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 24.1179 - regression_loss: 21.9548\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.3237 - regression_loss: 19.6017 - val_loss: 24.9701 - val_regression_loss: 17.8761 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.1626 - regression_loss: 19.6959 - val_loss: 24.0972 - val_regression_loss: 17.1891 - lr: 5.0000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.2199 - regression_loss: 19.8726 - val_loss: 24.2207 - val_regression_loss: 17.2492 - lr: 5.0000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.5807 - regression_loss: 19.8971 - val_loss: 24.7331 - val_regression_loss: 17.6484 - lr: 5.0000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.9510 - regression_loss: 19.5667 - val_loss: 23.9510 - val_regression_loss: 17.0718 - lr: 5.0000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.0046 - regression_loss: 19.4147 - val_loss: 24.4185 - val_regression_loss: 17.4273 - lr: 5.0000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.0314 - regression_loss: 19.5234 - val_loss: 24.0928 - val_regression_loss: 17.1865 - lr: 5.0000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.6165 - regression_loss: 19.2628 - val_loss: 24.1117 - val_regression_loss: 17.1848 - lr: 5.0000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.5423 - regression_loss: 19.3658 - val_loss: 24.2707 - val_regression_loss: 17.3198 - lr: 5.0000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.6094 - regression_loss: 19.2973 - val_loss: 24.2248 - val_regression_loss: 17.2675 - lr: 5.0000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.8879 - regression_loss: 19.2967 - val_loss: 24.1357 - val_regression_loss: 17.1972 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.7428 - regression_loss: 19.2115 - val_loss: 23.8821 - val_regression_loss: 17.0059 - lr: 5.0000e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.8861 - regression_loss: 19.2907 - val_loss: 24.1385 - val_regression_loss: 17.2035 - lr: 5.0000e-05\n",
      "Epoch 76/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 23.9493 - regression_loss: 21.7878\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.9510 - regression_loss: 19.2085 - val_loss: 24.1065 - val_regression_loss: 17.1904 - lr: 5.0000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.3475 - regression_loss: 19.1345 - val_loss: 24.1163 - val_regression_loss: 17.1885 - lr: 2.5000e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.5777 - regression_loss: 19.0961 - val_loss: 24.1283 - val_regression_loss: 17.1900 - lr: 2.5000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.4848 - regression_loss: 19.1055 - val_loss: 24.1330 - val_regression_loss: 17.1959 - lr: 2.5000e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.8883 - regression_loss: 19.1058 - val_loss: 24.1802 - val_regression_loss: 17.2273 - lr: 2.5000e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.2723 - regression_loss: 19.0843 - val_loss: 24.0128 - val_regression_loss: 17.1014 - lr: 2.5000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.5143 - regression_loss: 19.0471 - val_loss: 24.0853 - val_regression_loss: 17.1652 - lr: 2.5000e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.5782 - regression_loss: 19.0845 - val_loss: 24.0228 - val_regression_loss: 17.1268 - lr: 2.5000e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.8875 - regression_loss: 19.0235 - val_loss: 24.0566 - val_regression_loss: 17.1593 - lr: 2.5000e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.6274 - regression_loss: 19.0483 - val_loss: 24.0308 - val_regression_loss: 17.1338 - lr: 2.5000e-05\n",
      "Epoch 86/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 23.4149 - regression_loss: 21.2541\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.4155 - regression_loss: 19.0050 - val_loss: 24.1061 - val_regression_loss: 17.1830 - lr: 2.5000e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.3021 - regression_loss: 19.0290 - val_loss: 24.1481 - val_regression_loss: 17.2156 - lr: 1.2500e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.3649 - regression_loss: 19.0291 - val_loss: 24.0183 - val_regression_loss: 17.1167 - lr: 1.2500e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.2069 - regression_loss: 18.9951 - val_loss: 24.0260 - val_regression_loss: 17.1176 - lr: 1.2500e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.1498 - regression_loss: 18.9722 - val_loss: 24.0596 - val_regression_loss: 17.1390 - lr: 1.2500e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.2972 - regression_loss: 18.9760 - val_loss: 24.1194 - val_regression_loss: 17.1914 - lr: 1.2500e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.4570 - regression_loss: 18.9710 - val_loss: 24.0866 - val_regression_loss: 17.1691 - lr: 1.2500e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.2122 - regression_loss: 18.9691 - val_loss: 24.0030 - val_regression_loss: 17.1022 - lr: 1.2500e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.5920 - regression_loss: 18.9540 - val_loss: 24.0279 - val_regression_loss: 17.1181 - lr: 1.2500e-05\n",
      "Epoch 95/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 29.4685 - regression_loss: 27.3080\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.4490 - regression_loss: 18.9708 - val_loss: 23.9634 - val_regression_loss: 17.0727 - lr: 1.2500e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.5726 - regression_loss: 18.9405 - val_loss: 23.9902 - val_regression_loss: 17.0952 - lr: 6.2500e-06\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.5147 - regression_loss: 18.9310 - val_loss: 24.0315 - val_regression_loss: 17.1265 - lr: 6.2500e-06\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.6997 - regression_loss: 18.9351 - val_loss: 24.0533 - val_regression_loss: 17.1387 - lr: 6.2500e-06\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.4327 - regression_loss: 18.9296 - val_loss: 24.0648 - val_regression_loss: 17.1479 - lr: 6.2500e-06\n",
      "Epoch 100/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 23.1740 - regression_loss: 21.0136\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.6865 - regression_loss: 18.9405 - val_loss: 24.0192 - val_regression_loss: 17.1142 - lr: 6.2500e-06\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.1211 - regression_loss: 18.9302 - val_loss: 24.0381 - val_regression_loss: 17.1298 - lr: 3.1250e-06\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.4015 - regression_loss: 18.9221 - val_loss: 24.0147 - val_regression_loss: 17.1126 - lr: 3.1250e-06\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 121.4854 - regression_loss: 109.3224 - val_loss: 63.9574 - val_regression_loss: 52.8008 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 83.0089 - regression_loss: 74.3980 - val_loss: 49.9491 - val_regression_loss: 41.3675 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 63.5427 - regression_loss: 57.2363 - val_loss: 37.6990 - val_regression_loss: 31.4546 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 53.0611 - regression_loss: 46.6491 - val_loss: 31.9363 - val_regression_loss: 26.3075 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 42.6743 - regression_loss: 38.4465 - val_loss: 28.0417 - val_regression_loss: 22.6420 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 37.0541 - regression_loss: 32.6792 - val_loss: 24.9693 - val_regression_loss: 19.8180 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 33.7389 - regression_loss: 28.9567 - val_loss: 23.7142 - val_regression_loss: 18.5824 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.3498 - regression_loss: 25.4209 - val_loss: 22.1734 - val_regression_loss: 17.3060 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.7843 - regression_loss: 22.8289 - val_loss: 20.7194 - val_regression_loss: 16.1226 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.4855 - regression_loss: 20.6350 - val_loss: 19.7534 - val_regression_loss: 15.3541 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.2352 - regression_loss: 18.8561 - val_loss: 18.3944 - val_regression_loss: 14.2510 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.0481 - regression_loss: 17.0793 - val_loss: 18.1039 - val_regression_loss: 14.0137 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.8329 - regression_loss: 15.6217 - val_loss: 16.8722 - val_regression_loss: 12.9963 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5839 - regression_loss: 14.3692 - val_loss: 16.2104 - val_regression_loss: 12.4675 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8698 - regression_loss: 13.4853 - val_loss: 15.6345 - val_regression_loss: 11.9722 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5812 - regression_loss: 12.6163 - val_loss: 15.4904 - val_regression_loss: 11.8475 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.4257 - regression_loss: 11.8420 - val_loss: 15.0101 - val_regression_loss: 11.4590 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1081 - regression_loss: 11.2666 - val_loss: 14.2042 - val_regression_loss: 10.7855 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.4893 - regression_loss: 10.8053 - val_loss: 14.3045 - val_regression_loss: 10.8730 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.3180 - regression_loss: 10.4058 - val_loss: 13.5743 - val_regression_loss: 10.2526 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6939 - regression_loss: 9.9485 - val_loss: 13.3667 - val_regression_loss: 10.0944 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4963 - regression_loss: 9.6533 - val_loss: 12.9567 - val_regression_loss: 9.7183 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0690 - regression_loss: 9.3051 - val_loss: 12.5411 - val_regression_loss: 9.3877 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0275 - regression_loss: 9.2051 - val_loss: 12.1169 - val_regression_loss: 8.9966 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6207 - regression_loss: 8.9852 - val_loss: 11.9876 - val_regression_loss: 8.9151 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3702 - regression_loss: 8.7322 - val_loss: 11.6037 - val_regression_loss: 8.5411 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4131 - regression_loss: 8.5510 - val_loss: 11.5291 - val_regression_loss: 8.4774 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9496 - regression_loss: 8.2847 - val_loss: 11.3160 - val_regression_loss: 8.2806 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0478 - regression_loss: 8.2450 - val_loss: 11.0624 - val_regression_loss: 8.0249 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9699 - regression_loss: 8.2178 - val_loss: 11.1797 - val_regression_loss: 8.1725 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8295 - regression_loss: 8.0980 - val_loss: 10.8191 - val_regression_loss: 7.7635 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8218 - regression_loss: 8.1443 - val_loss: 11.0884 - val_regression_loss: 8.0707 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.5709 - regression_loss: 7.9036 - val_loss: 10.3563 - val_regression_loss: 7.3798 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2069 - regression_loss: 7.5290 - val_loss: 10.2955 - val_regression_loss: 7.3224 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2286 - regression_loss: 7.4700 - val_loss: 10.2764 - val_regression_loss: 7.2794 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9265 - regression_loss: 7.3162 - val_loss: 9.9582 - val_regression_loss: 6.9777 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8778 - regression_loss: 7.1691 - val_loss: 9.8314 - val_regression_loss: 6.8634 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7311 - regression_loss: 7.0580 - val_loss: 9.7821 - val_regression_loss: 6.8003 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6606 - regression_loss: 7.0280 - val_loss: 9.7283 - val_regression_loss: 6.6942 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5522 - regression_loss: 6.9871 - val_loss: 9.6510 - val_regression_loss: 6.6742 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4856 - regression_loss: 6.8266 - val_loss: 9.5022 - val_regression_loss: 6.5154 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5303 - regression_loss: 6.7969 - val_loss: 9.5784 - val_regression_loss: 6.5677 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3275 - regression_loss: 6.7384 - val_loss: 9.5499 - val_regression_loss: 6.4695 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3959 - regression_loss: 6.7106 - val_loss: 9.3920 - val_regression_loss: 6.3677 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1635 - regression_loss: 6.6374 - val_loss: 9.4351 - val_regression_loss: 6.4020 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2896 - regression_loss: 6.7098 - val_loss: 9.2088 - val_regression_loss: 6.1791 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1119 - regression_loss: 6.4913 - val_loss: 9.3066 - val_regression_loss: 6.2737 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1921 - regression_loss: 6.5340 - val_loss: 9.1713 - val_regression_loss: 6.1065 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.0827 - regression_loss: 6.4444 - val_loss: 9.1544 - val_regression_loss: 6.1075 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0471 - regression_loss: 6.3579 - val_loss: 9.0659 - val_regression_loss: 6.0117 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8254 - regression_loss: 6.2643 - val_loss: 9.0744 - val_regression_loss: 6.0279 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8275 - regression_loss: 6.2321 - val_loss: 9.0358 - val_regression_loss: 5.9699 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7144 - regression_loss: 6.2122 - val_loss: 9.0320 - val_regression_loss: 5.9349 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7628 - regression_loss: 6.1790 - val_loss: 9.0076 - val_regression_loss: 5.9347 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6215 - regression_loss: 6.1407 - val_loss: 8.9778 - val_regression_loss: 5.8868 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4956 - regression_loss: 6.1105 - val_loss: 9.0466 - val_regression_loss: 5.9102 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7103 - regression_loss: 6.0694 - val_loss: 8.9025 - val_regression_loss: 5.8258 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5662 - regression_loss: 6.0494 - val_loss: 8.8540 - val_regression_loss: 5.7869 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6915 - regression_loss: 6.0274 - val_loss: 8.7685 - val_regression_loss: 5.6952 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3986 - regression_loss: 5.9809 - val_loss: 8.8944 - val_regression_loss: 5.8091 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5762 - regression_loss: 6.0395 - val_loss: 9.0785 - val_regression_loss: 5.8684 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4261 - regression_loss: 5.9178 - val_loss: 9.1369 - val_regression_loss: 6.0485 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4940 - regression_loss: 6.0353 - val_loss: 9.0488 - val_regression_loss: 5.8321 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4726 - regression_loss: 5.9704 - val_loss: 9.2792 - val_regression_loss: 6.1564 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.4666 - regression_loss: 5.3282\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5105 - regression_loss: 6.1696 - val_loss: 9.6501 - val_regression_loss: 6.2775 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8645 - regression_loss: 6.3036 - val_loss: 9.0249 - val_regression_loss: 5.9363 - lr: 5.0000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.6509 - regression_loss: 6.1391 - val_loss: 8.7730 - val_regression_loss: 5.6194 - lr: 5.0000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6212 - regression_loss: 6.0034 - val_loss: 8.7328 - val_regression_loss: 5.6170 - lr: 5.0000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3658 - regression_loss: 5.7537 - val_loss: 8.7538 - val_regression_loss: 5.6547 - lr: 5.0000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2866 - regression_loss: 5.7759 - val_loss: 8.6447 - val_regression_loss: 5.5428 - lr: 5.0000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2079 - regression_loss: 5.7440 - val_loss: 8.6399 - val_regression_loss: 5.5510 - lr: 5.0000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2408 - regression_loss: 5.7322 - val_loss: 8.6767 - val_regression_loss: 5.5677 - lr: 5.0000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2801 - regression_loss: 5.7152 - val_loss: 8.7422 - val_regression_loss: 5.6116 - lr: 5.0000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1813 - regression_loss: 5.7031 - val_loss: 8.7323 - val_regression_loss: 5.5855 - lr: 5.0000e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2709 - regression_loss: 5.7193 - val_loss: 8.6551 - val_regression_loss: 5.5615 - lr: 5.0000e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1663 - regression_loss: 5.7031 - val_loss: 8.6348 - val_regression_loss: 5.5442 - lr: 5.0000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2300 - regression_loss: 5.6893 - val_loss: 8.6735 - val_regression_loss: 5.5483 - lr: 5.0000e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9943 - regression_loss: 5.6420 - val_loss: 8.6706 - val_regression_loss: 5.5683 - lr: 5.0000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2049 - regression_loss: 5.6909 - val_loss: 8.6632 - val_regression_loss: 5.5234 - lr: 5.0000e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0364 - regression_loss: 5.6498 - val_loss: 8.6576 - val_regression_loss: 5.5555 - lr: 5.0000e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1289 - regression_loss: 5.6253 - val_loss: 8.6141 - val_regression_loss: 5.5036 - lr: 5.0000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1483 - regression_loss: 5.6776 - val_loss: 8.6374 - val_regression_loss: 5.5310 - lr: 5.0000e-05\n",
      "Epoch 83/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.6606 - regression_loss: 5.5256\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0832 - regression_loss: 5.6471 - val_loss: 8.6238 - val_regression_loss: 5.5227 - lr: 5.0000e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0592 - regression_loss: 5.5895 - val_loss: 8.6331 - val_regression_loss: 5.5135 - lr: 2.5000e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1025 - regression_loss: 5.5932 - val_loss: 8.6360 - val_regression_loss: 5.5189 - lr: 2.5000e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2019 - regression_loss: 5.5784 - val_loss: 8.6555 - val_regression_loss: 5.5290 - lr: 2.5000e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1531 - regression_loss: 5.5729 - val_loss: 8.6178 - val_regression_loss: 5.4976 - lr: 2.5000e-05\n",
      "Epoch 88/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.7893 - regression_loss: 5.6549\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0486 - regression_loss: 5.5787 - val_loss: 8.5998 - val_regression_loss: 5.4869 - lr: 2.5000e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0476 - regression_loss: 5.5632 - val_loss: 8.6211 - val_regression_loss: 5.5114 - lr: 1.2500e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1462 - regression_loss: 5.5580 - val_loss: 8.6213 - val_regression_loss: 5.5070 - lr: 1.2500e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1090 - regression_loss: 5.5577 - val_loss: 8.6179 - val_regression_loss: 5.4985 - lr: 1.2500e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1407 - regression_loss: 5.5527 - val_loss: 8.6229 - val_regression_loss: 5.4993 - lr: 1.2500e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9808 - regression_loss: 5.5552 - val_loss: 8.6263 - val_regression_loss: 5.5106 - lr: 1.2500e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0954 - regression_loss: 5.5573 - val_loss: 8.6022 - val_regression_loss: 5.4859 - lr: 1.2500e-05\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0558 - regression_loss: 5.5453 - val_loss: 8.5974 - val_regression_loss: 5.4844 - lr: 1.2500e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0006 - regression_loss: 5.5482 - val_loss: 8.6122 - val_regression_loss: 5.4896 - lr: 1.2500e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0711 - regression_loss: 5.5448 - val_loss: 8.6154 - val_regression_loss: 5.4985 - lr: 1.2500e-05\n",
      "Epoch 98/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.4274 - regression_loss: 6.2935\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0546 - regression_loss: 5.5440 - val_loss: 8.6123 - val_regression_loss: 5.4951 - lr: 1.2500e-05\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9466 - regression_loss: 5.5346 - val_loss: 8.6056 - val_regression_loss: 5.4849 - lr: 6.2500e-06\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0737 - regression_loss: 5.5362 - val_loss: 8.6042 - val_regression_loss: 5.4864 - lr: 6.2500e-06\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0798 - regression_loss: 5.5344 - val_loss: 8.6050 - val_regression_loss: 5.4844 - lr: 6.2500e-06\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0979 - regression_loss: 5.5314 - val_loss: 8.6059 - val_regression_loss: 5.4872 - lr: 6.2500e-06\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1604 - regression_loss: 5.5318 - val_loss: 8.6109 - val_regression_loss: 5.4917 - lr: 6.2500e-06\n",
      "Epoch 104/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.6705 - regression_loss: 5.5368\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0097 - regression_loss: 5.5288 - val_loss: 8.6140 - val_regression_loss: 5.4951 - lr: 6.2500e-06\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0118 - regression_loss: 5.5254 - val_loss: 8.6130 - val_regression_loss: 5.4922 - lr: 3.1250e-06\n",
      "Epoch 106/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1131 - regression_loss: 5.5261 - val_loss: 8.6149 - val_regression_loss: 5.4935 - lr: 3.1250e-06\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9904 - regression_loss: 5.5237 - val_loss: 8.6118 - val_regression_loss: 5.4900 - lr: 3.1250e-06\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0914 - regression_loss: 5.5235 - val_loss: 8.6130 - val_regression_loss: 5.4898 - lr: 3.1250e-06\n",
      "Epoch 109/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.6811 - regression_loss: 6.5475\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0687 - regression_loss: 5.5241 - val_loss: 8.6085 - val_regression_loss: 5.4880 - lr: 3.1250e-06\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.1343 - regression_loss: 5.5237 - val_loss: 8.6066 - val_regression_loss: 5.4858 - lr: 1.5625e-06\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0332 - regression_loss: 5.5212 - val_loss: 8.6075 - val_regression_loss: 5.4869 - lr: 1.5625e-06\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1453 - regression_loss: 5.5210 - val_loss: 8.6074 - val_regression_loss: 5.4873 - lr: 1.5625e-06\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0497 - regression_loss: 5.5225 - val_loss: 8.6067 - val_regression_loss: 5.4872 - lr: 1.5625e-06\n",
      "Epoch 114/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.8553 - regression_loss: 6.7217\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1087 - regression_loss: 5.5197 - val_loss: 8.6060 - val_regression_loss: 5.4865 - lr: 1.5625e-06\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0518 - regression_loss: 5.5191 - val_loss: 8.6060 - val_regression_loss: 5.4861 - lr: 7.8125e-07\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0768 - regression_loss: 5.5190 - val_loss: 8.6061 - val_regression_loss: 5.4860 - lr: 7.8125e-07\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0446 - regression_loss: 5.5189 - val_loss: 8.6068 - val_regression_loss: 5.4863 - lr: 7.8125e-07\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0940 - regression_loss: 5.5192 - val_loss: 8.6056 - val_regression_loss: 5.4858 - lr: 7.8125e-07\n",
      "Epoch 119/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.2742 - regression_loss: 6.1406\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0527 - regression_loss: 5.5187 - val_loss: 8.6051 - val_regression_loss: 5.4849 - lr: 7.8125e-07\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0517 - regression_loss: 5.5183 - val_loss: 8.6049 - val_regression_loss: 5.4848 - lr: 3.9062e-07\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0874 - regression_loss: 5.5182 - val_loss: 8.6050 - val_regression_loss: 5.4847 - lr: 3.9062e-07\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0057 - regression_loss: 5.5181 - val_loss: 8.6050 - val_regression_loss: 5.4847 - lr: 3.9062e-07\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0594 - regression_loss: 5.5181 - val_loss: 8.6053 - val_regression_loss: 5.4849 - lr: 3.9062e-07\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9077 - regression_loss: 5.5181 - val_loss: 8.6053 - val_regression_loss: 5.4850 - lr: 3.9062e-07\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0081 - regression_loss: 5.5179 - val_loss: 8.6055 - val_regression_loss: 5.4853 - lr: 3.9062e-07\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0602 - regression_loss: 5.5179 - val_loss: 8.6058 - val_regression_loss: 5.4854 - lr: 3.9062e-07\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9881 - regression_loss: 5.5179 - val_loss: 8.6051 - val_regression_loss: 5.4847 - lr: 3.9062e-07\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0501 - regression_loss: 5.5179 - val_loss: 8.6053 - val_regression_loss: 5.4850 - lr: 3.9062e-07\n",
      "Epoch 129/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.6617 - regression_loss: 7.5281\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0224 - regression_loss: 5.5176 - val_loss: 8.6053 - val_regression_loss: 5.4848 - lr: 3.9062e-07\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0653 - regression_loss: 5.5176 - val_loss: 8.6053 - val_regression_loss: 5.4849 - lr: 1.9531e-07\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9272 - regression_loss: 5.5174 - val_loss: 8.6050 - val_regression_loss: 5.4847 - lr: 1.9531e-07\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0026 - regression_loss: 5.5173 - val_loss: 8.6051 - val_regression_loss: 5.4847 - lr: 1.9531e-07\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.9843 - regression_loss: 5.5173 - val_loss: 8.6049 - val_regression_loss: 5.4846 - lr: 1.9531e-07\n",
      "Epoch 134/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.9291 - regression_loss: 5.7955\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0967 - regression_loss: 5.5172 - val_loss: 8.6052 - val_regression_loss: 5.4847 - lr: 1.9531e-07\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0282 - regression_loss: 5.5172 - val_loss: 8.6052 - val_regression_loss: 5.4848 - lr: 9.7656e-08\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 53.1275 - regression_loss: 48.7477 - val_loss: 41.3797 - val_regression_loss: 29.0241 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 37.2037 - regression_loss: 32.6959 - val_loss: 32.9112 - val_regression_loss: 22.7413 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.5772 - regression_loss: 25.3174 - val_loss: 28.2526 - val_regression_loss: 19.5079 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.2855 - regression_loss: 21.1537 - val_loss: 24.4968 - val_regression_loss: 16.6365 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4999 - regression_loss: 17.2785 - val_loss: 21.2153 - val_regression_loss: 14.1907 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.0409 - regression_loss: 13.9478 - val_loss: 18.2156 - val_regression_loss: 11.8876 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.3289 - regression_loss: 11.6717 - val_loss: 15.8881 - val_regression_loss: 10.1228 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5831 - regression_loss: 9.7619 - val_loss: 13.8064 - val_regression_loss: 8.5729 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1877 - regression_loss: 8.3550 - val_loss: 11.8445 - val_regression_loss: 7.1784 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6171 - regression_loss: 7.1580 - val_loss: 10.5587 - val_regression_loss: 6.2017 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0339 - regression_loss: 6.3703 - val_loss: 9.7814 - val_regression_loss: 5.5982 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1105 - regression_loss: 5.5285 - val_loss: 8.6351 - val_regression_loss: 4.8113 - lr: 1.0000e-04\n",
      "Epoch 13/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5347 - regression_loss: 4.9272 - val_loss: 8.0419 - val_regression_loss: 4.3083 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9697 - regression_loss: 4.4453 - val_loss: 7.2903 - val_regression_loss: 3.7990 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5358 - regression_loss: 4.0981 - val_loss: 6.9709 - val_regression_loss: 3.5853 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9190 - regression_loss: 3.6709 - val_loss: 6.7992 - val_regression_loss: 3.3774 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9538 - regression_loss: 3.5380 - val_loss: 6.5899 - val_regression_loss: 3.4184 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9771 - regression_loss: 3.5128 - val_loss: 6.3659 - val_regression_loss: 3.0483 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5434 - regression_loss: 3.1539 - val_loss: 5.8256 - val_regression_loss: 2.7721 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.4086 - regression_loss: 2.9847 - val_loss: 5.5790 - val_regression_loss: 2.5428 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0892 - regression_loss: 2.7444 - val_loss: 5.3739 - val_regression_loss: 2.3915 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9846 - regression_loss: 2.6245 - val_loss: 5.3613 - val_regression_loss: 2.3540 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7907 - regression_loss: 2.5783 - val_loss: 5.1911 - val_regression_loss: 2.2697 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6986 - regression_loss: 2.4472 - val_loss: 5.2437 - val_regression_loss: 2.2779 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6195 - regression_loss: 2.3597 - val_loss: 5.0359 - val_regression_loss: 2.1220 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3306 - regression_loss: 2.2764 - val_loss: 4.9228 - val_regression_loss: 2.0800 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6040 - regression_loss: 2.2800 - val_loss: 4.9147 - val_regression_loss: 2.0228 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.4122 - regression_loss: 2.0977 - val_loss: 4.8247 - val_regression_loss: 2.0322 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3700 - regression_loss: 2.0754 - val_loss: 4.6813 - val_regression_loss: 1.8901 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3051 - regression_loss: 2.0182 - val_loss: 4.8222 - val_regression_loss: 1.9487 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2428 - regression_loss: 1.9757 - val_loss: 4.7094 - val_regression_loss: 1.9601 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2861 - regression_loss: 2.0240 - val_loss: 4.5009 - val_regression_loss: 1.7743 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0527 - regression_loss: 1.8374 - val_loss: 4.4619 - val_regression_loss: 1.6975 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0380 - regression_loss: 1.7566 - val_loss: 4.4894 - val_regression_loss: 1.7716 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9926 - regression_loss: 1.7521 - val_loss: 4.4949 - val_regression_loss: 1.7270 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9077 - regression_loss: 1.6731 - val_loss: 4.2890 - val_regression_loss: 1.6068 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8617 - regression_loss: 1.6093 - val_loss: 4.3366 - val_regression_loss: 1.6159 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8513 - regression_loss: 1.5767 - val_loss: 4.2834 - val_regression_loss: 1.6058 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7774 - regression_loss: 1.5533 - val_loss: 4.1379 - val_regression_loss: 1.4781 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7153 - regression_loss: 1.4823 - val_loss: 4.1329 - val_regression_loss: 1.4895 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6599 - regression_loss: 1.4442 - val_loss: 4.1228 - val_regression_loss: 1.4797 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5120 - regression_loss: 1.4017 - val_loss: 4.0395 - val_regression_loss: 1.4444 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6043 - regression_loss: 1.4000 - val_loss: 4.2653 - val_regression_loss: 1.5546 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6333 - regression_loss: 1.4394 - val_loss: 4.2016 - val_regression_loss: 1.6088 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7346 - regression_loss: 1.5326 - val_loss: 3.9764 - val_regression_loss: 1.3600 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5159 - regression_loss: 1.3060 - val_loss: 3.8378 - val_regression_loss: 1.2678 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.9240 - regression_loss: 0.8030\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5241 - regression_loss: 1.2792 - val_loss: 3.9099 - val_regression_loss: 1.3626 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3515 - regression_loss: 1.2970 - val_loss: 3.9678 - val_regression_loss: 1.3518 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3862 - regression_loss: 1.2248 - val_loss: 3.8081 - val_regression_loss: 1.2757 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4885 - regression_loss: 1.2755 - val_loss: 3.9092 - val_regression_loss: 1.3095 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5110 - regression_loss: 1.2633 - val_loss: 3.8424 - val_regression_loss: 1.3145 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4810 - regression_loss: 1.2528 - val_loss: 3.8707 - val_regression_loss: 1.2810 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.3671 - regression_loss: 1.2475\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4457 - regression_loss: 1.2125 - val_loss: 3.7418 - val_regression_loss: 1.2215 - lr: 5.0000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3771 - regression_loss: 1.1620 - val_loss: 3.7337 - val_regression_loss: 1.2002 - lr: 2.5000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3071 - regression_loss: 1.1369 - val_loss: 3.7795 - val_regression_loss: 1.2249 - lr: 2.5000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2938 - regression_loss: 1.1049 - val_loss: 3.7123 - val_regression_loss: 1.1934 - lr: 2.5000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3130 - regression_loss: 1.1411 - val_loss: 3.6664 - val_regression_loss: 1.1574 - lr: 2.5000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2845 - regression_loss: 1.1017 - val_loss: 3.7146 - val_regression_loss: 1.1774 - lr: 2.5000e-05\n",
      "Epoch 59/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3198 - regression_loss: 1.0977 - val_loss: 3.6903 - val_regression_loss: 1.1746 - lr: 2.5000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3027 - regression_loss: 1.0889 - val_loss: 3.6980 - val_regression_loss: 1.1821 - lr: 2.5000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2967 - regression_loss: 1.0860 - val_loss: 3.6680 - val_regression_loss: 1.1497 - lr: 2.5000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2738 - regression_loss: 1.0750 - val_loss: 3.6605 - val_regression_loss: 1.1468 - lr: 2.5000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.2776 - regression_loss: 1.0624 - val_loss: 3.6440 - val_regression_loss: 1.1412 - lr: 2.5000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.2512 - regression_loss: 1.0565 - val_loss: 3.6625 - val_regression_loss: 1.1451 - lr: 2.5000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1614 - regression_loss: 1.0535 - val_loss: 3.6343 - val_regression_loss: 1.1296 - lr: 2.5000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2578 - regression_loss: 1.0494 - val_loss: 3.6350 - val_regression_loss: 1.1313 - lr: 2.5000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.2478 - regression_loss: 1.0358 - val_loss: 3.6374 - val_regression_loss: 1.1297 - lr: 2.5000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2353 - regression_loss: 1.0267 - val_loss: 3.6210 - val_regression_loss: 1.1231 - lr: 2.5000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1961 - regression_loss: 1.0212 - val_loss: 3.6211 - val_regression_loss: 1.1222 - lr: 2.5000e-05\n",
      "Epoch 70/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.0673 - regression_loss: 1.9498\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.2229 - regression_loss: 1.0135 - val_loss: 3.6088 - val_regression_loss: 1.1118 - lr: 2.5000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1994 - regression_loss: 1.0059 - val_loss: 3.5927 - val_regression_loss: 1.1009 - lr: 1.2500e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1990 - regression_loss: 1.0027 - val_loss: 3.5845 - val_regression_loss: 1.0953 - lr: 1.2500e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.2106 - regression_loss: 1.0024 - val_loss: 3.5916 - val_regression_loss: 1.1002 - lr: 1.2500e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1684 - regression_loss: 0.9996 - val_loss: 3.5967 - val_regression_loss: 1.1037 - lr: 1.2500e-05\n",
      "Epoch 75/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.1757 - regression_loss: 1.0586\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.2086 - regression_loss: 0.9955 - val_loss: 3.5799 - val_regression_loss: 1.0946 - lr: 1.2500e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1749 - regression_loss: 0.9889 - val_loss: 3.5718 - val_regression_loss: 1.0882 - lr: 6.2500e-06\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1878 - regression_loss: 0.9870 - val_loss: 3.5713 - val_regression_loss: 1.0860 - lr: 6.2500e-06\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1571 - regression_loss: 0.9873 - val_loss: 3.5772 - val_regression_loss: 1.0894 - lr: 6.2500e-06\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1964 - regression_loss: 0.9844 - val_loss: 3.5679 - val_regression_loss: 1.0840 - lr: 6.2500e-06\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1727 - regression_loss: 0.9829 - val_loss: 3.5657 - val_regression_loss: 1.0828 - lr: 6.2500e-06\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1760 - regression_loss: 0.9809 - val_loss: 3.5634 - val_regression_loss: 1.0816 - lr: 6.2500e-06\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1831 - regression_loss: 0.9824 - val_loss: 3.5690 - val_regression_loss: 1.0835 - lr: 6.2500e-06\n",
      "Epoch 83/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.8021 - regression_loss: 0.6853\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1749 - regression_loss: 0.9791 - val_loss: 3.5691 - val_regression_loss: 1.0845 - lr: 6.2500e-06\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1856 - regression_loss: 0.9757 - val_loss: 3.5637 - val_regression_loss: 1.0818 - lr: 3.1250e-06\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1450 - regression_loss: 0.9739 - val_loss: 3.5625 - val_regression_loss: 1.0811 - lr: 3.1250e-06\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1781 - regression_loss: 0.9734 - val_loss: 3.5607 - val_regression_loss: 1.0799 - lr: 3.1250e-06\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1754 - regression_loss: 0.9740 - val_loss: 3.5614 - val_regression_loss: 1.0796 - lr: 3.1250e-06\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1711 - regression_loss: 0.9716 - val_loss: 3.5579 - val_regression_loss: 1.0772 - lr: 3.1250e-06\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1455 - regression_loss: 0.9707 - val_loss: 3.5557 - val_regression_loss: 1.0760 - lr: 3.1250e-06\n",
      "Epoch 90/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.3750 - regression_loss: 1.2583\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1602 - regression_loss: 0.9730 - val_loss: 3.5590 - val_regression_loss: 1.0777 - lr: 3.1250e-06\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1423 - regression_loss: 0.9694 - val_loss: 3.5564 - val_regression_loss: 1.0763 - lr: 1.5625e-06\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1789 - regression_loss: 0.9686 - val_loss: 3.5555 - val_regression_loss: 1.0757 - lr: 1.5625e-06\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1765 - regression_loss: 0.9684 - val_loss: 3.5544 - val_regression_loss: 1.0748 - lr: 1.5625e-06\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1529 - regression_loss: 0.9678 - val_loss: 3.5535 - val_regression_loss: 1.0742 - lr: 1.5625e-06\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1737 - regression_loss: 0.9676 - val_loss: 3.5523 - val_regression_loss: 1.0735 - lr: 1.5625e-06\n",
      "Epoch 96/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.2035 - regression_loss: 1.0869\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1468 - regression_loss: 0.9678 - val_loss: 3.5540 - val_regression_loss: 1.0744 - lr: 1.5625e-06\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1615 - regression_loss: 0.9666 - val_loss: 3.5533 - val_regression_loss: 1.0740 - lr: 7.8125e-07\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1588 - regression_loss: 0.9662 - val_loss: 3.5527 - val_regression_loss: 1.0735 - lr: 7.8125e-07\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1175 - regression_loss: 0.9663 - val_loss: 3.5524 - val_regression_loss: 1.0733 - lr: 7.8125e-07\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1540 - regression_loss: 0.9657 - val_loss: 3.5520 - val_regression_loss: 1.0731 - lr: 7.8125e-07\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1189 - regression_loss: 0.9656 - val_loss: 3.5523 - val_regression_loss: 1.0733 - lr: 7.8125e-07\n",
      "Epoch 102/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1612 - regression_loss: 0.9655 - val_loss: 3.5521 - val_regression_loss: 1.0732 - lr: 7.8125e-07\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1635 - regression_loss: 0.9650 - val_loss: 3.5514 - val_regression_loss: 1.0727 - lr: 7.8125e-07\n",
      "Epoch 104/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.2444 - regression_loss: 1.1279\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1479 - regression_loss: 0.9650 - val_loss: 3.5514 - val_regression_loss: 1.0727 - lr: 7.8125e-07\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1427 - regression_loss: 0.9646 - val_loss: 3.5513 - val_regression_loss: 1.0726 - lr: 3.9062e-07\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1633 - regression_loss: 0.9645 - val_loss: 3.5509 - val_regression_loss: 1.0723 - lr: 3.9062e-07\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1680 - regression_loss: 0.9644 - val_loss: 3.5505 - val_regression_loss: 1.0721 - lr: 3.9062e-07\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1511 - regression_loss: 0.9644 - val_loss: 3.5498 - val_regression_loss: 1.0717 - lr: 3.9062e-07\n",
      "Epoch 109/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.2864 - regression_loss: 1.1698\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 3.1309 - regression_loss: 0.9643 - val_loss: 3.5503 - val_regression_loss: 1.0719 - lr: 3.9062e-07\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1465 - regression_loss: 0.9640 - val_loss: 3.5502 - val_regression_loss: 1.0718 - lr: 1.9531e-07\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.0678 - regression_loss: 0.9640 - val_loss: 3.5499 - val_regression_loss: 1.0716 - lr: 1.9531e-07\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1463 - regression_loss: 0.9641 - val_loss: 3.5496 - val_regression_loss: 1.0715 - lr: 1.9531e-07\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1660 - regression_loss: 0.9639 - val_loss: 3.5495 - val_regression_loss: 1.0714 - lr: 1.9531e-07\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.0767 - regression_loss: 0.9638 - val_loss: 3.5495 - val_regression_loss: 1.0714 - lr: 1.9531e-07\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1443 - regression_loss: 0.9637 - val_loss: 3.5496 - val_regression_loss: 1.0714 - lr: 1.9531e-07\n",
      "Epoch 116/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.8307 - regression_loss: 0.7142\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1518 - regression_loss: 0.9636 - val_loss: 3.5496 - val_regression_loss: 1.0714 - lr: 1.9531e-07\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1715 - regression_loss: 0.9636 - val_loss: 3.5496 - val_regression_loss: 1.0714 - lr: 9.7656e-08\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1734 - regression_loss: 0.9635 - val_loss: 3.5496 - val_regression_loss: 1.0714 - lr: 9.7656e-08\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1605 - regression_loss: 0.9635 - val_loss: 3.5495 - val_regression_loss: 1.0714 - lr: 9.7656e-08\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1474 - regression_loss: 0.9636 - val_loss: 3.5494 - val_regression_loss: 1.0713 - lr: 9.7656e-08\n",
      "Epoch 121/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.2165 - regression_loss: 1.1000\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1369 - regression_loss: 0.9635 - val_loss: 3.5493 - val_regression_loss: 1.0712 - lr: 9.7656e-08\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1644 - regression_loss: 0.9634 - val_loss: 3.5493 - val_regression_loss: 1.0712 - lr: 4.8828e-08\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1761 - regression_loss: 0.9634 - val_loss: 3.5493 - val_regression_loss: 1.0712 - lr: 4.8828e-08\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1567 - regression_loss: 0.9634 - val_loss: 3.5493 - val_regression_loss: 1.0712 - lr: 4.8828e-08\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1585 - regression_loss: 0.9634 - val_loss: 3.5493 - val_regression_loss: 1.0712 - lr: 4.8828e-08\n",
      "Epoch 126/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.6934 - regression_loss: 0.5769\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1401 - regression_loss: 0.9634 - val_loss: 3.5493 - val_regression_loss: 1.0712 - lr: 4.8828e-08\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.0648 - regression_loss: 0.9633 - val_loss: 3.5493 - val_regression_loss: 1.0712 - lr: 2.4414e-08\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1505 - regression_loss: 0.9633 - val_loss: 3.5493 - val_regression_loss: 1.0712 - lr: 2.4414e-08\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1384 - regression_loss: 0.9633 - val_loss: 3.5493 - val_regression_loss: 1.0712 - lr: 2.4414e-08\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1473 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0712 - lr: 2.4414e-08\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1684 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0712 - lr: 2.4414e-08\n",
      "Epoch 132/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.4173 - regression_loss: 1.3008\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1740 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0712 - lr: 2.4414e-08\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1679 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 1.2207e-08\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1563 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 1.2207e-08\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1745 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 1.2207e-08\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1485 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 1.2207e-08\n",
      "Epoch 137/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.9439 - regression_loss: 0.8274\n",
      "Epoch 137: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1242 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 1.2207e-08\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1492 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 6.1035e-09\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1402 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 6.1035e-09\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1559 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 6.1035e-09\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1508 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 6.1035e-09\n",
      "Epoch 142/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.1054 - regression_loss: 0.9889\n",
      "Epoch 142: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-09.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1520 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 6.1035e-09\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1499 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 3.0518e-09\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1332 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 3.0518e-09\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1583 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 3.0518e-09\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1727 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 3.0518e-09\n",
      "Epoch 147/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.4860 - regression_loss: 1.3695\n",
      "Epoch 147: ReduceLROnPlateau reducing learning rate to 1.5258788677030566e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1534 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 3.0518e-09\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.0823 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 1.5259e-09\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1324 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 1.5259e-09\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1604 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 1.5259e-09\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1116 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 1.5259e-09\n",
      "Epoch 152/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.1759 - regression_loss: 1.0594\n",
      "Epoch 152: ReduceLROnPlateau reducing learning rate to 7.629394338515283e-10.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1368 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 1.5259e-09\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.0576 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 7.6294e-10\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1485 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 7.6294e-10\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1281 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 7.6294e-10\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1083 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 7.6294e-10\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1609 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 7.6294e-10\n",
      "Epoch 158/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.6750 - regression_loss: 0.5585\n",
      "Epoch 158: ReduceLROnPlateau reducing learning rate to 3.8146971692576415e-10.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1395 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 7.6294e-10\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1801 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 3.8147e-10\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.0802 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 3.8147e-10\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1609 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 3.8147e-10\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1423 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 3.8147e-10\n",
      "Epoch 163/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.9415 - regression_loss: 0.8250\n",
      "Epoch 163: ReduceLROnPlateau reducing learning rate to 1.9073485846288207e-10.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1679 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 3.8147e-10\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1567 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 1.9073e-10\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1719 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 1.9073e-10\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.0800 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 1.9073e-10\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1208 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 1.9073e-10\n",
      "Epoch 168/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.2384 - regression_loss: 1.1219\n",
      "Epoch 168: ReduceLROnPlateau reducing learning rate to 9.536742923144104e-11.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1611 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 1.9073e-10\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1676 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 9.5367e-11\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1502 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 9.5367e-11\n",
      "Epoch 171/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1732 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 9.5367e-11\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1508 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 9.5367e-11\n",
      "Epoch 173/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.0770 - regression_loss: 0.9605\n",
      "Epoch 173: ReduceLROnPlateau reducing learning rate to 4.768371461572052e-11.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1137 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 9.5367e-11\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1604 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 4.7684e-11\n",
      "Epoch 175/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1589 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 4.7684e-11\n",
      "Epoch 176/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1075 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 4.7684e-11\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1421 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 4.7684e-11\n",
      "Epoch 178/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.2904 - regression_loss: 1.1739\n",
      "Epoch 178: ReduceLROnPlateau reducing learning rate to 2.384185730786026e-11.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1682 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 4.7684e-11\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1748 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 2.3842e-11\n",
      "Epoch 180/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1717 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 2.3842e-11\n",
      "Epoch 181/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1708 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 2.3842e-11\n",
      "Epoch 182/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1659 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 2.3842e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.0570 - regression_loss: 0.9405\n",
      "Epoch 183: ReduceLROnPlateau reducing learning rate to 1.192092865393013e-11.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1493 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 2.3842e-11\n",
      "Epoch 184/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1373 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 1.1921e-11\n",
      "Epoch 185/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1303 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 1.1921e-11\n",
      "Epoch 186/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1655 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 1.1921e-11\n",
      "Epoch 187/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1366 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 1.1921e-11\n",
      "Epoch 188/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.1302 - regression_loss: 1.0137\n",
      "Epoch 188: ReduceLROnPlateau reducing learning rate to 5.960464326965065e-12.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1631 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 1.1921e-11\n",
      "Epoch 189/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1530 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 5.9605e-12\n",
      "Epoch 190/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.0533 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 5.9605e-12\n",
      "Epoch 191/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1697 - regression_loss: 0.9633 - val_loss: 3.5492 - val_regression_loss: 1.0711 - lr: 5.9605e-12\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 26ms/step - loss: 121.0731 - regression_loss: 112.5194 - val_loss: 102.5105 - val_regression_loss: 75.9835 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 82.1692 - regression_loss: 73.6099 - val_loss: 76.8365 - val_regression_loss: 56.8211 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 67.4590 - regression_loss: 60.6063 - val_loss: 71.9298 - val_regression_loss: 53.2558 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 61.1844 - regression_loss: 56.3768 - val_loss: 68.8669 - val_regression_loss: 50.7113 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 58.7470 - regression_loss: 52.2391 - val_loss: 60.4403 - val_regression_loss: 44.3502 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 53.5396 - regression_loss: 47.5567 - val_loss: 59.6516 - val_regression_loss: 43.1327 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 48.6568 - regression_loss: 44.2300 - val_loss: 56.4153 - val_regression_loss: 40.5353 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 47.3240 - regression_loss: 41.8069 - val_loss: 51.3589 - val_regression_loss: 36.8289 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 42.1264 - regression_loss: 37.8919 - val_loss: 50.4740 - val_regression_loss: 35.9203 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 40.3623 - regression_loss: 35.2694 - val_loss: 45.4610 - val_regression_loss: 32.2990 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 36.6625 - regression_loss: 32.3633 - val_loss: 42.2360 - val_regression_loss: 29.8173 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 33.2019 - regression_loss: 29.4343 - val_loss: 39.0818 - val_regression_loss: 27.3203 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31.4335 - regression_loss: 27.0421 - val_loss: 36.5420 - val_regression_loss: 25.3025 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.5952 - regression_loss: 24.5213 - val_loss: 33.2662 - val_regression_loss: 22.8118 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.2387 - regression_loss: 22.1070 - val_loss: 30.3261 - val_regression_loss: 20.6091 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.0645 - regression_loss: 20.2444 - val_loss: 27.4023 - val_regression_loss: 18.4787 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.7828 - regression_loss: 18.2241 - val_loss: 25.9647 - val_regression_loss: 17.3247 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.9985 - regression_loss: 16.6142 - val_loss: 23.1257 - val_regression_loss: 15.2568 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.2553 - regression_loss: 15.0121 - val_loss: 22.0751 - val_regression_loss: 14.4372 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.1950 - regression_loss: 13.8036 - val_loss: 19.7377 - val_regression_loss: 12.7718 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.2715 - regression_loss: 12.6930 - val_loss: 19.1887 - val_regression_loss: 12.3136 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.0865 - regression_loss: 11.9027 - val_loss: 17.3375 - val_regression_loss: 11.0258 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.8553 - regression_loss: 10.9668 - val_loss: 16.7004 - val_regression_loss: 10.5380 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.2763 - regression_loss: 10.3415 - val_loss: 15.6025 - val_regression_loss: 9.7849 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.1527 - regression_loss: 9.5910 - val_loss: 15.3418 - val_regression_loss: 9.5593 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.5409 - regression_loss: 9.2228 - val_loss: 14.3227 - val_regression_loss: 8.8934 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.7131 - regression_loss: 8.7793 - val_loss: 14.9541 - val_regression_loss: 9.2918 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.4995 - regression_loss: 8.6197 - val_loss: 13.6746 - val_regression_loss: 8.4068 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.5608 - regression_loss: 8.1450 - val_loss: 13.9791 - val_regression_loss: 8.6159 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.8129 - regression_loss: 8.0998 - val_loss: 12.9829 - val_regression_loss: 7.9131 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2008 - regression_loss: 7.5876 - val_loss: 13.2096 - val_regression_loss: 8.0656 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.1701 - regression_loss: 7.3886 - val_loss: 12.1705 - val_regression_loss: 7.3828 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.0964 - regression_loss: 7.4295 - val_loss: 12.2767 - val_regression_loss: 7.4183 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5749 - regression_loss: 7.0876 - val_loss: 12.2052 - val_regression_loss: 7.3659 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.6065 - regression_loss: 6.8999 - val_loss: 11.6135 - val_regression_loss: 6.9734 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3222 - regression_loss: 6.7457 - val_loss: 12.0402 - val_regression_loss: 7.2513 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4427 - regression_loss: 6.7164 - val_loss: 11.2673 - val_regression_loss: 6.8247 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4897 - regression_loss: 6.8210 - val_loss: 14.3072 - val_regression_loss: 9.0046 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.8859 - regression_loss: 7.2496 - val_loss: 11.4207 - val_regression_loss: 7.0868 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.7251 - regression_loss: 7.0958 - val_loss: 13.7621 - val_regression_loss: 8.5698 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.4590 - regression_loss: 10.3541\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4229 - regression_loss: 6.9052 - val_loss: 10.5995 - val_regression_loss: 6.3424 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.7913 - regression_loss: 6.1242 - val_loss: 11.5767 - val_regression_loss: 6.9586 - lr: 5.0000e-05\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.6157 - regression_loss: 6.2334 - val_loss: 10.6142 - val_regression_loss: 6.2691 - lr: 5.0000e-05\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5525 - regression_loss: 6.0131 - val_loss: 10.4487 - val_regression_loss: 6.1588 - lr: 5.0000e-05\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3554 - regression_loss: 5.7978 - val_loss: 10.9427 - val_regression_loss: 6.4945 - lr: 5.0000e-05\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.4131 - regression_loss: 5.8648 - val_loss: 10.3879 - val_regression_loss: 6.1124 - lr: 5.0000e-05\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.0766 - regression_loss: 5.7589 - val_loss: 10.4320 - val_regression_loss: 6.1384 - lr: 5.0000e-05\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2500 - regression_loss: 5.7233 - val_loss: 10.5096 - val_regression_loss: 6.1948 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.4422 - regression_loss: 5.8084 - val_loss: 10.1513 - val_regression_loss: 5.9545 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.9586 - regression_loss: 5.5349 - val_loss: 10.5508 - val_regression_loss: 6.2301 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1121 - regression_loss: 5.5488 - val_loss: 10.0825 - val_regression_loss: 5.9078 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8494 - regression_loss: 5.5303 - val_loss: 10.1367 - val_regression_loss: 5.9327 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.9997 - regression_loss: 5.4333 - val_loss: 10.1601 - val_regression_loss: 5.9454 - lr: 5.0000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.9383 - regression_loss: 5.4093 - val_loss: 9.8504 - val_regression_loss: 5.7467 - lr: 5.0000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8165 - regression_loss: 5.3678 - val_loss: 9.9363 - val_regression_loss: 5.7978 - lr: 5.0000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8801 - regression_loss: 5.3557 - val_loss: 9.8929 - val_regression_loss: 5.7686 - lr: 5.0000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7852 - regression_loss: 5.2994 - val_loss: 9.6403 - val_regression_loss: 5.5984 - lr: 5.0000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4866 - regression_loss: 5.2355 - val_loss: 9.9458 - val_regression_loss: 5.8018 - lr: 5.0000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.4460 - regression_loss: 5.1491 - val_loss: 9.5530 - val_regression_loss: 5.5335 - lr: 5.0000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7005 - regression_loss: 5.1824 - val_loss: 9.8287 - val_regression_loss: 5.7347 - lr: 5.0000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.5520 - regression_loss: 5.1203 - val_loss: 9.5496 - val_regression_loss: 5.5353 - lr: 5.0000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.5306 - regression_loss: 5.0467 - val_loss: 9.5329 - val_regression_loss: 5.5203 - lr: 5.0000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4744 - regression_loss: 4.9992 - val_loss: 9.4996 - val_regression_loss: 5.4881 - lr: 5.0000e-05\n",
      "Epoch 64/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.2121 - regression_loss: 7.1124\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.4860 - regression_loss: 4.9562 - val_loss: 9.3515 - val_regression_loss: 5.3993 - lr: 5.0000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.3292 - regression_loss: 4.8979 - val_loss: 9.3824 - val_regression_loss: 5.4226 - lr: 2.5000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.3136 - regression_loss: 4.9017 - val_loss: 9.4569 - val_regression_loss: 5.4744 - lr: 2.5000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.3941 - regression_loss: 4.8521 - val_loss: 9.2762 - val_regression_loss: 5.3463 - lr: 2.5000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.2019 - regression_loss: 4.8574 - val_loss: 9.2507 - val_regression_loss: 5.3262 - lr: 2.5000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.3159 - regression_loss: 4.8105 - val_loss: 9.3368 - val_regression_loss: 5.3840 - lr: 2.5000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.2791 - regression_loss: 4.7967 - val_loss: 9.3041 - val_regression_loss: 5.3622 - lr: 2.5000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.2984 - regression_loss: 4.7890 - val_loss: 9.2791 - val_regression_loss: 5.3435 - lr: 2.5000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.3290 - regression_loss: 4.7699 - val_loss: 9.1592 - val_regression_loss: 5.2602 - lr: 2.5000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1740 - regression_loss: 4.7367 - val_loss: 9.2640 - val_regression_loss: 5.3304 - lr: 2.5000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.2155 - regression_loss: 4.7545 - val_loss: 9.2602 - val_regression_loss: 5.3283 - lr: 2.5000e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1565 - regression_loss: 4.7159 - val_loss: 9.0425 - val_regression_loss: 5.1820 - lr: 2.5000e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1839 - regression_loss: 4.7022 - val_loss: 9.1655 - val_regression_loss: 5.2612 - lr: 2.5000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1298 - regression_loss: 4.6827 - val_loss: 9.1773 - val_regression_loss: 5.2726 - lr: 2.5000e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0312 - regression_loss: 4.6526 - val_loss: 9.1440 - val_regression_loss: 5.2468 - lr: 2.5000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0712 - regression_loss: 4.6338 - val_loss: 9.0836 - val_regression_loss: 5.2076 - lr: 2.5000e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0264 - regression_loss: 4.6059 - val_loss: 9.0156 - val_regression_loss: 5.1589 - lr: 2.5000e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0816 - regression_loss: 4.6049 - val_loss: 9.0168 - val_regression_loss: 5.1589 - lr: 2.5000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9920 - regression_loss: 4.5856 - val_loss: 9.1351 - val_regression_loss: 5.2405 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0243 - regression_loss: 4.5489 - val_loss: 8.9223 - val_regression_loss: 5.0936 - lr: 2.5000e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9909 - regression_loss: 4.5370 - val_loss: 8.8903 - val_regression_loss: 5.0708 - lr: 2.5000e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.9253 - regression_loss: 4.5252 - val_loss: 9.0201 - val_regression_loss: 5.1575 - lr: 2.5000e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.8270 - regression_loss: 4.5055 - val_loss: 8.9305 - val_regression_loss: 5.0992 - lr: 2.5000e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.9299 - regression_loss: 4.4772 - val_loss: 8.8433 - val_regression_loss: 5.0405 - lr: 2.5000e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.9511 - regression_loss: 4.4826 - val_loss: 8.8562 - val_regression_loss: 5.0469 - lr: 2.5000e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.7521 - regression_loss: 4.5143 - val_loss: 9.0197 - val_regression_loss: 5.1581 - lr: 2.5000e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.8292 - regression_loss: 4.4624 - val_loss: 8.7185 - val_regression_loss: 4.9556 - lr: 2.5000e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.6215 - regression_loss: 4.4223 - val_loss: 8.8879 - val_regression_loss: 5.0643 - lr: 2.5000e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.8028 - regression_loss: 4.4111 - val_loss: 8.7534 - val_regression_loss: 4.9771 - lr: 2.5000e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.7435 - regression_loss: 4.3861 - val_loss: 8.7958 - val_regression_loss: 5.0010 - lr: 2.5000e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7963 - regression_loss: 4.3937 - val_loss: 8.6979 - val_regression_loss: 4.9343 - lr: 2.5000e-05\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.6165 - regression_loss: 4.3454 - val_loss: 8.9002 - val_regression_loss: 5.0706 - lr: 2.5000e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.8048 - regression_loss: 4.3460 - val_loss: 8.6564 - val_regression_loss: 4.9045 - lr: 2.5000e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.6310 - regression_loss: 4.3198 - val_loss: 8.7436 - val_regression_loss: 4.9605 - lr: 2.5000e-05\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.7341 - regression_loss: 4.3015 - val_loss: 8.5967 - val_regression_loss: 4.8622 - lr: 2.5000e-05\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.6307 - regression_loss: 4.2970 - val_loss: 8.6919 - val_regression_loss: 4.9280 - lr: 2.5000e-05\n",
      "Epoch 100/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.6293 - regression_loss: 5.5341\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.6948 - regression_loss: 4.2596 - val_loss: 8.6254 - val_regression_loss: 4.8830 - lr: 2.5000e-05\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5473 - regression_loss: 4.2460 - val_loss: 8.6229 - val_regression_loss: 4.8812 - lr: 1.2500e-05\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.7026 - regression_loss: 4.2360 - val_loss: 8.6407 - val_regression_loss: 4.8923 - lr: 1.2500e-05\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.6941 - regression_loss: 4.2325 - val_loss: 8.6469 - val_regression_loss: 4.8934 - lr: 1.2500e-05\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.6926 - regression_loss: 4.2182 - val_loss: 8.6012 - val_regression_loss: 4.8627 - lr: 1.2500e-05\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.6283 - regression_loss: 4.2169 - val_loss: 8.6010 - val_regression_loss: 4.8613 - lr: 1.2500e-05\n",
      "Epoch 106/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.5606 - regression_loss: 5.4658\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6419 - regression_loss: 4.2035 - val_loss: 8.5552 - val_regression_loss: 4.8298 - lr: 1.2500e-05\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.6567 - regression_loss: 4.1947 - val_loss: 8.5626 - val_regression_loss: 4.8341 - lr: 6.2500e-06\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.6230 - regression_loss: 4.1885 - val_loss: 8.5780 - val_regression_loss: 4.8442 - lr: 6.2500e-06\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.6509 - regression_loss: 4.1897 - val_loss: 8.6100 - val_regression_loss: 4.8655 - lr: 6.2500e-06\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.6045 - regression_loss: 4.1819 - val_loss: 8.5721 - val_regression_loss: 4.8403 - lr: 6.2500e-06\n",
      "Epoch 111/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.6861 - regression_loss: 4.5915\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.6127 - regression_loss: 4.1772 - val_loss: 8.5475 - val_regression_loss: 4.8237 - lr: 6.2500e-06\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5788 - regression_loss: 4.1740 - val_loss: 8.5557 - val_regression_loss: 4.8291 - lr: 3.1250e-06\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.6118 - regression_loss: 4.1711 - val_loss: 8.5424 - val_regression_loss: 4.8200 - lr: 3.1250e-06\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5375 - regression_loss: 4.1689 - val_loss: 8.5610 - val_regression_loss: 4.8322 - lr: 3.1250e-06\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5639 - regression_loss: 4.1676 - val_loss: 8.5726 - val_regression_loss: 4.8402 - lr: 3.1250e-06\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5664 - regression_loss: 4.1661 - val_loss: 8.5515 - val_regression_loss: 4.8261 - lr: 3.1250e-06\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5356 - regression_loss: 4.1646 - val_loss: 8.5635 - val_regression_loss: 4.8344 - lr: 3.1250e-06\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5721 - regression_loss: 4.1607 - val_loss: 8.5497 - val_regression_loss: 4.8249 - lr: 3.1250e-06\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5660 - regression_loss: 4.1580 - val_loss: 8.5511 - val_regression_loss: 4.8254 - lr: 3.1250e-06\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5010 - regression_loss: 4.1599 - val_loss: 8.5175 - val_regression_loss: 4.8028 - lr: 3.1250e-06\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5047 - regression_loss: 4.1555 - val_loss: 8.5141 - val_regression_loss: 4.8005 - lr: 3.1250e-06\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4457 - regression_loss: 4.1523 - val_loss: 8.5285 - val_regression_loss: 4.8100 - lr: 3.1250e-06\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5289 - regression_loss: 4.1508 - val_loss: 8.5508 - val_regression_loss: 4.8248 - lr: 3.1250e-06\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5385 - regression_loss: 4.1510 - val_loss: 8.5386 - val_regression_loss: 4.8161 - lr: 3.1250e-06\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4936 - regression_loss: 4.1475 - val_loss: 8.5438 - val_regression_loss: 4.8197 - lr: 3.1250e-06\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.4763 - regression_loss: 4.1448 - val_loss: 8.5449 - val_regression_loss: 4.8205 - lr: 3.1250e-06\n",
      "Epoch 127/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.6900 - regression_loss: 3.5957\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5368 - regression_loss: 4.1449 - val_loss: 8.5198 - val_regression_loss: 4.8038 - lr: 3.1250e-06\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5838 - regression_loss: 4.1398 - val_loss: 8.5201 - val_regression_loss: 4.8039 - lr: 1.5625e-06\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5420 - regression_loss: 4.1385 - val_loss: 8.5226 - val_regression_loss: 4.8053 - lr: 1.5625e-06\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5173 - regression_loss: 4.1376 - val_loss: 8.5253 - val_regression_loss: 4.8072 - lr: 1.5625e-06\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.4816 - regression_loss: 4.1372 - val_loss: 8.5343 - val_regression_loss: 4.8130 - lr: 1.5625e-06\n",
      "Epoch 132/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.3803 - regression_loss: 5.2860\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5660 - regression_loss: 4.1389 - val_loss: 8.5130 - val_regression_loss: 4.7988 - lr: 1.5625e-06\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5970 - regression_loss: 4.1344 - val_loss: 8.5169 - val_regression_loss: 4.8013 - lr: 7.8125e-07\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5216 - regression_loss: 4.1338 - val_loss: 8.5168 - val_regression_loss: 4.8013 - lr: 7.8125e-07\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5678 - regression_loss: 4.1333 - val_loss: 8.5172 - val_regression_loss: 4.8014 - lr: 7.8125e-07\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5913 - regression_loss: 4.1336 - val_loss: 8.5132 - val_regression_loss: 4.7987 - lr: 7.8125e-07\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4449 - regression_loss: 4.1329 - val_loss: 8.5192 - val_regression_loss: 4.8027 - lr: 7.8125e-07\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5379 - regression_loss: 4.1316 - val_loss: 8.5210 - val_regression_loss: 4.8038 - lr: 7.8125e-07\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5773 - regression_loss: 4.1317 - val_loss: 8.5164 - val_regression_loss: 4.8006 - lr: 7.8125e-07\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5877 - regression_loss: 4.1308 - val_loss: 8.5196 - val_regression_loss: 4.8028 - lr: 7.8125e-07\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5725 - regression_loss: 4.1300 - val_loss: 8.5182 - val_regression_loss: 4.8019 - lr: 7.8125e-07\n",
      "Epoch 142/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.4567 - regression_loss: 5.3625\n",
      "Epoch 142: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5912 - regression_loss: 4.1298 - val_loss: 8.5144 - val_regression_loss: 4.7994 - lr: 7.8125e-07\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5614 - regression_loss: 4.1291 - val_loss: 8.5111 - val_regression_loss: 4.7972 - lr: 3.9062e-07\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5601 - regression_loss: 4.1285 - val_loss: 8.5135 - val_regression_loss: 4.7988 - lr: 3.9062e-07\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5742 - regression_loss: 4.1283 - val_loss: 8.5115 - val_regression_loss: 4.7974 - lr: 3.9062e-07\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5320 - regression_loss: 4.1285 - val_loss: 8.5154 - val_regression_loss: 4.8000 - lr: 3.9062e-07\n",
      "Epoch 147/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.0439 - regression_loss: 2.9497\n",
      "Epoch 147: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6.5511 - regression_loss: 4.1278 - val_loss: 8.5142 - val_regression_loss: 4.7992 - lr: 3.9062e-07\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.4569 - regression_loss: 4.1278 - val_loss: 8.5165 - val_regression_loss: 4.8007 - lr: 1.9531e-07\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4528 - regression_loss: 4.1272 - val_loss: 8.5152 - val_regression_loss: 4.7998 - lr: 1.9531e-07\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5057 - regression_loss: 4.1270 - val_loss: 8.5146 - val_regression_loss: 4.7994 - lr: 1.9531e-07\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3397 - regression_loss: 4.1269 - val_loss: 8.5149 - val_regression_loss: 4.7996 - lr: 1.9531e-07\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5608 - regression_loss: 4.1267 - val_loss: 8.5145 - val_regression_loss: 4.7993 - lr: 1.9531e-07\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3270 - regression_loss: 4.1271 - val_loss: 8.5163 - val_regression_loss: 4.8005 - lr: 1.9531e-07\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5499 - regression_loss: 4.1267 - val_loss: 8.5135 - val_regression_loss: 4.7986 - lr: 1.9531e-07\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5495 - regression_loss: 4.1265 - val_loss: 8.5127 - val_regression_loss: 4.7981 - lr: 1.9531e-07\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6312 - regression_loss: 4.1262 - val_loss: 8.5118 - val_regression_loss: 4.7975 - lr: 1.9531e-07\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4728 - regression_loss: 4.1261 - val_loss: 8.5114 - val_regression_loss: 4.7972 - lr: 1.9531e-07\n",
      "Epoch 158/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.1888 - regression_loss: 6.0946\n",
      "Epoch 158: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5853 - regression_loss: 4.1260 - val_loss: 8.5108 - val_regression_loss: 4.7968 - lr: 1.9531e-07\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4887 - regression_loss: 4.1258 - val_loss: 8.5104 - val_regression_loss: 4.7965 - lr: 9.7656e-08\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5292 - regression_loss: 4.1257 - val_loss: 8.5109 - val_regression_loss: 4.7968 - lr: 9.7656e-08\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5117 - regression_loss: 4.1257 - val_loss: 8.5114 - val_regression_loss: 4.7972 - lr: 9.7656e-08\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4387 - regression_loss: 4.1255 - val_loss: 8.5116 - val_regression_loss: 4.7973 - lr: 9.7656e-08\n",
      "Epoch 163/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.9313 - regression_loss: 3.8371\n",
      "Epoch 163: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5839 - regression_loss: 4.1254 - val_loss: 8.5113 - val_regression_loss: 4.7971 - lr: 9.7656e-08\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5175 - regression_loss: 4.1254 - val_loss: 8.5111 - val_regression_loss: 4.7970 - lr: 4.8828e-08\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3488 - regression_loss: 4.1254 - val_loss: 8.5110 - val_regression_loss: 4.7969 - lr: 4.8828e-08\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5596 - regression_loss: 4.1253 - val_loss: 8.5108 - val_regression_loss: 4.7968 - lr: 4.8828e-08\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5294 - regression_loss: 4.1253 - val_loss: 8.5110 - val_regression_loss: 4.7969 - lr: 4.8828e-08\n",
      "Epoch 168/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.7764 - regression_loss: 4.6822\n",
      "Epoch 168: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5377 - regression_loss: 4.1253 - val_loss: 8.5108 - val_regression_loss: 4.7968 - lr: 4.8828e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4995 - regression_loss: 4.1252 - val_loss: 8.5109 - val_regression_loss: 4.7968 - lr: 2.4414e-08\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4219 - regression_loss: 4.1252 - val_loss: 8.5108 - val_regression_loss: 4.7968 - lr: 2.4414e-08\n",
      "Epoch 171/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5415 - regression_loss: 4.1252 - val_loss: 8.5110 - val_regression_loss: 4.7969 - lr: 2.4414e-08\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4936 - regression_loss: 4.1252 - val_loss: 8.5109 - val_regression_loss: 4.7968 - lr: 2.4414e-08\n",
      "Epoch 173/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.9421 - regression_loss: 3.8479\n",
      "Epoch 173: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5634 - regression_loss: 4.1251 - val_loss: 8.5109 - val_regression_loss: 4.7968 - lr: 2.4414e-08\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4407 - regression_loss: 4.1251 - val_loss: 8.5108 - val_regression_loss: 4.7968 - lr: 1.2207e-08\n",
      "Epoch 175/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5567 - regression_loss: 4.1251 - val_loss: 8.5109 - val_regression_loss: 4.7968 - lr: 1.2207e-08\n",
      "Epoch 176/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5003 - regression_loss: 4.1251 - val_loss: 8.5108 - val_regression_loss: 4.7968 - lr: 1.2207e-08\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5560 - regression_loss: 4.1251 - val_loss: 8.5108 - val_regression_loss: 4.7968 - lr: 1.2207e-08\n",
      "Epoch 178/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.6851 - regression_loss: 3.5909\n",
      "Epoch 178: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.4749 - regression_loss: 4.1251 - val_loss: 8.5108 - val_regression_loss: 4.7968 - lr: 1.2207e-08\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4229 - regression_loss: 4.1251 - val_loss: 8.5108 - val_regression_loss: 4.7968 - lr: 6.1035e-09\n",
      "Epoch 180/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5706 - regression_loss: 4.1251 - val_loss: 8.5108 - val_regression_loss: 4.7968 - lr: 6.1035e-09\n",
      "Epoch 181/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4993 - regression_loss: 4.1251 - val_loss: 8.5108 - val_regression_loss: 4.7968 - lr: 6.1035e-09\n",
      "Epoch 182/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4970 - regression_loss: 4.1251 - val_loss: 8.5108 - val_regression_loss: 4.7968 - lr: 6.1035e-09\n",
      "Epoch 183/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.5203 - regression_loss: 5.4262\n",
      "Epoch 183: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4838 - regression_loss: 4.1251 - val_loss: 8.5108 - val_regression_loss: 4.7968 - lr: 6.1035e-09\n",
      "Epoch 184/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5099 - regression_loss: 4.1251 - val_loss: 8.5108 - val_regression_loss: 4.7968 - lr: 3.0518e-09\n",
      "Epoch 185/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4694 - regression_loss: 4.1251 - val_loss: 8.5108 - val_regression_loss: 4.7968 - lr: 3.0518e-09\n",
      "Epoch 186/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5513 - regression_loss: 4.1251 - val_loss: 8.5108 - val_regression_loss: 4.7968 - lr: 3.0518e-09\n",
      "Epoch 187/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4768 - regression_loss: 4.1251 - val_loss: 8.5108 - val_regression_loss: 4.7968 - lr: 3.0518e-09\n",
      "Epoch 188/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.8675 - regression_loss: 4.7733\n",
      "Epoch 188: ReduceLROnPlateau reducing learning rate to 1.5258788677030566e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4928 - regression_loss: 4.1251 - val_loss: 8.5108 - val_regression_loss: 4.7968 - lr: 3.0518e-09\n",
      "Epoch 189/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5120 - regression_loss: 4.1251 - val_loss: 8.5108 - val_regression_loss: 4.7968 - lr: 1.5259e-09\n",
      "Epoch 190/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5769 - regression_loss: 4.1251 - val_loss: 8.5108 - val_regression_loss: 4.7968 - lr: 1.5259e-09\n",
      "Epoch 191/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5661 - regression_loss: 4.1251 - val_loss: 8.5108 - val_regression_loss: 4.7968 - lr: 1.5259e-09\n",
      "Epoch 192/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5685 - regression_loss: 4.1251 - val_loss: 8.5108 - val_regression_loss: 4.7968 - lr: 1.5259e-09\n",
      "Epoch 193/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.4038 - regression_loss: 6.3096\n",
      "Epoch 193: ReduceLROnPlateau reducing learning rate to 7.629394338515283e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4934 - regression_loss: 4.1251 - val_loss: 8.5108 - val_regression_loss: 4.7968 - lr: 1.5259e-09\n",
      "Epoch 194/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5959 - regression_loss: 4.1251 - val_loss: 8.5108 - val_regression_loss: 4.7968 - lr: 7.6294e-10\n",
      "Epoch 195/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.4958 - regression_loss: 4.1251 - val_loss: 8.5108 - val_regression_loss: 4.7968 - lr: 7.6294e-10\n",
      "Epoch 196/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4853 - regression_loss: 4.1251 - val_loss: 8.5108 - val_regression_loss: 4.7968 - lr: 7.6294e-10\n",
      "Epoch 197/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5450 - regression_loss: 4.1251 - val_loss: 8.5108 - val_regression_loss: 4.7968 - lr: 7.6294e-10\n",
      "Epoch 198/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.3615 - regression_loss: 6.2673\n",
      "Epoch 198: ReduceLROnPlateau reducing learning rate to 3.8146971692576415e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4733 - regression_loss: 4.1251 - val_loss: 8.5108 - val_regression_loss: 4.7968 - lr: 7.6294e-10\n",
      "Epoch 199/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4175 - regression_loss: 4.1251 - val_loss: 8.5108 - val_regression_loss: 4.7968 - lr: 3.8147e-10\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 163.6145 - regression_loss: 151.1763 - val_loss: 122.7774 - val_regression_loss: 90.8481 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 126.9190 - regression_loss: 115.7730 - val_loss: 101.6801 - val_regression_loss: 74.8362 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 102.7648 - regression_loss: 94.7014 - val_loss: 84.5003 - val_regression_loss: 61.7094 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 90.1977 - regression_loss: 81.1798 - val_loss: 76.4524 - val_regression_loss: 55.6176 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 78.7101 - regression_loss: 70.8206 - val_loss: 67.6940 - val_regression_loss: 48.8404 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 70.3535 - regression_loss: 63.0699 - val_loss: 62.5719 - val_regression_loss: 44.9793 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 63.3589 - regression_loss: 57.7547 - val_loss: 57.5875 - val_regression_loss: 41.2768 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 58.2219 - regression_loss: 52.8260 - val_loss: 53.6562 - val_regression_loss: 38.3704 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 54.8542 - regression_loss: 49.2604 - val_loss: 50.0667 - val_regression_loss: 35.7312 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 50.7516 - regression_loss: 45.4204 - val_loss: 47.1673 - val_regression_loss: 33.5896 - lr: 1.0000e-04\n",
      "Epoch 11/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 48.0002 - regression_loss: 42.3827 - val_loss: 44.4395 - val_regression_loss: 31.5937 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 43.6155 - regression_loss: 39.1326 - val_loss: 41.7788 - val_regression_loss: 29.5779 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 40.3401 - regression_loss: 36.5198 - val_loss: 39.1195 - val_regression_loss: 27.6718 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 38.9916 - regression_loss: 33.8525 - val_loss: 36.5244 - val_regression_loss: 25.7395 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 34.4867 - regression_loss: 31.1705 - val_loss: 34.1936 - val_regression_loss: 24.0007 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 33.0530 - regression_loss: 29.2794 - val_loss: 31.8940 - val_regression_loss: 22.3298 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.9847 - regression_loss: 27.2857 - val_loss: 29.7266 - val_regression_loss: 20.6712 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.6972 - regression_loss: 24.6580 - val_loss: 27.4978 - val_regression_loss: 19.1210 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.0323 - regression_loss: 22.5476 - val_loss: 25.3498 - val_regression_loss: 17.4339 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.8404 - regression_loss: 20.9143 - val_loss: 23.2946 - val_regression_loss: 15.9838 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.4248 - regression_loss: 19.0293 - val_loss: 21.4989 - val_regression_loss: 14.5851 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8242 - regression_loss: 17.1955 - val_loss: 19.7891 - val_regression_loss: 13.3823 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.9532 - regression_loss: 15.9633 - val_loss: 18.0499 - val_regression_loss: 12.0034 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.7903 - regression_loss: 14.6377 - val_loss: 16.5413 - val_regression_loss: 10.8941 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.2611 - regression_loss: 13.2349 - val_loss: 15.4534 - val_regression_loss: 10.0054 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.3388 - regression_loss: 12.3169 - val_loss: 14.0536 - val_regression_loss: 9.0046 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.4909 - regression_loss: 11.2712 - val_loss: 13.0914 - val_regression_loss: 8.2457 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8313 - regression_loss: 10.4123 - val_loss: 12.1436 - val_regression_loss: 7.5404 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4189 - regression_loss: 9.6864 - val_loss: 11.5195 - val_regression_loss: 7.0260 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9941 - regression_loss: 9.1159 - val_loss: 10.9984 - val_regression_loss: 6.7069 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5620 - regression_loss: 8.6816 - val_loss: 10.2622 - val_regression_loss: 6.0927 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8148 - regression_loss: 8.1635 - val_loss: 9.7982 - val_regression_loss: 5.7754 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3434 - regression_loss: 7.7724 - val_loss: 9.4226 - val_regression_loss: 5.4857 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0970 - regression_loss: 7.4296 - val_loss: 9.1172 - val_regression_loss: 5.2560 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9320 - regression_loss: 7.2235 - val_loss: 9.0095 - val_regression_loss: 5.2116 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8450 - regression_loss: 7.1670 - val_loss: 8.8939 - val_regression_loss: 5.0733 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6338 - regression_loss: 6.9930 - val_loss: 8.8485 - val_regression_loss: 5.1259 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3339 - regression_loss: 6.6620 - val_loss: 8.5129 - val_regression_loss: 4.8006 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5286 - regression_loss: 6.8832 - val_loss: 8.2010 - val_regression_loss: 4.5919 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7056 - regression_loss: 6.3235 - val_loss: 8.2163 - val_regression_loss: 4.6151 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3644 - regression_loss: 6.1459 - val_loss: 8.1988 - val_regression_loss: 4.6299 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6494 - regression_loss: 6.0549 - val_loss: 7.9344 - val_regression_loss: 4.4055 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4938 - regression_loss: 5.9682 - val_loss: 8.0194 - val_regression_loss: 4.4788 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4049 - regression_loss: 5.8413 - val_loss: 7.9039 - val_regression_loss: 4.3794 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2422 - regression_loss: 5.7628 - val_loss: 8.0777 - val_regression_loss: 4.5701 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3576 - regression_loss: 5.8526 - val_loss: 7.7985 - val_regression_loss: 4.3432 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9544 - regression_loss: 5.6107 - val_loss: 7.7889 - val_regression_loss: 4.3247 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1679 - regression_loss: 5.5856 - val_loss: 8.0716 - val_regression_loss: 4.5793 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1922 - regression_loss: 5.6603 - val_loss: 7.6103 - val_regression_loss: 4.2061 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9715 - regression_loss: 5.4214 - val_loss: 7.5854 - val_regression_loss: 4.1834 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8655 - regression_loss: 5.3217 - val_loss: 7.7305 - val_regression_loss: 4.3104 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7248 - regression_loss: 5.2523 - val_loss: 7.4890 - val_regression_loss: 4.1126 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5874 - regression_loss: 5.1996 - val_loss: 7.9535 - val_regression_loss: 4.5147 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6983 - regression_loss: 5.2865 - val_loss: 7.4451 - val_regression_loss: 4.0916 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5412 - regression_loss: 5.0593 - val_loss: 7.5083 - val_regression_loss: 4.1552 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.3414 - regression_loss: 4.9598 - val_loss: 7.3414 - val_regression_loss: 4.0269 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4383 - regression_loss: 5.1019 - val_loss: 7.4908 - val_regression_loss: 4.1539 - lr: 1.0000e-04\n",
      "Epoch 58/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 7.3353 - regression_loss: 5.0664 - val_loss: 7.7813 - val_regression_loss: 4.4020 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4363 - regression_loss: 4.9608 - val_loss: 7.4480 - val_regression_loss: 4.1316 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5331 - regression_loss: 5.0810 - val_loss: 7.7158 - val_regression_loss: 4.3465 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0949 - regression_loss: 4.8444 - val_loss: 7.1584 - val_regression_loss: 3.9053 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0233 - regression_loss: 4.9544 - val_loss: 7.4759 - val_regression_loss: 4.1659 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5312 - regression_loss: 5.0591 - val_loss: 7.3887 - val_regression_loss: 4.0969 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9984 - regression_loss: 4.6621 - val_loss: 7.2794 - val_regression_loss: 4.0053 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8089 - regression_loss: 4.5087 - val_loss: 7.0500 - val_regression_loss: 3.8289 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9093 - regression_loss: 4.5101 - val_loss: 7.3334 - val_regression_loss: 4.0650 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9609 - regression_loss: 4.5346 - val_loss: 7.2338 - val_regression_loss: 3.9882 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9078 - regression_loss: 4.4533 - val_loss: 7.0621 - val_regression_loss: 3.8516 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6432 - regression_loss: 4.2672 - val_loss: 6.9080 - val_regression_loss: 3.7414 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5847 - regression_loss: 4.2529 - val_loss: 7.0450 - val_regression_loss: 3.8417 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5070 - regression_loss: 4.2008 - val_loss: 6.8104 - val_regression_loss: 3.6704 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5384 - regression_loss: 4.1574 - val_loss: 7.1230 - val_regression_loss: 3.9204 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5590 - regression_loss: 4.1552 - val_loss: 6.7469 - val_regression_loss: 3.6273 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3790 - regression_loss: 4.0583 - val_loss: 6.9624 - val_regression_loss: 3.7889 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3294 - regression_loss: 4.1698 - val_loss: 7.1366 - val_regression_loss: 3.9338 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4521 - regression_loss: 4.0843 - val_loss: 6.6611 - val_regression_loss: 3.5791 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4999 - regression_loss: 4.0993 - val_loss: 6.6510 - val_regression_loss: 3.5561 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2717 - regression_loss: 3.9421 - val_loss: 6.9745 - val_regression_loss: 3.8024 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2194 - regression_loss: 3.8777 - val_loss: 6.6123 - val_regression_loss: 3.5385 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.0942 - regression_loss: 3.7859 - val_loss: 7.0710 - val_regression_loss: 3.8948 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3111 - regression_loss: 3.9752 - val_loss: 6.4860 - val_regression_loss: 3.4436 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.0638 - regression_loss: 3.7866 - val_loss: 6.5651 - val_regression_loss: 3.5126 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.0477 - regression_loss: 3.7444 - val_loss: 7.2481 - val_regression_loss: 4.0450 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3521 - regression_loss: 3.9934 - val_loss: 6.5192 - val_regression_loss: 3.4688 - lr: 1.0000e-04\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9667 - regression_loss: 3.6200 - val_loss: 6.6709 - val_regression_loss: 3.5948 - lr: 1.0000e-04\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9301 - regression_loss: 3.6254 - val_loss: 6.4757 - val_regression_loss: 3.4503 - lr: 1.0000e-04\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.9404 - regression_loss: 3.6100 - val_loss: 6.5299 - val_regression_loss: 3.4966 - lr: 1.0000e-04\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.0446 - regression_loss: 3.7161 - val_loss: 6.5460 - val_regression_loss: 3.4936 - lr: 1.0000e-04\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8608 - regression_loss: 3.5028 - val_loss: 6.3623 - val_regression_loss: 3.3678 - lr: 1.0000e-04\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8202 - regression_loss: 3.5011 - val_loss: 6.4055 - val_regression_loss: 3.4162 - lr: 1.0000e-04\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9989 - regression_loss: 3.6608 - val_loss: 6.2845 - val_regression_loss: 3.3143 - lr: 1.0000e-04\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9047 - regression_loss: 3.7074 - val_loss: 6.6360 - val_regression_loss: 3.5660 - lr: 1.0000e-04\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8451 - regression_loss: 3.5306 - val_loss: 6.6789 - val_regression_loss: 3.6090 - lr: 1.0000e-04\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7641 - regression_loss: 3.4858 - val_loss: 6.3009 - val_regression_loss: 3.3111 - lr: 1.0000e-04\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5999 - regression_loss: 3.3143 - val_loss: 6.2478 - val_regression_loss: 3.2748 - lr: 1.0000e-04\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5387 - regression_loss: 3.2972 - val_loss: 6.4826 - val_regression_loss: 3.4686 - lr: 1.0000e-04\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7044 - regression_loss: 3.3862 - val_loss: 6.2166 - val_regression_loss: 3.2562 - lr: 1.0000e-04\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.5842 - regression_loss: 3.2997 - val_loss: 6.1948 - val_regression_loss: 3.2556 - lr: 1.0000e-04\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5553 - regression_loss: 3.2790 - val_loss: 6.7616 - val_regression_loss: 3.6873 - lr: 1.0000e-04\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9450 - regression_loss: 3.6131 - val_loss: 6.5946 - val_regression_loss: 3.5554 - lr: 1.0000e-04\n",
      "Epoch 101/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.1816 - regression_loss: 4.1313\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9091 - regression_loss: 3.6510 - val_loss: 6.4505 - val_regression_loss: 3.5024 - lr: 1.0000e-04\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8485 - regression_loss: 3.5821 - val_loss: 6.7568 - val_regression_loss: 3.6785 - lr: 5.0000e-05\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5140 - regression_loss: 3.2076 - val_loss: 6.2417 - val_regression_loss: 3.3202 - lr: 5.0000e-05\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5942 - regression_loss: 3.3122 - val_loss: 6.8100 - val_regression_loss: 3.7281 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6778 - regression_loss: 3.3672 - val_loss: 6.1341 - val_regression_loss: 3.2380 - lr: 5.0000e-05\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5241 - regression_loss: 3.2101 - val_loss: 6.3990 - val_regression_loss: 3.4095 - lr: 5.0000e-05\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.4195 - regression_loss: 3.2216 - val_loss: 6.0177 - val_regression_loss: 3.1324 - lr: 5.0000e-05\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.3312 - regression_loss: 3.0729 - val_loss: 6.2279 - val_regression_loss: 3.2825 - lr: 5.0000e-05\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.4849 - regression_loss: 3.1853 - val_loss: 6.1240 - val_regression_loss: 3.2315 - lr: 5.0000e-05\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.4972 - regression_loss: 3.2358 - val_loss: 6.4515 - val_regression_loss: 3.4579 - lr: 5.0000e-05\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.4680 - regression_loss: 3.1649 - val_loss: 6.0194 - val_regression_loss: 3.1453 - lr: 5.0000e-05\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.4188 - regression_loss: 3.1565 - val_loss: 6.2139 - val_regression_loss: 3.2674 - lr: 5.0000e-05\n",
      "Epoch 113/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.4388 - regression_loss: 2.3919\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.3433 - regression_loss: 3.0857 - val_loss: 5.9904 - val_regression_loss: 3.1122 - lr: 5.0000e-05\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.3365 - regression_loss: 3.0395 - val_loss: 6.0655 - val_regression_loss: 3.1608 - lr: 2.5000e-05\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1797 - regression_loss: 3.0045 - val_loss: 6.1197 - val_regression_loss: 3.2014 - lr: 2.5000e-05\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2215 - regression_loss: 2.9938 - val_loss: 6.0115 - val_regression_loss: 3.1237 - lr: 2.5000e-05\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.3334 - regression_loss: 3.0011 - val_loss: 5.9699 - val_regression_loss: 3.0943 - lr: 2.5000e-05\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2267 - regression_loss: 2.9907 - val_loss: 6.0330 - val_regression_loss: 3.1388 - lr: 2.5000e-05\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2104 - regression_loss: 2.9897 - val_loss: 6.0042 - val_regression_loss: 3.1188 - lr: 2.5000e-05\n",
      "Epoch 120/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.3278 - regression_loss: 3.2819\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.2466 - regression_loss: 2.9834 - val_loss: 6.0796 - val_regression_loss: 3.1725 - lr: 2.5000e-05\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2075 - regression_loss: 2.9820 - val_loss: 6.0759 - val_regression_loss: 3.1690 - lr: 1.2500e-05\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2360 - regression_loss: 2.9763 - val_loss: 5.9938 - val_regression_loss: 3.1114 - lr: 1.2500e-05\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.2338 - regression_loss: 2.9634 - val_loss: 5.9754 - val_regression_loss: 3.0994 - lr: 1.2500e-05\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.1457 - regression_loss: 2.9604 - val_loss: 5.9908 - val_regression_loss: 3.1099 - lr: 1.2500e-05\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.2141 - regression_loss: 2.9593 - val_loss: 6.0178 - val_regression_loss: 3.1287 - lr: 1.2500e-05\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2028 - regression_loss: 2.9562 - val_loss: 6.0193 - val_regression_loss: 3.1311 - lr: 1.2500e-05\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2334 - regression_loss: 2.9569 - val_loss: 6.0027 - val_regression_loss: 3.1199 - lr: 1.2500e-05\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2446 - regression_loss: 2.9549 - val_loss: 5.9965 - val_regression_loss: 3.1158 - lr: 1.2500e-05\n",
      "Epoch 129/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.2983 - regression_loss: 3.2531\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2405 - regression_loss: 2.9510 - val_loss: 6.0026 - val_regression_loss: 3.1193 - lr: 1.2500e-05\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1845 - regression_loss: 2.9437 - val_loss: 6.0076 - val_regression_loss: 3.1229 - lr: 6.2500e-06\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2427 - regression_loss: 2.9568 - val_loss: 6.0302 - val_regression_loss: 3.1394 - lr: 6.2500e-06\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2622 - regression_loss: 2.9435 - val_loss: 6.0002 - val_regression_loss: 3.1179 - lr: 6.2500e-06\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2100 - regression_loss: 2.9431 - val_loss: 5.9809 - val_regression_loss: 3.1044 - lr: 6.2500e-06\n",
      "Epoch 134/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.7598 - regression_loss: 3.7148\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2180 - regression_loss: 2.9404 - val_loss: 5.9881 - val_regression_loss: 3.1095 - lr: 6.2500e-06\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2122 - regression_loss: 2.9414 - val_loss: 5.9822 - val_regression_loss: 3.1054 - lr: 3.1250e-06\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1838 - regression_loss: 2.9430 - val_loss: 6.0013 - val_regression_loss: 3.1188 - lr: 3.1250e-06\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1395 - regression_loss: 2.9384 - val_loss: 6.0078 - val_regression_loss: 3.1233 - lr: 3.1250e-06\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1285 - regression_loss: 2.9381 - val_loss: 5.9928 - val_regression_loss: 3.1129 - lr: 3.1250e-06\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1477 - regression_loss: 2.9356 - val_loss: 5.9960 - val_regression_loss: 3.1152 - lr: 3.1250e-06\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1883 - regression_loss: 2.9350 - val_loss: 5.9939 - val_regression_loss: 3.1139 - lr: 3.1250e-06\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1997 - regression_loss: 2.9341 - val_loss: 5.9921 - val_regression_loss: 3.1126 - lr: 3.1250e-06\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2282 - regression_loss: 2.9345 - val_loss: 5.9949 - val_regression_loss: 3.1144 - lr: 3.1250e-06\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.1247 - regression_loss: 2.9332 - val_loss: 5.9923 - val_regression_loss: 3.1127 - lr: 3.1250e-06\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2361 - regression_loss: 2.9327 - val_loss: 5.9946 - val_regression_loss: 3.1143 - lr: 3.1250e-06\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2052 - regression_loss: 2.9362 - val_loss: 5.9858 - val_regression_loss: 3.1083 - lr: 3.1250e-06\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1543 - regression_loss: 2.9313 - val_loss: 5.9879 - val_regression_loss: 3.1097 - lr: 3.1250e-06\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0970 - regression_loss: 2.9312 - val_loss: 5.9895 - val_regression_loss: 3.1106 - lr: 3.1250e-06\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1931 - regression_loss: 2.9307 - val_loss: 5.9942 - val_regression_loss: 3.1140 - lr: 3.1250e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1615 - regression_loss: 2.9314 - val_loss: 6.0003 - val_regression_loss: 3.1184 - lr: 3.1250e-06\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1665 - regression_loss: 2.9308 - val_loss: 5.9959 - val_regression_loss: 3.1154 - lr: 3.1250e-06\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1182 - regression_loss: 2.9290 - val_loss: 5.9974 - val_regression_loss: 3.1164 - lr: 3.1250e-06\n",
      "Epoch 152/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.9723 - regression_loss: 2.9277\n",
      "Epoch 152: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2280 - regression_loss: 2.9282 - val_loss: 5.9918 - val_regression_loss: 3.1124 - lr: 3.1250e-06\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2133 - regression_loss: 2.9281 - val_loss: 5.9880 - val_regression_loss: 3.1099 - lr: 1.5625e-06\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1328 - regression_loss: 2.9269 - val_loss: 5.9893 - val_regression_loss: 3.1108 - lr: 1.5625e-06\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2119 - regression_loss: 2.9269 - val_loss: 5.9901 - val_regression_loss: 3.1115 - lr: 1.5625e-06\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2009 - regression_loss: 2.9264 - val_loss: 5.9901 - val_regression_loss: 3.1113 - lr: 1.5625e-06\n",
      "Epoch 157/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.0488 - regression_loss: 3.0042\n",
      "Epoch 157: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2082 - regression_loss: 2.9261 - val_loss: 5.9899 - val_regression_loss: 3.1113 - lr: 1.5625e-06\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 26ms/step - loss: 141.5306 - regression_loss: 128.8195 - val_loss: 97.8354 - val_regression_loss: 74.7527 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 98.0772 - regression_loss: 89.1877 - val_loss: 74.9290 - val_regression_loss: 57.0007 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 72.9151 - regression_loss: 65.7198 - val_loss: 56.8503 - val_regression_loss: 42.9738 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 56.5763 - regression_loss: 50.8302 - val_loss: 44.8465 - val_regression_loss: 33.7596 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 43.0321 - regression_loss: 38.8981 - val_loss: 36.7870 - val_regression_loss: 27.5109 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 35.1373 - regression_loss: 31.5802 - val_loss: 30.8890 - val_regression_loss: 22.9646 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30.3412 - regression_loss: 26.2784 - val_loss: 26.6020 - val_regression_loss: 19.5974 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 24.8837 - regression_loss: 21.9864 - val_loss: 23.2128 - val_regression_loss: 16.9606 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.3926 - regression_loss: 18.9257 - val_loss: 20.6161 - val_regression_loss: 14.7769 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.0470 - regression_loss: 16.5349 - val_loss: 18.5256 - val_regression_loss: 13.1496 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.2031 - regression_loss: 14.2566 - val_loss: 16.4722 - val_regression_loss: 11.3751 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.0777 - regression_loss: 12.1949 - val_loss: 15.0686 - val_regression_loss: 10.2286 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.9266 - regression_loss: 10.8542 - val_loss: 13.9331 - val_regression_loss: 9.1703 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.0252 - regression_loss: 9.5048 - val_loss: 13.1931 - val_regression_loss: 8.6572 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.1840 - regression_loss: 8.7044 - val_loss: 12.2926 - val_regression_loss: 7.7463 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.4777 - regression_loss: 7.7839 - val_loss: 12.1881 - val_regression_loss: 7.7951 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4236 - regression_loss: 7.0291 - val_loss: 11.0371 - val_regression_loss: 6.7086 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.0179 - regression_loss: 6.5314 - val_loss: 10.9134 - val_regression_loss: 6.7492 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2293 - regression_loss: 5.9183 - val_loss: 10.0191 - val_regression_loss: 5.9159 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7695 - regression_loss: 5.4201 - val_loss: 9.9340 - val_regression_loss: 5.8861 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.4666 - regression_loss: 5.0822 - val_loss: 9.5547 - val_regression_loss: 5.5072 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.4579 - regression_loss: 5.0054 - val_loss: 9.7311 - val_regression_loss: 5.7206 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.8532 - regression_loss: 4.6970 - val_loss: 9.1569 - val_regression_loss: 5.1970 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.7313 - regression_loss: 4.5094 - val_loss: 9.4236 - val_regression_loss: 5.4724 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.6304 - regression_loss: 4.3888 - val_loss: 8.7870 - val_regression_loss: 4.9022 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7556 - regression_loss: 4.3658 - val_loss: 9.0970 - val_regression_loss: 5.2192 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.4163 - regression_loss: 4.1856 - val_loss: 8.6079 - val_regression_loss: 4.8143 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3747 - regression_loss: 4.0061 - val_loss: 8.5486 - val_regression_loss: 4.7759 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.1210 - regression_loss: 3.9164 - val_loss: 8.3649 - val_regression_loss: 4.6078 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3343 - regression_loss: 3.9107 - val_loss: 8.2268 - val_regression_loss: 4.4945 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.0505 - regression_loss: 3.7736 - val_loss: 8.3815 - val_regression_loss: 4.6701 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.9746 - regression_loss: 3.6608 - val_loss: 7.9624 - val_regression_loss: 4.3299 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.8884 - regression_loss: 3.6851 - val_loss: 7.7949 - val_regression_loss: 4.2124 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.9156 - regression_loss: 3.5540 - val_loss: 8.0017 - val_regression_loss: 4.4057 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7059 - regression_loss: 3.4542 - val_loss: 7.6304 - val_regression_loss: 4.0921 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6406 - regression_loss: 3.3287 - val_loss: 7.7744 - val_regression_loss: 4.2229 - lr: 1.0000e-04\n",
      "Epoch 37/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6706 - regression_loss: 3.3640 - val_loss: 7.4931 - val_regression_loss: 3.9746 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6043 - regression_loss: 3.3253 - val_loss: 7.6128 - val_regression_loss: 4.1188 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.4848 - regression_loss: 3.1893 - val_loss: 7.4016 - val_regression_loss: 3.9525 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.4007 - regression_loss: 3.1296 - val_loss: 7.2451 - val_regression_loss: 3.8090 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.4480 - regression_loss: 3.2302 - val_loss: 7.1537 - val_regression_loss: 3.7721 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5.2627 - regression_loss: 2.9674 - val_loss: 7.0623 - val_regression_loss: 3.6861 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.3704 - regression_loss: 3.1526 - val_loss: 7.3175 - val_regression_loss: 3.9323 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.4101 - regression_loss: 3.1537 - val_loss: 7.4298 - val_regression_loss: 4.0412 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.2482 - regression_loss: 2.9555 - val_loss: 7.0372 - val_regression_loss: 3.6789 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.0832 - regression_loss: 2.9327 - val_loss: 7.7910 - val_regression_loss: 4.3645 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.4335 - regression_loss: 3.1959 - val_loss: 6.7658 - val_regression_loss: 3.5073 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.0691 - regression_loss: 2.8307 - val_loss: 6.7471 - val_regression_loss: 3.4731 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9257 - regression_loss: 2.6917 - val_loss: 6.9951 - val_regression_loss: 3.7130 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9832 - regression_loss: 2.7364 - val_loss: 6.5260 - val_regression_loss: 3.3508 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8608 - regression_loss: 2.6134 - val_loss: 6.5035 - val_regression_loss: 3.3170 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9118 - regression_loss: 2.7576 - val_loss: 7.2416 - val_regression_loss: 3.9398 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2929 - regression_loss: 3.0242 - val_loss: 6.5461 - val_regression_loss: 3.3429 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.7168 - regression_loss: 2.5729 - val_loss: 6.4028 - val_regression_loss: 3.2403 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7185 - regression_loss: 2.5285 - val_loss: 6.3390 - val_regression_loss: 3.2028 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.7021 - regression_loss: 2.4458 - val_loss: 6.4383 - val_regression_loss: 3.2891 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7064 - regression_loss: 2.4679 - val_loss: 6.4721 - val_regression_loss: 3.3500 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6314 - regression_loss: 2.4160 - val_loss: 6.1792 - val_regression_loss: 3.0907 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.5978 - regression_loss: 2.4388 - val_loss: 6.6803 - val_regression_loss: 3.5302 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6347 - regression_loss: 2.4353 - val_loss: 6.1424 - val_regression_loss: 3.0628 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.5044 - regression_loss: 2.3032 - val_loss: 6.1150 - val_regression_loss: 3.0505 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.4562 - regression_loss: 2.2828 - val_loss: 6.3065 - val_regression_loss: 3.2277 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.4433 - regression_loss: 2.2824 - val_loss: 6.0778 - val_regression_loss: 3.0157 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.4243 - regression_loss: 2.2090 - val_loss: 6.1167 - val_regression_loss: 3.0746 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3589 - regression_loss: 2.1977 - val_loss: 5.9133 - val_regression_loss: 2.9138 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.4009 - regression_loss: 2.2087 - val_loss: 6.4157 - val_regression_loss: 3.3223 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.4484 - regression_loss: 2.3270 - val_loss: 5.9456 - val_regression_loss: 2.9346 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3830 - regression_loss: 2.1966 - val_loss: 6.0073 - val_regression_loss: 2.9684 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.4402 - regression_loss: 2.2841 - val_loss: 6.0222 - val_regression_loss: 3.0274 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.2854 - regression_loss: 2.1266 - val_loss: 5.8090 - val_regression_loss: 2.8333 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.2615 - regression_loss: 2.1245 - val_loss: 5.8804 - val_regression_loss: 2.8680 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2070 - regression_loss: 2.0615 - val_loss: 5.8106 - val_regression_loss: 2.8493 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.1838 - regression_loss: 2.0481 - val_loss: 5.6762 - val_regression_loss: 2.7505 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.2154 - regression_loss: 2.0285 - val_loss: 5.7052 - val_regression_loss: 2.7582 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0918 - regression_loss: 1.9960 - val_loss: 5.9324 - val_regression_loss: 2.9444 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0635 - regression_loss: 2.0066 - val_loss: 5.6230 - val_regression_loss: 2.7028 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.1697 - regression_loss: 2.0303 - val_loss: 5.6246 - val_regression_loss: 2.6987 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.1600 - regression_loss: 1.9734 - val_loss: 5.7439 - val_regression_loss: 2.8043 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.1494 - regression_loss: 1.9982 - val_loss: 5.6579 - val_regression_loss: 2.7562 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0864 - regression_loss: 1.9203 - val_loss: 5.5538 - val_regression_loss: 2.6449 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0140 - regression_loss: 1.8847 - val_loss: 5.6238 - val_regression_loss: 2.7267 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0577 - regression_loss: 1.8750 - val_loss: 5.4540 - val_regression_loss: 2.5785 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9880 - regression_loss: 1.8554 - val_loss: 5.5131 - val_regression_loss: 2.6129 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9976 - regression_loss: 1.8808 - val_loss: 5.4989 - val_regression_loss: 2.6146 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0821 - regression_loss: 1.9411 - val_loss: 5.4143 - val_regression_loss: 2.5598 - lr: 1.0000e-04\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0263 - regression_loss: 1.8693 - val_loss: 5.6357 - val_regression_loss: 2.7524 - lr: 1.0000e-04\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0863 - regression_loss: 1.9624 - val_loss: 5.4433 - val_regression_loss: 2.5651 - lr: 1.0000e-04\n",
      "Epoch 88/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.7947 - regression_loss: 1.7866\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0896 - regression_loss: 1.9571 - val_loss: 5.4583 - val_regression_loss: 2.5801 - lr: 1.0000e-04\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0546 - regression_loss: 1.9156 - val_loss: 5.4496 - val_regression_loss: 2.6066 - lr: 5.0000e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.8619 - regression_loss: 1.7548 - val_loss: 5.4354 - val_regression_loss: 2.5618 - lr: 5.0000e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9551 - regression_loss: 1.8142 - val_loss: 5.4847 - val_regression_loss: 2.6187 - lr: 5.0000e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.8609 - regression_loss: 1.7256 - val_loss: 5.4072 - val_regression_loss: 2.5352 - lr: 5.0000e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7799 - regression_loss: 1.7593 - val_loss: 5.3682 - val_regression_loss: 2.5291 - lr: 5.0000e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.8910 - regression_loss: 1.7430 - val_loss: 5.2611 - val_regression_loss: 2.4415 - lr: 5.0000e-05\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8041 - regression_loss: 1.7319 - val_loss: 5.3129 - val_regression_loss: 2.4786 - lr: 5.0000e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.8437 - regression_loss: 1.7297 - val_loss: 5.2885 - val_regression_loss: 2.4525 - lr: 5.0000e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7961 - regression_loss: 1.7002 - val_loss: 5.2679 - val_regression_loss: 2.4453 - lr: 5.0000e-05\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7329 - regression_loss: 1.6832 - val_loss: 5.2147 - val_regression_loss: 2.4074 - lr: 5.0000e-05\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7671 - regression_loss: 1.6866 - val_loss: 5.2471 - val_regression_loss: 2.4245 - lr: 5.0000e-05\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.8166 - regression_loss: 1.6738 - val_loss: 5.2677 - val_regression_loss: 2.4421 - lr: 5.0000e-05\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7526 - regression_loss: 1.6676 - val_loss: 5.2041 - val_regression_loss: 2.3948 - lr: 5.0000e-05\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7502 - regression_loss: 1.6657 - val_loss: 5.2075 - val_regression_loss: 2.3983 - lr: 5.0000e-05\n",
      "Epoch 103/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.9699 - regression_loss: 1.9664\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.8056 - regression_loss: 1.7030 - val_loss: 5.2569 - val_regression_loss: 2.4210 - lr: 5.0000e-05\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.8321 - regression_loss: 1.6790 - val_loss: 5.2171 - val_regression_loss: 2.4091 - lr: 2.5000e-05\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8126 - regression_loss: 1.6632 - val_loss: 5.1754 - val_regression_loss: 2.3777 - lr: 2.5000e-05\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7617 - regression_loss: 1.6318 - val_loss: 5.1775 - val_regression_loss: 2.3740 - lr: 2.5000e-05\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7599 - regression_loss: 1.6347 - val_loss: 5.1876 - val_regression_loss: 2.3826 - lr: 2.5000e-05\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7298 - regression_loss: 1.6277 - val_loss: 5.1775 - val_regression_loss: 2.3778 - lr: 2.5000e-05\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7450 - regression_loss: 1.6245 - val_loss: 5.1648 - val_regression_loss: 2.3651 - lr: 2.5000e-05\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7296 - regression_loss: 1.6207 - val_loss: 5.1585 - val_regression_loss: 2.3617 - lr: 2.5000e-05\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6972 - regression_loss: 1.6253 - val_loss: 5.1548 - val_regression_loss: 2.3567 - lr: 2.5000e-05\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7375 - regression_loss: 1.6185 - val_loss: 5.1545 - val_regression_loss: 2.3572 - lr: 2.5000e-05\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7322 - regression_loss: 1.6123 - val_loss: 5.1501 - val_regression_loss: 2.3544 - lr: 2.5000e-05\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7470 - regression_loss: 1.6299 - val_loss: 5.1705 - val_regression_loss: 2.3661 - lr: 2.5000e-05\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7069 - regression_loss: 1.6113 - val_loss: 5.1681 - val_regression_loss: 2.3709 - lr: 2.5000e-05\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6878 - regression_loss: 1.6115 - val_loss: 5.1267 - val_regression_loss: 2.3342 - lr: 2.5000e-05\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7374 - regression_loss: 1.6338 - val_loss: 5.1078 - val_regression_loss: 2.3317 - lr: 2.5000e-05\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7120 - regression_loss: 1.6010 - val_loss: 5.1153 - val_regression_loss: 2.3279 - lr: 2.5000e-05\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7264 - regression_loss: 1.6017 - val_loss: 5.1216 - val_regression_loss: 2.3351 - lr: 2.5000e-05\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7172 - regression_loss: 1.5917 - val_loss: 5.1038 - val_regression_loss: 2.3197 - lr: 2.5000e-05\n",
      "Epoch 121/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.1576 - regression_loss: 2.1570\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7205 - regression_loss: 1.5906 - val_loss: 5.1017 - val_regression_loss: 2.3185 - lr: 2.5000e-05\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7080 - regression_loss: 1.5829 - val_loss: 5.1075 - val_regression_loss: 2.3219 - lr: 1.2500e-05\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6817 - regression_loss: 1.5872 - val_loss: 5.1094 - val_regression_loss: 2.3251 - lr: 1.2500e-05\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6992 - regression_loss: 1.5809 - val_loss: 5.1097 - val_regression_loss: 2.3238 - lr: 1.2500e-05\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7281 - regression_loss: 1.5876 - val_loss: 5.0994 - val_regression_loss: 2.3178 - lr: 1.2500e-05\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7172 - regression_loss: 1.5785 - val_loss: 5.0972 - val_regression_loss: 2.3130 - lr: 1.2500e-05\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7073 - regression_loss: 1.5757 - val_loss: 5.0910 - val_regression_loss: 2.3108 - lr: 1.2500e-05\n",
      "Epoch 128/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.5767 - regression_loss: 2.5767\n",
      "Epoch 128: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7099 - regression_loss: 1.5839 - val_loss: 5.0853 - val_regression_loss: 2.3109 - lr: 1.2500e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6629 - regression_loss: 1.5746 - val_loss: 5.0787 - val_regression_loss: 2.3018 - lr: 6.2500e-06\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7041 - regression_loss: 1.5714 - val_loss: 5.0795 - val_regression_loss: 2.3019 - lr: 6.2500e-06\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6651 - regression_loss: 1.5700 - val_loss: 5.0824 - val_regression_loss: 2.3033 - lr: 6.2500e-06\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6783 - regression_loss: 1.5704 - val_loss: 5.0873 - val_regression_loss: 2.3085 - lr: 6.2500e-06\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6732 - regression_loss: 1.5694 - val_loss: 5.0899 - val_regression_loss: 2.3091 - lr: 6.2500e-06\n",
      "Epoch 134/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.6626 - regression_loss: 1.6629\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6746 - regression_loss: 1.5674 - val_loss: 5.0889 - val_regression_loss: 2.3079 - lr: 6.2500e-06\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6645 - regression_loss: 1.5664 - val_loss: 5.0859 - val_regression_loss: 2.3058 - lr: 3.1250e-06\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6870 - regression_loss: 1.5663 - val_loss: 5.0816 - val_regression_loss: 2.3033 - lr: 3.1250e-06\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6911 - regression_loss: 1.5662 - val_loss: 5.0809 - val_regression_loss: 2.3025 - lr: 3.1250e-06\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6533 - regression_loss: 1.5650 - val_loss: 5.0809 - val_regression_loss: 2.3023 - lr: 3.1250e-06\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6742 - regression_loss: 1.5650 - val_loss: 5.0819 - val_regression_loss: 2.3028 - lr: 3.1250e-06\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7049 - regression_loss: 1.5647 - val_loss: 5.0813 - val_regression_loss: 2.3031 - lr: 3.1250e-06\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6666 - regression_loss: 1.5646 - val_loss: 5.0809 - val_regression_loss: 2.3030 - lr: 3.1250e-06\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6821 - regression_loss: 1.5637 - val_loss: 5.0801 - val_regression_loss: 2.3019 - lr: 3.1250e-06\n",
      "Epoch 143/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.3320 - regression_loss: 1.3325\n",
      "Epoch 143: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7086 - regression_loss: 1.5647 - val_loss: 5.0774 - val_regression_loss: 2.2992 - lr: 3.1250e-06\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6822 - regression_loss: 1.5625 - val_loss: 5.0765 - val_regression_loss: 2.2990 - lr: 1.5625e-06\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6704 - regression_loss: 1.5622 - val_loss: 5.0769 - val_regression_loss: 2.2994 - lr: 1.5625e-06\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6440 - regression_loss: 1.5618 - val_loss: 5.0766 - val_regression_loss: 2.2994 - lr: 1.5625e-06\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6826 - regression_loss: 1.5624 - val_loss: 5.0758 - val_regression_loss: 2.2987 - lr: 1.5625e-06\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6845 - regression_loss: 1.5616 - val_loss: 5.0757 - val_regression_loss: 2.2988 - lr: 1.5625e-06\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6633 - regression_loss: 1.5612 - val_loss: 5.0756 - val_regression_loss: 2.2990 - lr: 1.5625e-06\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6549 - regression_loss: 1.5616 - val_loss: 5.0754 - val_regression_loss: 2.2993 - lr: 1.5625e-06\n",
      "Epoch 151/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.5744 - regression_loss: 1.5750\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6981 - regression_loss: 1.5615 - val_loss: 5.0754 - val_regression_loss: 2.2990 - lr: 1.5625e-06\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6772 - regression_loss: 1.5607 - val_loss: 5.0749 - val_regression_loss: 2.2985 - lr: 7.8125e-07\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7195 - regression_loss: 1.5609 - val_loss: 5.0749 - val_regression_loss: 2.2982 - lr: 7.8125e-07\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6556 - regression_loss: 1.5604 - val_loss: 5.0743 - val_regression_loss: 2.2978 - lr: 7.8125e-07\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6949 - regression_loss: 1.5603 - val_loss: 5.0743 - val_regression_loss: 2.2979 - lr: 7.8125e-07\n",
      "Epoch 156/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.7527 - regression_loss: 1.7533\n",
      "Epoch 156: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6539 - regression_loss: 1.5603 - val_loss: 5.0743 - val_regression_loss: 2.2980 - lr: 7.8125e-07\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6926 - regression_loss: 1.5602 - val_loss: 5.0741 - val_regression_loss: 2.2978 - lr: 3.9062e-07\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6327 - regression_loss: 1.5600 - val_loss: 5.0741 - val_regression_loss: 2.2979 - lr: 3.9062e-07\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6734 - regression_loss: 1.5600 - val_loss: 5.0736 - val_regression_loss: 2.2977 - lr: 3.9062e-07\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6929 - regression_loss: 1.5599 - val_loss: 5.0736 - val_regression_loss: 2.2976 - lr: 3.9062e-07\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6795 - regression_loss: 1.5603 - val_loss: 5.0734 - val_regression_loss: 2.2973 - lr: 3.9062e-07\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6876 - regression_loss: 1.5598 - val_loss: 5.0731 - val_regression_loss: 2.2971 - lr: 3.9062e-07\n",
      "Epoch 163/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.8398 - regression_loss: 1.8405\n",
      "Epoch 163: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6799 - regression_loss: 1.5598 - val_loss: 5.0730 - val_regression_loss: 2.2971 - lr: 3.9062e-07\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6384 - regression_loss: 1.5596 - val_loss: 5.0730 - val_regression_loss: 2.2970 - lr: 1.9531e-07\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6812 - regression_loss: 1.5595 - val_loss: 5.0729 - val_regression_loss: 2.2969 - lr: 1.9531e-07\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6741 - regression_loss: 1.5596 - val_loss: 5.0728 - val_regression_loss: 2.2969 - lr: 1.9531e-07\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6633 - regression_loss: 1.5596 - val_loss: 5.0727 - val_regression_loss: 2.2967 - lr: 1.9531e-07\n",
      "Epoch 168/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.0991 - regression_loss: 2.0998\n",
      "Epoch 168: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6593 - regression_loss: 1.5595 - val_loss: 5.0727 - val_regression_loss: 2.2967 - lr: 1.9531e-07\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5916 - regression_loss: 1.5594 - val_loss: 5.0727 - val_regression_loss: 2.2967 - lr: 9.7656e-08\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6255 - regression_loss: 1.5594 - val_loss: 5.0727 - val_regression_loss: 2.2967 - lr: 9.7656e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6647 - regression_loss: 1.5595 - val_loss: 5.0726 - val_regression_loss: 2.2966 - lr: 9.7656e-08\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6416 - regression_loss: 1.5594 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 9.7656e-08\n",
      "Epoch 173/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6434 - regression_loss: 1.5594 - val_loss: 5.0726 - val_regression_loss: 2.2966 - lr: 9.7656e-08\n",
      "Epoch 174/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.5090 - regression_loss: 1.5097\n",
      "Epoch 174: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6464 - regression_loss: 1.5594 - val_loss: 5.0726 - val_regression_loss: 2.2966 - lr: 9.7656e-08\n",
      "Epoch 175/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6800 - regression_loss: 1.5593 - val_loss: 5.0726 - val_regression_loss: 2.2966 - lr: 4.8828e-08\n",
      "Epoch 176/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6783 - regression_loss: 1.5594 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 4.8828e-08\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6217 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 4.8828e-08\n",
      "Epoch 178/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6782 - regression_loss: 1.5594 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 4.8828e-08\n",
      "Epoch 179/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.2636 - regression_loss: 2.2643\n",
      "Epoch 179: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6841 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 4.8828e-08\n",
      "Epoch 180/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6860 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 2.4414e-08\n",
      "Epoch 181/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6693 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 2.4414e-08\n",
      "Epoch 182/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7011 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 2.4414e-08\n",
      "Epoch 183/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6618 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 2.4414e-08\n",
      "Epoch 184/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.6766 - regression_loss: 1.6773\n",
      "Epoch 184: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6514 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 2.4414e-08\n",
      "Epoch 185/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7111 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 1.2207e-08\n",
      "Epoch 186/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6403 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 1.2207e-08\n",
      "Epoch 187/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6602 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 1.2207e-08\n",
      "Epoch 188/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6199 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 1.2207e-08\n",
      "Epoch 189/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.2986 - regression_loss: 1.2993\n",
      "Epoch 189: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6399 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 1.2207e-08\n",
      "Epoch 190/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6496 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 6.1035e-09\n",
      "Epoch 191/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6691 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 6.1035e-09\n",
      "Epoch 192/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6833 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 6.1035e-09\n",
      "Epoch 193/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6428 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 6.1035e-09\n",
      "Epoch 194/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.0636 - regression_loss: 2.0643\n",
      "Epoch 194: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6786 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 6.1035e-09\n",
      "Epoch 195/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6775 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 3.0518e-09\n",
      "Epoch 196/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6784 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 3.0518e-09\n",
      "Epoch 197/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6653 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 3.0518e-09\n",
      "Epoch 198/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6596 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 3.0518e-09\n",
      "Epoch 199/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.8513 - regression_loss: 1.8520\n",
      "Epoch 199: ReduceLROnPlateau reducing learning rate to 1.5258788677030566e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6680 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 3.0518e-09\n",
      "Epoch 200/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6441 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 1.5259e-09\n",
      "Epoch 201/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6908 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 1.5259e-09\n",
      "Epoch 202/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6916 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 1.5259e-09\n",
      "Epoch 203/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6869 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 1.5259e-09\n",
      "Epoch 204/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.5325 - regression_loss: 1.5332\n",
      "Epoch 204: ReduceLROnPlateau reducing learning rate to 7.629394338515283e-10.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6765 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 1.5259e-09\n",
      "Epoch 205/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6899 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 7.6294e-10\n",
      "Epoch 206/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6909 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 7.6294e-10\n",
      "Epoch 207/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6224 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 7.6294e-10\n",
      "Epoch 208/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6316 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 7.6294e-10\n",
      "Epoch 209/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.3909 - regression_loss: 1.3916\n",
      "Epoch 209: ReduceLROnPlateau reducing learning rate to 3.8146971692576415e-10.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6764 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 7.6294e-10\n",
      "Epoch 210/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7001 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 3.8147e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 211/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6066 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 3.8147e-10\n",
      "Epoch 212/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6591 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 3.8147e-10\n",
      "Epoch 213/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6450 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 3.8147e-10\n",
      "Epoch 214/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.5708 - regression_loss: 1.5715\n",
      "Epoch 214: ReduceLROnPlateau reducing learning rate to 1.9073485846288207e-10.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6550 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 3.8147e-10\n",
      "Epoch 215/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6473 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 1.9073e-10\n",
      "Epoch 216/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6739 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 1.9073e-10\n",
      "Epoch 217/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6433 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 1.9073e-10\n",
      "Epoch 218/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6650 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 1.9073e-10\n",
      "Epoch 219/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.1075 - regression_loss: 2.1082\n",
      "Epoch 219: ReduceLROnPlateau reducing learning rate to 9.536742923144104e-11.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6833 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 1.9073e-10\n",
      "Epoch 220/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6628 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 9.5367e-11\n",
      "Epoch 221/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6501 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 9.5367e-11\n",
      "Epoch 222/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6677 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 9.5367e-11\n",
      "Epoch 223/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6676 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 9.5367e-11\n",
      "Epoch 224/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.3662 - regression_loss: 2.3669\n",
      "Epoch 224: ReduceLROnPlateau reducing learning rate to 4.768371461572052e-11.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6792 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 9.5367e-11\n",
      "Epoch 225/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6519 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 4.7684e-11\n",
      "Epoch 226/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6748 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 4.7684e-11\n",
      "Epoch 227/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6656 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 4.7684e-11\n",
      "Epoch 228/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6573 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 4.7684e-11\n",
      "Epoch 229/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.9917 - regression_loss: 1.9924\n",
      "Epoch 229: ReduceLROnPlateau reducing learning rate to 2.384185730786026e-11.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6914 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 4.7684e-11\n",
      "Epoch 230/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6725 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 2.3842e-11\n",
      "Epoch 231/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6749 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 2.3842e-11\n",
      "Epoch 232/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6961 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 2.3842e-11\n",
      "Epoch 233/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6635 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 2.3842e-11\n",
      "Epoch 234/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.3961 - regression_loss: 2.3968\n",
      "Epoch 234: ReduceLROnPlateau reducing learning rate to 1.192092865393013e-11.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6053 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 2.3842e-11\n",
      "Epoch 235/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6732 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 1.1921e-11\n",
      "Epoch 236/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6805 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 1.1921e-11\n",
      "Epoch 237/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6243 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 1.1921e-11\n",
      "Epoch 238/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6731 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 1.1921e-11\n",
      "Epoch 239/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.1938 - regression_loss: 1.1945\n",
      "Epoch 239: ReduceLROnPlateau reducing learning rate to 5.960464326965065e-12.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6961 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 1.1921e-11\n",
      "Epoch 240/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6559 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 5.9605e-12\n",
      "Epoch 241/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6850 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 5.9605e-12\n",
      "Epoch 242/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6506 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 5.9605e-12\n",
      "Epoch 243/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6685 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 5.9605e-12\n",
      "Epoch 244/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.8728 - regression_loss: 1.8735\n",
      "Epoch 244: ReduceLROnPlateau reducing learning rate to 2.9802321634825324e-12.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6719 - regression_loss: 1.5593 - val_loss: 5.0725 - val_regression_loss: 2.2966 - lr: 5.9605e-12\n",
      "3/3 [==============================] - 0s 970us/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 26ms/step - loss: 142.7553 - regression_loss: 132.1014 - val_loss: 94.7476 - val_regression_loss: 74.3645 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 94.1770 - regression_loss: 85.8510 - val_loss: 76.1736 - val_regression_loss: 57.9924 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 74.0131 - regression_loss: 67.7625 - val_loss: 55.8349 - val_regression_loss: 42.8448 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 64.5169 - regression_loss: 58.4487 - val_loss: 46.0320 - val_regression_loss: 35.3174 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 56.7918 - regression_loss: 50.9729 - val_loss: 40.9248 - val_regression_loss: 31.1254 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 50.6436 - regression_loss: 45.6119 - val_loss: 35.5295 - val_regression_loss: 27.1342 - lr: 1.0000e-04\n",
      "Epoch 7/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 47.9368 - regression_loss: 41.9857 - val_loss: 31.6119 - val_regression_loss: 24.2692 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 42.1552 - regression_loss: 38.1734 - val_loss: 30.6836 - val_regression_loss: 23.2477 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 40.2199 - regression_loss: 35.8311 - val_loss: 28.1125 - val_regression_loss: 21.2582 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 37.9779 - regression_loss: 33.6342 - val_loss: 26.7355 - val_regression_loss: 20.0595 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 36.2385 - regression_loss: 31.4689 - val_loss: 25.2755 - val_regression_loss: 18.9583 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 33.9797 - regression_loss: 29.6996 - val_loss: 24.9877 - val_regression_loss: 18.5958 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30.8735 - regression_loss: 28.1355 - val_loss: 23.5617 - val_regression_loss: 17.5357 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30.7642 - regression_loss: 26.6797 - val_loss: 23.1695 - val_regression_loss: 17.1235 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.5277 - regression_loss: 25.6266 - val_loss: 22.2611 - val_regression_loss: 16.4140 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.5776 - regression_loss: 24.1502 - val_loss: 21.3024 - val_regression_loss: 15.7356 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.0752 - regression_loss: 23.2459 - val_loss: 21.7023 - val_regression_loss: 15.8996 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.7103 - regression_loss: 22.2359 - val_loss: 20.4109 - val_regression_loss: 14.9371 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.7435 - regression_loss: 21.2902 - val_loss: 20.0016 - val_regression_loss: 14.5485 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.1757 - regression_loss: 20.4932 - val_loss: 20.2523 - val_regression_loss: 14.6614 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.1515 - regression_loss: 19.7868 - val_loss: 19.3058 - val_regression_loss: 13.9453 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.1900 - regression_loss: 19.0461 - val_loss: 19.0127 - val_regression_loss: 13.7052 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.3232 - regression_loss: 18.3113 - val_loss: 18.7840 - val_regression_loss: 13.4722 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.8030 - regression_loss: 17.7679 - val_loss: 18.4736 - val_regression_loss: 13.2004 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.1749 - regression_loss: 17.2142 - val_loss: 18.2594 - val_regression_loss: 13.0022 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.2898 - regression_loss: 16.9068 - val_loss: 17.8108 - val_regression_loss: 12.6338 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.6137 - regression_loss: 16.3763 - val_loss: 17.7499 - val_regression_loss: 12.5721 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.7458 - regression_loss: 16.3760 - val_loss: 17.3945 - val_regression_loss: 12.2646 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.3450 - regression_loss: 16.1202 - val_loss: 17.4321 - val_regression_loss: 12.2975 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.9733 - regression_loss: 15.7163 - val_loss: 16.9623 - val_regression_loss: 11.8705 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.8357 - regression_loss: 14.8191 - val_loss: 16.7898 - val_regression_loss: 11.7320 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.1432 - regression_loss: 14.4054 - val_loss: 16.7770 - val_regression_loss: 11.7347 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.1636 - regression_loss: 14.1210 - val_loss: 16.5508 - val_regression_loss: 11.5036 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.9375 - regression_loss: 13.8661 - val_loss: 16.6840 - val_regression_loss: 11.6256 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.7680 - regression_loss: 13.7703 - val_loss: 16.2823 - val_regression_loss: 11.2494 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.7156 - regression_loss: 13.6536 - val_loss: 16.5699 - val_regression_loss: 11.5628 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.2484 - regression_loss: 13.3375 - val_loss: 16.0983 - val_regression_loss: 11.0796 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.3485 - regression_loss: 13.3124 - val_loss: 15.9403 - val_regression_loss: 11.0013 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.9451 - regression_loss: 12.8330 - val_loss: 15.7573 - val_regression_loss: 10.8643 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.7894 - regression_loss: 12.6331 - val_loss: 15.7163 - val_regression_loss: 10.7910 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.3690 - regression_loss: 12.4523 - val_loss: 15.6823 - val_regression_loss: 10.7831 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.3111 - regression_loss: 12.3239 - val_loss: 15.5272 - val_regression_loss: 10.6334 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.2132 - regression_loss: 12.2687 - val_loss: 15.5567 - val_regression_loss: 10.7138 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.9571 - regression_loss: 12.0482 - val_loss: 15.3700 - val_regression_loss: 10.5361 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.6801 - regression_loss: 11.8708 - val_loss: 15.2039 - val_regression_loss: 10.4051 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.5392 - regression_loss: 11.7277 - val_loss: 15.1379 - val_regression_loss: 10.3051 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.4677 - regression_loss: 11.6568 - val_loss: 15.2765 - val_regression_loss: 10.5404 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.2402 - regression_loss: 11.5794 - val_loss: 15.0292 - val_regression_loss: 10.1775 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.3358 - regression_loss: 11.5978 - val_loss: 15.3184 - val_regression_loss: 10.5884 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.1861 - regression_loss: 11.4261 - val_loss: 14.9061 - val_regression_loss: 10.0810 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.9311 - regression_loss: 11.3406 - val_loss: 14.8508 - val_regression_loss: 10.1327 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.9929 - regression_loss: 11.0599 - val_loss: 14.5359 - val_regression_loss: 9.8233 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7088 - regression_loss: 11.1469 - val_loss: 14.6594 - val_regression_loss: 10.0176 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7402 - regression_loss: 10.9615 - val_loss: 14.5273 - val_regression_loss: 9.7948 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6962 - regression_loss: 10.8015 - val_loss: 14.9241 - val_regression_loss: 10.2909 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.9347 - regression_loss: 11.1353 - val_loss: 14.9373 - val_regression_loss: 10.0403 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.9690 - regression_loss: 11.0933 - val_loss: 14.8931 - val_regression_loss: 10.2994 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.5241 - regression_loss: 10.8118 - val_loss: 14.4491 - val_regression_loss: 9.6883 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6462 - regression_loss: 10.7788 - val_loss: 14.8056 - val_regression_loss: 10.2353 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6024 - regression_loss: 10.7092 - val_loss: 14.4655 - val_regression_loss: 9.6964 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.2708 - regression_loss: 10.5700 - val_loss: 14.7719 - val_regression_loss: 10.1977 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.5098 - regression_loss: 10.6788 - val_loss: 14.4148 - val_regression_loss: 9.6397 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.2464 - regression_loss: 10.6446 - val_loss: 14.3067 - val_regression_loss: 9.7958 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.3561 - regression_loss: 10.6264 - val_loss: 14.2150 - val_regression_loss: 9.4790 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.3827 - regression_loss: 10.6481 - val_loss: 14.2201 - val_regression_loss: 9.7119 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 12.9158 - regression_loss: 10.3169 - val_loss: 14.1032 - val_regression_loss: 9.4084 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.1184 - regression_loss: 10.5618 - val_loss: 14.4651 - val_regression_loss: 9.9823 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.9386 - regression_loss: 10.4725 - val_loss: 14.1229 - val_regression_loss: 9.4162 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.5646 - regression_loss: 10.7566 - val_loss: 14.9634 - val_regression_loss: 10.4796 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6975 - regression_loss: 10.7506 - val_loss: 14.4823 - val_regression_loss: 9.6391 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 14.6571 - regression_loss: 12.6696\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.4946 - regression_loss: 10.7072 - val_loss: 14.6369 - val_regression_loss: 10.1661 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9244 - regression_loss: 10.4394 - val_loss: 13.5691 - val_regression_loss: 9.0748 - lr: 5.0000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0244 - regression_loss: 10.1966 - val_loss: 13.5443 - val_regression_loss: 9.0773 - lr: 5.0000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6561 - regression_loss: 9.9209 - val_loss: 13.8798 - val_regression_loss: 9.4626 - lr: 5.0000e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6646 - regression_loss: 9.9159 - val_loss: 13.6755 - val_regression_loss: 9.1470 - lr: 5.0000e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.5902 - regression_loss: 9.9133 - val_loss: 13.6975 - val_regression_loss: 9.2236 - lr: 5.0000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.4832 - regression_loss: 9.8668 - val_loss: 13.6214 - val_regression_loss: 9.1754 - lr: 5.0000e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.3755 - regression_loss: 9.7988 - val_loss: 13.5379 - val_regression_loss: 9.0546 - lr: 5.0000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5055 - regression_loss: 9.7864 - val_loss: 13.5653 - val_regression_loss: 9.1215 - lr: 5.0000e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2457 - regression_loss: 9.7731 - val_loss: 13.6082 - val_regression_loss: 9.1819 - lr: 5.0000e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5698 - regression_loss: 9.7958 - val_loss: 13.4850 - val_regression_loss: 9.0146 - lr: 5.0000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5668 - regression_loss: 9.8124 - val_loss: 13.4844 - val_regression_loss: 9.0552 - lr: 5.0000e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4275 - regression_loss: 9.7566 - val_loss: 13.5061 - val_regression_loss: 9.0759 - lr: 5.0000e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3923 - regression_loss: 9.7188 - val_loss: 13.4859 - val_regression_loss: 9.0115 - lr: 5.0000e-05\n",
      "Epoch 85/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.8168 - regression_loss: 9.8307\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4350 - regression_loss: 9.7099 - val_loss: 13.4898 - val_regression_loss: 9.0535 - lr: 5.0000e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3115 - regression_loss: 9.6731 - val_loss: 13.4816 - val_regression_loss: 9.0700 - lr: 2.5000e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2291 - regression_loss: 9.6450 - val_loss: 13.4358 - val_regression_loss: 9.0016 - lr: 2.5000e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3147 - regression_loss: 9.6259 - val_loss: 13.4465 - val_regression_loss: 9.0140 - lr: 2.5000e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1982 - regression_loss: 9.6530 - val_loss: 13.5016 - val_regression_loss: 9.0826 - lr: 2.5000e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.0970 - regression_loss: 9.6358 - val_loss: 13.4322 - val_regression_loss: 8.9825 - lr: 2.5000e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2426 - regression_loss: 9.6049 - val_loss: 13.4243 - val_regression_loss: 8.9993 - lr: 2.5000e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1391 - regression_loss: 9.5935 - val_loss: 13.4066 - val_regression_loss: 9.0022 - lr: 2.5000e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2811 - regression_loss: 9.5900 - val_loss: 13.3784 - val_regression_loss: 8.9653 - lr: 2.5000e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4365 - regression_loss: 9.5804 - val_loss: 13.3822 - val_regression_loss: 8.9560 - lr: 2.5000e-05\n",
      "Epoch 95/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.8974 - regression_loss: 7.9119\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1556 - regression_loss: 9.5673 - val_loss: 13.3860 - val_regression_loss: 8.9708 - lr: 2.5000e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2134 - regression_loss: 9.5551 - val_loss: 13.4105 - val_regression_loss: 9.0013 - lr: 1.2500e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3633 - regression_loss: 9.5549 - val_loss: 13.3768 - val_regression_loss: 8.9570 - lr: 1.2500e-05\n",
      "Epoch 98/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0771 - regression_loss: 9.5450 - val_loss: 13.3876 - val_regression_loss: 8.9709 - lr: 1.2500e-05\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3583 - regression_loss: 9.5387 - val_loss: 13.3648 - val_regression_loss: 8.9506 - lr: 1.2500e-05\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2072 - regression_loss: 9.5405 - val_loss: 13.3589 - val_regression_loss: 8.9406 - lr: 1.2500e-05\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0893 - regression_loss: 9.5349 - val_loss: 13.3576 - val_regression_loss: 8.9447 - lr: 1.2500e-05\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1055 - regression_loss: 9.5246 - val_loss: 13.3670 - val_regression_loss: 8.9621 - lr: 1.2500e-05\n",
      "Epoch 103/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.3937 - regression_loss: 11.4084\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1375 - regression_loss: 9.5370 - val_loss: 13.3831 - val_regression_loss: 8.9801 - lr: 1.2500e-05\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1943 - regression_loss: 9.5126 - val_loss: 13.3648 - val_regression_loss: 8.9558 - lr: 6.2500e-06\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2569 - regression_loss: 9.5146 - val_loss: 13.3434 - val_regression_loss: 8.9270 - lr: 6.2500e-06\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9999 - regression_loss: 9.5093 - val_loss: 13.3440 - val_regression_loss: 8.9265 - lr: 6.2500e-06\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2751 - regression_loss: 9.5053 - val_loss: 13.3422 - val_regression_loss: 8.9299 - lr: 6.2500e-06\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1522 - regression_loss: 9.5070 - val_loss: 13.3422 - val_regression_loss: 8.9323 - lr: 6.2500e-06\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9546 - regression_loss: 9.5016 - val_loss: 13.3443 - val_regression_loss: 8.9379 - lr: 6.2500e-06\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0442 - regression_loss: 9.4991 - val_loss: 13.3499 - val_regression_loss: 8.9497 - lr: 6.2500e-06\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1325 - regression_loss: 9.4985 - val_loss: 13.3419 - val_regression_loss: 8.9434 - lr: 6.2500e-06\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.2666 - regression_loss: 9.4985 - val_loss: 13.3437 - val_regression_loss: 8.9443 - lr: 6.2500e-06\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2448 - regression_loss: 9.4963 - val_loss: 13.3350 - val_regression_loss: 8.9308 - lr: 6.2500e-06\n",
      "Epoch 114/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 14.9440 - regression_loss: 12.9588\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0054 - regression_loss: 9.4951 - val_loss: 13.3401 - val_regression_loss: 8.9364 - lr: 6.2500e-06\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0562 - regression_loss: 9.4893 - val_loss: 13.3419 - val_regression_loss: 8.9407 - lr: 3.1250e-06\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0012 - regression_loss: 9.4864 - val_loss: 13.3336 - val_regression_loss: 8.9309 - lr: 3.1250e-06\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1730 - regression_loss: 9.4842 - val_loss: 13.3309 - val_regression_loss: 8.9268 - lr: 3.1250e-06\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.1932 - regression_loss: 9.4849 - val_loss: 13.3290 - val_regression_loss: 8.9218 - lr: 3.1250e-06\n",
      "Epoch 119/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.4938 - regression_loss: 11.5087\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.1221 - regression_loss: 9.4855 - val_loss: 13.3262 - val_regression_loss: 8.9190 - lr: 3.1250e-06\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.0433 - regression_loss: 9.4808 - val_loss: 13.3255 - val_regression_loss: 8.9189 - lr: 1.5625e-06\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.0480 - regression_loss: 9.4810 - val_loss: 13.3292 - val_regression_loss: 8.9246 - lr: 1.5625e-06\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.0805 - regression_loss: 9.4794 - val_loss: 13.3279 - val_regression_loss: 8.9229 - lr: 1.5625e-06\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.2129 - regression_loss: 9.4792 - val_loss: 13.3293 - val_regression_loss: 8.9255 - lr: 1.5625e-06\n",
      "Epoch 124/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.4974 - regression_loss: 11.5123\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.1441 - regression_loss: 9.4816 - val_loss: 13.3250 - val_regression_loss: 8.9202 - lr: 1.5625e-06\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.0472 - regression_loss: 9.4771 - val_loss: 13.3262 - val_regression_loss: 8.9220 - lr: 7.8125e-07\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.9423 - regression_loss: 9.4777 - val_loss: 13.3277 - val_regression_loss: 8.9246 - lr: 7.8125e-07\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.1307 - regression_loss: 9.4768 - val_loss: 13.3267 - val_regression_loss: 8.9225 - lr: 7.8125e-07\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.1825 - regression_loss: 9.4762 - val_loss: 13.3257 - val_regression_loss: 8.9212 - lr: 7.8125e-07\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0232 - regression_loss: 9.4761 - val_loss: 13.3259 - val_regression_loss: 8.9218 - lr: 7.8125e-07\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.0365 - regression_loss: 9.4761 - val_loss: 13.3243 - val_regression_loss: 8.9201 - lr: 7.8125e-07\n",
      "Epoch 131/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.3310 - regression_loss: 10.3459\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0802 - regression_loss: 9.4755 - val_loss: 13.3259 - val_regression_loss: 8.9227 - lr: 7.8125e-07\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0111 - regression_loss: 9.4746 - val_loss: 13.3260 - val_regression_loss: 8.9232 - lr: 3.9062e-07\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.0335 - regression_loss: 9.4749 - val_loss: 13.3256 - val_regression_loss: 8.9224 - lr: 3.9062e-07\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1290 - regression_loss: 9.4744 - val_loss: 13.3255 - val_regression_loss: 8.9222 - lr: 3.9062e-07\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1543 - regression_loss: 9.4741 - val_loss: 13.3252 - val_regression_loss: 8.9220 - lr: 3.9062e-07\n",
      "Epoch 136/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.1520 - regression_loss: 11.1669\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1846 - regression_loss: 9.4740 - val_loss: 13.3256 - val_regression_loss: 8.9228 - lr: 3.9062e-07\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0969 - regression_loss: 9.4737 - val_loss: 13.3255 - val_regression_loss: 8.9225 - lr: 1.9531e-07\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2338 - regression_loss: 9.4736 - val_loss: 13.3254 - val_regression_loss: 8.9226 - lr: 1.9531e-07\n",
      "Epoch 139/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1685 - regression_loss: 9.4736 - val_loss: 13.3253 - val_regression_loss: 8.9222 - lr: 1.9531e-07\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0099 - regression_loss: 9.4735 - val_loss: 13.3250 - val_regression_loss: 8.9221 - lr: 1.9531e-07\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.9277 - regression_loss: 9.4735 - val_loss: 13.3252 - val_regression_loss: 8.9224 - lr: 1.9531e-07\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2332 - regression_loss: 9.4734 - val_loss: 13.3251 - val_regression_loss: 8.9224 - lr: 1.9531e-07\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.2442 - regression_loss: 9.4733 - val_loss: 13.3252 - val_regression_loss: 8.9225 - lr: 1.9531e-07\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.9998 - regression_loss: 9.4732 - val_loss: 13.3249 - val_regression_loss: 8.9221 - lr: 1.9531e-07\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.1424 - regression_loss: 9.4731 - val_loss: 13.3248 - val_regression_loss: 8.9221 - lr: 1.9531e-07\n",
      "Epoch 146/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.9650 - regression_loss: 10.9800\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.2008 - regression_loss: 9.4730 - val_loss: 13.3247 - val_regression_loss: 8.9220 - lr: 1.9531e-07\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.9804 - regression_loss: 9.4729 - val_loss: 13.3247 - val_regression_loss: 8.9220 - lr: 9.7656e-08\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2314 - regression_loss: 9.4729 - val_loss: 13.3248 - val_regression_loss: 8.9222 - lr: 9.7656e-08\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9990 - regression_loss: 9.4728 - val_loss: 13.3247 - val_regression_loss: 8.9220 - lr: 9.7656e-08\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1082 - regression_loss: 9.4728 - val_loss: 13.3246 - val_regression_loss: 8.9219 - lr: 9.7656e-08\n",
      "Epoch 151/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.7039 - regression_loss: 9.7189\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1309 - regression_loss: 9.4728 - val_loss: 13.3245 - val_regression_loss: 8.9217 - lr: 9.7656e-08\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1700 - regression_loss: 9.4728 - val_loss: 13.3246 - val_regression_loss: 8.9219 - lr: 4.8828e-08\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1527 - regression_loss: 9.4727 - val_loss: 13.3245 - val_regression_loss: 8.9217 - lr: 4.8828e-08\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2934 - regression_loss: 9.4727 - val_loss: 13.3245 - val_regression_loss: 8.9217 - lr: 4.8828e-08\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8544 - regression_loss: 9.4727 - val_loss: 13.3245 - val_regression_loss: 8.9218 - lr: 4.8828e-08\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1027 - regression_loss: 9.4726 - val_loss: 13.3245 - val_regression_loss: 8.9218 - lr: 4.8828e-08\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1309 - regression_loss: 9.4726 - val_loss: 13.3245 - val_regression_loss: 8.9218 - lr: 4.8828e-08\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1280 - regression_loss: 9.4726 - val_loss: 13.3244 - val_regression_loss: 8.9216 - lr: 4.8828e-08\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0364 - regression_loss: 9.4726 - val_loss: 13.3245 - val_regression_loss: 8.9218 - lr: 4.8828e-08\n",
      "Epoch 160/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.7407 - regression_loss: 10.7557\n",
      "Epoch 160: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1479 - regression_loss: 9.4725 - val_loss: 13.3244 - val_regression_loss: 8.9217 - lr: 4.8828e-08\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0849 - regression_loss: 9.4725 - val_loss: 13.3243 - val_regression_loss: 8.9216 - lr: 2.4414e-08\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1228 - regression_loss: 9.4725 - val_loss: 13.3243 - val_regression_loss: 8.9216 - lr: 2.4414e-08\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9366 - regression_loss: 9.4725 - val_loss: 13.3244 - val_regression_loss: 8.9217 - lr: 2.4414e-08\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0300 - regression_loss: 9.4724 - val_loss: 13.3244 - val_regression_loss: 8.9217 - lr: 2.4414e-08\n",
      "Epoch 165/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.9772 - regression_loss: 9.9922\n",
      "Epoch 165: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.0992 - regression_loss: 9.4725 - val_loss: 13.3243 - val_regression_loss: 8.9216 - lr: 2.4414e-08\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.2116 - regression_loss: 9.4724 - val_loss: 13.3243 - val_regression_loss: 8.9216 - lr: 1.2207e-08\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8516 - regression_loss: 9.4724 - val_loss: 13.3243 - val_regression_loss: 8.9216 - lr: 1.2207e-08\n",
      "Epoch 168/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2093 - regression_loss: 9.4724 - val_loss: 13.3243 - val_regression_loss: 8.9216 - lr: 1.2207e-08\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2647 - regression_loss: 9.4724 - val_loss: 13.3243 - val_regression_loss: 8.9216 - lr: 1.2207e-08\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0233 - regression_loss: 9.4724 - val_loss: 13.3243 - val_regression_loss: 8.9216 - lr: 1.2207e-08\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 130.3009 - regression_loss: 118.2837 - val_loss: 67.7771 - val_regression_loss: 56.9154 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 89.0232 - regression_loss: 80.1481 - val_loss: 45.6957 - val_regression_loss: 37.9481 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 63.4728 - regression_loss: 56.6695 - val_loss: 31.6543 - val_regression_loss: 26.0701 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 47.6659 - regression_loss: 42.1600 - val_loss: 26.3536 - val_regression_loss: 20.8680 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 36.8195 - regression_loss: 32.3603 - val_loss: 22.0283 - val_regression_loss: 17.0541 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.2001 - regression_loss: 26.7971 - val_loss: 19.0076 - val_regression_loss: 14.4791 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.6624 - regression_loss: 22.6131 - val_loss: 19.2731 - val_regression_loss: 14.5964 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.8216 - regression_loss: 20.2447 - val_loss: 15.8641 - val_regression_loss: 11.7614 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.2889 - regression_loss: 18.6625 - val_loss: 14.6702 - val_regression_loss: 10.7563 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.8264 - regression_loss: 16.7713 - val_loss: 15.2983 - val_regression_loss: 11.2339 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.0790 - regression_loss: 14.9862 - val_loss: 12.0888 - val_regression_loss: 8.5493 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.4728 - regression_loss: 13.3403 - val_loss: 12.6534 - val_regression_loss: 8.9811 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.1157 - regression_loss: 12.0552 - val_loss: 10.5108 - val_regression_loss: 7.2020 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7934 - regression_loss: 11.0053 - val_loss: 10.3581 - val_regression_loss: 7.0589 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8201 - regression_loss: 10.0705 - val_loss: 9.6390 - val_regression_loss: 6.4635 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8144 - regression_loss: 9.1604 - val_loss: 8.5579 - val_regression_loss: 5.5716 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1743 - regression_loss: 8.5820 - val_loss: 8.7782 - val_regression_loss: 5.7448 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.4473 - regression_loss: 7.8259 - val_loss: 7.7546 - val_regression_loss: 4.8973 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6775 - regression_loss: 7.2886 - val_loss: 7.1702 - val_regression_loss: 4.4108 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2989 - regression_loss: 6.6733 - val_loss: 7.1608 - val_regression_loss: 4.4108 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.6701 - regression_loss: 6.1389 - val_loss: 6.6054 - val_regression_loss: 3.9488 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8030 - regression_loss: 5.7804 - val_loss: 6.6655 - val_regression_loss: 4.0139 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8147 - regression_loss: 5.4116 - val_loss: 6.0503 - val_regression_loss: 3.5024 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4574 - regression_loss: 5.0896 - val_loss: 5.7276 - val_regression_loss: 3.2269 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.2073 - regression_loss: 4.7971 - val_loss: 5.7897 - val_regression_loss: 3.2698 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9555 - regression_loss: 4.5530 - val_loss: 5.7597 - val_regression_loss: 3.2409 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3391 - regression_loss: 4.4087 - val_loss: 5.1591 - val_regression_loss: 2.7598 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5276 - regression_loss: 4.2218 - val_loss: 5.3697 - val_regression_loss: 2.9160 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.1592 - regression_loss: 3.9633 - val_loss: 5.2554 - val_regression_loss: 2.8098 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.1462 - regression_loss: 3.8363 - val_loss: 4.9003 - val_regression_loss: 2.5118 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9046 - regression_loss: 3.6446 - val_loss: 5.6328 - val_regression_loss: 3.1071 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9641 - regression_loss: 3.6903 - val_loss: 4.7848 - val_regression_loss: 2.4126 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6137 - regression_loss: 3.3680 - val_loss: 4.7676 - val_regression_loss: 2.3935 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5076 - regression_loss: 3.2488 - val_loss: 4.8685 - val_regression_loss: 2.4632 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.4571 - regression_loss: 3.1856 - val_loss: 4.6069 - val_regression_loss: 2.2337 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2669 - regression_loss: 3.0489 - val_loss: 4.9037 - val_regression_loss: 2.4804 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1866 - regression_loss: 2.9816 - val_loss: 4.4787 - val_regression_loss: 2.1321 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1085 - regression_loss: 2.9229 - val_loss: 4.7247 - val_regression_loss: 2.3182 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1465 - regression_loss: 2.9390 - val_loss: 4.8596 - val_regression_loss: 2.4253 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9678 - regression_loss: 2.8148 - val_loss: 4.4005 - val_regression_loss: 2.0516 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7880 - regression_loss: 2.6496 - val_loss: 4.5258 - val_regression_loss: 2.1379 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7542 - regression_loss: 2.5720 - val_loss: 4.3588 - val_regression_loss: 2.0036 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6815 - regression_loss: 2.5501 - val_loss: 4.3685 - val_regression_loss: 2.0065 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6631 - regression_loss: 2.4866 - val_loss: 4.5839 - val_regression_loss: 2.1749 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6574 - regression_loss: 2.4852 - val_loss: 4.2991 - val_regression_loss: 1.9472 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5735 - regression_loss: 2.4164 - val_loss: 4.7753 - val_regression_loss: 2.3207 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5480 - regression_loss: 2.4327 - val_loss: 4.2398 - val_regression_loss: 1.8931 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.4835 - regression_loss: 2.3384 - val_loss: 4.3482 - val_regression_loss: 1.9775 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.4471 - regression_loss: 2.2932 - val_loss: 4.3032 - val_regression_loss: 1.9345 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3507 - regression_loss: 2.2143 - val_loss: 4.1903 - val_regression_loss: 1.8435 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3373 - regression_loss: 2.1853 - val_loss: 4.5990 - val_regression_loss: 2.1682 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3622 - regression_loss: 2.3628 - val_loss: 4.1512 - val_regression_loss: 1.8081 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3590 - regression_loss: 2.2069 - val_loss: 4.1261 - val_regression_loss: 1.7853 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2805 - regression_loss: 2.1287 - val_loss: 4.3392 - val_regression_loss: 1.9495 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.1978 - regression_loss: 2.0769 - val_loss: 4.0837 - val_regression_loss: 1.7563 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2715 - regression_loss: 2.1155 - val_loss: 4.0847 - val_regression_loss: 1.7478 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2496 - regression_loss: 2.1322 - val_loss: 4.6121 - val_regression_loss: 2.1681 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2855 - regression_loss: 2.2841 - val_loss: 4.0923 - val_regression_loss: 1.7593 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2647 - regression_loss: 2.1059 - val_loss: 4.1228 - val_regression_loss: 1.7774 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.8939 - regression_loss: 1.9274\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2546 - regression_loss: 2.1493 - val_loss: 4.6886 - val_regression_loss: 2.2321 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.1829 - regression_loss: 2.0587 - val_loss: 4.1372 - val_regression_loss: 1.8003 - lr: 5.0000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2206 - regression_loss: 2.1191 - val_loss: 4.4272 - val_regression_loss: 2.0154 - lr: 5.0000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.1071 - regression_loss: 2.0011 - val_loss: 4.0286 - val_regression_loss: 1.7097 - lr: 5.0000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2087 - regression_loss: 2.0588 - val_loss: 4.2262 - val_regression_loss: 1.8559 - lr: 5.0000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9990 - regression_loss: 1.9494 - val_loss: 3.9644 - val_regression_loss: 1.6486 - lr: 5.0000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0165 - regression_loss: 1.9487 - val_loss: 4.0678 - val_regression_loss: 1.7280 - lr: 5.0000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9623 - regression_loss: 1.8606 - val_loss: 3.9654 - val_regression_loss: 1.6472 - lr: 5.0000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9630 - regression_loss: 1.8487 - val_loss: 3.9456 - val_regression_loss: 1.6293 - lr: 5.0000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9512 - regression_loss: 1.8439 - val_loss: 3.9858 - val_regression_loss: 1.6610 - lr: 5.0000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8749 - regression_loss: 1.8284 - val_loss: 3.9550 - val_regression_loss: 1.6363 - lr: 5.0000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9310 - regression_loss: 1.8438 - val_loss: 4.0017 - val_regression_loss: 1.6740 - lr: 5.0000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9164 - regression_loss: 1.8226 - val_loss: 3.9667 - val_regression_loss: 1.6439 - lr: 5.0000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9144 - regression_loss: 1.8088 - val_loss: 3.9296 - val_regression_loss: 1.6156 - lr: 5.0000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8903 - regression_loss: 1.7903 - val_loss: 3.9474 - val_regression_loss: 1.6300 - lr: 5.0000e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8514 - regression_loss: 1.7822 - val_loss: 3.9304 - val_regression_loss: 1.6174 - lr: 5.0000e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8808 - regression_loss: 1.7785 - val_loss: 3.9357 - val_regression_loss: 1.6189 - lr: 5.0000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8538 - regression_loss: 1.7644 - val_loss: 3.9075 - val_regression_loss: 1.5971 - lr: 5.0000e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8684 - regression_loss: 1.7536 - val_loss: 3.9218 - val_regression_loss: 1.6077 - lr: 5.0000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8355 - regression_loss: 1.7478 - val_loss: 3.9182 - val_regression_loss: 1.6040 - lr: 5.0000e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8227 - regression_loss: 1.7414 - val_loss: 3.8673 - val_regression_loss: 1.5662 - lr: 5.0000e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8252 - regression_loss: 1.7420 - val_loss: 3.9493 - val_regression_loss: 1.6289 - lr: 5.0000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8445 - regression_loss: 1.7401 - val_loss: 3.8554 - val_regression_loss: 1.5592 - lr: 5.0000e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8062 - regression_loss: 1.7345 - val_loss: 3.9651 - val_regression_loss: 1.6409 - lr: 5.0000e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8355 - regression_loss: 1.7246 - val_loss: 3.8410 - val_regression_loss: 1.5453 - lr: 5.0000e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7999 - regression_loss: 1.7248 - val_loss: 3.9599 - val_regression_loss: 1.6391 - lr: 5.0000e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8743 - regression_loss: 1.7737 - val_loss: 3.8401 - val_regression_loss: 1.5478 - lr: 5.0000e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8479 - regression_loss: 1.7504 - val_loss: 4.0079 - val_regression_loss: 1.6741 - lr: 5.0000e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8297 - regression_loss: 1.7672 - val_loss: 3.8202 - val_regression_loss: 1.5291 - lr: 5.0000e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7613 - regression_loss: 1.7302 - val_loss: 3.9568 - val_regression_loss: 1.6361 - lr: 5.0000e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7896 - regression_loss: 1.6931 - val_loss: 3.8063 - val_regression_loss: 1.5184 - lr: 5.0000e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7791 - regression_loss: 1.6959 - val_loss: 3.9239 - val_regression_loss: 1.6084 - lr: 5.0000e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7263 - regression_loss: 1.6711 - val_loss: 3.8001 - val_regression_loss: 1.5133 - lr: 5.0000e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7444 - regression_loss: 1.6649 - val_loss: 3.8147 - val_regression_loss: 1.5234 - lr: 5.0000e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7144 - regression_loss: 1.6453 - val_loss: 3.7855 - val_regression_loss: 1.5021 - lr: 5.0000e-05\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7324 - regression_loss: 1.6530 - val_loss: 3.8906 - val_regression_loss: 1.5831 - lr: 5.0000e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7428 - regression_loss: 1.6644 - val_loss: 3.7694 - val_regression_loss: 1.4870 - lr: 5.0000e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7234 - regression_loss: 1.6535 - val_loss: 3.8969 - val_regression_loss: 1.5875 - lr: 5.0000e-05\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7081 - regression_loss: 1.6416 - val_loss: 3.7826 - val_regression_loss: 1.5011 - lr: 5.0000e-05\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8020 - regression_loss: 1.6999 - val_loss: 4.0264 - val_regression_loss: 1.6929 - lr: 5.0000e-05\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7270 - regression_loss: 1.6476 - val_loss: 3.7767 - val_regression_loss: 1.4993 - lr: 5.0000e-05\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7817 - regression_loss: 1.6856 - val_loss: 3.9250 - val_regression_loss: 1.6114 - lr: 5.0000e-05\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7047 - regression_loss: 1.6649 - val_loss: 3.7941 - val_regression_loss: 1.5207 - lr: 5.0000e-05\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7281 - regression_loss: 1.6735 - val_loss: 3.9744 - val_regression_loss: 1.6523 - lr: 5.0000e-05\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7260 - regression_loss: 1.7011 - val_loss: 3.7163 - val_regression_loss: 1.4498 - lr: 5.0000e-05\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6699 - regression_loss: 1.6364 - val_loss: 3.8068 - val_regression_loss: 1.5194 - lr: 5.0000e-05\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6153 - regression_loss: 1.5854 - val_loss: 3.7083 - val_regression_loss: 1.4464 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6788 - regression_loss: 1.6143 - val_loss: 3.7586 - val_regression_loss: 1.4792 - lr: 5.0000e-05\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6327 - regression_loss: 1.5842 - val_loss: 3.6936 - val_regression_loss: 1.4349 - lr: 5.0000e-05\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6650 - regression_loss: 1.6487 - val_loss: 3.7567 - val_regression_loss: 1.4806 - lr: 5.0000e-05\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6857 - regression_loss: 1.6065 - val_loss: 3.6923 - val_regression_loss: 1.4290 - lr: 5.0000e-05\n",
      "Epoch 111/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.5790 - regression_loss: 2.6244\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6637 - regression_loss: 1.5901 - val_loss: 3.6775 - val_regression_loss: 1.4163 - lr: 5.0000e-05\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6133 - regression_loss: 1.5437 - val_loss: 3.6935 - val_regression_loss: 1.4280 - lr: 2.5000e-05\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6224 - regression_loss: 1.5367 - val_loss: 3.6876 - val_regression_loss: 1.4245 - lr: 2.5000e-05\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5981 - regression_loss: 1.5303 - val_loss: 3.6884 - val_regression_loss: 1.4264 - lr: 2.5000e-05\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5742 - regression_loss: 1.5292 - val_loss: 3.6931 - val_regression_loss: 1.4299 - lr: 2.5000e-05\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6031 - regression_loss: 1.5266 - val_loss: 3.6789 - val_regression_loss: 1.4175 - lr: 2.5000e-05\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5643 - regression_loss: 1.5222 - val_loss: 3.6807 - val_regression_loss: 1.4193 - lr: 2.5000e-05\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6026 - regression_loss: 1.5207 - val_loss: 3.6733 - val_regression_loss: 1.4131 - lr: 2.5000e-05\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5997 - regression_loss: 1.5187 - val_loss: 3.6859 - val_regression_loss: 1.4239 - lr: 2.5000e-05\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5590 - regression_loss: 1.5177 - val_loss: 3.6751 - val_regression_loss: 1.4159 - lr: 2.5000e-05\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5680 - regression_loss: 1.5112 - val_loss: 3.6781 - val_regression_loss: 1.4180 - lr: 2.5000e-05\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5755 - regression_loss: 1.5115 - val_loss: 3.6595 - val_regression_loss: 1.4037 - lr: 2.5000e-05\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5427 - regression_loss: 1.5092 - val_loss: 3.6740 - val_regression_loss: 1.4138 - lr: 2.5000e-05\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5409 - regression_loss: 1.5085 - val_loss: 3.6604 - val_regression_loss: 1.4037 - lr: 2.5000e-05\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5266 - regression_loss: 1.5077 - val_loss: 3.6499 - val_regression_loss: 1.3965 - lr: 2.5000e-05\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5771 - regression_loss: 1.5089 - val_loss: 3.6670 - val_regression_loss: 1.4092 - lr: 2.5000e-05\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5572 - regression_loss: 1.4963 - val_loss: 3.6429 - val_regression_loss: 1.3907 - lr: 2.5000e-05\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5627 - regression_loss: 1.5036 - val_loss: 3.6484 - val_regression_loss: 1.3951 - lr: 2.5000e-05\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5720 - regression_loss: 1.5201 - val_loss: 3.6718 - val_regression_loss: 1.4132 - lr: 2.5000e-05\n",
      "Epoch 130/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.4668 - regression_loss: 1.5149\n",
      "Epoch 130: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5804 - regression_loss: 1.5293 - val_loss: 3.6255 - val_regression_loss: 1.3785 - lr: 2.5000e-05\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5575 - regression_loss: 1.4891 - val_loss: 3.6779 - val_regression_loss: 1.4179 - lr: 1.2500e-05\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5596 - regression_loss: 1.4910 - val_loss: 3.6598 - val_regression_loss: 1.4041 - lr: 1.2500e-05\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5435 - regression_loss: 1.4995 - val_loss: 3.6284 - val_regression_loss: 1.3803 - lr: 1.2500e-05\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5646 - regression_loss: 1.4835 - val_loss: 3.6576 - val_regression_loss: 1.4023 - lr: 1.2500e-05\n",
      "Epoch 135/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.3552 - regression_loss: 1.4037\n",
      "Epoch 135: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5507 - regression_loss: 1.4820 - val_loss: 3.6433 - val_regression_loss: 1.3916 - lr: 1.2500e-05\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5425 - regression_loss: 1.4791 - val_loss: 3.6411 - val_regression_loss: 1.3898 - lr: 6.2500e-06\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5297 - regression_loss: 1.4759 - val_loss: 3.6339 - val_regression_loss: 1.3842 - lr: 6.2500e-06\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5259 - regression_loss: 1.4776 - val_loss: 3.6304 - val_regression_loss: 1.3816 - lr: 6.2500e-06\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5462 - regression_loss: 1.4747 - val_loss: 3.6389 - val_regression_loss: 1.3880 - lr: 6.2500e-06\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5130 - regression_loss: 1.4743 - val_loss: 3.6471 - val_regression_loss: 1.3942 - lr: 6.2500e-06\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5088 - regression_loss: 1.4746 - val_loss: 3.6419 - val_regression_loss: 1.3902 - lr: 6.2500e-06\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5226 - regression_loss: 1.4734 - val_loss: 3.6321 - val_regression_loss: 1.3826 - lr: 6.2500e-06\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5199 - regression_loss: 1.4775 - val_loss: 3.6254 - val_regression_loss: 1.3777 - lr: 6.2500e-06\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5489 - regression_loss: 1.4723 - val_loss: 3.6376 - val_regression_loss: 1.3869 - lr: 6.2500e-06\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5071 - regression_loss: 1.4753 - val_loss: 3.6515 - val_regression_loss: 1.3977 - lr: 6.2500e-06\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5474 - regression_loss: 1.4753 - val_loss: 3.6413 - val_regression_loss: 1.3898 - lr: 6.2500e-06\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5059 - regression_loss: 1.4687 - val_loss: 3.6226 - val_regression_loss: 1.3754 - lr: 6.2500e-06\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5101 - regression_loss: 1.4713 - val_loss: 3.6209 - val_regression_loss: 1.3744 - lr: 6.2500e-06\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5235 - regression_loss: 1.4718 - val_loss: 3.6315 - val_regression_loss: 1.3825 - lr: 6.2500e-06\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5332 - regression_loss: 1.4680 - val_loss: 3.6329 - val_regression_loss: 1.3835 - lr: 6.2500e-06\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5224 - regression_loss: 1.4715 - val_loss: 3.6255 - val_regression_loss: 1.3782 - lr: 6.2500e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 152/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.2760 - regression_loss: 1.3251\n",
      "Epoch 152: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5100 - regression_loss: 1.4668 - val_loss: 3.6348 - val_regression_loss: 1.3856 - lr: 6.2500e-06\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5378 - regression_loss: 1.4662 - val_loss: 3.6353 - val_regression_loss: 1.3858 - lr: 3.1250e-06\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5096 - regression_loss: 1.4655 - val_loss: 3.6310 - val_regression_loss: 1.3825 - lr: 3.1250e-06\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5137 - regression_loss: 1.4652 - val_loss: 3.6303 - val_regression_loss: 1.3819 - lr: 3.1250e-06\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5490 - regression_loss: 1.4658 - val_loss: 3.6244 - val_regression_loss: 1.3774 - lr: 3.1250e-06\n",
      "Epoch 157/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.8753 - regression_loss: 1.9245\n",
      "Epoch 157: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5166 - regression_loss: 1.4650 - val_loss: 3.6256 - val_regression_loss: 1.3783 - lr: 3.1250e-06\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4811 - regression_loss: 1.4635 - val_loss: 3.6270 - val_regression_loss: 1.3793 - lr: 1.5625e-06\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5275 - regression_loss: 1.4636 - val_loss: 3.6277 - val_regression_loss: 1.3798 - lr: 1.5625e-06\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5352 - regression_loss: 1.4631 - val_loss: 3.6279 - val_regression_loss: 1.3800 - lr: 1.5625e-06\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5125 - regression_loss: 1.4632 - val_loss: 3.6271 - val_regression_loss: 1.3794 - lr: 1.5625e-06\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5079 - regression_loss: 1.4631 - val_loss: 3.6259 - val_regression_loss: 1.3784 - lr: 1.5625e-06\n",
      "Epoch 163/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.2605 - regression_loss: 1.3099\n",
      "Epoch 163: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5299 - regression_loss: 1.4627 - val_loss: 3.6264 - val_regression_loss: 1.3788 - lr: 1.5625e-06\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5017 - regression_loss: 1.4625 - val_loss: 3.6268 - val_regression_loss: 1.3791 - lr: 7.8125e-07\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5351 - regression_loss: 1.4625 - val_loss: 3.6259 - val_regression_loss: 1.3785 - lr: 7.8125e-07\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5221 - regression_loss: 1.4624 - val_loss: 3.6262 - val_regression_loss: 1.3787 - lr: 7.8125e-07\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5165 - regression_loss: 1.4625 - val_loss: 3.6247 - val_regression_loss: 1.3775 - lr: 7.8125e-07\n",
      "Epoch 168/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.3819 - regression_loss: 1.4312\n",
      "Epoch 168: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5000 - regression_loss: 1.4622 - val_loss: 3.6247 - val_regression_loss: 1.3775 - lr: 7.8125e-07\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5023 - regression_loss: 1.4620 - val_loss: 3.6247 - val_regression_loss: 1.3775 - lr: 3.9062e-07\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5386 - regression_loss: 1.4619 - val_loss: 3.6250 - val_regression_loss: 1.3777 - lr: 3.9062e-07\n",
      "Epoch 171/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5106 - regression_loss: 1.4619 - val_loss: 3.6255 - val_regression_loss: 1.3781 - lr: 3.9062e-07\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5345 - regression_loss: 1.4619 - val_loss: 3.6252 - val_regression_loss: 1.3779 - lr: 3.9062e-07\n",
      "Epoch 173/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4437 - regression_loss: 1.4618 - val_loss: 3.6252 - val_regression_loss: 1.3779 - lr: 3.9062e-07\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5464 - regression_loss: 1.4617 - val_loss: 3.6255 - val_regression_loss: 1.3781 - lr: 3.9062e-07\n",
      "Epoch 175/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5410 - regression_loss: 1.4616 - val_loss: 3.6255 - val_regression_loss: 1.3781 - lr: 3.9062e-07\n",
      "Epoch 176/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4938 - regression_loss: 1.4616 - val_loss: 3.6254 - val_regression_loss: 1.3780 - lr: 3.9062e-07\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5178 - regression_loss: 1.4616 - val_loss: 3.6255 - val_regression_loss: 1.3781 - lr: 3.9062e-07\n",
      "Epoch 178/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.8501 - regression_loss: 1.8995\n",
      "Epoch 178: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5151 - regression_loss: 1.4616 - val_loss: 3.6258 - val_regression_loss: 1.3783 - lr: 3.9062e-07\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5430 - regression_loss: 1.4615 - val_loss: 3.6259 - val_regression_loss: 1.3784 - lr: 1.9531e-07\n",
      "Epoch 180/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4456 - regression_loss: 1.4615 - val_loss: 3.6256 - val_regression_loss: 1.3782 - lr: 1.9531e-07\n",
      "Epoch 181/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4750 - regression_loss: 1.4614 - val_loss: 3.6256 - val_regression_loss: 1.3782 - lr: 1.9531e-07\n",
      "Epoch 182/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.5337 - regression_loss: 1.4614 - val_loss: 3.6255 - val_regression_loss: 1.3781 - lr: 1.9531e-07\n",
      "Epoch 183/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.8811 - regression_loss: 1.9305\n",
      "Epoch 183: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5064 - regression_loss: 1.4614 - val_loss: 3.6257 - val_regression_loss: 1.3783 - lr: 1.9531e-07\n",
      "Epoch 184/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5251 - regression_loss: 1.4613 - val_loss: 3.6257 - val_regression_loss: 1.3782 - lr: 9.7656e-08\n",
      "Epoch 185/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5123 - regression_loss: 1.4613 - val_loss: 3.6256 - val_regression_loss: 1.3782 - lr: 9.7656e-08\n",
      "Epoch 186/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5089 - regression_loss: 1.4613 - val_loss: 3.6255 - val_regression_loss: 1.3781 - lr: 9.7656e-08\n",
      "Epoch 187/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4971 - regression_loss: 1.4613 - val_loss: 3.6254 - val_regression_loss: 1.3781 - lr: 9.7656e-08\n",
      "Epoch 188/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.1984 - regression_loss: 1.2478\n",
      "Epoch 188: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4819 - regression_loss: 1.4613 - val_loss: 3.6254 - val_regression_loss: 1.3781 - lr: 9.7656e-08\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 26ms/step - loss: 64.6789 - regression_loss: 58.0187 - val_loss: 61.7761 - val_regression_loss: 44.9078 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 45.0315 - regression_loss: 41.1251 - val_loss: 48.8413 - val_regression_loss: 34.2608 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 37.3587 - regression_loss: 32.2905 - val_loss: 37.9895 - val_regression_loss: 26.5557 - lr: 1.0000e-04\n",
      "Epoch 4/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 29.6409 - regression_loss: 25.8404 - val_loss: 31.9670 - val_regression_loss: 22.2669 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.6178 - regression_loss: 22.0986 - val_loss: 27.5984 - val_regression_loss: 19.1496 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.6676 - regression_loss: 19.4548 - val_loss: 24.9697 - val_regression_loss: 17.1707 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.3202 - regression_loss: 17.0335 - val_loss: 21.4565 - val_regression_loss: 14.6732 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.5273 - regression_loss: 15.3676 - val_loss: 19.8441 - val_regression_loss: 13.4306 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.6700 - regression_loss: 14.0010 - val_loss: 18.2319 - val_regression_loss: 12.2578 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.8848 - regression_loss: 13.0106 - val_loss: 17.3904 - val_regression_loss: 11.6692 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.8400 - regression_loss: 12.0543 - val_loss: 15.6248 - val_regression_loss: 10.4644 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1165 - regression_loss: 11.1337 - val_loss: 15.3576 - val_regression_loss: 10.2337 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.2420 - regression_loss: 10.5087 - val_loss: 13.8120 - val_regression_loss: 9.1697 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3749 - regression_loss: 9.8096 - val_loss: 14.3878 - val_regression_loss: 9.5708 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9571 - regression_loss: 9.3320 - val_loss: 12.6614 - val_regression_loss: 8.4314 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.5074 - regression_loss: 8.9805 - val_loss: 13.8692 - val_regression_loss: 9.2379 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4245 - regression_loss: 8.7619 - val_loss: 11.8572 - val_regression_loss: 7.8432 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8590 - regression_loss: 8.3184 - val_loss: 12.7649 - val_regression_loss: 8.4778 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2666 - regression_loss: 8.1737 - val_loss: 11.0951 - val_regression_loss: 7.3502 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2987 - regression_loss: 7.7691 - val_loss: 12.3136 - val_regression_loss: 8.1901 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2410 - regression_loss: 7.7621 - val_loss: 10.8381 - val_regression_loss: 7.1956 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9173 - regression_loss: 7.3977 - val_loss: 11.8729 - val_regression_loss: 7.8792 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6620 - regression_loss: 7.2948 - val_loss: 10.4349 - val_regression_loss: 6.8981 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.7252 - regression_loss: 7.2136 - val_loss: 11.3831 - val_regression_loss: 7.5472 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3547 - regression_loss: 6.9673 - val_loss: 10.1745 - val_regression_loss: 6.7294 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3436 - regression_loss: 6.9076 - val_loss: 11.0444 - val_regression_loss: 7.2629 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2257 - regression_loss: 6.7650 - val_loss: 9.8633 - val_regression_loss: 6.4700 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1911 - regression_loss: 6.7811 - val_loss: 10.6654 - val_regression_loss: 6.9853 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0178 - regression_loss: 6.6315 - val_loss: 9.9478 - val_regression_loss: 6.4844 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8535 - regression_loss: 6.5194 - val_loss: 9.6325 - val_regression_loss: 6.2690 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0109 - regression_loss: 6.6125 - val_loss: 11.0538 - val_regression_loss: 7.2613 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8880 - regression_loss: 6.4259 - val_loss: 9.4155 - val_regression_loss: 6.2086 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9810 - regression_loss: 6.5929 - val_loss: 11.7849 - val_regression_loss: 7.7898 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9017 - regression_loss: 6.6648 - val_loss: 9.2697 - val_regression_loss: 6.0885 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5114 - regression_loss: 6.2801 - val_loss: 10.8648 - val_regression_loss: 7.0849 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6675 - regression_loss: 6.2968 - val_loss: 8.9953 - val_regression_loss: 5.8349 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4150 - regression_loss: 6.0562 - val_loss: 9.8482 - val_regression_loss: 6.3463 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2670 - regression_loss: 5.9127 - val_loss: 8.9238 - val_regression_loss: 5.7485 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3707 - regression_loss: 5.9577 - val_loss: 9.2799 - val_regression_loss: 5.9410 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2088 - regression_loss: 5.7864 - val_loss: 8.9580 - val_regression_loss: 5.7166 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9262 - regression_loss: 5.6836 - val_loss: 9.1567 - val_regression_loss: 5.8239 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1046 - regression_loss: 5.7325 - val_loss: 8.8935 - val_regression_loss: 5.6301 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9071 - regression_loss: 5.6038 - val_loss: 8.8311 - val_regression_loss: 5.6091 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9461 - regression_loss: 5.5844 - val_loss: 8.5990 - val_regression_loss: 5.4591 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9612 - regression_loss: 5.6589 - val_loss: 9.5610 - val_regression_loss: 6.0990 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9058 - regression_loss: 5.5959 - val_loss: 8.4104 - val_regression_loss: 5.3735 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0103 - regression_loss: 5.6802 - val_loss: 10.1333 - val_regression_loss: 6.5052 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1584 - regression_loss: 5.8465 - val_loss: 8.4416 - val_regression_loss: 5.4312 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8500 - regression_loss: 5.5918 - val_loss: 9.1872 - val_regression_loss: 5.8171 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6967 - regression_loss: 5.3914 - val_loss: 8.1603 - val_regression_loss: 5.1021 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5565 - regression_loss: 5.2807 - val_loss: 8.5197 - val_regression_loss: 5.3196 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5785 - regression_loss: 5.2344 - val_loss: 8.0913 - val_regression_loss: 5.0593 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5529 - regression_loss: 5.2891 - val_loss: 8.6125 - val_regression_loss: 5.3830 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5943 - regression_loss: 5.2536 - val_loss: 8.0862 - val_regression_loss: 5.0560 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4218 - regression_loss: 5.1541 - val_loss: 8.5200 - val_regression_loss: 5.2968 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.3810 - regression_loss: 5.2418 - val_loss: 8.0079 - val_regression_loss: 4.9465 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.3160 - regression_loss: 5.0971 - val_loss: 7.9649 - val_regression_loss: 4.9312 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.3213 - regression_loss: 5.0674 - val_loss: 7.9510 - val_regression_loss: 4.9079 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4604 - regression_loss: 5.0917 - val_loss: 7.8163 - val_regression_loss: 4.8488 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.2094 - regression_loss: 5.0305 - val_loss: 8.0248 - val_regression_loss: 4.9237 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1508 - regression_loss: 5.0104 - val_loss: 7.7479 - val_regression_loss: 4.7759 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.2633 - regression_loss: 4.9752 - val_loss: 8.6467 - val_regression_loss: 5.3836 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4344 - regression_loss: 5.1209 - val_loss: 7.6677 - val_regression_loss: 4.7569 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.3824 - regression_loss: 5.0271 - val_loss: 7.9212 - val_regression_loss: 4.8458 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.1426 - regression_loss: 4.8777 - val_loss: 7.6681 - val_regression_loss: 4.6944 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.1579 - regression_loss: 4.8638 - val_loss: 7.7848 - val_regression_loss: 4.7630 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9384 - regression_loss: 4.7949 - val_loss: 7.5647 - val_regression_loss: 4.6090 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.1476 - regression_loss: 4.8488 - val_loss: 8.2078 - val_regression_loss: 5.0311 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9550 - regression_loss: 4.9155 - val_loss: 7.5586 - val_regression_loss: 4.6634 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0851 - regression_loss: 4.8788 - val_loss: 8.1180 - val_regression_loss: 4.9684 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.1465 - regression_loss: 4.8221 - val_loss: 7.4946 - val_regression_loss: 4.5593 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.4338 - regression_loss: 5.5169\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9743 - regression_loss: 4.7225 - val_loss: 7.7337 - val_regression_loss: 4.6897 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9172 - regression_loss: 4.6761 - val_loss: 7.4820 - val_regression_loss: 4.5336 - lr: 5.0000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9423 - regression_loss: 4.6393 - val_loss: 7.5062 - val_regression_loss: 4.5475 - lr: 5.0000e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9431 - regression_loss: 4.6558 - val_loss: 7.4761 - val_regression_loss: 4.5273 - lr: 5.0000e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8147 - regression_loss: 4.6213 - val_loss: 7.5215 - val_regression_loss: 4.5523 - lr: 5.0000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7872 - regression_loss: 4.6157 - val_loss: 7.4752 - val_regression_loss: 4.5096 - lr: 5.0000e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7830 - regression_loss: 4.6463 - val_loss: 7.4786 - val_regression_loss: 4.5203 - lr: 5.0000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7998 - regression_loss: 4.6413 - val_loss: 7.3918 - val_regression_loss: 4.4738 - lr: 5.0000e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7526 - regression_loss: 4.5843 - val_loss: 7.5373 - val_regression_loss: 4.5526 - lr: 5.0000e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6438 - regression_loss: 4.5758 - val_loss: 7.4096 - val_regression_loss: 4.4702 - lr: 5.0000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7307 - regression_loss: 4.5717 - val_loss: 7.4238 - val_regression_loss: 4.4782 - lr: 5.0000e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8291 - regression_loss: 4.5538 - val_loss: 7.4827 - val_regression_loss: 4.5082 - lr: 5.0000e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.8564 - regression_loss: 4.5442 - val_loss: 7.3684 - val_regression_loss: 4.4423 - lr: 5.0000e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8252 - regression_loss: 4.5518 - val_loss: 7.3834 - val_regression_loss: 4.4428 - lr: 5.0000e-05\n",
      "Epoch 86/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.5037 - regression_loss: 4.5901\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7900 - regression_loss: 4.5296 - val_loss: 7.4193 - val_regression_loss: 4.4765 - lr: 5.0000e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7913 - regression_loss: 4.5628 - val_loss: 7.3891 - val_regression_loss: 4.4622 - lr: 2.5000e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7495 - regression_loss: 4.4970 - val_loss: 7.3219 - val_regression_loss: 4.4094 - lr: 2.5000e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6135 - regression_loss: 4.5269 - val_loss: 7.3083 - val_regression_loss: 4.3914 - lr: 2.5000e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7040 - regression_loss: 4.5630 - val_loss: 7.4737 - val_regression_loss: 4.4807 - lr: 2.5000e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7221 - regression_loss: 4.4950 - val_loss: 7.3153 - val_regression_loss: 4.3965 - lr: 2.5000e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6106 - regression_loss: 4.5194 - val_loss: 7.2797 - val_regression_loss: 4.3782 - lr: 2.5000e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6923 - regression_loss: 4.5285 - val_loss: 7.4120 - val_regression_loss: 4.4512 - lr: 2.5000e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6345 - regression_loss: 4.4914 - val_loss: 7.3027 - val_regression_loss: 4.3851 - lr: 2.5000e-05\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7415 - regression_loss: 4.4885 - val_loss: 7.2964 - val_regression_loss: 4.3839 - lr: 2.5000e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6444 - regression_loss: 4.4637 - val_loss: 7.3449 - val_regression_loss: 4.4065 - lr: 2.5000e-05\n",
      "Epoch 97/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.0924 - regression_loss: 6.1802\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7420 - regression_loss: 4.4699 - val_loss: 7.3660 - val_regression_loss: 4.4195 - lr: 2.5000e-05\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7146 - regression_loss: 4.4583 - val_loss: 7.3304 - val_regression_loss: 4.3989 - lr: 1.2500e-05\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5246 - regression_loss: 4.4544 - val_loss: 7.2855 - val_regression_loss: 4.3732 - lr: 1.2500e-05\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6626 - regression_loss: 4.4619 - val_loss: 7.2938 - val_regression_loss: 4.3824 - lr: 1.2500e-05\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7086 - regression_loss: 4.4509 - val_loss: 7.3252 - val_regression_loss: 4.3954 - lr: 1.2500e-05\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7417 - regression_loss: 4.4474 - val_loss: 7.3256 - val_regression_loss: 4.3947 - lr: 1.2500e-05\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6635 - regression_loss: 4.4475 - val_loss: 7.3066 - val_regression_loss: 4.3832 - lr: 1.2500e-05\n",
      "Epoch 104/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.2517 - regression_loss: 4.3399\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6509 - regression_loss: 4.4759 - val_loss: 7.2608 - val_regression_loss: 4.3584 - lr: 1.2500e-05\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6908 - regression_loss: 4.4475 - val_loss: 7.2986 - val_regression_loss: 4.3757 - lr: 6.2500e-06\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6206 - regression_loss: 4.4430 - val_loss: 7.3315 - val_regression_loss: 4.3946 - lr: 6.2500e-06\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7103 - regression_loss: 4.4390 - val_loss: 7.3237 - val_regression_loss: 4.3918 - lr: 6.2500e-06\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6398 - regression_loss: 4.4356 - val_loss: 7.3011 - val_regression_loss: 4.3791 - lr: 6.2500e-06\n",
      "Epoch 109/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.7237 - regression_loss: 4.8121\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6918 - regression_loss: 4.4345 - val_loss: 7.2851 - val_regression_loss: 4.3699 - lr: 6.2500e-06\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6447 - regression_loss: 4.4333 - val_loss: 7.2829 - val_regression_loss: 4.3681 - lr: 3.1250e-06\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5935 - regression_loss: 4.4318 - val_loss: 7.2852 - val_regression_loss: 4.3689 - lr: 3.1250e-06\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7323 - regression_loss: 4.4325 - val_loss: 7.2968 - val_regression_loss: 4.3742 - lr: 3.1250e-06\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6526 - regression_loss: 4.4313 - val_loss: 7.3020 - val_regression_loss: 4.3764 - lr: 3.1250e-06\n",
      "Epoch 114/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.9860 - regression_loss: 6.0745\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6490 - regression_loss: 4.4309 - val_loss: 7.2999 - val_regression_loss: 4.3757 - lr: 3.1250e-06\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6565 - regression_loss: 4.4290 - val_loss: 7.2957 - val_regression_loss: 4.3732 - lr: 1.5625e-06\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5183 - regression_loss: 4.4300 - val_loss: 7.2908 - val_regression_loss: 4.3709 - lr: 1.5625e-06\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5841 - regression_loss: 4.4284 - val_loss: 7.2923 - val_regression_loss: 4.3713 - lr: 1.5625e-06\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6197 - regression_loss: 4.4279 - val_loss: 7.2940 - val_regression_loss: 4.3721 - lr: 1.5625e-06\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6122 - regression_loss: 4.4278 - val_loss: 7.2960 - val_regression_loss: 4.3731 - lr: 1.5625e-06\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6585 - regression_loss: 4.4273 - val_loss: 7.2948 - val_regression_loss: 4.3724 - lr: 1.5625e-06\n",
      "Epoch 121/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.4427 - regression_loss: 5.5313\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6625 - regression_loss: 4.4273 - val_loss: 7.2953 - val_regression_loss: 4.3724 - lr: 1.5625e-06\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5499 - regression_loss: 4.4266 - val_loss: 7.2932 - val_regression_loss: 4.3713 - lr: 7.8125e-07\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6.6198 - regression_loss: 4.4269 - val_loss: 7.2945 - val_regression_loss: 4.3719 - lr: 7.8125e-07\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.6320 - regression_loss: 4.4264 - val_loss: 7.2930 - val_regression_loss: 4.3712 - lr: 7.8125e-07\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6111 - regression_loss: 4.4266 - val_loss: 7.2911 - val_regression_loss: 4.3700 - lr: 7.8125e-07\n",
      "Epoch 126/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.7452 - regression_loss: 3.8338\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6195 - regression_loss: 4.4265 - val_loss: 7.2932 - val_regression_loss: 4.3710 - lr: 7.8125e-07\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6444 - regression_loss: 4.4258 - val_loss: 7.2937 - val_regression_loss: 4.3713 - lr: 3.9062e-07\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6715 - regression_loss: 4.4256 - val_loss: 7.2935 - val_regression_loss: 4.3713 - lr: 3.9062e-07\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6035 - regression_loss: 4.4259 - val_loss: 7.2916 - val_regression_loss: 4.3703 - lr: 3.9062e-07\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4825 - regression_loss: 4.4256 - val_loss: 7.2918 - val_regression_loss: 4.3703 - lr: 3.9062e-07\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7272 - regression_loss: 4.4253 - val_loss: 7.2915 - val_regression_loss: 4.3702 - lr: 3.9062e-07\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6521 - regression_loss: 4.4255 - val_loss: 7.2922 - val_regression_loss: 4.3706 - lr: 3.9062e-07\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6535 - regression_loss: 4.4253 - val_loss: 7.2913 - val_regression_loss: 4.3699 - lr: 3.9062e-07\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6858 - regression_loss: 4.4252 - val_loss: 7.2918 - val_regression_loss: 4.3702 - lr: 3.9062e-07\n",
      "Epoch 135/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.6928 - regression_loss: 5.7814\n",
      "Epoch 135: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5840 - regression_loss: 4.4251 - val_loss: 7.2913 - val_regression_loss: 4.3698 - lr: 3.9062e-07\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6654 - regression_loss: 4.4249 - val_loss: 7.2914 - val_regression_loss: 4.3699 - lr: 1.9531e-07\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5985 - regression_loss: 4.4252 - val_loss: 7.2920 - val_regression_loss: 4.3703 - lr: 1.9531e-07\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7274 - regression_loss: 4.4249 - val_loss: 7.2914 - val_regression_loss: 4.3699 - lr: 1.9531e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5745 - regression_loss: 4.4248 - val_loss: 7.2912 - val_regression_loss: 4.3698 - lr: 1.9531e-07\n",
      "Epoch 140/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.7914 - regression_loss: 4.8800\n",
      "Epoch 140: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6311 - regression_loss: 4.4249 - val_loss: 7.2911 - val_regression_loss: 4.3698 - lr: 1.9531e-07\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6841 - regression_loss: 4.4247 - val_loss: 7.2911 - val_regression_loss: 4.3698 - lr: 9.7656e-08\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5863 - regression_loss: 4.4247 - val_loss: 7.2909 - val_regression_loss: 4.3696 - lr: 9.7656e-08\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6421 - regression_loss: 4.4247 - val_loss: 7.2910 - val_regression_loss: 4.3697 - lr: 9.7656e-08\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6089 - regression_loss: 4.4246 - val_loss: 7.2910 - val_regression_loss: 4.3697 - lr: 9.7656e-08\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 26ms/step - loss: 57.7944 - regression_loss: 51.8103 - val_loss: 59.8047 - val_regression_loss: 47.7429 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 38.6252 - regression_loss: 34.5040 - val_loss: 52.8726 - val_regression_loss: 42.9395 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 33.9714 - regression_loss: 30.0021 - val_loss: 48.8846 - val_regression_loss: 39.8827 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31.5640 - regression_loss: 28.0004 - val_loss: 42.9329 - val_regression_loss: 34.7100 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.2234 - regression_loss: 25.3198 - val_loss: 38.8367 - val_regression_loss: 31.0959 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.8687 - regression_loss: 23.4620 - val_loss: 36.1007 - val_regression_loss: 28.6650 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.9562 - regression_loss: 22.4404 - val_loss: 34.8736 - val_regression_loss: 27.6766 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.2857 - regression_loss: 21.5125 - val_loss: 33.5886 - val_regression_loss: 26.6407 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.8128 - regression_loss: 20.6477 - val_loss: 32.7578 - val_regression_loss: 25.9860 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.2171 - regression_loss: 19.8715 - val_loss: 31.7717 - val_regression_loss: 25.1433 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.4884 - regression_loss: 19.2824 - val_loss: 30.7803 - val_regression_loss: 24.2790 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.0628 - regression_loss: 18.7001 - val_loss: 29.9737 - val_regression_loss: 23.5799 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.9153 - regression_loss: 18.1008 - val_loss: 29.1340 - val_regression_loss: 22.8187 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.8727 - regression_loss: 17.6098 - val_loss: 28.4999 - val_regression_loss: 22.2874 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.8864 - regression_loss: 17.1813 - val_loss: 27.8103 - val_regression_loss: 21.6746 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.0434 - regression_loss: 16.7526 - val_loss: 26.8972 - val_regression_loss: 20.8708 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.0106 - regression_loss: 16.4173 - val_loss: 26.4391 - val_regression_loss: 20.4795 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1063 - regression_loss: 16.0310 - val_loss: 26.2426 - val_regression_loss: 20.3195 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.6049 - regression_loss: 15.7550 - val_loss: 25.1822 - val_regression_loss: 19.3632 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.5590 - regression_loss: 15.4492 - val_loss: 24.4826 - val_regression_loss: 18.7298 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.2291 - regression_loss: 15.1824 - val_loss: 24.0687 - val_regression_loss: 18.3849 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.1217 - regression_loss: 15.0645 - val_loss: 24.1178 - val_regression_loss: 18.4275 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.7907 - regression_loss: 14.8764 - val_loss: 23.3629 - val_regression_loss: 17.7493 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.7265 - regression_loss: 14.6763 - val_loss: 22.8890 - val_regression_loss: 17.3181 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4352 - regression_loss: 14.3896 - val_loss: 22.6267 - val_regression_loss: 17.0856 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2203 - regression_loss: 14.3212 - val_loss: 22.1257 - val_regression_loss: 16.6215 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.1905 - regression_loss: 14.0896 - val_loss: 21.9588 - val_regression_loss: 16.4777 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7890 - regression_loss: 13.9943 - val_loss: 21.8205 - val_regression_loss: 16.3483 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8345 - regression_loss: 14.1266 - val_loss: 21.5486 - val_regression_loss: 16.1033 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8846 - regression_loss: 13.9840 - val_loss: 22.3088 - val_regression_loss: 16.7586 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.6712 - regression_loss: 13.8411 - val_loss: 21.2622 - val_regression_loss: 15.8333 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.0265 - regression_loss: 14.0594 - val_loss: 21.5234 - val_regression_loss: 16.0770 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8683 - regression_loss: 13.7823 - val_loss: 20.7575 - val_regression_loss: 15.4173 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.3600 - regression_loss: 13.6308 - val_loss: 20.7748 - val_regression_loss: 15.4175 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.4397 - regression_loss: 13.5544 - val_loss: 20.4704 - val_regression_loss: 15.1812 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5452 - regression_loss: 13.5176 - val_loss: 20.7727 - val_regression_loss: 15.4522 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.4077 - regression_loss: 13.5467 - val_loss: 20.6670 - val_regression_loss: 15.3590 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.4227 - regression_loss: 13.4947 - val_loss: 20.3396 - val_regression_loss: 15.0656 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.1523 - regression_loss: 13.4679 - val_loss: 20.4808 - val_regression_loss: 15.1638 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.0490 - regression_loss: 13.4030 - val_loss: 20.1135 - val_regression_loss: 14.8588 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.6659 - regression_loss: 13.8355 - val_loss: 21.0406 - val_regression_loss: 15.6839 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.3427 - regression_loss: 13.5295 - val_loss: 20.3318 - val_regression_loss: 15.1096 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.0295 - regression_loss: 13.4399 - val_loss: 20.0380 - val_regression_loss: 14.8281 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.1647 - regression_loss: 13.3514 - val_loss: 20.2315 - val_regression_loss: 15.0134 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.1195 - regression_loss: 13.2593 - val_loss: 19.8845 - val_regression_loss: 14.7174 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.1886 - regression_loss: 13.2101 - val_loss: 20.9127 - val_regression_loss: 15.5902 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.4226 - regression_loss: 13.5501 - val_loss: 20.1684 - val_regression_loss: 15.0057 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 14.1048 - regression_loss: 12.2033\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.1271 - regression_loss: 13.2235 - val_loss: 20.7699 - val_regression_loss: 15.4855 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.2837 - regression_loss: 13.4660 - val_loss: 19.7682 - val_regression_loss: 14.6412 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.0957 - regression_loss: 13.1466 - val_loss: 19.9063 - val_regression_loss: 14.7679 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.9256 - regression_loss: 13.0861 - val_loss: 19.9375 - val_regression_loss: 14.8120 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.7851 - regression_loss: 13.0796 - val_loss: 19.9688 - val_regression_loss: 14.8517 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.7844 - regression_loss: 13.0485 - val_loss: 20.0744 - val_regression_loss: 14.9444 - lr: 5.0000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.8225 - regression_loss: 13.0347 - val_loss: 19.8919 - val_regression_loss: 14.7854 - lr: 5.0000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.6986 - regression_loss: 13.0096 - val_loss: 19.9336 - val_regression_loss: 14.8262 - lr: 5.0000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.8446 - regression_loss: 12.9977 - val_loss: 19.9345 - val_regression_loss: 14.8346 - lr: 5.0000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5451 - regression_loss: 12.9802 - val_loss: 19.8599 - val_regression_loss: 14.7661 - lr: 5.0000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.9023 - regression_loss: 12.9741 - val_loss: 19.9127 - val_regression_loss: 14.8132 - lr: 5.0000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.7400 - regression_loss: 12.9631 - val_loss: 19.8053 - val_regression_loss: 14.7201 - lr: 5.0000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.8576 - regression_loss: 12.9939 - val_loss: 19.7876 - val_regression_loss: 14.7209 - lr: 5.0000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.9303 - regression_loss: 13.0207 - val_loss: 19.8443 - val_regression_loss: 14.7721 - lr: 5.0000e-05\n",
      "Epoch 62/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 17.1210 - regression_loss: 15.2217\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.7882 - regression_loss: 13.0340 - val_loss: 20.0448 - val_regression_loss: 14.9542 - lr: 5.0000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.8621 - regression_loss: 12.8912 - val_loss: 19.7473 - val_regression_loss: 14.6881 - lr: 2.5000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.7140 - regression_loss: 12.8931 - val_loss: 19.7646 - val_regression_loss: 14.7012 - lr: 2.5000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5375 - regression_loss: 12.8642 - val_loss: 19.8113 - val_regression_loss: 14.7457 - lr: 2.5000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.8179 - regression_loss: 12.8599 - val_loss: 19.8054 - val_regression_loss: 14.7440 - lr: 2.5000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.8109 - regression_loss: 12.8510 - val_loss: 19.8460 - val_regression_loss: 14.7789 - lr: 2.5000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.4513 - regression_loss: 12.8491 - val_loss: 19.7931 - val_regression_loss: 14.7343 - lr: 2.5000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.8450 - regression_loss: 12.8735 - val_loss: 19.7807 - val_regression_loss: 14.7269 - lr: 2.5000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.6248 - regression_loss: 12.8231 - val_loss: 19.7035 - val_regression_loss: 14.6610 - lr: 2.5000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.6665 - regression_loss: 12.8485 - val_loss: 19.7868 - val_regression_loss: 14.7440 - lr: 2.5000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5260 - regression_loss: 12.8316 - val_loss: 19.7576 - val_regression_loss: 14.7193 - lr: 2.5000e-05\n",
      "Epoch 73/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.5132 - regression_loss: 13.6147\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.7792 - regression_loss: 12.8098 - val_loss: 19.8410 - val_regression_loss: 14.7947 - lr: 2.5000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.7503 - regression_loss: 12.7950 - val_loss: 19.8203 - val_regression_loss: 14.7749 - lr: 1.2500e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5140 - regression_loss: 12.7905 - val_loss: 19.7906 - val_regression_loss: 14.7471 - lr: 1.2500e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.2537 - regression_loss: 12.7936 - val_loss: 19.7406 - val_regression_loss: 14.7036 - lr: 1.2500e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5890 - regression_loss: 12.7834 - val_loss: 19.7404 - val_regression_loss: 14.7056 - lr: 1.2500e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5739 - regression_loss: 12.7762 - val_loss: 19.7807 - val_regression_loss: 14.7411 - lr: 1.2500e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5306 - regression_loss: 12.7982 - val_loss: 19.7243 - val_regression_loss: 14.6904 - lr: 1.2500e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5070 - regression_loss: 12.7750 - val_loss: 19.7865 - val_regression_loss: 14.7477 - lr: 1.2500e-05\n",
      "Epoch 81/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.2087 - regression_loss: 16.3106\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.7895 - regression_loss: 12.7802 - val_loss: 19.8083 - val_regression_loss: 14.7677 - lr: 1.2500e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.7052 - regression_loss: 12.7642 - val_loss: 19.7725 - val_regression_loss: 14.7368 - lr: 6.2500e-06\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.7243 - regression_loss: 12.7697 - val_loss: 19.7737 - val_regression_loss: 14.7385 - lr: 6.2500e-06\n",
      "Epoch 84/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5709 - regression_loss: 12.7586 - val_loss: 19.7444 - val_regression_loss: 14.7121 - lr: 6.2500e-06\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.6131 - regression_loss: 12.7611 - val_loss: 19.7493 - val_regression_loss: 14.7171 - lr: 6.2500e-06\n",
      "Epoch 86/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 21.1852 - regression_loss: 19.2871\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.7206 - regression_loss: 12.7529 - val_loss: 19.7332 - val_regression_loss: 14.7019 - lr: 6.2500e-06\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.6014 - regression_loss: 12.7511 - val_loss: 19.7333 - val_regression_loss: 14.7029 - lr: 3.1250e-06\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.6530 - regression_loss: 12.7488 - val_loss: 19.7385 - val_regression_loss: 14.7080 - lr: 3.1250e-06\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5758 - regression_loss: 12.7481 - val_loss: 19.7366 - val_regression_loss: 14.7065 - lr: 3.1250e-06\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.6797 - regression_loss: 12.7497 - val_loss: 19.7518 - val_regression_loss: 14.7202 - lr: 3.1250e-06\n",
      "Epoch 91/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.5083 - regression_loss: 16.6103\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.4508 - regression_loss: 12.7484 - val_loss: 19.7496 - val_regression_loss: 14.7193 - lr: 3.1250e-06\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.6894 - regression_loss: 12.7424 - val_loss: 19.7496 - val_regression_loss: 14.7193 - lr: 1.5625e-06\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.6590 - regression_loss: 12.7428 - val_loss: 19.7523 - val_regression_loss: 14.7215 - lr: 1.5625e-06\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.6116 - regression_loss: 12.7418 - val_loss: 19.7532 - val_regression_loss: 14.7226 - lr: 1.5625e-06\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.1944 - regression_loss: 12.7431 - val_loss: 19.7508 - val_regression_loss: 14.7205 - lr: 1.5625e-06\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.6839 - regression_loss: 12.7417 - val_loss: 19.7504 - val_regression_loss: 14.7204 - lr: 1.5625e-06\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.6640 - regression_loss: 12.7410 - val_loss: 19.7518 - val_regression_loss: 14.7217 - lr: 1.5625e-06\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.6437 - regression_loss: 12.7410 - val_loss: 19.7521 - val_regression_loss: 14.7224 - lr: 1.5625e-06\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.4654 - regression_loss: 12.7404 - val_loss: 19.7512 - val_regression_loss: 14.7214 - lr: 1.5625e-06\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.1336 - regression_loss: 12.7414 - val_loss: 19.7479 - val_regression_loss: 14.7184 - lr: 1.5625e-06\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5177 - regression_loss: 12.7405 - val_loss: 19.7525 - val_regression_loss: 14.7227 - lr: 1.5625e-06\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.4359 - regression_loss: 12.7399 - val_loss: 19.7446 - val_regression_loss: 14.7157 - lr: 1.5625e-06\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.1034 - regression_loss: 12.7405 - val_loss: 19.7496 - val_regression_loss: 14.7203 - lr: 1.5625e-06\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.6437 - regression_loss: 12.7387 - val_loss: 19.7528 - val_regression_loss: 14.7236 - lr: 1.5625e-06\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5268 - regression_loss: 12.7385 - val_loss: 19.7493 - val_regression_loss: 14.7202 - lr: 1.5625e-06\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.4818 - regression_loss: 12.7374 - val_loss: 19.7457 - val_regression_loss: 14.7170 - lr: 1.5625e-06\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.6293 - regression_loss: 12.7415 - val_loss: 19.7504 - val_regression_loss: 14.7213 - lr: 1.5625e-06\n",
      "Epoch 108/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 17.9589 - regression_loss: 16.0610\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.4974 - regression_loss: 12.7358 - val_loss: 19.7446 - val_regression_loss: 14.7165 - lr: 1.5625e-06\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.6485 - regression_loss: 12.7351 - val_loss: 19.7456 - val_regression_loss: 14.7175 - lr: 7.8125e-07\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.4512 - regression_loss: 12.7355 - val_loss: 19.7418 - val_regression_loss: 14.7142 - lr: 7.8125e-07\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 57.8261 - regression_loss: 52.1539 - val_loss: 35.5712 - val_regression_loss: 26.6813 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 48.2062 - regression_loss: 42.8096 - val_loss: 34.8192 - val_regression_loss: 26.3195 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 42.4838 - regression_loss: 39.1077 - val_loss: 33.2520 - val_regression_loss: 25.3205 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 40.9354 - regression_loss: 36.2064 - val_loss: 31.6210 - val_regression_loss: 24.1610 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 39.5571 - regression_loss: 34.9832 - val_loss: 30.8135 - val_regression_loss: 23.7068 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 37.1733 - regression_loss: 32.9073 - val_loss: 30.1856 - val_regression_loss: 23.2156 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 35.6992 - regression_loss: 31.6412 - val_loss: 29.5792 - val_regression_loss: 22.7998 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 35.3339 - regression_loss: 30.8511 - val_loss: 28.8864 - val_regression_loss: 22.2481 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 33.9349 - regression_loss: 30.1948 - val_loss: 28.1188 - val_regression_loss: 21.6363 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 33.3199 - regression_loss: 29.4224 - val_loss: 27.9630 - val_regression_loss: 21.5875 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.3041 - regression_loss: 28.3217 - val_loss: 27.3741 - val_regression_loss: 21.0703 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.2146 - regression_loss: 27.8537 - val_loss: 26.9418 - val_regression_loss: 20.7654 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.2901 - regression_loss: 27.2678 - val_loss: 26.9026 - val_regression_loss: 20.7836 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.7644 - regression_loss: 26.7736 - val_loss: 26.3374 - val_regression_loss: 20.2829 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.9359 - regression_loss: 26.5505 - val_loss: 26.1212 - val_regression_loss: 20.1410 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.7248 - regression_loss: 25.9841 - val_loss: 26.1585 - val_regression_loss: 20.2086 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.0655 - regression_loss: 25.5654 - val_loss: 25.6987 - val_regression_loss: 19.7760 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.1833 - regression_loss: 25.4580 - val_loss: 25.9675 - val_regression_loss: 20.0716 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.8726 - regression_loss: 25.0185 - val_loss: 25.2665 - val_regression_loss: 19.4587 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.9383 - regression_loss: 24.6988 - val_loss: 25.5948 - val_regression_loss: 19.7816 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.7945 - regression_loss: 24.4744 - val_loss: 25.0848 - val_regression_loss: 19.3667 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.8862 - regression_loss: 24.2421 - val_loss: 25.2367 - val_regression_loss: 19.5390 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.3459 - regression_loss: 24.0285 - val_loss: 24.5400 - val_regression_loss: 18.9115 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.2153 - regression_loss: 23.8522 - val_loss: 24.6500 - val_regression_loss: 19.0218 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.3041 - regression_loss: 23.8273 - val_loss: 24.3504 - val_regression_loss: 18.7832 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.4569 - regression_loss: 23.6232 - val_loss: 24.9416 - val_regression_loss: 19.3494 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.1193 - regression_loss: 23.5283 - val_loss: 24.3065 - val_regression_loss: 18.7915 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.3133 - regression_loss: 23.6189 - val_loss: 25.2882 - val_regression_loss: 19.7049 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.6280 - regression_loss: 24.1483 - val_loss: 24.0803 - val_regression_loss: 18.6060 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.7305 - regression_loss: 23.8315 - val_loss: 24.3374 - val_regression_loss: 18.8878 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.4070 - regression_loss: 23.0661 - val_loss: 24.4752 - val_regression_loss: 19.0418 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.2274 - regression_loss: 22.8215 - val_loss: 23.9003 - val_regression_loss: 18.5361 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.0288 - regression_loss: 22.7337 - val_loss: 24.7047 - val_regression_loss: 19.2927 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.1689 - regression_loss: 22.7710 - val_loss: 23.9132 - val_regression_loss: 18.5602 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.6732 - regression_loss: 22.9312 - val_loss: 24.6071 - val_regression_loss: 19.2182 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.4158 - regression_loss: 23.0160 - val_loss: 23.6970 - val_regression_loss: 18.3776 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.0196 - regression_loss: 22.7913 - val_loss: 24.3912 - val_regression_loss: 19.0111 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.8587 - regression_loss: 22.4747 - val_loss: 24.0829 - val_regression_loss: 18.7523 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.7733 - regression_loss: 22.6712 - val_loss: 23.8919 - val_regression_loss: 18.6039 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.2437 - regression_loss: 22.6574 - val_loss: 24.0480 - val_regression_loss: 18.7785 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.6075 - regression_loss: 22.7381 - val_loss: 23.8786 - val_regression_loss: 18.6152 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.1044 - regression_loss: 22.8641 - val_loss: 24.1917 - val_regression_loss: 18.9022 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.0661 - regression_loss: 23.2748 - val_loss: 23.7434 - val_regression_loss: 18.4781 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.3822 - regression_loss: 22.1392 - val_loss: 24.9181 - val_regression_loss: 19.5649 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.4747 - regression_loss: 22.2989 - val_loss: 23.5728 - val_regression_loss: 18.3131 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.6646 - regression_loss: 22.1962 - val_loss: 24.0998 - val_regression_loss: 18.8242 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.3715 - regression_loss: 22.2493 - val_loss: 23.8676 - val_regression_loss: 18.5710 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.6636 - regression_loss: 22.1828 - val_loss: 24.1841 - val_regression_loss: 18.8968 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.4288 - regression_loss: 22.0291 - val_loss: 23.8549 - val_regression_loss: 18.5887 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.1904 - regression_loss: 21.9131 - val_loss: 23.8446 - val_regression_loss: 18.5778 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.5513 - regression_loss: 21.8971 - val_loss: 24.4299 - val_regression_loss: 19.0956 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.1954 - regression_loss: 21.8593 - val_loss: 24.1202 - val_regression_loss: 18.8204 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.3420 - regression_loss: 21.8427 - val_loss: 24.2339 - val_regression_loss: 18.9043 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.4405 - regression_loss: 21.8069 - val_loss: 23.9192 - val_regression_loss: 18.6470 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.0136 - regression_loss: 21.7352 - val_loss: 24.2276 - val_regression_loss: 18.9289 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.7902 - regression_loss: 21.6638 - val_loss: 24.2083 - val_regression_loss: 18.8589 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.9836 - regression_loss: 21.7994 - val_loss: 24.3052 - val_regression_loss: 18.9326 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.3881 - regression_loss: 21.7646 - val_loss: 24.1984 - val_regression_loss: 18.8559 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.3525 - regression_loss: 21.7606 - val_loss: 24.7164 - val_regression_loss: 19.3104 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.5811 - regression_loss: 21.5175 - val_loss: 24.0167 - val_regression_loss: 18.6667 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.8433 - regression_loss: 21.7777 - val_loss: 25.0886 - val_regression_loss: 19.6585 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.0340 - regression_loss: 21.6203 - val_loss: 24.1472 - val_regression_loss: 18.7297 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.3267 - regression_loss: 21.9913 - val_loss: 25.0099 - val_regression_loss: 19.5158 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.4329 - regression_loss: 22.0132 - val_loss: 24.4099 - val_regression_loss: 19.0041 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 28.6011 - regression_loss: 26.7209\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.4599 - regression_loss: 21.6939 - val_loss: 24.4330 - val_regression_loss: 19.0418 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.7537 - regression_loss: 21.3576 - val_loss: 24.1969 - val_regression_loss: 18.8156 - lr: 5.0000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.4785 - regression_loss: 21.3473 - val_loss: 24.4348 - val_regression_loss: 19.0178 - lr: 5.0000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.6098 - regression_loss: 21.3367 - val_loss: 24.7139 - val_regression_loss: 19.2550 - lr: 5.0000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.5385 - regression_loss: 21.2847 - val_loss: 24.6456 - val_regression_loss: 19.1940 - lr: 5.0000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.8711 - regression_loss: 21.2590 - val_loss: 24.5312 - val_regression_loss: 19.1032 - lr: 5.0000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.3910 - regression_loss: 21.3242 - val_loss: 24.6183 - val_regression_loss: 19.1776 - lr: 5.0000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.4625 - regression_loss: 21.2629 - val_loss: 24.3124 - val_regression_loss: 18.8925 - lr: 5.0000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.8046 - regression_loss: 21.2693 - val_loss: 24.5874 - val_regression_loss: 19.1435 - lr: 5.0000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.8972 - regression_loss: 21.2100 - val_loss: 24.5036 - val_regression_loss: 19.0526 - lr: 5.0000e-05\n",
      "Epoch 75/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 26.4409 - regression_loss: 24.5619\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.9175 - regression_loss: 21.2596 - val_loss: 24.4135 - val_regression_loss: 18.9630 - lr: 5.0000e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.4147 - regression_loss: 21.1672 - val_loss: 24.8126 - val_regression_loss: 19.3246 - lr: 2.5000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.5115 - regression_loss: 21.1650 - val_loss: 24.7394 - val_regression_loss: 19.2611 - lr: 2.5000e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.6786 - regression_loss: 21.1398 - val_loss: 24.6527 - val_regression_loss: 19.1768 - lr: 2.5000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.2934 - regression_loss: 21.1225 - val_loss: 24.6532 - val_regression_loss: 19.1709 - lr: 2.5000e-05\n",
      "Epoch 80/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 23.1686 - regression_loss: 21.2899\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.3208 - regression_loss: 21.1474 - val_loss: 24.5341 - val_regression_loss: 19.0553 - lr: 2.5000e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.4354 - regression_loss: 21.1217 - val_loss: 24.5591 - val_regression_loss: 19.0795 - lr: 1.2500e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.5856 - regression_loss: 21.0797 - val_loss: 24.6592 - val_regression_loss: 19.1731 - lr: 1.2500e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.4551 - regression_loss: 21.0732 - val_loss: 24.8462 - val_regression_loss: 19.3447 - lr: 1.2500e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.4966 - regression_loss: 21.1062 - val_loss: 24.8938 - val_regression_loss: 19.3849 - lr: 1.2500e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.4575 - regression_loss: 21.0817 - val_loss: 24.7263 - val_regression_loss: 19.2293 - lr: 1.2500e-05\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 24ms/step - loss: 68.0395 - regression_loss: 63.8978 - val_loss: 44.2981 - val_regression_loss: 34.3424 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 60.0166 - regression_loss: 54.2167 - val_loss: 41.4977 - val_regression_loss: 32.3207 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 52.8814 - regression_loss: 47.3518 - val_loss: 38.9618 - val_regression_loss: 30.2803 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 48.6642 - regression_loss: 43.4223 - val_loss: 36.1423 - val_regression_loss: 28.0917 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 45.0940 - regression_loss: 39.8919 - val_loss: 33.5152 - val_regression_loss: 26.0759 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 42.7878 - regression_loss: 37.6220 - val_loss: 31.3068 - val_regression_loss: 24.0845 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 39.5009 - regression_loss: 34.9590 - val_loss: 30.1450 - val_regression_loss: 23.4827 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 37.5223 - regression_loss: 33.3427 - val_loss: 28.8905 - val_regression_loss: 22.3949 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 35.9000 - regression_loss: 31.5638 - val_loss: 27.6751 - val_regression_loss: 21.4115 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 33.4654 - regression_loss: 29.9105 - val_loss: 26.3102 - val_regression_loss: 20.4319 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.4214 - regression_loss: 28.4397 - val_loss: 25.2213 - val_regression_loss: 19.5320 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.2442 - regression_loss: 27.0557 - val_loss: 24.4853 - val_regression_loss: 18.9226 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.9431 - regression_loss: 25.8180 - val_loss: 23.5322 - val_regression_loss: 18.2540 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.5464 - regression_loss: 24.6566 - val_loss: 22.6427 - val_regression_loss: 17.3979 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.8100 - regression_loss: 23.4347 - val_loss: 21.8852 - val_regression_loss: 16.9316 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.2512 - regression_loss: 22.6268 - val_loss: 21.1319 - val_regression_loss: 16.2289 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.7722 - regression_loss: 21.6506 - val_loss: 21.0240 - val_regression_loss: 16.2595 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.8607 - regression_loss: 20.7692 - val_loss: 20.3049 - val_regression_loss: 15.5643 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.2476 - regression_loss: 19.8943 - val_loss: 19.6734 - val_regression_loss: 14.9857 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.8417 - regression_loss: 19.2412 - val_loss: 19.3949 - val_regression_loss: 14.9076 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.5116 - regression_loss: 18.6016 - val_loss: 18.9285 - val_regression_loss: 14.3979 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.1403 - regression_loss: 17.9535 - val_loss: 18.8001 - val_regression_loss: 14.3918 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.2109 - regression_loss: 17.4242 - val_loss: 18.3322 - val_regression_loss: 13.8360 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.0942 - regression_loss: 17.0503 - val_loss: 18.1271 - val_regression_loss: 13.8788 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.8536 - regression_loss: 16.6728 - val_loss: 18.0195 - val_regression_loss: 13.4925 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.6414 - regression_loss: 16.5278 - val_loss: 18.6041 - val_regression_loss: 14.4208 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.3551 - regression_loss: 17.0012 - val_loss: 19.0190 - val_regression_loss: 14.0415 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4460 - regression_loss: 16.9394 - val_loss: 18.3206 - val_regression_loss: 14.1210 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1565 - regression_loss: 16.0030 - val_loss: 18.1979 - val_regression_loss: 13.4570 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.9917 - regression_loss: 15.8425 - val_loss: 17.5469 - val_regression_loss: 13.3918 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.5989 - regression_loss: 15.6980 - val_loss: 16.8702 - val_regression_loss: 12.4655 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4240 - regression_loss: 14.6780 - val_loss: 16.8559 - val_regression_loss: 12.7327 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2222 - regression_loss: 14.5300 - val_loss: 17.3885 - val_regression_loss: 12.8435 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.8211 - regression_loss: 14.6694 - val_loss: 16.6598 - val_regression_loss: 12.5641 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.1821 - regression_loss: 14.9365 - val_loss: 16.8740 - val_regression_loss: 12.3524 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6634 - regression_loss: 14.5388 - val_loss: 16.4974 - val_regression_loss: 12.3778 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5798 - regression_loss: 14.1276 - val_loss: 16.6616 - val_regression_loss: 12.2038 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7577 - regression_loss: 14.3073 - val_loss: 16.0119 - val_regression_loss: 11.9102 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7386 - regression_loss: 13.9703 - val_loss: 15.8526 - val_regression_loss: 11.6490 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.7073 - regression_loss: 13.6821 - val_loss: 15.7276 - val_regression_loss: 11.6233 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.0233 - regression_loss: 13.4416 - val_loss: 15.6627 - val_regression_loss: 11.4690 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.4797 - regression_loss: 13.5819 - val_loss: 15.7666 - val_regression_loss: 11.5469 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.1770 - regression_loss: 13.4024 - val_loss: 15.6595 - val_regression_loss: 11.5461 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.9671 - regression_loss: 13.3594 - val_loss: 15.5584 - val_regression_loss: 11.3862 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.8260 - regression_loss: 13.0637 - val_loss: 15.5369 - val_regression_loss: 11.4023 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5213 - regression_loss: 13.5552 - val_loss: 15.7018 - val_regression_loss: 11.4167 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.8541 - regression_loss: 13.0944 - val_loss: 15.3315 - val_regression_loss: 11.1574 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.6904 - regression_loss: 12.8852 - val_loss: 15.4093 - val_regression_loss: 11.1857 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.9092 - regression_loss: 12.9249 - val_loss: 15.6920 - val_regression_loss: 11.4977 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.7016 - regression_loss: 12.7596 - val_loss: 15.3344 - val_regression_loss: 11.1046 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.7640 - regression_loss: 12.7838 - val_loss: 15.2159 - val_regression_loss: 11.0373 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.6861 - regression_loss: 12.6616 - val_loss: 15.5162 - val_regression_loss: 11.2075 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.6623 - regression_loss: 12.6598 - val_loss: 15.5598 - val_regression_loss: 11.3751 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5609 - regression_loss: 12.6607 - val_loss: 15.0952 - val_regression_loss: 10.8703 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.1290 - regression_loss: 12.5962 - val_loss: 15.2412 - val_regression_loss: 11.0730 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.1416 - regression_loss: 12.5538 - val_loss: 15.5349 - val_regression_loss: 11.1942 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.0464 - regression_loss: 12.5465 - val_loss: 15.1891 - val_regression_loss: 10.9613 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.1787 - regression_loss: 12.3897 - val_loss: 14.9820 - val_regression_loss: 10.7455 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.3428 - regression_loss: 12.6044 - val_loss: 15.0826 - val_regression_loss: 10.9301 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.0541 - regression_loss: 12.3559 - val_loss: 15.2493 - val_regression_loss: 10.9647 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.9510 - regression_loss: 12.2823 - val_loss: 15.1287 - val_regression_loss: 10.9275 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.0475 - regression_loss: 12.2482 - val_loss: 14.9843 - val_regression_loss: 10.7714 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.9387 - regression_loss: 12.1669 - val_loss: 15.0848 - val_regression_loss: 10.8068 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.7610 - regression_loss: 12.2796 - val_loss: 15.1117 - val_regression_loss: 10.8953 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.0572 - regression_loss: 12.1557 - val_loss: 14.9289 - val_regression_loss: 10.6678 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.6249 - regression_loss: 12.1047 - val_loss: 15.2347 - val_regression_loss: 10.8990 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.7599 - regression_loss: 12.0859 - val_loss: 15.1271 - val_regression_loss: 10.8392 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.8066 - regression_loss: 12.1446 - val_loss: 14.8433 - val_regression_loss: 10.5996 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.5896 - regression_loss: 11.9402 - val_loss: 15.0713 - val_regression_loss: 10.7551 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.5826 - regression_loss: 11.9419 - val_loss: 15.3481 - val_regression_loss: 11.0435 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.8289 - regression_loss: 11.9969 - val_loss: 15.5971 - val_regression_loss: 11.0810 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.8748 - regression_loss: 12.0201 - val_loss: 15.7867 - val_regression_loss: 11.4826 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.7245 - regression_loss: 12.2643 - val_loss: 15.6952 - val_regression_loss: 11.1918 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.6156 - regression_loss: 12.6106 - val_loss: 14.8967 - val_regression_loss: 10.6167 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.6672 - regression_loss: 11.8026\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.5935 - regression_loss: 11.9663 - val_loss: 14.9002 - val_regression_loss: 10.6753 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.2601 - regression_loss: 11.7394 - val_loss: 14.8515 - val_regression_loss: 10.5816 - lr: 5.0000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.2067 - regression_loss: 11.6466 - val_loss: 14.8705 - val_regression_loss: 10.6085 - lr: 5.0000e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.4382 - regression_loss: 11.6127 - val_loss: 14.8728 - val_regression_loss: 10.6073 - lr: 5.0000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.2518 - regression_loss: 11.5848 - val_loss: 14.8910 - val_regression_loss: 10.6110 - lr: 5.0000e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1474 - regression_loss: 11.5575 - val_loss: 14.8195 - val_regression_loss: 10.5682 - lr: 5.0000e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.3000 - regression_loss: 11.6108 - val_loss: 14.8121 - val_regression_loss: 10.5448 - lr: 5.0000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1286 - regression_loss: 11.4935 - val_loss: 14.9032 - val_regression_loss: 10.6359 - lr: 5.0000e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.2613 - regression_loss: 11.5114 - val_loss: 14.9643 - val_regression_loss: 10.6643 - lr: 5.0000e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1732 - regression_loss: 11.5227 - val_loss: 14.8375 - val_regression_loss: 10.5668 - lr: 5.0000e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.0496 - regression_loss: 11.4706 - val_loss: 14.8962 - val_regression_loss: 10.6164 - lr: 5.0000e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.0683 - regression_loss: 11.4644 - val_loss: 14.7707 - val_regression_loss: 10.5006 - lr: 5.0000e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.2309 - regression_loss: 11.4255 - val_loss: 14.8814 - val_regression_loss: 10.5948 - lr: 5.0000e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.3264 - regression_loss: 11.6033 - val_loss: 14.9685 - val_regression_loss: 10.6482 - lr: 5.0000e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.2236 - regression_loss: 11.4280 - val_loss: 14.8984 - val_regression_loss: 10.6018 - lr: 5.0000e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8272 - regression_loss: 11.4011 - val_loss: 14.8503 - val_regression_loss: 10.5856 - lr: 5.0000e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.0896 - regression_loss: 11.4188 - val_loss: 14.8458 - val_regression_loss: 10.5403 - lr: 5.0000e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1238 - regression_loss: 11.3738 - val_loss: 14.8841 - val_regression_loss: 10.6178 - lr: 5.0000e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.0506 - regression_loss: 11.3206 - val_loss: 14.9300 - val_regression_loss: 10.6144 - lr: 5.0000e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.8552 - regression_loss: 11.3579 - val_loss: 14.9384 - val_regression_loss: 10.6395 - lr: 5.0000e-05\n",
      "Epoch 95/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.0639 - regression_loss: 13.2011\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.0724 - regression_loss: 11.3133 - val_loss: 14.8291 - val_regression_loss: 10.5514 - lr: 5.0000e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.9781 - regression_loss: 11.2487 - val_loss: 14.8454 - val_regression_loss: 10.5590 - lr: 2.5000e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8119 - regression_loss: 11.2568 - val_loss: 14.8240 - val_regression_loss: 10.5400 - lr: 2.5000e-05\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8346 - regression_loss: 11.2195 - val_loss: 14.8263 - val_regression_loss: 10.5365 - lr: 2.5000e-05\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8759 - regression_loss: 11.2112 - val_loss: 14.8571 - val_regression_loss: 10.5653 - lr: 2.5000e-05\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.9736 - regression_loss: 11.3068 - val_loss: 14.8784 - val_regression_loss: 10.5859 - lr: 2.5000e-05\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8738 - regression_loss: 11.1876 - val_loss: 14.9064 - val_regression_loss: 10.5901 - lr: 2.5000e-05\n",
      "Epoch 102/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.4257 - regression_loss: 10.5631\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.8511 - regression_loss: 11.2439 - val_loss: 14.8943 - val_regression_loss: 10.5781 - lr: 2.5000e-05\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6114 - regression_loss: 11.1775 - val_loss: 14.9019 - val_regression_loss: 10.5978 - lr: 1.2500e-05\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.9875 - regression_loss: 11.1945 - val_loss: 14.8612 - val_regression_loss: 10.5722 - lr: 1.2500e-05\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.9886 - regression_loss: 11.1847 - val_loss: 14.8605 - val_regression_loss: 10.5612 - lr: 1.2500e-05\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8262 - regression_loss: 11.1697 - val_loss: 14.8721 - val_regression_loss: 10.5695 - lr: 1.2500e-05\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7906 - regression_loss: 11.1595 - val_loss: 14.8728 - val_regression_loss: 10.5653 - lr: 1.2500e-05\n",
      "Epoch 108/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 14.3771 - regression_loss: 12.5147\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.9586 - regression_loss: 11.1546 - val_loss: 14.8696 - val_regression_loss: 10.5730 - lr: 1.2500e-05\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6787 - regression_loss: 11.1477 - val_loss: 14.8717 - val_regression_loss: 10.5755 - lr: 6.2500e-06\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8214 - regression_loss: 11.1476 - val_loss: 14.8542 - val_regression_loss: 10.5567 - lr: 6.2500e-06\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6860 - regression_loss: 11.1356 - val_loss: 14.8648 - val_regression_loss: 10.5637 - lr: 6.2500e-06\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8347 - regression_loss: 11.1397 - val_loss: 14.8747 - val_regression_loss: 10.5744 - lr: 6.2500e-06\n",
      "Epoch 113/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.7174 - regression_loss: 10.8551\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8058 - regression_loss: 11.1306 - val_loss: 14.8706 - val_regression_loss: 10.5694 - lr: 6.2500e-06\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6484 - regression_loss: 11.1264 - val_loss: 14.8722 - val_regression_loss: 10.5712 - lr: 3.1250e-06\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8958 - regression_loss: 11.1291 - val_loss: 14.8641 - val_regression_loss: 10.5650 - lr: 3.1250e-06\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7503 - regression_loss: 11.1261 - val_loss: 14.8693 - val_regression_loss: 10.5680 - lr: 3.1250e-06\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.9002 - regression_loss: 11.1250 - val_loss: 14.8720 - val_regression_loss: 10.5684 - lr: 3.1250e-06\n",
      "Epoch 118/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 16.7401 - regression_loss: 14.8778\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8535 - regression_loss: 11.1295 - val_loss: 14.8723 - val_regression_loss: 10.5661 - lr: 3.1250e-06\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7361 - regression_loss: 11.1226 - val_loss: 14.8698 - val_regression_loss: 10.5661 - lr: 1.5625e-06\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8086 - regression_loss: 11.1194 - val_loss: 14.8698 - val_regression_loss: 10.5672 - lr: 1.5625e-06\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7827 - regression_loss: 11.1176 - val_loss: 14.8692 - val_regression_loss: 10.5672 - lr: 1.5625e-06\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7679 - regression_loss: 11.1188 - val_loss: 14.8702 - val_regression_loss: 10.5678 - lr: 1.5625e-06\n",
      "Epoch 123/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 16.6944 - regression_loss: 14.8322\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6876 - regression_loss: 11.1178 - val_loss: 14.8712 - val_regression_loss: 10.5681 - lr: 1.5625e-06\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8922 - regression_loss: 11.1168 - val_loss: 14.8712 - val_regression_loss: 10.5690 - lr: 7.8125e-07\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7662 - regression_loss: 11.1156 - val_loss: 14.8712 - val_regression_loss: 10.5686 - lr: 7.8125e-07\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.9002 - regression_loss: 11.1147 - val_loss: 14.8722 - val_regression_loss: 10.5696 - lr: 7.8125e-07\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 81.4487 - regression_loss: 73.6952 - val_loss: 56.5900 - val_regression_loss: 47.9344 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 65.4066 - regression_loss: 58.2664 - val_loss: 44.1703 - val_regression_loss: 37.8912 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 55.9251 - regression_loss: 50.3113 - val_loss: 38.1422 - val_regression_loss: 33.0304 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 49.0744 - regression_loss: 44.0440 - val_loss: 33.0423 - val_regression_loss: 28.4002 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 44.0190 - regression_loss: 39.0655 - val_loss: 29.3783 - val_regression_loss: 25.1335 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 38.8542 - regression_loss: 34.5423 - val_loss: 26.3529 - val_regression_loss: 22.3422 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 35.0852 - regression_loss: 30.6333 - val_loss: 23.4844 - val_regression_loss: 19.6333 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.1962 - regression_loss: 27.1141 - val_loss: 21.2714 - val_regression_loss: 17.4628 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.3676 - regression_loss: 24.0478 - val_loss: 19.0595 - val_regression_loss: 15.3745 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.7560 - regression_loss: 21.3669 - val_loss: 17.5168 - val_regression_loss: 13.8455 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.7790 - regression_loss: 19.3302 - val_loss: 16.0862 - val_regression_loss: 12.4283 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6763 - regression_loss: 17.5341 - val_loss: 15.4473 - val_regression_loss: 11.6691 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.7801 - regression_loss: 16.5838 - val_loss: 13.9483 - val_regression_loss: 10.2285 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.0901 - regression_loss: 15.1582 - val_loss: 13.7839 - val_regression_loss: 10.0631 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.9225 - regression_loss: 15.0139 - val_loss: 12.8281 - val_regression_loss: 9.2324 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5940 - regression_loss: 13.9080 - val_loss: 11.9330 - val_regression_loss: 8.3590 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.3286 - regression_loss: 13.5465 - val_loss: 12.8983 - val_regression_loss: 9.0996 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.9891 - regression_loss: 12.9976 - val_loss: 11.4812 - val_regression_loss: 7.8091 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.3488 - regression_loss: 12.6500 - val_loss: 12.3507 - val_regression_loss: 8.5876 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.1264 - regression_loss: 12.4099 - val_loss: 11.1478 - val_regression_loss: 7.4768 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.9410 - regression_loss: 12.0114 - val_loss: 11.5283 - val_regression_loss: 7.8068 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.4760 - regression_loss: 11.8402 - val_loss: 11.0749 - val_regression_loss: 7.4033 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.2997 - regression_loss: 11.5340 - val_loss: 11.0307 - val_regression_loss: 7.3314 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8482 - regression_loss: 11.2676 - val_loss: 10.6050 - val_regression_loss: 6.9270 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8352 - regression_loss: 11.2729 - val_loss: 11.2182 - val_regression_loss: 7.4659 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.5763 - regression_loss: 11.0969 - val_loss: 10.9458 - val_regression_loss: 7.1988 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.4690 - regression_loss: 10.9150 - val_loss: 10.7472 - val_regression_loss: 7.0249 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.4306 - regression_loss: 10.8296 - val_loss: 11.8051 - val_regression_loss: 7.9071 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.4813 - regression_loss: 10.7747 - val_loss: 10.3208 - val_regression_loss: 6.6333 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.4579 - regression_loss: 11.0838 - val_loss: 12.3316 - val_regression_loss: 8.3062 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.7261 - regression_loss: 11.0852 - val_loss: 10.5476 - val_regression_loss: 6.8152 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.2638 - regression_loss: 10.6289 - val_loss: 10.2801 - val_regression_loss: 6.5773 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1688 - regression_loss: 10.5008 - val_loss: 11.7792 - val_regression_loss: 7.8600 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0556 - regression_loss: 10.4967 - val_loss: 10.4499 - val_regression_loss: 6.7087 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0307 - regression_loss: 10.3341 - val_loss: 10.5520 - val_regression_loss: 6.8127 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6283 - regression_loss: 10.1797 - val_loss: 10.7784 - val_regression_loss: 6.9954 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7184 - regression_loss: 10.0801 - val_loss: 10.5326 - val_regression_loss: 6.7919 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6504 - regression_loss: 10.0897 - val_loss: 11.0074 - val_regression_loss: 7.1679 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6014 - regression_loss: 10.1294 - val_loss: 10.4639 - val_regression_loss: 6.7085 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3467 - regression_loss: 9.8754 - val_loss: 11.5727 - val_regression_loss: 7.6336 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8719 - regression_loss: 10.2618 - val_loss: 10.7548 - val_regression_loss: 6.9623 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4181 - regression_loss: 10.0141 - val_loss: 10.5793 - val_regression_loss: 6.8064 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4767 - regression_loss: 9.8478 - val_loss: 10.5930 - val_regression_loss: 6.8246 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3918 - regression_loss: 9.7981 - val_loss: 10.5043 - val_regression_loss: 6.7269 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2533 - regression_loss: 9.7728 - val_loss: 10.9716 - val_regression_loss: 7.0994 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1322 - regression_loss: 9.6807 - val_loss: 10.4810 - val_regression_loss: 6.7258 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2586 - regression_loss: 9.7814 - val_loss: 11.1711 - val_regression_loss: 7.2862 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3451 - regression_loss: 9.8588 - val_loss: 11.2025 - val_regression_loss: 7.3047 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1550 - regression_loss: 9.7410 - val_loss: 10.3573 - val_regression_loss: 6.6327 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2846 - regression_loss: 9.7043 - val_loss: 10.7556 - val_regression_loss: 6.9561 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.6569 - regression_loss: 11.8112\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1697 - regression_loss: 9.6393 - val_loss: 11.5440 - val_regression_loss: 7.5916 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1742 - regression_loss: 9.6084 - val_loss: 10.3912 - val_regression_loss: 6.6470 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0832 - regression_loss: 9.6490 - val_loss: 11.1474 - val_regression_loss: 7.2585 - lr: 5.0000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2732 - regression_loss: 9.6530 - val_loss: 10.8188 - val_regression_loss: 6.9877 - lr: 5.0000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0843 - regression_loss: 9.4762 - val_loss: 10.7135 - val_regression_loss: 6.8966 - lr: 5.0000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9265 - regression_loss: 9.4737 - val_loss: 10.7973 - val_regression_loss: 6.9697 - lr: 5.0000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8796 - regression_loss: 9.4342 - val_loss: 10.7101 - val_regression_loss: 6.9108 - lr: 5.0000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8269 - regression_loss: 9.4107 - val_loss: 10.6098 - val_regression_loss: 6.8293 - lr: 5.0000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8747 - regression_loss: 9.4226 - val_loss: 10.5705 - val_regression_loss: 6.7997 - lr: 5.0000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0225 - regression_loss: 9.3898 - val_loss: 10.8854 - val_regression_loss: 7.0398 - lr: 5.0000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7436 - regression_loss: 9.4016 - val_loss: 10.5920 - val_regression_loss: 6.7985 - lr: 5.0000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9420 - regression_loss: 9.4558 - val_loss: 10.8095 - val_regression_loss: 6.9845 - lr: 5.0000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.8703 - regression_loss: 9.3968 - val_loss: 10.8179 - val_regression_loss: 6.9974 - lr: 5.0000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.7988 - regression_loss: 9.3537 - val_loss: 10.4613 - val_regression_loss: 6.6945 - lr: 5.0000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.7012 - regression_loss: 9.4691 - val_loss: 11.3353 - val_regression_loss: 7.3947 - lr: 5.0000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.0675 - regression_loss: 9.5967 - val_loss: 10.4810 - val_regression_loss: 6.7078 - lr: 5.0000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.9600 - regression_loss: 9.3224 - val_loss: 11.1420 - val_regression_loss: 7.2530 - lr: 5.0000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.0426 - regression_loss: 9.3992 - val_loss: 10.4657 - val_regression_loss: 6.6987 - lr: 5.0000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.9129 - regression_loss: 9.5246 - val_loss: 11.2102 - val_regression_loss: 7.3039 - lr: 5.0000e-05\n",
      "Epoch 70/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.1979 - regression_loss: 10.3552\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9388 - regression_loss: 9.4217 - val_loss: 10.5636 - val_regression_loss: 6.7693 - lr: 5.0000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9112 - regression_loss: 9.2918 - val_loss: 10.7468 - val_regression_loss: 6.9213 - lr: 2.5000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6715 - regression_loss: 9.2895 - val_loss: 10.9530 - val_regression_loss: 7.0891 - lr: 2.5000e-05\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 156.0752 - regression_loss: 141.8490 - val_loss: 119.2171 - val_regression_loss: 93.6755 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 117.7696 - regression_loss: 107.8881 - val_loss: 95.2448 - val_regression_loss: 74.3658 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 98.3080 - regression_loss: 89.2424 - val_loss: 82.2170 - val_regression_loss: 64.3875 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 84.8508 - regression_loss: 76.4034 - val_loss: 71.3884 - val_regression_loss: 55.8925 - lr: 1.0000e-04\n",
      "Epoch 5/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 73.1748 - regression_loss: 65.7901 - val_loss: 64.1231 - val_regression_loss: 49.6490 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 64.5703 - regression_loss: 58.5491 - val_loss: 58.5066 - val_regression_loss: 44.5500 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 54.5653 - regression_loss: 52.6232 - val_loss: 53.7112 - val_regression_loss: 40.8034 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 54.8768 - regression_loss: 48.6949 - val_loss: 49.5384 - val_regression_loss: 37.7032 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 49.4926 - regression_loss: 43.8624 - val_loss: 45.8522 - val_regression_loss: 34.4315 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 45.4491 - regression_loss: 40.4837 - val_loss: 42.5512 - val_regression_loss: 31.9909 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 41.6781 - regression_loss: 37.2924 - val_loss: 39.6226 - val_regression_loss: 29.5648 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 39.4767 - regression_loss: 34.4700 - val_loss: 36.8392 - val_regression_loss: 27.2422 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 36.5595 - regression_loss: 32.0098 - val_loss: 34.2926 - val_regression_loss: 25.1960 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 33.3582 - regression_loss: 29.6778 - val_loss: 31.8551 - val_regression_loss: 23.2269 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31.4494 - regression_loss: 27.5741 - val_loss: 29.5011 - val_regression_loss: 21.3208 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.1649 - regression_loss: 25.6178 - val_loss: 27.3116 - val_regression_loss: 19.5096 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.1529 - regression_loss: 24.1026 - val_loss: 25.7400 - val_regression_loss: 18.4752 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.6708 - regression_loss: 22.3696 - val_loss: 23.5917 - val_regression_loss: 16.5147 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.4088 - regression_loss: 20.9072 - val_loss: 22.2938 - val_regression_loss: 15.8096 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.2199 - regression_loss: 19.5218 - val_loss: 20.2317 - val_regression_loss: 13.8673 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.7690 - regression_loss: 18.5907 - val_loss: 19.2963 - val_regression_loss: 13.4275 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.3427 - regression_loss: 17.5852 - val_loss: 17.3667 - val_regression_loss: 11.7799 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.7097 - regression_loss: 16.3841 - val_loss: 16.3187 - val_regression_loss: 11.0638 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.6861 - regression_loss: 15.5282 - val_loss: 15.5126 - val_regression_loss: 10.5111 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6729 - regression_loss: 14.5782 - val_loss: 14.3227 - val_regression_loss: 9.5130 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9592 - regression_loss: 13.8649 - val_loss: 13.6353 - val_regression_loss: 8.9895 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.4305 - regression_loss: 13.2994 - val_loss: 12.8926 - val_regression_loss: 8.4072 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5552 - regression_loss: 12.7989 - val_loss: 12.3785 - val_regression_loss: 8.0968 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1177 - regression_loss: 12.3370 - val_loss: 12.0513 - val_regression_loss: 7.9120 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.6862 - regression_loss: 11.8478 - val_loss: 11.3704 - val_regression_loss: 7.3657 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.0273 - regression_loss: 11.3924 - val_loss: 11.0299 - val_regression_loss: 7.1104 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8439 - regression_loss: 11.0368 - val_loss: 10.6527 - val_regression_loss: 6.8163 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.3812 - regression_loss: 10.7687 - val_loss: 10.3986 - val_regression_loss: 6.6643 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9835 - regression_loss: 10.4501 - val_loss: 10.2302 - val_regression_loss: 6.5070 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8605 - regression_loss: 10.2637 - val_loss: 10.8422 - val_regression_loss: 7.0853 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.9606 - regression_loss: 10.2912 - val_loss: 9.9239 - val_regression_loss: 6.2988 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.5136 - regression_loss: 9.7557 - val_loss: 9.8377 - val_regression_loss: 6.3087 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5157 - regression_loss: 9.7192 - val_loss: 9.5346 - val_regression_loss: 6.0563 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0452 - regression_loss: 9.3322 - val_loss: 9.3868 - val_regression_loss: 5.8733 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5716 - regression_loss: 9.1714 - val_loss: 9.8937 - val_regression_loss: 6.3608 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8069 - regression_loss: 9.3713 - val_loss: 9.6618 - val_regression_loss: 6.1151 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9070 - regression_loss: 9.2808 - val_loss: 9.1012 - val_regression_loss: 5.7255 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1526 - regression_loss: 8.7779 - val_loss: 9.1945 - val_regression_loss: 5.8065 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9926 - regression_loss: 8.7496 - val_loss: 9.0602 - val_regression_loss: 5.6745 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7756 - regression_loss: 8.3824 - val_loss: 9.1812 - val_regression_loss: 5.8258 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8498 - regression_loss: 8.3598 - val_loss: 9.0360 - val_regression_loss: 5.6468 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7717 - regression_loss: 8.2830 - val_loss: 8.7288 - val_regression_loss: 5.4232 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.4222 - regression_loss: 8.1063 - val_loss: 8.7111 - val_regression_loss: 5.4201 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6741 - regression_loss: 8.0383 - val_loss: 8.5955 - val_regression_loss: 5.3404 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.4805 - regression_loss: 7.9246 - val_loss: 8.5980 - val_regression_loss: 5.3439 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0314 - regression_loss: 7.9381 - val_loss: 8.6194 - val_regression_loss: 5.3463 - lr: 1.0000e-04\n",
      "Epoch 52/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2820 - regression_loss: 7.7403 - val_loss: 8.4522 - val_regression_loss: 5.2123 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.1437 - regression_loss: 7.7345 - val_loss: 8.4859 - val_regression_loss: 5.2585 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4944 - regression_loss: 7.5626 - val_loss: 8.7224 - val_regression_loss: 5.4685 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0130 - regression_loss: 7.7184 - val_loss: 10.1699 - val_regression_loss: 6.5498 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1226 - regression_loss: 8.6780 - val_loss: 8.4333 - val_regression_loss: 5.2408 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0858 - regression_loss: 7.5484 - val_loss: 8.4996 - val_regression_loss: 5.2729 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2285 - regression_loss: 7.7056 - val_loss: 10.8725 - val_regression_loss: 7.1139 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.5707 - regression_loss: 13.7351\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4695 - regression_loss: 8.9835 - val_loss: 8.3018 - val_regression_loss: 5.1312 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4839 - regression_loss: 7.1437 - val_loss: 8.3772 - val_regression_loss: 5.1729 - lr: 5.0000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7169 - regression_loss: 7.2490 - val_loss: 8.2499 - val_regression_loss: 5.0782 - lr: 5.0000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3212 - regression_loss: 7.1843 - val_loss: 8.2613 - val_regression_loss: 5.0640 - lr: 5.0000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6388 - regression_loss: 7.2008 - val_loss: 8.1596 - val_regression_loss: 4.9927 - lr: 5.0000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5605 - regression_loss: 7.1251 - val_loss: 8.1121 - val_regression_loss: 4.9563 - lr: 5.0000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9496 - regression_loss: 7.0774 - val_loss: 8.1305 - val_regression_loss: 4.9703 - lr: 5.0000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3758 - regression_loss: 7.0956 - val_loss: 8.1619 - val_regression_loss: 5.0152 - lr: 5.0000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3682 - regression_loss: 7.0351 - val_loss: 8.3175 - val_regression_loss: 5.1313 - lr: 5.0000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4770 - regression_loss: 7.0950 - val_loss: 8.2048 - val_regression_loss: 5.0484 - lr: 5.0000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1952 - regression_loss: 7.1193 - val_loss: 8.1922 - val_regression_loss: 5.0173 - lr: 5.0000e-05\n",
      "Epoch 70/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.5598 - regression_loss: 9.7257\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3498 - regression_loss: 7.0670 - val_loss: 8.0400 - val_regression_loss: 4.9042 - lr: 5.0000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3114 - regression_loss: 6.9153 - val_loss: 8.0461 - val_regression_loss: 4.9097 - lr: 2.5000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3485 - regression_loss: 6.9056 - val_loss: 8.0419 - val_regression_loss: 4.9148 - lr: 2.5000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2367 - regression_loss: 6.8911 - val_loss: 8.0498 - val_regression_loss: 4.9285 - lr: 2.5000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3527 - regression_loss: 6.8639 - val_loss: 8.0345 - val_regression_loss: 4.9063 - lr: 2.5000e-05\n",
      "Epoch 75/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.8165 - regression_loss: 9.9827\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2431 - regression_loss: 6.9081 - val_loss: 8.0078 - val_regression_loss: 4.8778 - lr: 2.5000e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3786 - regression_loss: 6.8892 - val_loss: 8.0125 - val_regression_loss: 4.8816 - lr: 1.2500e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2118 - regression_loss: 6.8727 - val_loss: 8.0100 - val_regression_loss: 4.8890 - lr: 1.2500e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3315 - regression_loss: 6.8536 - val_loss: 8.0016 - val_regression_loss: 4.8769 - lr: 1.2500e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0077 - regression_loss: 6.8285 - val_loss: 7.9895 - val_regression_loss: 4.8687 - lr: 1.2500e-05\n",
      "Epoch 80/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.6786 - regression_loss: 4.8450\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3056 - regression_loss: 6.8444 - val_loss: 7.9880 - val_regression_loss: 4.8684 - lr: 1.2500e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9013 - regression_loss: 6.8029 - val_loss: 7.9841 - val_regression_loss: 4.8676 - lr: 6.2500e-06\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2062 - regression_loss: 6.8009 - val_loss: 7.9847 - val_regression_loss: 4.8677 - lr: 6.2500e-06\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1986 - regression_loss: 6.8006 - val_loss: 7.9911 - val_regression_loss: 4.8722 - lr: 6.2500e-06\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8674 - regression_loss: 6.7963 - val_loss: 7.9879 - val_regression_loss: 4.8705 - lr: 6.2500e-06\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1375 - regression_loss: 6.7951 - val_loss: 7.9812 - val_regression_loss: 4.8662 - lr: 6.2500e-06\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9720 - regression_loss: 6.7926 - val_loss: 7.9817 - val_regression_loss: 4.8646 - lr: 6.2500e-06\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1331 - regression_loss: 6.7901 - val_loss: 7.9849 - val_regression_loss: 4.8696 - lr: 6.2500e-06\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0773 - regression_loss: 6.7968 - val_loss: 7.9854 - val_regression_loss: 4.8677 - lr: 6.2500e-06\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5024 - regression_loss: 6.7812 - val_loss: 7.9748 - val_regression_loss: 4.8600 - lr: 6.2500e-06\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2302 - regression_loss: 6.7730 - val_loss: 7.9742 - val_regression_loss: 4.8594 - lr: 6.2500e-06\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1084 - regression_loss: 6.7711 - val_loss: 7.9728 - val_regression_loss: 4.8570 - lr: 6.2500e-06\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1732 - regression_loss: 6.7680 - val_loss: 7.9667 - val_regression_loss: 4.8521 - lr: 6.2500e-06\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2278 - regression_loss: 6.7651 - val_loss: 7.9675 - val_regression_loss: 4.8531 - lr: 6.2500e-06\n",
      "Epoch 94/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.1206 - regression_loss: 9.2873\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0326 - regression_loss: 6.7653 - val_loss: 7.9644 - val_regression_loss: 4.8504 - lr: 6.2500e-06\n",
      "Epoch 95/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0907 - regression_loss: 6.7642 - val_loss: 7.9663 - val_regression_loss: 4.8518 - lr: 3.1250e-06\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0710 - regression_loss: 6.7552 - val_loss: 7.9648 - val_regression_loss: 4.8509 - lr: 3.1250e-06\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1329 - regression_loss: 6.7543 - val_loss: 7.9615 - val_regression_loss: 4.8500 - lr: 3.1250e-06\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6078 - regression_loss: 6.7529 - val_loss: 7.9614 - val_regression_loss: 4.8507 - lr: 3.1250e-06\n",
      "Epoch 99/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.8587 - regression_loss: 6.0254\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1545 - regression_loss: 6.7521 - val_loss: 7.9629 - val_regression_loss: 4.8514 - lr: 3.1250e-06\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1062 - regression_loss: 6.7483 - val_loss: 7.9628 - val_regression_loss: 4.8515 - lr: 1.5625e-06\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2146 - regression_loss: 6.7496 - val_loss: 7.9603 - val_regression_loss: 4.8498 - lr: 1.5625e-06\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0346 - regression_loss: 6.7482 - val_loss: 7.9608 - val_regression_loss: 4.8497 - lr: 1.5625e-06\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6610 - regression_loss: 6.7455 - val_loss: 7.9601 - val_regression_loss: 4.8491 - lr: 1.5625e-06\n",
      "Epoch 104/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.6489 - regression_loss: 9.8157\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2277 - regression_loss: 6.7452 - val_loss: 7.9591 - val_regression_loss: 4.8476 - lr: 1.5625e-06\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1975 - regression_loss: 6.7439 - val_loss: 7.9591 - val_regression_loss: 4.8478 - lr: 7.8125e-07\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1028 - regression_loss: 6.7435 - val_loss: 7.9594 - val_regression_loss: 4.8484 - lr: 7.8125e-07\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1288 - regression_loss: 6.7433 - val_loss: 7.9592 - val_regression_loss: 4.8480 - lr: 7.8125e-07\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0053 - regression_loss: 6.7427 - val_loss: 7.9590 - val_regression_loss: 4.8480 - lr: 7.8125e-07\n",
      "Epoch 109/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.3630 - regression_loss: 5.5297\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1483 - regression_loss: 6.7420 - val_loss: 7.9588 - val_regression_loss: 4.8479 - lr: 7.8125e-07\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0669 - regression_loss: 6.7417 - val_loss: 7.9585 - val_regression_loss: 4.8475 - lr: 3.9062e-07\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0981 - regression_loss: 6.7415 - val_loss: 7.9583 - val_regression_loss: 4.8475 - lr: 3.9062e-07\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0728 - regression_loss: 6.7413 - val_loss: 7.9579 - val_regression_loss: 4.8472 - lr: 3.9062e-07\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6986 - regression_loss: 6.7410 - val_loss: 7.9578 - val_regression_loss: 4.8470 - lr: 3.9062e-07\n",
      "Epoch 114/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.9541 - regression_loss: 6.1209\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.1682 - regression_loss: 6.7406 - val_loss: 7.9576 - val_regression_loss: 4.8468 - lr: 3.9062e-07\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1213 - regression_loss: 6.7408 - val_loss: 7.9578 - val_regression_loss: 4.8468 - lr: 1.9531e-07\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.1241 - regression_loss: 6.7405 - val_loss: 7.9574 - val_regression_loss: 4.8467 - lr: 1.9531e-07\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1183 - regression_loss: 6.7402 - val_loss: 7.9575 - val_regression_loss: 4.8468 - lr: 1.9531e-07\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0778 - regression_loss: 6.7403 - val_loss: 7.9576 - val_regression_loss: 4.8468 - lr: 1.9531e-07\n",
      "Epoch 119/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.5863 - regression_loss: 5.7531\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1289 - regression_loss: 6.7402 - val_loss: 7.9574 - val_regression_loss: 4.8466 - lr: 1.9531e-07\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9489 - regression_loss: 6.7397 - val_loss: 7.9573 - val_regression_loss: 4.8465 - lr: 9.7656e-08\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7112 - regression_loss: 6.7398 - val_loss: 7.9572 - val_regression_loss: 4.8465 - lr: 9.7656e-08\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1767 - regression_loss: 6.7397 - val_loss: 7.9572 - val_regression_loss: 4.8466 - lr: 9.7656e-08\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0067 - regression_loss: 6.7397 - val_loss: 7.9573 - val_regression_loss: 4.8466 - lr: 9.7656e-08\n",
      "Epoch 124/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.5848 - regression_loss: 5.7515\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0396 - regression_loss: 6.7396 - val_loss: 7.9572 - val_regression_loss: 4.8465 - lr: 9.7656e-08\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4524 - regression_loss: 6.7395 - val_loss: 7.9572 - val_regression_loss: 4.8465 - lr: 4.8828e-08\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0246 - regression_loss: 6.7395 - val_loss: 7.9572 - val_regression_loss: 4.8465 - lr: 4.8828e-08\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1815 - regression_loss: 6.7394 - val_loss: 7.9572 - val_regression_loss: 4.8465 - lr: 4.8828e-08\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0918 - regression_loss: 6.7394 - val_loss: 7.9571 - val_regression_loss: 4.8465 - lr: 4.8828e-08\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1367 - regression_loss: 6.7394 - val_loss: 7.9571 - val_regression_loss: 4.8464 - lr: 4.8828e-08\n",
      "Epoch 130/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.5289 - regression_loss: 6.6956\n",
      "Epoch 130: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7955 - regression_loss: 6.7393 - val_loss: 7.9571 - val_regression_loss: 4.8464 - lr: 4.8828e-08\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1240 - regression_loss: 6.7392 - val_loss: 7.9571 - val_regression_loss: 4.8464 - lr: 2.4414e-08\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0260 - regression_loss: 6.7392 - val_loss: 7.9571 - val_regression_loss: 4.8464 - lr: 2.4414e-08\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0092 - regression_loss: 6.7392 - val_loss: 7.9571 - val_regression_loss: 4.8464 - lr: 2.4414e-08\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0106 - regression_loss: 6.7392 - val_loss: 7.9571 - val_regression_loss: 4.8464 - lr: 2.4414e-08\n",
      "Epoch 135/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.9195 - regression_loss: 6.0863\n",
      "Epoch 135: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 9.1531 - regression_loss: 6.7392 - val_loss: 7.9571 - val_regression_loss: 4.8464 - lr: 2.4414e-08\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1222 - regression_loss: 6.7392 - val_loss: 7.9571 - val_regression_loss: 4.8464 - lr: 1.2207e-08\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0361 - regression_loss: 6.7392 - val_loss: 7.9571 - val_regression_loss: 4.8464 - lr: 1.2207e-08\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0895 - regression_loss: 6.7392 - val_loss: 7.9571 - val_regression_loss: 4.8464 - lr: 1.2207e-08\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.1585 - regression_loss: 6.7392 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 1.2207e-08\n",
      "Epoch 140/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.2842 - regression_loss: 6.4509\n",
      "Epoch 140: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1078 - regression_loss: 6.7392 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 1.2207e-08\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1073 - regression_loss: 6.7392 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 6.1035e-09\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1309 - regression_loss: 6.7392 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 6.1035e-09\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1676 - regression_loss: 6.7392 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 6.1035e-09\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9949 - regression_loss: 6.7392 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 6.1035e-09\n",
      "Epoch 145/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.5630 - regression_loss: 7.7297\n",
      "Epoch 145: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6318 - regression_loss: 6.7392 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 6.1035e-09\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0347 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 3.0518e-09\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9231 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 3.0518e-09\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0017 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 3.0518e-09\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7149 - regression_loss: 6.7392 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 3.0518e-09\n",
      "Epoch 150/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.6898 - regression_loss: 5.8566\n",
      "Epoch 150: ReduceLROnPlateau reducing learning rate to 1.5258788677030566e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1785 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 3.0518e-09\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1082 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 1.5259e-09\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0539 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 1.5259e-09\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1563 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 1.5259e-09\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0380 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 1.5259e-09\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9691 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 1.5259e-09\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1319 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 1.5259e-09\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0679 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 1.5259e-09\n",
      "Epoch 158/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.2318 - regression_loss: 7.3986\n",
      "Epoch 158: ReduceLROnPlateau reducing learning rate to 7.629394338515283e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1775 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 1.5259e-09\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8644 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 7.6294e-10\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9500 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 7.6294e-10\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9765 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 7.6294e-10\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1051 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 7.6294e-10\n",
      "Epoch 163/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.5456 - regression_loss: 4.7123\n",
      "Epoch 163: ReduceLROnPlateau reducing learning rate to 3.8146971692576415e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1241 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 7.6294e-10\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1354 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 3.8147e-10\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0948 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 3.8147e-10\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1177 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 3.8147e-10\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0461 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 3.8147e-10\n",
      "Epoch 168/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.9885 - regression_loss: 9.1553\n",
      "Epoch 168: ReduceLROnPlateau reducing learning rate to 1.9073485846288207e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1244 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 3.8147e-10\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1337 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 1.9073e-10\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0386 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 1.9073e-10\n",
      "Epoch 171/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0862 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 1.9073e-10\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1884 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 1.9073e-10\n",
      "Epoch 173/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.7516 - regression_loss: 6.9183\n",
      "Epoch 173: ReduceLROnPlateau reducing learning rate to 9.536742923144104e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1531 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 1.9073e-10\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1098 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 9.5367e-11\n",
      "Epoch 175/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9467 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 9.5367e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1288 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 9.5367e-11\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0606 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 9.5367e-11\n",
      "Epoch 178/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.1384 - regression_loss: 8.3051\n",
      "Epoch 178: ReduceLROnPlateau reducing learning rate to 4.768371461572052e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0964 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 9.5367e-11\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0845 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 4.7684e-11\n",
      "Epoch 180/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1525 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 4.7684e-11\n",
      "Epoch 181/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1543 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 4.7684e-11\n",
      "Epoch 182/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5430 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 4.7684e-11\n",
      "Epoch 183/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.5295 - regression_loss: 6.6963\n",
      "Epoch 183: ReduceLROnPlateau reducing learning rate to 2.384185730786026e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0649 - regression_loss: 6.7391 - val_loss: 7.9570 - val_regression_loss: 4.8464 - lr: 4.7684e-11\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 130.6502 - regression_loss: 118.2871 - val_loss: 94.6678 - val_regression_loss: 72.6830 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 84.2134 - regression_loss: 78.5416 - val_loss: 74.8267 - val_regression_loss: 57.3101 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 64.9319 - regression_loss: 58.8714 - val_loss: 56.1120 - val_regression_loss: 42.5738 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 49.2040 - regression_loss: 43.9799 - val_loss: 44.3857 - val_regression_loss: 33.1697 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 39.4966 - regression_loss: 34.6476 - val_loss: 36.7201 - val_regression_loss: 26.9137 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 33.3783 - regression_loss: 29.4010 - val_loss: 31.5519 - val_regression_loss: 22.7873 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30.4227 - regression_loss: 26.2735 - val_loss: 28.7403 - val_regression_loss: 20.6607 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.2638 - regression_loss: 24.2449 - val_loss: 26.1003 - val_regression_loss: 18.7408 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.2104 - regression_loss: 22.5534 - val_loss: 24.1020 - val_regression_loss: 17.2929 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.9534 - regression_loss: 20.7125 - val_loss: 22.4858 - val_regression_loss: 16.0805 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.8629 - regression_loss: 19.4041 - val_loss: 21.4679 - val_regression_loss: 15.3304 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.7247 - regression_loss: 18.2187 - val_loss: 20.6097 - val_regression_loss: 14.7285 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.3433 - regression_loss: 17.1268 - val_loss: 19.5564 - val_regression_loss: 13.9601 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 19.5356 - regression_loss: 16.4117 - val_loss: 19.5502 - val_regression_loss: 14.0064 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.2769 - regression_loss: 15.6119 - val_loss: 18.3628 - val_regression_loss: 13.1334 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.6558 - regression_loss: 15.7842 - val_loss: 19.8370 - val_regression_loss: 14.2934 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.2592 - regression_loss: 15.0638 - val_loss: 18.1819 - val_regression_loss: 13.0592 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.9440 - regression_loss: 14.9371 - val_loss: 18.1947 - val_regression_loss: 13.0382 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.9563 - regression_loss: 14.1424 - val_loss: 16.0456 - val_regression_loss: 11.4020 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.3576 - regression_loss: 13.5007 - val_loss: 16.1298 - val_regression_loss: 11.4070 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.4059 - regression_loss: 12.8502 - val_loss: 14.9975 - val_regression_loss: 10.5472 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.0135 - regression_loss: 12.2774 - val_loss: 14.7583 - val_regression_loss: 10.3501 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.7326 - regression_loss: 11.8670 - val_loss: 14.1080 - val_regression_loss: 9.8578 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.3746 - regression_loss: 11.8398 - val_loss: 13.8076 - val_regression_loss: 9.6288 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.0029 - regression_loss: 11.4297 - val_loss: 13.8176 - val_regression_loss: 9.6132 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.9344 - regression_loss: 11.2218 - val_loss: 13.3382 - val_regression_loss: 9.2651 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.2829 - regression_loss: 11.6009 - val_loss: 13.2408 - val_regression_loss: 9.1552 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.4435 - regression_loss: 10.7941 - val_loss: 13.0844 - val_regression_loss: 9.0300 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.2660 - regression_loss: 10.6285 - val_loss: 12.7377 - val_regression_loss: 8.7887 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.9809 - regression_loss: 10.4664 - val_loss: 12.8799 - val_regression_loss: 8.8622 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.9752 - regression_loss: 10.4208 - val_loss: 12.4304 - val_regression_loss: 8.5212 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.5225 - regression_loss: 9.9405 - val_loss: 12.1593 - val_regression_loss: 8.3042 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.5064 - regression_loss: 9.8064 - val_loss: 11.9311 - val_regression_loss: 8.0985 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.1680 - regression_loss: 9.5952 - val_loss: 11.9059 - val_regression_loss: 8.1006 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.8677 - regression_loss: 9.3581 - val_loss: 11.6123 - val_regression_loss: 7.8570 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.6562 - regression_loss: 9.2241 - val_loss: 11.4779 - val_regression_loss: 7.7308 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.5084 - regression_loss: 9.1252 - val_loss: 11.3374 - val_regression_loss: 7.6267 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.5367 - regression_loss: 9.0001 - val_loss: 11.2019 - val_regression_loss: 7.5102 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.4065 - regression_loss: 8.8083 - val_loss: 11.0609 - val_regression_loss: 7.4064 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.0733 - regression_loss: 8.7131 - val_loss: 10.9159 - val_regression_loss: 7.2865 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.9842 - regression_loss: 8.4740 - val_loss: 10.8754 - val_regression_loss: 7.2309 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.8161 - regression_loss: 8.3926 - val_loss: 10.5946 - val_regression_loss: 7.0382 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7375 - regression_loss: 8.2261 - val_loss: 10.6143 - val_regression_loss: 7.0390 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.5897 - regression_loss: 8.1537 - val_loss: 10.5013 - val_regression_loss: 6.9345 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.4520 - regression_loss: 7.9738 - val_loss: 10.3290 - val_regression_loss: 6.8002 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.1145 - regression_loss: 7.8503 - val_loss: 10.4886 - val_regression_loss: 6.9145 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.1127 - regression_loss: 7.7092 - val_loss: 10.1729 - val_regression_loss: 6.7020 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.0014 - regression_loss: 7.6652 - val_loss: 10.0713 - val_regression_loss: 6.5847 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.8934 - regression_loss: 7.5004 - val_loss: 9.8817 - val_regression_loss: 6.4619 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.9219 - regression_loss: 7.4158 - val_loss: 9.9793 - val_regression_loss: 6.5248 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.7461 - regression_loss: 7.4199 - val_loss: 9.9454 - val_regression_loss: 6.5043 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5519 - regression_loss: 7.1956 - val_loss: 9.7022 - val_regression_loss: 6.3315 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4519 - regression_loss: 7.1240 - val_loss: 9.6230 - val_regression_loss: 6.2582 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3150 - regression_loss: 7.1376 - val_loss: 9.8324 - val_regression_loss: 6.3945 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3900 - regression_loss: 7.1802 - val_loss: 9.6317 - val_regression_loss: 6.3116 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3595 - regression_loss: 6.9455 - val_loss: 9.6878 - val_regression_loss: 6.2703 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3186 - regression_loss: 6.9430 - val_loss: 9.2570 - val_regression_loss: 5.9618 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.6269 - regression_loss: 6.6656 - val_loss: 9.1668 - val_regression_loss: 5.9182 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.4288 - regression_loss: 6.6218 - val_loss: 9.2071 - val_regression_loss: 5.9339 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.8420 - regression_loss: 6.5479 - val_loss: 9.4008 - val_regression_loss: 6.0444 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.8559 - regression_loss: 6.5743 - val_loss: 9.5092 - val_regression_loss: 6.2525 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.9389 - regression_loss: 6.7982 - val_loss: 9.8621 - val_regression_loss: 6.3828 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.6676 - regression_loss: 6.4408 - val_loss: 9.0692 - val_regression_loss: 5.8858 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.9207 - regression_loss: 7.1147\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.0471 - regression_loss: 6.7099 - val_loss: 8.8415 - val_regression_loss: 5.6355 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.5041 - regression_loss: 6.3094 - val_loss: 8.7121 - val_regression_loss: 5.5515 - lr: 5.0000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3528 - regression_loss: 6.1627 - val_loss: 8.7489 - val_regression_loss: 5.5667 - lr: 5.0000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3347 - regression_loss: 6.0486 - val_loss: 8.6072 - val_regression_loss: 5.4708 - lr: 5.0000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3349 - regression_loss: 6.0461 - val_loss: 8.6084 - val_regression_loss: 5.4554 - lr: 5.0000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2949 - regression_loss: 5.9966 - val_loss: 8.5663 - val_regression_loss: 5.4324 - lr: 5.0000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.9591 - regression_loss: 5.9745 - val_loss: 8.5122 - val_regression_loss: 5.3924 - lr: 5.0000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2105 - regression_loss: 6.0267 - val_loss: 8.5046 - val_regression_loss: 5.3972 - lr: 5.0000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2470 - regression_loss: 5.9627 - val_loss: 8.6577 - val_regression_loss: 5.4978 - lr: 5.0000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2411 - regression_loss: 5.9234 - val_loss: 8.4478 - val_regression_loss: 5.3607 - lr: 5.0000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8839 - regression_loss: 5.8880 - val_loss: 8.6292 - val_regression_loss: 5.4676 - lr: 5.0000e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.0191 - regression_loss: 5.8134 - val_loss: 8.3996 - val_regression_loss: 5.3176 - lr: 5.0000e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0477 - regression_loss: 5.8727 - val_loss: 8.3776 - val_regression_loss: 5.2775 - lr: 5.0000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.1071 - regression_loss: 5.7952 - val_loss: 8.2671 - val_regression_loss: 5.2046 - lr: 5.0000e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.0538 - regression_loss: 5.7728 - val_loss: 8.2937 - val_regression_loss: 5.2308 - lr: 5.0000e-05\n",
      "Epoch 79/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.9127 - regression_loss: 6.1092\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.9847 - regression_loss: 5.7149 - val_loss: 8.2906 - val_regression_loss: 5.2219 - lr: 5.0000e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6164 - regression_loss: 5.6586 - val_loss: 8.2720 - val_regression_loss: 5.2107 - lr: 2.5000e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8874 - regression_loss: 5.6548 - val_loss: 8.2393 - val_regression_loss: 5.1820 - lr: 2.5000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8854 - regression_loss: 5.6354 - val_loss: 8.2271 - val_regression_loss: 5.1783 - lr: 2.5000e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8951 - regression_loss: 5.6299 - val_loss: 8.2243 - val_regression_loss: 5.1757 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8576 - regression_loss: 5.6087 - val_loss: 8.2345 - val_regression_loss: 5.1804 - lr: 2.5000e-05\n",
      "Epoch 85/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.0589 - regression_loss: 5.2559\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.9247 - regression_loss: 5.6267 - val_loss: 8.2153 - val_regression_loss: 5.1740 - lr: 2.5000e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8448 - regression_loss: 5.5772 - val_loss: 8.1821 - val_regression_loss: 5.1522 - lr: 1.2500e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7725 - regression_loss: 5.5774 - val_loss: 8.1856 - val_regression_loss: 5.1461 - lr: 1.2500e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8399 - regression_loss: 5.5674 - val_loss: 8.1750 - val_regression_loss: 5.1380 - lr: 1.2500e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7097 - regression_loss: 5.5518 - val_loss: 8.1900 - val_regression_loss: 5.1462 - lr: 1.2500e-05\n",
      "Epoch 90/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.5504 - regression_loss: 6.7477\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7811 - regression_loss: 5.5625 - val_loss: 8.1606 - val_regression_loss: 5.1220 - lr: 1.2500e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6770 - regression_loss: 5.5430 - val_loss: 8.1512 - val_regression_loss: 5.1159 - lr: 6.2500e-06\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7102 - regression_loss: 5.5402 - val_loss: 8.1513 - val_regression_loss: 5.1166 - lr: 6.2500e-06\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7587 - regression_loss: 5.5359 - val_loss: 8.1458 - val_regression_loss: 5.1137 - lr: 6.2500e-06\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6964 - regression_loss: 5.5338 - val_loss: 8.1324 - val_regression_loss: 5.1041 - lr: 6.2500e-06\n",
      "Epoch 95/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.2598 - regression_loss: 5.4573\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7990 - regression_loss: 5.5333 - val_loss: 8.1294 - val_regression_loss: 5.1026 - lr: 6.2500e-06\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.5424 - regression_loss: 5.5238 - val_loss: 8.1328 - val_regression_loss: 5.1039 - lr: 3.1250e-06\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6904 - regression_loss: 5.5225 - val_loss: 8.1385 - val_regression_loss: 5.1073 - lr: 3.1250e-06\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7264 - regression_loss: 5.5196 - val_loss: 8.1418 - val_regression_loss: 5.1089 - lr: 3.1250e-06\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6484 - regression_loss: 5.5187 - val_loss: 8.1447 - val_regression_loss: 5.1109 - lr: 3.1250e-06\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6645 - regression_loss: 5.5205 - val_loss: 8.1328 - val_regression_loss: 5.1029 - lr: 3.1250e-06\n",
      "Epoch 101/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.8530 - regression_loss: 7.0505\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6235 - regression_loss: 5.5152 - val_loss: 8.1329 - val_regression_loss: 5.1035 - lr: 3.1250e-06\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7540 - regression_loss: 5.5125 - val_loss: 8.1309 - val_regression_loss: 5.1022 - lr: 1.5625e-06\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6539 - regression_loss: 5.5119 - val_loss: 8.1301 - val_regression_loss: 5.1014 - lr: 1.5625e-06\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6120 - regression_loss: 5.5114 - val_loss: 8.1269 - val_regression_loss: 5.0992 - lr: 1.5625e-06\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.4955 - regression_loss: 5.5101 - val_loss: 8.1284 - val_regression_loss: 5.1000 - lr: 1.5625e-06\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6606 - regression_loss: 5.5089 - val_loss: 8.1272 - val_regression_loss: 5.0991 - lr: 1.5625e-06\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7205 - regression_loss: 5.5085 - val_loss: 8.1245 - val_regression_loss: 5.0973 - lr: 1.5625e-06\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6484 - regression_loss: 5.5073 - val_loss: 8.1225 - val_regression_loss: 5.0959 - lr: 1.5625e-06\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6388 - regression_loss: 5.5069 - val_loss: 8.1197 - val_regression_loss: 5.0935 - lr: 1.5625e-06\n",
      "Epoch 110/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.6856 - regression_loss: 5.8831\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7646 - regression_loss: 5.5061 - val_loss: 8.1211 - val_regression_loss: 5.0944 - lr: 1.5625e-06\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8261 - regression_loss: 5.5047 - val_loss: 8.1223 - val_regression_loss: 5.0954 - lr: 7.8125e-07\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6989 - regression_loss: 5.5043 - val_loss: 8.1192 - val_regression_loss: 5.0932 - lr: 7.8125e-07\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7269 - regression_loss: 5.5033 - val_loss: 8.1179 - val_regression_loss: 5.0922 - lr: 7.8125e-07\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7535 - regression_loss: 5.5035 - val_loss: 8.1193 - val_regression_loss: 5.0931 - lr: 7.8125e-07\n",
      "Epoch 115/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.7699 - regression_loss: 3.9675\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7036 - regression_loss: 5.5038 - val_loss: 8.1161 - val_regression_loss: 5.0912 - lr: 7.8125e-07\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8002 - regression_loss: 5.5020 - val_loss: 8.1155 - val_regression_loss: 5.0907 - lr: 3.9062e-07\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7415 - regression_loss: 5.5018 - val_loss: 8.1160 - val_regression_loss: 5.0909 - lr: 3.9062e-07\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7229 - regression_loss: 5.5015 - val_loss: 8.1160 - val_regression_loss: 5.0909 - lr: 3.9062e-07\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6106 - regression_loss: 5.5013 - val_loss: 8.1158 - val_regression_loss: 5.0907 - lr: 3.9062e-07\n",
      "Epoch 120/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.2395 - regression_loss: 7.4371\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7242 - regression_loss: 5.5014 - val_loss: 8.1154 - val_regression_loss: 5.0905 - lr: 3.9062e-07\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6628 - regression_loss: 5.5007 - val_loss: 8.1156 - val_regression_loss: 5.0906 - lr: 1.9531e-07\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.5261 - regression_loss: 5.5007 - val_loss: 8.1162 - val_regression_loss: 5.0910 - lr: 1.9531e-07\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7225 - regression_loss: 5.5006 - val_loss: 8.1156 - val_regression_loss: 5.0905 - lr: 1.9531e-07\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6729 - regression_loss: 5.5003 - val_loss: 8.1155 - val_regression_loss: 5.0905 - lr: 1.9531e-07\n",
      "Epoch 125/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 [====>.........................] - ETA: 0s - loss: 7.4282 - regression_loss: 5.6258\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.5399 - regression_loss: 5.5003 - val_loss: 8.1155 - val_regression_loss: 5.0904 - lr: 1.9531e-07\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7094 - regression_loss: 5.5002 - val_loss: 8.1152 - val_regression_loss: 5.0902 - lr: 9.7656e-08\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7886 - regression_loss: 5.5001 - val_loss: 8.1152 - val_regression_loss: 5.0902 - lr: 9.7656e-08\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7387 - regression_loss: 5.5001 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 9.7656e-08\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.5325 - regression_loss: 5.5001 - val_loss: 8.1154 - val_regression_loss: 5.0904 - lr: 9.7656e-08\n",
      "Epoch 130/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.4941 - regression_loss: 6.6917\n",
      "Epoch 130: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6421 - regression_loss: 5.4999 - val_loss: 8.1153 - val_regression_loss: 5.0903 - lr: 9.7656e-08\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7611 - regression_loss: 5.4998 - val_loss: 8.1152 - val_regression_loss: 5.0903 - lr: 4.8828e-08\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7182 - regression_loss: 5.4997 - val_loss: 8.1151 - val_regression_loss: 5.0902 - lr: 4.8828e-08\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6986 - regression_loss: 5.4997 - val_loss: 8.1152 - val_regression_loss: 5.0902 - lr: 4.8828e-08\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7037 - regression_loss: 5.4997 - val_loss: 8.1152 - val_regression_loss: 5.0902 - lr: 4.8828e-08\n",
      "Epoch 135/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.6773 - regression_loss: 5.8749\n",
      "Epoch 135: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6232 - regression_loss: 5.4997 - val_loss: 8.1151 - val_regression_loss: 5.0901 - lr: 4.8828e-08\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.5201 - regression_loss: 5.4996 - val_loss: 8.1151 - val_regression_loss: 5.0902 - lr: 2.4414e-08\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.5828 - regression_loss: 5.4996 - val_loss: 8.1151 - val_regression_loss: 5.0901 - lr: 2.4414e-08\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.5849 - regression_loss: 5.4996 - val_loss: 8.1151 - val_regression_loss: 5.0901 - lr: 2.4414e-08\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6289 - regression_loss: 5.4996 - val_loss: 8.1151 - val_regression_loss: 5.0901 - lr: 2.4414e-08\n",
      "Epoch 140/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.5834 - regression_loss: 6.7810\n",
      "Epoch 140: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7742 - regression_loss: 5.4996 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 2.4414e-08\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6988 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 1.2207e-08\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6523 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 1.2207e-08\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6837 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 1.2207e-08\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6893 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 1.2207e-08\n",
      "Epoch 145/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.5589 - regression_loss: 7.7566\n",
      "Epoch 145: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6558 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 1.2207e-08\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7733 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 6.1035e-09\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7121 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 6.1035e-09\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7860 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 6.1035e-09\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7796 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 6.1035e-09\n",
      "Epoch 150/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.5921 - regression_loss: 4.7897\n",
      "Epoch 150: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7086 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 6.1035e-09\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.5370 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 3.0518e-09\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7983 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 3.0518e-09\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6414 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 3.0518e-09\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6159 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 3.0518e-09\n",
      "Epoch 155/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.4840 - regression_loss: 6.6816\n",
      "Epoch 155: ReduceLROnPlateau reducing learning rate to 1.5258788677030566e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6973 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 3.0518e-09\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6730 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 1.5259e-09\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6914 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 1.5259e-09\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7161 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 1.5259e-09\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6162 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 1.5259e-09\n",
      "Epoch 160/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.5878 - regression_loss: 6.7855\n",
      "Epoch 160: ReduceLROnPlateau reducing learning rate to 7.629394338515283e-10.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7174 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 1.5259e-09\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7205 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 7.6294e-10\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6976 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 7.6294e-10\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6658 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 7.6294e-10\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.5882 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 7.6294e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.3662 - regression_loss: 5.5638\n",
      "Epoch 165: ReduceLROnPlateau reducing learning rate to 3.8146971692576415e-10.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7578 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 7.6294e-10\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7287 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 3.8147e-10\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6639 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 3.8147e-10\n",
      "Epoch 168/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7146 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 3.8147e-10\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7241 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 3.8147e-10\n",
      "Epoch 170/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.0483 - regression_loss: 7.2459\n",
      "Epoch 170: ReduceLROnPlateau reducing learning rate to 1.9073485846288207e-10.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6651 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 3.8147e-10\n",
      "Epoch 171/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.4952 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 1.9073e-10\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7121 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 1.9073e-10\n",
      "Epoch 173/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7284 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 1.9073e-10\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6649 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 1.9073e-10\n",
      "Epoch 175/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7671 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 1.9073e-10\n",
      "Epoch 176/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.1321 - regression_loss: 7.3297\n",
      "Epoch 176: ReduceLROnPlateau reducing learning rate to 9.536742923144104e-11.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7913 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 1.9073e-10\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.4740 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 9.5367e-11\n",
      "Epoch 178/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6948 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 9.5367e-11\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6726 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 9.5367e-11\n",
      "Epoch 180/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6817 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 9.5367e-11\n",
      "Epoch 181/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7697 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 9.5367e-11\n",
      "Epoch 182/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.6561 - regression_loss: 6.8537\n",
      "Epoch 182: ReduceLROnPlateau reducing learning rate to 4.768371461572052e-11.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6718 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 9.5367e-11\n",
      "Epoch 183/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7759 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 4.7684e-11\n",
      "Epoch 184/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6260 - regression_loss: 5.4995 - val_loss: 8.1150 - val_regression_loss: 5.0901 - lr: 4.7684e-11\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 27ms/step - loss: 107.7414 - regression_loss: 97.9366 - val_loss: 81.4174 - val_regression_loss: 61.6694 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 76.2438 - regression_loss: 68.6852 - val_loss: 63.8786 - val_regression_loss: 48.0034 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 61.4236 - regression_loss: 55.6235 - val_loss: 53.8499 - val_regression_loss: 40.2632 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 53.1488 - regression_loss: 47.6397 - val_loss: 46.7836 - val_regression_loss: 34.6753 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 45.4017 - regression_loss: 41.0261 - val_loss: 39.9646 - val_regression_loss: 29.4501 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 39.8678 - regression_loss: 35.3975 - val_loss: 34.0052 - val_regression_loss: 24.9905 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 35.5296 - regression_loss: 31.6642 - val_loss: 30.7748 - val_regression_loss: 22.4595 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.2769 - regression_loss: 28.1699 - val_loss: 26.8110 - val_regression_loss: 19.4771 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.8536 - regression_loss: 24.9858 - val_loss: 24.4314 - val_regression_loss: 17.5603 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.7194 - regression_loss: 22.4219 - val_loss: 21.4441 - val_regression_loss: 15.3798 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.6947 - regression_loss: 20.0713 - val_loss: 19.8184 - val_regression_loss: 14.0093 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9666 - regression_loss: 18.0253 - val_loss: 17.4925 - val_regression_loss: 12.3494 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.6393 - regression_loss: 16.6784 - val_loss: 17.3815 - val_regression_loss: 12.0229 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.9028 - regression_loss: 15.0491 - val_loss: 14.4880 - val_regression_loss: 10.0328 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.8276 - regression_loss: 13.2403 - val_loss: 13.5292 - val_regression_loss: 9.1574 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.3676 - regression_loss: 11.8911 - val_loss: 12.1796 - val_regression_loss: 8.2342 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.4796 - regression_loss: 11.0549 - val_loss: 11.6856 - val_regression_loss: 7.7461 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8015 - regression_loss: 10.1243 - val_loss: 10.7853 - val_regression_loss: 7.1203 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0987 - regression_loss: 9.6050 - val_loss: 10.5198 - val_regression_loss: 6.8183 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4058 - regression_loss: 9.0514 - val_loss: 9.7006 - val_regression_loss: 6.2774 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2386 - regression_loss: 8.7109 - val_loss: 9.5800 - val_regression_loss: 6.1000 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.5604 - regression_loss: 8.3690 - val_loss: 9.1690 - val_regression_loss: 5.8471 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6612 - regression_loss: 8.0825 - val_loss: 9.1492 - val_regression_loss: 5.7604 - lr: 1.0000e-04\n",
      "Epoch 24/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3645 - regression_loss: 7.8770 - val_loss: 8.8642 - val_regression_loss: 5.6067 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0756 - regression_loss: 7.6861 - val_loss: 8.6840 - val_regression_loss: 5.4187 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8592 - regression_loss: 7.4829 - val_loss: 8.7792 - val_regression_loss: 5.5646 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4760 - regression_loss: 7.4413 - val_loss: 8.6779 - val_regression_loss: 5.4067 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6269 - regression_loss: 7.1785 - val_loss: 8.5390 - val_regression_loss: 5.3676 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4450 - regression_loss: 7.1009 - val_loss: 8.4143 - val_regression_loss: 5.2696 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9275 - regression_loss: 6.8642 - val_loss: 8.2788 - val_regression_loss: 5.0797 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1341 - regression_loss: 6.8887 - val_loss: 8.2554 - val_regression_loss: 5.0467 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2005 - regression_loss: 6.8057 - val_loss: 8.6695 - val_regression_loss: 5.5465 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9879 - regression_loss: 6.7365 - val_loss: 9.1473 - val_regression_loss: 5.6379 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5697 - regression_loss: 7.1770 - val_loss: 8.6275 - val_regression_loss: 5.4945 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.9501 - regression_loss: 8.1596\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9289 - regression_loss: 6.6219 - val_loss: 8.1544 - val_regression_loss: 4.9558 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3908 - regression_loss: 6.3960 - val_loss: 8.2476 - val_regression_loss: 5.1715 - lr: 5.0000e-05\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5466 - regression_loss: 6.2588 - val_loss: 7.9342 - val_regression_loss: 4.8100 - lr: 5.0000e-05\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5008 - regression_loss: 6.1709 - val_loss: 7.9874 - val_regression_loss: 4.9236 - lr: 5.0000e-05\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4005 - regression_loss: 6.0995 - val_loss: 7.8480 - val_regression_loss: 4.7576 - lr: 5.0000e-05\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3803 - regression_loss: 6.0709 - val_loss: 7.8821 - val_regression_loss: 4.8367 - lr: 5.0000e-05\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3031 - regression_loss: 6.0295 - val_loss: 7.7971 - val_regression_loss: 4.7291 - lr: 5.0000e-05\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0473 - regression_loss: 5.9251 - val_loss: 7.7497 - val_regression_loss: 4.7009 - lr: 5.0000e-05\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0720 - regression_loss: 5.8651 - val_loss: 7.7641 - val_regression_loss: 4.7329 - lr: 5.0000e-05\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8845 - regression_loss: 5.8387 - val_loss: 7.7337 - val_regression_loss: 4.6836 - lr: 5.0000e-05\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9316 - regression_loss: 5.8387 - val_loss: 7.7286 - val_regression_loss: 4.6864 - lr: 5.0000e-05\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8712 - regression_loss: 5.7544 - val_loss: 7.7329 - val_regression_loss: 4.7114 - lr: 5.0000e-05\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.9328 - regression_loss: 5.7236 - val_loss: 7.6837 - val_regression_loss: 4.6250 - lr: 5.0000e-05\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9012 - regression_loss: 5.7122 - val_loss: 7.6893 - val_regression_loss: 4.6735 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9807 - regression_loss: 5.6818 - val_loss: 7.5932 - val_regression_loss: 4.5622 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8324 - regression_loss: 5.5983 - val_loss: 7.5437 - val_regression_loss: 4.5400 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8006 - regression_loss: 5.5838 - val_loss: 7.5023 - val_regression_loss: 4.5008 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5329 - regression_loss: 5.5633 - val_loss: 7.4470 - val_regression_loss: 4.4455 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6806 - regression_loss: 5.4844 - val_loss: 7.5633 - val_regression_loss: 4.5838 - lr: 5.0000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6937 - regression_loss: 5.4200 - val_loss: 7.4891 - val_regression_loss: 4.4842 - lr: 5.0000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5784 - regression_loss: 5.3909 - val_loss: 7.4417 - val_regression_loss: 4.4680 - lr: 5.0000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5029 - regression_loss: 5.3911 - val_loss: 7.3564 - val_regression_loss: 4.3969 - lr: 5.0000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4952 - regression_loss: 5.3357 - val_loss: 7.3449 - val_regression_loss: 4.3938 - lr: 5.0000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5084 - regression_loss: 5.2899 - val_loss: 7.3055 - val_regression_loss: 4.3580 - lr: 5.0000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.1631 - regression_loss: 5.2792 - val_loss: 7.3243 - val_regression_loss: 4.3784 - lr: 5.0000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4622 - regression_loss: 5.2517 - val_loss: 7.3352 - val_regression_loss: 4.3865 - lr: 5.0000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.1276 - regression_loss: 5.2290 - val_loss: 7.2698 - val_regression_loss: 4.3231 - lr: 5.0000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.1625 - regression_loss: 5.1498 - val_loss: 7.2520 - val_regression_loss: 4.3223 - lr: 5.0000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.2850 - regression_loss: 5.1236 - val_loss: 7.2200 - val_regression_loss: 4.2943 - lr: 5.0000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0065 - regression_loss: 5.0996 - val_loss: 7.1151 - val_regression_loss: 4.2112 - lr: 5.0000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.2695 - regression_loss: 5.0785 - val_loss: 7.2167 - val_regression_loss: 4.3121 - lr: 5.0000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.1677 - regression_loss: 5.0400 - val_loss: 7.1094 - val_regression_loss: 4.1793 - lr: 5.0000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.1946 - regression_loss: 4.9975 - val_loss: 7.1076 - val_regression_loss: 4.2127 - lr: 5.0000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9582 - regression_loss: 4.9411 - val_loss: 7.0505 - val_regression_loss: 4.1527 - lr: 5.0000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9310 - regression_loss: 4.9104 - val_loss: 7.0431 - val_regression_loss: 4.1447 - lr: 5.0000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8623 - regression_loss: 4.8947 - val_loss: 7.0261 - val_regression_loss: 4.1549 - lr: 5.0000e-05\n",
      "Epoch 71/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7996 - regression_loss: 4.8830 - val_loss: 6.9284 - val_regression_loss: 4.0509 - lr: 5.0000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9845 - regression_loss: 4.8436 - val_loss: 7.1329 - val_regression_loss: 4.2564 - lr: 5.0000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8809 - regression_loss: 4.8306 - val_loss: 6.9953 - val_regression_loss: 4.0880 - lr: 5.0000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9423 - regression_loss: 4.8725 - val_loss: 6.9582 - val_regression_loss: 4.1152 - lr: 5.0000e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7324 - regression_loss: 4.7726 - val_loss: 6.8334 - val_regression_loss: 3.9615 - lr: 5.0000e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0572 - regression_loss: 4.8470 - val_loss: 7.0245 - val_regression_loss: 4.1848 - lr: 5.0000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0046 - regression_loss: 4.8780 - val_loss: 6.8146 - val_regression_loss: 3.9480 - lr: 5.0000e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7697 - regression_loss: 4.7735 - val_loss: 6.8873 - val_regression_loss: 4.0412 - lr: 5.0000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8452 - regression_loss: 4.6698 - val_loss: 6.7969 - val_regression_loss: 3.9458 - lr: 5.0000e-05\n",
      "Epoch 80/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.2127 - regression_loss: 5.4301\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7735 - regression_loss: 4.6401 - val_loss: 6.7966 - val_regression_loss: 3.9789 - lr: 5.0000e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7509 - regression_loss: 4.6315 - val_loss: 6.7639 - val_regression_loss: 3.9551 - lr: 2.5000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7274 - regression_loss: 4.5580 - val_loss: 6.6788 - val_regression_loss: 3.8504 - lr: 2.5000e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5821 - regression_loss: 4.5826 - val_loss: 6.7754 - val_regression_loss: 3.9689 - lr: 2.5000e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4307 - regression_loss: 4.5828 - val_loss: 6.7108 - val_regression_loss: 3.9070 - lr: 2.5000e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7003 - regression_loss: 4.5218 - val_loss: 6.6688 - val_regression_loss: 3.8481 - lr: 2.5000e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5583 - regression_loss: 4.4971 - val_loss: 6.7242 - val_regression_loss: 3.9167 - lr: 2.5000e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6378 - regression_loss: 4.4766 - val_loss: 6.7066 - val_regression_loss: 3.9070 - lr: 2.5000e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6260 - regression_loss: 4.4667 - val_loss: 6.6222 - val_regression_loss: 3.8232 - lr: 2.5000e-05\n",
      "Epoch 89/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.2569 - regression_loss: 4.4751\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5783 - regression_loss: 4.4619 - val_loss: 6.6425 - val_regression_loss: 3.8497 - lr: 2.5000e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5534 - regression_loss: 4.4276 - val_loss: 6.6564 - val_regression_loss: 3.8653 - lr: 1.2500e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4854 - regression_loss: 4.4280 - val_loss: 6.6595 - val_regression_loss: 3.8713 - lr: 1.2500e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5403 - regression_loss: 4.4213 - val_loss: 6.6187 - val_regression_loss: 3.8301 - lr: 1.2500e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6010 - regression_loss: 4.4171 - val_loss: 6.6077 - val_regression_loss: 3.8206 - lr: 1.2500e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4014 - regression_loss: 4.4074 - val_loss: 6.6076 - val_regression_loss: 3.8243 - lr: 1.2500e-05\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3771 - regression_loss: 4.4048 - val_loss: 6.6070 - val_regression_loss: 3.8227 - lr: 1.2500e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5457 - regression_loss: 4.3985 - val_loss: 6.5927 - val_regression_loss: 3.8080 - lr: 1.2500e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5312 - regression_loss: 4.3932 - val_loss: 6.6154 - val_regression_loss: 3.8330 - lr: 1.2500e-05\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5400 - regression_loss: 4.3906 - val_loss: 6.5705 - val_regression_loss: 3.7900 - lr: 1.2500e-05\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3596 - regression_loss: 4.3746 - val_loss: 6.5812 - val_regression_loss: 3.8027 - lr: 1.2500e-05\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4934 - regression_loss: 4.3774 - val_loss: 6.6141 - val_regression_loss: 3.8374 - lr: 1.2500e-05\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5611 - regression_loss: 4.3599 - val_loss: 6.5734 - val_regression_loss: 3.7963 - lr: 1.2500e-05\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3271 - regression_loss: 4.3658 - val_loss: 6.5382 - val_regression_loss: 3.7573 - lr: 1.2500e-05\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5116 - regression_loss: 4.3529 - val_loss: 6.5610 - val_regression_loss: 3.7915 - lr: 1.2500e-05\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4585 - regression_loss: 4.3389 - val_loss: 6.5746 - val_regression_loss: 3.8076 - lr: 1.2500e-05\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3868 - regression_loss: 4.3435 - val_loss: 6.5530 - val_regression_loss: 3.7820 - lr: 1.2500e-05\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2453 - regression_loss: 4.3462 - val_loss: 6.5085 - val_regression_loss: 3.7377 - lr: 1.2500e-05\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4262 - regression_loss: 4.3211 - val_loss: 6.5294 - val_regression_loss: 3.7689 - lr: 1.2500e-05\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4822 - regression_loss: 4.3200 - val_loss: 6.5548 - val_regression_loss: 3.7914 - lr: 1.2500e-05\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3712 - regression_loss: 4.3150 - val_loss: 6.5243 - val_regression_loss: 3.7581 - lr: 1.2500e-05\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2743 - regression_loss: 4.3110 - val_loss: 6.4849 - val_regression_loss: 3.7218 - lr: 1.2500e-05\n",
      "Epoch 111/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.0425 - regression_loss: 5.2618\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4039 - regression_loss: 4.3055 - val_loss: 6.5358 - val_regression_loss: 3.7776 - lr: 1.2500e-05\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4089 - regression_loss: 4.2929 - val_loss: 6.5234 - val_regression_loss: 3.7661 - lr: 6.2500e-06\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3270 - regression_loss: 4.2873 - val_loss: 6.4846 - val_regression_loss: 3.7247 - lr: 6.2500e-06\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4764 - regression_loss: 4.2818 - val_loss: 6.4847 - val_regression_loss: 3.7266 - lr: 6.2500e-06\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4294 - regression_loss: 4.2822 - val_loss: 6.4851 - val_regression_loss: 3.7278 - lr: 6.2500e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.6358 - regression_loss: 5.8553\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3544 - regression_loss: 4.2783 - val_loss: 6.4944 - val_regression_loss: 3.7406 - lr: 6.2500e-06\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3668 - regression_loss: 4.2693 - val_loss: 6.4941 - val_regression_loss: 3.7402 - lr: 3.1250e-06\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3904 - regression_loss: 4.2670 - val_loss: 6.4870 - val_regression_loss: 3.7332 - lr: 3.1250e-06\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3769 - regression_loss: 4.2641 - val_loss: 6.4790 - val_regression_loss: 3.7246 - lr: 3.1250e-06\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4145 - regression_loss: 4.2631 - val_loss: 6.4767 - val_regression_loss: 3.7230 - lr: 3.1250e-06\n",
      "Epoch 121/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.0451 - regression_loss: 5.2646\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.2884 - regression_loss: 4.2628 - val_loss: 6.4683 - val_regression_loss: 3.7144 - lr: 3.1250e-06\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2878 - regression_loss: 4.2596 - val_loss: 6.4706 - val_regression_loss: 3.7173 - lr: 1.5625e-06\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3830 - regression_loss: 4.2615 - val_loss: 6.4650 - val_regression_loss: 3.7114 - lr: 1.5625e-06\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3860 - regression_loss: 4.2590 - val_loss: 6.4704 - val_regression_loss: 3.7177 - lr: 1.5625e-06\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.1921 - regression_loss: 4.2573 - val_loss: 6.4703 - val_regression_loss: 3.7172 - lr: 1.5625e-06\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3545 - regression_loss: 4.2560 - val_loss: 6.4719 - val_regression_loss: 3.7194 - lr: 1.5625e-06\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2892 - regression_loss: 4.2564 - val_loss: 6.4776 - val_regression_loss: 3.7255 - lr: 1.5625e-06\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3991 - regression_loss: 4.2550 - val_loss: 6.4770 - val_regression_loss: 3.7255 - lr: 1.5625e-06\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4107 - regression_loss: 4.2564 - val_loss: 6.4708 - val_regression_loss: 3.7185 - lr: 1.5625e-06\n",
      "Epoch 130/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.9250 - regression_loss: 6.1446\n",
      "Epoch 130: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3348 - regression_loss: 4.2531 - val_loss: 6.4728 - val_regression_loss: 3.7208 - lr: 1.5625e-06\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2728 - regression_loss: 4.2521 - val_loss: 6.4724 - val_regression_loss: 3.7208 - lr: 7.8125e-07\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3845 - regression_loss: 4.2515 - val_loss: 6.4693 - val_regression_loss: 3.7178 - lr: 7.8125e-07\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3123 - regression_loss: 4.2549 - val_loss: 6.4745 - val_regression_loss: 3.7233 - lr: 7.8125e-07\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3920 - regression_loss: 4.2500 - val_loss: 6.4713 - val_regression_loss: 3.7199 - lr: 7.8125e-07\n",
      "Epoch 135/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.5384 - regression_loss: 4.7580\n",
      "Epoch 135: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3913 - regression_loss: 4.2505 - val_loss: 6.4673 - val_regression_loss: 3.7159 - lr: 7.8125e-07\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3702 - regression_loss: 4.2490 - val_loss: 6.4681 - val_regression_loss: 3.7167 - lr: 3.9062e-07\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3530 - regression_loss: 4.2490 - val_loss: 6.4664 - val_regression_loss: 3.7150 - lr: 3.9062e-07\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3883 - regression_loss: 4.2486 - val_loss: 6.4661 - val_regression_loss: 3.7147 - lr: 3.9062e-07\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3555 - regression_loss: 4.2482 - val_loss: 6.4665 - val_regression_loss: 3.7152 - lr: 3.9062e-07\n",
      "Epoch 140/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.1082 - regression_loss: 4.3279\n",
      "Epoch 140: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3308 - regression_loss: 4.2481 - val_loss: 6.4662 - val_regression_loss: 3.7150 - lr: 3.9062e-07\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3398 - regression_loss: 4.2479 - val_loss: 6.4666 - val_regression_loss: 3.7155 - lr: 1.9531e-07\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3447 - regression_loss: 4.2476 - val_loss: 6.4665 - val_regression_loss: 3.7153 - lr: 1.9531e-07\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3937 - regression_loss: 4.2479 - val_loss: 6.4672 - val_regression_loss: 3.7161 - lr: 1.9531e-07\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.1555 - regression_loss: 4.2475 - val_loss: 6.4670 - val_regression_loss: 3.7160 - lr: 1.9531e-07\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3479 - regression_loss: 4.2474 - val_loss: 6.4664 - val_regression_loss: 3.7153 - lr: 1.9531e-07\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3902 - regression_loss: 4.2472 - val_loss: 6.4666 - val_regression_loss: 3.7156 - lr: 1.9531e-07\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4034 - regression_loss: 4.2471 - val_loss: 6.4663 - val_regression_loss: 3.7153 - lr: 1.9531e-07\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3627 - regression_loss: 4.2471 - val_loss: 6.4661 - val_regression_loss: 3.7151 - lr: 1.9531e-07\n",
      "Epoch 149/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.0658 - regression_loss: 4.2854\n",
      "Epoch 149: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2415 - regression_loss: 4.2470 - val_loss: 6.4663 - val_regression_loss: 3.7154 - lr: 1.9531e-07\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2687 - regression_loss: 4.2467 - val_loss: 6.4663 - val_regression_loss: 3.7154 - lr: 9.7656e-08\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3469 - regression_loss: 4.2467 - val_loss: 6.4662 - val_regression_loss: 3.7153 - lr: 9.7656e-08\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3657 - regression_loss: 4.2467 - val_loss: 6.4659 - val_regression_loss: 3.7150 - lr: 9.7656e-08\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3502 - regression_loss: 4.2466 - val_loss: 6.4662 - val_regression_loss: 3.7153 - lr: 9.7656e-08\n",
      "Epoch 154/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.3834 - regression_loss: 3.6031\n",
      "Epoch 154: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2278 - regression_loss: 4.2466 - val_loss: 6.4657 - val_regression_loss: 3.7148 - lr: 9.7656e-08\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.3442 - regression_loss: 4.2464 - val_loss: 6.4658 - val_regression_loss: 3.7150 - lr: 4.8828e-08\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2027 - regression_loss: 4.2464 - val_loss: 6.4658 - val_regression_loss: 3.7149 - lr: 4.8828e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3406 - regression_loss: 4.2464 - val_loss: 6.4657 - val_regression_loss: 3.7149 - lr: 4.8828e-08\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3675 - regression_loss: 4.2463 - val_loss: 6.4657 - val_regression_loss: 3.7148 - lr: 4.8828e-08\n",
      "Epoch 159/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.4772 - regression_loss: 4.6968\n",
      "Epoch 159: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3180 - regression_loss: 4.2463 - val_loss: 6.4656 - val_regression_loss: 3.7148 - lr: 4.8828e-08\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3808 - regression_loss: 4.2463 - val_loss: 6.4657 - val_regression_loss: 3.7148 - lr: 2.4414e-08\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3703 - regression_loss: 4.2463 - val_loss: 6.4658 - val_regression_loss: 3.7149 - lr: 2.4414e-08\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2212 - regression_loss: 4.2462 - val_loss: 6.4657 - val_regression_loss: 3.7149 - lr: 2.4414e-08\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3474 - regression_loss: 4.2462 - val_loss: 6.4657 - val_regression_loss: 3.7149 - lr: 2.4414e-08\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 27ms/step - loss: 131.7557 - regression_loss: 121.8997 - val_loss: 99.2724 - val_regression_loss: 85.7121 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 93.9667 - regression_loss: 85.0187 - val_loss: 79.6065 - val_regression_loss: 68.1614 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 74.7705 - regression_loss: 68.4032 - val_loss: 65.0164 - val_regression_loss: 56.4164 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 59.1139 - regression_loss: 55.6927 - val_loss: 56.8067 - val_regression_loss: 48.8161 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 53.5886 - regression_loss: 48.7434 - val_loss: 51.0245 - val_regression_loss: 43.0411 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 46.8742 - regression_loss: 42.4170 - val_loss: 46.3789 - val_regression_loss: 39.1426 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 44.0470 - regression_loss: 39.1981 - val_loss: 42.5736 - val_regression_loss: 35.4758 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 40.2620 - regression_loss: 35.7154 - val_loss: 39.6182 - val_regression_loss: 32.8242 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 38.0870 - regression_loss: 33.5121 - val_loss: 37.3106 - val_regression_loss: 30.7370 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 35.7859 - regression_loss: 31.3893 - val_loss: 35.3534 - val_regression_loss: 28.9687 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 33.6546 - regression_loss: 29.6623 - val_loss: 33.5738 - val_regression_loss: 27.2594 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31.8421 - regression_loss: 28.1254 - val_loss: 32.2264 - val_regression_loss: 26.0608 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.2477 - regression_loss: 26.6860 - val_loss: 30.5937 - val_regression_loss: 24.5672 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.2350 - regression_loss: 25.5107 - val_loss: 29.3703 - val_regression_loss: 23.4558 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.4569 - regression_loss: 24.3540 - val_loss: 28.3410 - val_regression_loss: 22.5724 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.7230 - regression_loss: 23.3645 - val_loss: 27.2244 - val_regression_loss: 21.5296 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.9181 - regression_loss: 22.3144 - val_loss: 26.5256 - val_regression_loss: 20.9382 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.1167 - regression_loss: 21.5755 - val_loss: 25.6133 - val_regression_loss: 20.0826 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.7503 - regression_loss: 20.7222 - val_loss: 24.8047 - val_regression_loss: 19.3517 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.3072 - regression_loss: 20.2243 - val_loss: 24.3644 - val_regression_loss: 18.9584 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.4991 - regression_loss: 19.6481 - val_loss: 23.8041 - val_regression_loss: 18.4864 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.1093 - regression_loss: 19.1320 - val_loss: 23.1179 - val_regression_loss: 17.8614 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.5192 - regression_loss: 18.7722 - val_loss: 23.0054 - val_regression_loss: 17.7436 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.3553 - regression_loss: 18.3555 - val_loss: 22.4969 - val_regression_loss: 17.3070 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.4666 - regression_loss: 18.1004 - val_loss: 22.2205 - val_regression_loss: 17.0588 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.1193 - regression_loss: 17.7506 - val_loss: 22.2648 - val_regression_loss: 17.1053 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5333 - regression_loss: 17.4947 - val_loss: 21.9884 - val_regression_loss: 16.8587 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.3729 - regression_loss: 17.2942 - val_loss: 21.6742 - val_regression_loss: 16.5847 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.9149 - regression_loss: 17.1267 - val_loss: 21.6066 - val_regression_loss: 16.5128 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.8710 - regression_loss: 17.0174 - val_loss: 21.5974 - val_regression_loss: 16.4929 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.8391 - regression_loss: 16.8569 - val_loss: 21.1626 - val_regression_loss: 16.0995 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.3218 - regression_loss: 16.8943 - val_loss: 21.2740 - val_regression_loss: 16.2117 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.3889 - regression_loss: 16.5743 - val_loss: 21.1051 - val_regression_loss: 16.0549 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.3891 - regression_loss: 16.4223 - val_loss: 21.2104 - val_regression_loss: 16.1343 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.2178 - regression_loss: 16.3215 - val_loss: 20.9298 - val_regression_loss: 15.8928 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.6843 - regression_loss: 16.4703 - val_loss: 21.3740 - val_regression_loss: 16.2730 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.2792 - regression_loss: 16.2395 - val_loss: 20.9067 - val_regression_loss: 15.8734 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.9074 - regression_loss: 16.0548 - val_loss: 20.9561 - val_regression_loss: 15.9143 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1079 - regression_loss: 16.0494 - val_loss: 20.6753 - val_regression_loss: 15.6732 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.0481 - regression_loss: 16.0102 - val_loss: 21.1079 - val_regression_loss: 16.0312 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.7265 - regression_loss: 16.1217 - val_loss: 20.6665 - val_regression_loss: 15.6747 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.5218 - regression_loss: 15.9604 - val_loss: 20.8556 - val_regression_loss: 15.8340 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.4281 - regression_loss: 15.8758 - val_loss: 20.5594 - val_regression_loss: 15.5790 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.6788 - regression_loss: 15.7434 - val_loss: 20.6728 - val_regression_loss: 15.6685 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.4554 - regression_loss: 15.6198 - val_loss: 20.7679 - val_regression_loss: 15.7422 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.5734 - regression_loss: 15.6571 - val_loss: 21.3334 - val_regression_loss: 16.2138 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.2181 - regression_loss: 15.4771 - val_loss: 20.4224 - val_regression_loss: 15.4572 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.3806 - regression_loss: 15.4899 - val_loss: 21.0708 - val_regression_loss: 16.0233 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.4437 - regression_loss: 15.3928 - val_loss: 20.4278 - val_regression_loss: 15.4638 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.8482 - regression_loss: 15.2814 - val_loss: 20.7750 - val_regression_loss: 15.7440 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.5405 - regression_loss: 15.3235 - val_loss: 20.6491 - val_regression_loss: 15.6339 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.2147 - regression_loss: 15.3238 - val_loss: 20.3253 - val_regression_loss: 15.3604 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.1356 - regression_loss: 15.2328 - val_loss: 20.7404 - val_regression_loss: 15.7192 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.9010 - regression_loss: 15.1019 - val_loss: 20.4772 - val_regression_loss: 15.5058 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 16.9645 - regression_loss: 15.1888\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.8892 - regression_loss: 15.0154 - val_loss: 20.6192 - val_regression_loss: 15.6112 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4363 - regression_loss: 14.9925 - val_loss: 20.5377 - val_regression_loss: 15.5511 - lr: 5.0000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.8410 - regression_loss: 14.8982 - val_loss: 20.4868 - val_regression_loss: 15.5125 - lr: 5.0000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4606 - regression_loss: 14.8687 - val_loss: 20.6882 - val_regression_loss: 15.6834 - lr: 5.0000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.9306 - regression_loss: 14.9635 - val_loss: 20.5351 - val_regression_loss: 15.5415 - lr: 5.0000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.8593 - regression_loss: 14.9360 - val_loss: 20.4169 - val_regression_loss: 15.4431 - lr: 5.0000e-05\n",
      "Epoch 61/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 20.8389 - regression_loss: 19.0636\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.8520 - regression_loss: 14.8835 - val_loss: 21.0072 - val_regression_loss: 15.9408 - lr: 5.0000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.7370 - regression_loss: 14.7982 - val_loss: 20.5439 - val_regression_loss: 15.5508 - lr: 2.5000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5598 - regression_loss: 14.7929 - val_loss: 20.3958 - val_regression_loss: 15.4272 - lr: 2.5000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5762 - regression_loss: 14.7631 - val_loss: 20.6646 - val_regression_loss: 15.6514 - lr: 2.5000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6244 - regression_loss: 14.7336 - val_loss: 20.6712 - val_regression_loss: 15.6563 - lr: 2.5000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3994 - regression_loss: 14.7186 - val_loss: 20.5087 - val_regression_loss: 15.5169 - lr: 2.5000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4945 - regression_loss: 14.7161 - val_loss: 20.5512 - val_regression_loss: 15.5529 - lr: 2.5000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.7756 - regression_loss: 14.7052 - val_loss: 20.5323 - val_regression_loss: 15.5370 - lr: 2.5000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6203 - regression_loss: 14.7212 - val_loss: 20.6490 - val_regression_loss: 15.6354 - lr: 2.5000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4652 - regression_loss: 14.6886 - val_loss: 20.6292 - val_regression_loss: 15.6211 - lr: 2.5000e-05\n",
      "Epoch 71/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 16.2973 - regression_loss: 14.5224\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4889 - regression_loss: 14.6750 - val_loss: 20.4840 - val_regression_loss: 15.4926 - lr: 2.5000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3138 - regression_loss: 14.6466 - val_loss: 20.5002 - val_regression_loss: 15.5065 - lr: 1.2500e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.8049 - regression_loss: 14.6636 - val_loss: 20.5926 - val_regression_loss: 15.5851 - lr: 1.2500e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.1824 - regression_loss: 14.6309 - val_loss: 20.5460 - val_regression_loss: 15.5477 - lr: 1.2500e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2872 - regression_loss: 14.6357 - val_loss: 20.6196 - val_regression_loss: 15.6085 - lr: 1.2500e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.4344 - regression_loss: 14.6198 - val_loss: 20.5675 - val_regression_loss: 15.5663 - lr: 1.2500e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4730 - regression_loss: 14.6171 - val_loss: 20.5185 - val_regression_loss: 15.5240 - lr: 1.2500e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3400 - regression_loss: 14.6312 - val_loss: 20.5741 - val_regression_loss: 15.5702 - lr: 1.2500e-05\n",
      "Epoch 79/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.3143 - regression_loss: 16.5397\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3335 - regression_loss: 14.6152 - val_loss: 20.5249 - val_regression_loss: 15.5283 - lr: 1.2500e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6666 - regression_loss: 14.5974 - val_loss: 20.5365 - val_regression_loss: 15.5370 - lr: 6.2500e-06\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4954 - regression_loss: 14.5976 - val_loss: 20.5442 - val_regression_loss: 15.5422 - lr: 6.2500e-06\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4684 - regression_loss: 14.5895 - val_loss: 20.5970 - val_regression_loss: 15.5865 - lr: 6.2500e-06\n",
      "Epoch 83/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3143 - regression_loss: 14.5891 - val_loss: 20.6232 - val_regression_loss: 15.6086 - lr: 6.2500e-06\n",
      "Epoch 84/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 16.9837 - regression_loss: 15.2091\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.2879 - regression_loss: 14.5874 - val_loss: 20.5670 - val_regression_loss: 15.5615 - lr: 6.2500e-06\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3593 - regression_loss: 14.5773 - val_loss: 20.5730 - val_regression_loss: 15.5661 - lr: 3.1250e-06\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2155 - regression_loss: 14.5810 - val_loss: 20.5404 - val_regression_loss: 15.5386 - lr: 3.1250e-06\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3232 - regression_loss: 14.5736 - val_loss: 20.5557 - val_regression_loss: 15.5507 - lr: 3.1250e-06\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2612 - regression_loss: 14.5757 - val_loss: 20.5643 - val_regression_loss: 15.5575 - lr: 3.1250e-06\n",
      "Epoch 89/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.2696 - regression_loss: 16.4951\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6199 - regression_loss: 14.5760 - val_loss: 20.5426 - val_regression_loss: 15.5394 - lr: 3.1250e-06\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5443 - regression_loss: 14.5691 - val_loss: 20.5606 - val_regression_loss: 15.5546 - lr: 1.5625e-06\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2179 - regression_loss: 14.5681 - val_loss: 20.5548 - val_regression_loss: 15.5497 - lr: 1.5625e-06\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.1576 - regression_loss: 14.5686 - val_loss: 20.5750 - val_regression_loss: 15.5668 - lr: 1.5625e-06\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 80.0672 - regression_loss: 74.3404 - val_loss: 50.1240 - val_regression_loss: 43.0399 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 67.1935 - regression_loss: 60.8488 - val_loss: 41.7645 - val_regression_loss: 35.9343 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 57.4520 - regression_loss: 52.5618 - val_loss: 36.8422 - val_regression_loss: 31.5070 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 51.6670 - regression_loss: 46.7457 - val_loss: 33.1843 - val_regression_loss: 28.2415 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 46.9356 - regression_loss: 42.5786 - val_loss: 30.4668 - val_regression_loss: 25.7787 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 43.5906 - regression_loss: 38.8225 - val_loss: 28.3433 - val_regression_loss: 23.9577 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 40.2633 - regression_loss: 35.8721 - val_loss: 26.0766 - val_regression_loss: 21.8462 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 36.7211 - regression_loss: 33.0481 - val_loss: 24.0799 - val_regression_loss: 20.1296 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 34.3580 - regression_loss: 30.3591 - val_loss: 21.9081 - val_regression_loss: 18.1054 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.1537 - regression_loss: 28.0635 - val_loss: 20.1361 - val_regression_loss: 16.5303 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.0167 - regression_loss: 26.0156 - val_loss: 18.5890 - val_regression_loss: 15.1739 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.3216 - regression_loss: 23.9121 - val_loss: 17.1719 - val_regression_loss: 13.8612 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.0285 - regression_loss: 22.3145 - val_loss: 15.7606 - val_regression_loss: 12.5347 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.3933 - regression_loss: 21.1220 - val_loss: 15.4701 - val_regression_loss: 12.2637 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.7267 - regression_loss: 19.4659 - val_loss: 13.8442 - val_regression_loss: 10.7548 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.9062 - regression_loss: 18.5998 - val_loss: 13.4614 - val_regression_loss: 10.4302 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5249 - regression_loss: 17.8541 - val_loss: 12.0201 - val_regression_loss: 9.0950 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.4069 - regression_loss: 16.5929 - val_loss: 11.6858 - val_regression_loss: 8.7960 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.2545 - regression_loss: 15.9359 - val_loss: 11.1919 - val_regression_loss: 8.3497 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.2099 - regression_loss: 15.2833 - val_loss: 11.3673 - val_regression_loss: 8.5095 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.7211 - regression_loss: 14.7670 - val_loss: 10.2618 - val_regression_loss: 7.3954 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6518 - regression_loss: 14.7237 - val_loss: 9.9205 - val_regression_loss: 7.1524 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.4182 - regression_loss: 13.8271 - val_loss: 9.7995 - val_regression_loss: 7.0388 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.9297 - regression_loss: 13.3612 - val_loss: 9.5855 - val_regression_loss: 6.8063 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.9099 - regression_loss: 13.1810 - val_loss: 10.2605 - val_regression_loss: 7.4752 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5627 - regression_loss: 13.0706 - val_loss: 9.3566 - val_regression_loss: 6.4822 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.3640 - regression_loss: 12.7336 - val_loss: 9.4084 - val_regression_loss: 6.6755 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.0518 - regression_loss: 12.2581 - val_loss: 8.8660 - val_regression_loss: 6.1423 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.0393 - regression_loss: 11.9294 - val_loss: 8.7325 - val_regression_loss: 5.9401 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.3554 - regression_loss: 11.8354 - val_loss: 8.8673 - val_regression_loss: 6.1585 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.0079 - regression_loss: 11.3980 - val_loss: 8.5379 - val_regression_loss: 5.7690 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6369 - regression_loss: 11.3447 - val_loss: 8.4217 - val_regression_loss: 5.6175 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7256 - regression_loss: 11.0795 - val_loss: 8.5027 - val_regression_loss: 5.7653 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6086 - regression_loss: 10.8055 - val_loss: 8.5864 - val_regression_loss: 5.8630 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1493 - regression_loss: 10.6029 - val_loss: 8.3389 - val_regression_loss: 5.5378 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5762 - regression_loss: 10.4382 - val_loss: 8.5491 - val_regression_loss: 5.8212 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.5481 - regression_loss: 10.1735 - val_loss: 8.4386 - val_regression_loss: 5.4809 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7231 - regression_loss: 10.2560 - val_loss: 8.6954 - val_regression_loss: 5.9578 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6908 - regression_loss: 10.0409 - val_loss: 8.1184 - val_regression_loss: 5.3413 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3517 - regression_loss: 9.7668 - val_loss: 8.1998 - val_regression_loss: 5.2921 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9787 - regression_loss: 9.6151 - val_loss: 8.1963 - val_regression_loss: 5.4828 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7673 - regression_loss: 9.4054 - val_loss: 8.7167 - val_regression_loss: 5.6017 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8596 - regression_loss: 9.5137 - val_loss: 8.8532 - val_regression_loss: 6.0944 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8873 - regression_loss: 9.4811 - val_loss: 7.9688 - val_regression_loss: 5.1809 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2882 - regression_loss: 9.0013 - val_loss: 8.0548 - val_regression_loss: 5.3177 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4715 - regression_loss: 9.0052 - val_loss: 7.9447 - val_regression_loss: 5.1044 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4696 - regression_loss: 8.8540 - val_loss: 8.0023 - val_regression_loss: 5.2149 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1520 - regression_loss: 8.6598 - val_loss: 7.8653 - val_regression_loss: 5.0717 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9506 - regression_loss: 8.6962 - val_loss: 7.9643 - val_regression_loss: 5.0448 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1076 - regression_loss: 8.6594 - val_loss: 8.0108 - val_regression_loss: 5.0426 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0069 - regression_loss: 8.6309 - val_loss: 7.9389 - val_regression_loss: 5.2019 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.5172 - regression_loss: 8.3133 - val_loss: 8.0179 - val_regression_loss: 5.0957 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.5976 - regression_loss: 8.3993 - val_loss: 8.0245 - val_regression_loss: 5.0551 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8150 - regression_loss: 8.5683 - val_loss: 7.8897 - val_regression_loss: 4.9410 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7193 - regression_loss: 8.1962 - val_loss: 9.2823 - val_regression_loss: 6.4900 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3194 - regression_loss: 8.7811 - val_loss: 7.9984 - val_regression_loss: 5.1910 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.8012 - regression_loss: 9.0291\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.5570 - regression_loss: 8.4467 - val_loss: 10.9521 - val_regression_loss: 7.0647 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5852 - regression_loss: 9.1681 - val_loss: 9.8545 - val_regression_loss: 6.9974 - lr: 5.0000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7487 - regression_loss: 9.3287 - val_loss: 8.5171 - val_regression_loss: 5.3217 - lr: 5.0000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2573 - regression_loss: 7.8414 - val_loss: 8.5777 - val_regression_loss: 5.8303 - lr: 5.0000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3694 - regression_loss: 8.0315 - val_loss: 8.3601 - val_regression_loss: 5.2251 - lr: 5.0000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.3574 - regression_loss: 7.9826 - val_loss: 7.9768 - val_regression_loss: 5.2377 - lr: 5.0000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3266 - regression_loss: 7.9561 - val_loss: 8.3702 - val_regression_loss: 5.2312 - lr: 5.0000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2697 - regression_loss: 7.9626 - val_loss: 8.3370 - val_regression_loss: 5.6125 - lr: 5.0000e-05\n",
      "Epoch 65/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.8826 - regression_loss: 7.1109\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.5016 - regression_loss: 8.1545 - val_loss: 8.4354 - val_regression_loss: 5.2781 - lr: 5.0000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.4316 - regression_loss: 7.9527 - val_loss: 7.7739 - val_regression_loss: 5.0177 - lr: 2.5000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0336 - regression_loss: 7.7275 - val_loss: 7.7315 - val_regression_loss: 4.9717 - lr: 2.5000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8576 - regression_loss: 7.6292 - val_loss: 7.7697 - val_regression_loss: 4.8851 - lr: 2.5000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7640 - regression_loss: 7.5852 - val_loss: 7.7504 - val_regression_loss: 4.9749 - lr: 2.5000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9188 - regression_loss: 7.5284 - val_loss: 7.7202 - val_regression_loss: 4.8812 - lr: 2.5000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6376 - regression_loss: 7.4951 - val_loss: 7.7272 - val_regression_loss: 4.9036 - lr: 2.5000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8621 - regression_loss: 7.5210 - val_loss: 7.7713 - val_regression_loss: 4.9823 - lr: 2.5000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7318 - regression_loss: 7.4754 - val_loss: 7.7216 - val_regression_loss: 4.8821 - lr: 2.5000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7610 - regression_loss: 7.4674 - val_loss: 7.7061 - val_regression_loss: 4.8910 - lr: 2.5000e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7258 - regression_loss: 7.4500 - val_loss: 7.7238 - val_regression_loss: 4.9069 - lr: 2.5000e-05\n",
      "Epoch 76/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.6709 - regression_loss: 6.8994\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8240 - regression_loss: 7.4639 - val_loss: 7.7618 - val_regression_loss: 4.9284 - lr: 2.5000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5347 - regression_loss: 7.4502 - val_loss: 7.7576 - val_regression_loss: 4.9574 - lr: 1.2500e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6820 - regression_loss: 7.3975 - val_loss: 7.7305 - val_regression_loss: 4.9046 - lr: 1.2500e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7447 - regression_loss: 7.3933 - val_loss: 7.7330 - val_regression_loss: 4.9008 - lr: 1.2500e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5848 - regression_loss: 7.4029 - val_loss: 7.7300 - val_regression_loss: 4.9197 - lr: 1.2500e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7373 - regression_loss: 7.3883 - val_loss: 7.7108 - val_regression_loss: 4.8925 - lr: 1.2500e-05\n",
      "Epoch 82/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.8028 - regression_loss: 8.0315\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8361 - regression_loss: 7.4338 - val_loss: 7.7376 - val_regression_loss: 4.8844 - lr: 1.2500e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6908 - regression_loss: 7.3758 - val_loss: 7.7149 - val_regression_loss: 4.9079 - lr: 6.2500e-06\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6069 - regression_loss: 7.3615 - val_loss: 7.7150 - val_regression_loss: 4.9121 - lr: 6.2500e-06\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6914 - regression_loss: 7.3601 - val_loss: 7.7180 - val_regression_loss: 4.9109 - lr: 6.2500e-06\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7055 - regression_loss: 7.3571 - val_loss: 7.7204 - val_regression_loss: 4.9027 - lr: 6.2500e-06\n",
      "Epoch 87/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.5490 - regression_loss: 7.7777\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7321 - regression_loss: 7.3532 - val_loss: 7.7248 - val_regression_loss: 4.8935 - lr: 6.2500e-06\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7165 - regression_loss: 7.3487 - val_loss: 7.7221 - val_regression_loss: 4.8987 - lr: 3.1250e-06\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6451 - regression_loss: 7.3448 - val_loss: 7.7207 - val_regression_loss: 4.8974 - lr: 3.1250e-06\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3559 - regression_loss: 7.3401 - val_loss: 7.7226 - val_regression_loss: 4.9050 - lr: 3.1250e-06\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8153 - regression_loss: 7.3496 - val_loss: 7.7246 - val_regression_loss: 4.9163 - lr: 3.1250e-06\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5920 - regression_loss: 7.3380 - val_loss: 7.7239 - val_regression_loss: 4.9113 - lr: 3.1250e-06\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5189 - regression_loss: 7.3395 - val_loss: 7.7275 - val_regression_loss: 4.9032 - lr: 3.1250e-06\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7347 - regression_loss: 7.3371 - val_loss: 7.7265 - val_regression_loss: 4.9054 - lr: 3.1250e-06\n",
      "Epoch 95/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.7959 - regression_loss: 8.0246\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6257 - regression_loss: 7.3315 - val_loss: 7.7248 - val_regression_loss: 4.9016 - lr: 3.1250e-06\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5821 - regression_loss: 7.3298 - val_loss: 7.7258 - val_regression_loss: 4.9023 - lr: 1.5625e-06\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6461 - regression_loss: 7.3289 - val_loss: 7.7246 - val_regression_loss: 4.9029 - lr: 1.5625e-06\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6431 - regression_loss: 7.3291 - val_loss: 7.7234 - val_regression_loss: 4.9057 - lr: 1.5625e-06\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7265 - regression_loss: 7.3274 - val_loss: 7.7244 - val_regression_loss: 4.9058 - lr: 1.5625e-06\n",
      "Epoch 100/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.7088 - regression_loss: 8.9375\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.6302 - regression_loss: 7.3261 - val_loss: 7.7245 - val_regression_loss: 4.9056 - lr: 1.5625e-06\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5631 - regression_loss: 7.3248 - val_loss: 7.7249 - val_regression_loss: 4.9052 - lr: 7.8125e-07\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6532 - regression_loss: 7.3245 - val_loss: 7.7248 - val_regression_loss: 4.9058 - lr: 7.8125e-07\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5618 - regression_loss: 7.3240 - val_loss: 7.7237 - val_regression_loss: 4.9045 - lr: 7.8125e-07\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5427 - regression_loss: 7.3243 - val_loss: 7.7243 - val_regression_loss: 4.9058 - lr: 7.8125e-07\n",
      "Epoch 105/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.6074 - regression_loss: 7.8361\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5611 - regression_loss: 7.3235 - val_loss: 7.7245 - val_regression_loss: 4.9051 - lr: 7.8125e-07\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5904 - regression_loss: 7.3232 - val_loss: 7.7249 - val_regression_loss: 4.9043 - lr: 3.9062e-07\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6195 - regression_loss: 7.3236 - val_loss: 7.7248 - val_regression_loss: 4.9033 - lr: 3.9062e-07\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5153 - regression_loss: 7.3220 - val_loss: 7.7243 - val_regression_loss: 4.9037 - lr: 3.9062e-07\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7222 - regression_loss: 7.3226 - val_loss: 7.7241 - val_regression_loss: 4.9049 - lr: 3.9062e-07\n",
      "Epoch 110/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.0635 - regression_loss: 8.2922\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6510 - regression_loss: 7.3220 - val_loss: 7.7245 - val_regression_loss: 4.9047 - lr: 3.9062e-07\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5412 - regression_loss: 7.3215 - val_loss: 7.7245 - val_regression_loss: 4.9050 - lr: 1.9531e-07\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5444 - regression_loss: 7.3214 - val_loss: 7.7245 - val_regression_loss: 4.9047 - lr: 1.9531e-07\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6279 - regression_loss: 7.3211 - val_loss: 7.7246 - val_regression_loss: 4.9049 - lr: 1.9531e-07\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6779 - regression_loss: 7.3213 - val_loss: 7.7242 - val_regression_loss: 4.9050 - lr: 1.9531e-07\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 26ms/step - loss: 109.2865 - regression_loss: 100.8428 - val_loss: 108.3223 - val_regression_loss: 85.2067 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 82.2510 - regression_loss: 77.5971 - val_loss: 82.7182 - val_regression_loss: 63.9207 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 72.1703 - regression_loss: 65.6268 - val_loss: 72.8559 - val_regression_loss: 56.7402 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 59.4902 - regression_loss: 56.3058 - val_loss: 62.9834 - val_regression_loss: 48.9224 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 53.4039 - regression_loss: 48.9753 - val_loss: 55.4439 - val_regression_loss: 42.8602 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 48.6156 - regression_loss: 43.5838 - val_loss: 52.1499 - val_regression_loss: 40.4449 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 44.5277 - regression_loss: 39.1651 - val_loss: 46.0332 - val_regression_loss: 35.4010 - lr: 1.0000e-04\n",
      "Epoch 8/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 40.4323 - regression_loss: 36.5547 - val_loss: 43.2039 - val_regression_loss: 33.2229 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 37.2898 - regression_loss: 33.3291 - val_loss: 40.7047 - val_regression_loss: 31.2468 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 33.5293 - regression_loss: 30.9561 - val_loss: 37.7093 - val_regression_loss: 28.7577 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31.9924 - regression_loss: 29.5052 - val_loss: 36.2747 - val_regression_loss: 27.7044 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.1441 - regression_loss: 27.4598 - val_loss: 34.0614 - val_regression_loss: 25.8624 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.1239 - regression_loss: 26.2806 - val_loss: 32.7830 - val_regression_loss: 24.9794 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.5722 - regression_loss: 24.7675 - val_loss: 30.9516 - val_regression_loss: 23.4550 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.1145 - regression_loss: 23.6361 - val_loss: 29.8400 - val_regression_loss: 22.6652 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.2728 - regression_loss: 22.6889 - val_loss: 28.4418 - val_regression_loss: 21.4827 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.0969 - regression_loss: 21.7526 - val_loss: 27.5138 - val_regression_loss: 20.8025 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.6157 - regression_loss: 21.1075 - val_loss: 26.5717 - val_regression_loss: 20.0562 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.8295 - regression_loss: 20.3034 - val_loss: 25.9360 - val_regression_loss: 19.5304 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.9807 - regression_loss: 19.7861 - val_loss: 25.1075 - val_regression_loss: 18.8364 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.2987 - regression_loss: 19.3065 - val_loss: 24.4862 - val_regression_loss: 18.4842 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.8509 - regression_loss: 18.6484 - val_loss: 23.8276 - val_regression_loss: 17.8528 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.2935 - regression_loss: 18.1368 - val_loss: 23.2461 - val_regression_loss: 17.4512 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8503 - regression_loss: 17.8734 - val_loss: 22.6813 - val_regression_loss: 16.9592 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4677 - regression_loss: 17.5311 - val_loss: 22.1643 - val_regression_loss: 16.6336 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.2693 - regression_loss: 17.2264 - val_loss: 21.6838 - val_regression_loss: 16.1762 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.1155 - regression_loss: 17.2758 - val_loss: 21.2697 - val_regression_loss: 15.9077 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.0936 - regression_loss: 17.0766 - val_loss: 20.9039 - val_regression_loss: 15.6014 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.0086 - regression_loss: 16.2304 - val_loss: 20.5575 - val_regression_loss: 15.4001 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.7292 - regression_loss: 15.6341 - val_loss: 20.1912 - val_regression_loss: 15.0832 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6388 - regression_loss: 15.2425 - val_loss: 19.8125 - val_regression_loss: 14.7676 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.2832 - regression_loss: 15.2319 - val_loss: 19.5379 - val_regression_loss: 14.5140 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5443 - regression_loss: 14.9782 - val_loss: 19.1911 - val_regression_loss: 14.4312 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6734 - regression_loss: 14.8275 - val_loss: 18.9108 - val_regression_loss: 14.0165 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.1453 - regression_loss: 14.3006 - val_loss: 18.5819 - val_regression_loss: 13.8780 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9557 - regression_loss: 13.9785 - val_loss: 18.4847 - val_regression_loss: 13.7027 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5051 - regression_loss: 13.7295 - val_loss: 18.0492 - val_regression_loss: 13.5460 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.3653 - regression_loss: 13.4948 - val_loss: 17.8577 - val_regression_loss: 13.2608 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.2528 - regression_loss: 13.4982 - val_loss: 17.7414 - val_regression_loss: 13.1497 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.8316 - regression_loss: 13.3259 - val_loss: 17.7002 - val_regression_loss: 13.3010 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.0048 - regression_loss: 13.1732 - val_loss: 17.3811 - val_regression_loss: 12.8496 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.6929 - regression_loss: 12.8875 - val_loss: 17.0823 - val_regression_loss: 12.8461 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.4163 - regression_loss: 12.6736 - val_loss: 17.0858 - val_regression_loss: 12.5959 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.1108 - regression_loss: 12.4408 - val_loss: 16.6228 - val_regression_loss: 12.4329 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.6636 - regression_loss: 12.2325 - val_loss: 16.3465 - val_regression_loss: 12.1914 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.3323 - regression_loss: 11.9708 - val_loss: 16.3570 - val_regression_loss: 12.1011 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.6661 - regression_loss: 11.9146 - val_loss: 16.3642 - val_regression_loss: 12.2545 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.4765 - regression_loss: 11.8028 - val_loss: 16.3528 - val_regression_loss: 12.0430 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1375 - regression_loss: 11.6708 - val_loss: 15.8334 - val_regression_loss: 11.8914 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.4130 - regression_loss: 11.7787 - val_loss: 15.9830 - val_regression_loss: 11.7779 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7694 - regression_loss: 11.3296 - val_loss: 15.7473 - val_regression_loss: 11.7248 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.9069 - regression_loss: 11.2839 - val_loss: 15.8912 - val_regression_loss: 11.6913 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7588 - regression_loss: 11.1745 - val_loss: 15.4506 - val_regression_loss: 11.5832 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7435 - regression_loss: 11.0512 - val_loss: 15.6369 - val_regression_loss: 11.5443 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0797 - regression_loss: 10.8719 - val_loss: 15.3491 - val_regression_loss: 11.4295 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6183 - regression_loss: 10.9314 - val_loss: 15.7570 - val_regression_loss: 11.5152 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.5535 - regression_loss: 11.0558 - val_loss: 15.2520 - val_regression_loss: 11.4195 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.3306 - regression_loss: 10.7495 - val_loss: 15.3411 - val_regression_loss: 11.2757 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9752 - regression_loss: 10.4685 - val_loss: 14.9385 - val_regression_loss: 11.0352 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9679 - regression_loss: 10.3997 - val_loss: 14.9756 - val_regression_loss: 11.0116 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8134 - regression_loss: 10.2902 - val_loss: 14.9379 - val_regression_loss: 11.0173 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5857 - regression_loss: 10.1736 - val_loss: 14.8513 - val_regression_loss: 10.9727 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3491 - regression_loss: 10.0937 - val_loss: 15.0453 - val_regression_loss: 10.9891 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5676 - regression_loss: 10.2756 - val_loss: 14.8952 - val_regression_loss: 11.0852 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6107 - regression_loss: 10.1889 - val_loss: 15.4879 - val_regression_loss: 11.2627 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6620 - regression_loss: 10.0953 - val_loss: 14.6644 - val_regression_loss: 10.8484 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1795 - regression_loss: 9.9290 - val_loss: 15.0092 - val_regression_loss: 10.9418 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3368 - regression_loss: 9.8636 - val_loss: 14.6006 - val_regression_loss: 10.7380 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1359 - regression_loss: 9.7539 - val_loss: 14.6746 - val_regression_loss: 10.7346 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1047 - regression_loss: 9.6775 - val_loss: 14.7200 - val_regression_loss: 10.7905 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0394 - regression_loss: 9.6204 - val_loss: 14.6081 - val_regression_loss: 10.7270 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0373 - regression_loss: 9.5633 - val_loss: 14.5170 - val_regression_loss: 10.6190 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9872 - regression_loss: 9.5489 - val_loss: 14.7971 - val_regression_loss: 10.7606 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1315 - regression_loss: 9.5936 - val_loss: 14.4733 - val_regression_loss: 10.6424 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9563 - regression_loss: 9.5125 - val_loss: 14.3828 - val_regression_loss: 10.5335 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8439 - regression_loss: 9.3955 - val_loss: 14.5152 - val_regression_loss: 10.5934 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9256 - regression_loss: 9.5808 - val_loss: 15.1616 - val_regression_loss: 10.9587 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7850 - regression_loss: 9.3964 - val_loss: 14.3162 - val_regression_loss: 10.6262 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7970 - regression_loss: 9.4062 - val_loss: 15.0507 - val_regression_loss: 10.8929 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6863 - regression_loss: 9.3294 - val_loss: 14.3730 - val_regression_loss: 10.5043 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6609 - regression_loss: 9.2847 - val_loss: 14.3821 - val_regression_loss: 10.5174 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7440 - regression_loss: 9.2713 - val_loss: 14.6564 - val_regression_loss: 10.6745 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5398 - regression_loss: 9.2280 - val_loss: 14.2904 - val_regression_loss: 10.4701 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8328 - regression_loss: 9.3072 - val_loss: 14.6735 - val_regression_loss: 10.6029 - lr: 1.0000e-04\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8119 - regression_loss: 9.1577 - val_loss: 14.2568 - val_regression_loss: 10.4383 - lr: 1.0000e-04\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.4504 - regression_loss: 9.0773 - val_loss: 14.4368 - val_regression_loss: 10.5331 - lr: 1.0000e-04\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4508 - regression_loss: 9.0751 - val_loss: 14.6879 - val_regression_loss: 10.6392 - lr: 1.0000e-04\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4387 - regression_loss: 9.0485 - val_loss: 14.2775 - val_regression_loss: 10.4091 - lr: 1.0000e-04\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2617 - regression_loss: 9.0020 - val_loss: 14.2713 - val_regression_loss: 10.4556 - lr: 1.0000e-04\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5884 - regression_loss: 9.2039 - val_loss: 15.2443 - val_regression_loss: 10.9613 - lr: 1.0000e-04\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4670 - regression_loss: 9.1247 - val_loss: 14.3396 - val_regression_loss: 10.5884 - lr: 1.0000e-04\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3454 - regression_loss: 9.0985 - val_loss: 15.0532 - val_regression_loss: 10.8900 - lr: 1.0000e-04\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1970 - regression_loss: 9.0911 - val_loss: 14.2410 - val_regression_loss: 10.4342 - lr: 1.0000e-04\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2928 - regression_loss: 8.9290 - val_loss: 14.7050 - val_regression_loss: 10.6284 - lr: 1.0000e-04\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2722 - regression_loss: 8.9728 - val_loss: 14.3771 - val_regression_loss: 10.4165 - lr: 1.0000e-04\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1984 - regression_loss: 8.8662 - val_loss: 14.3468 - val_regression_loss: 10.4713 - lr: 1.0000e-04\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3064 - regression_loss: 8.9145 - val_loss: 14.3068 - val_regression_loss: 10.3734 - lr: 1.0000e-04\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1138 - regression_loss: 8.8334 - val_loss: 14.4159 - val_regression_loss: 10.4917 - lr: 1.0000e-04\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1337 - regression_loss: 8.7835 - val_loss: 14.4000 - val_regression_loss: 10.4837 - lr: 1.0000e-04\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0392 - regression_loss: 8.8299 - val_loss: 14.7303 - val_regression_loss: 10.6464 - lr: 1.0000e-04\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0963 - regression_loss: 8.8272 - val_loss: 14.3172 - val_regression_loss: 10.4184 - lr: 1.0000e-04\n",
      "Epoch 102/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1598 - regression_loss: 8.7004 - val_loss: 14.8211 - val_regression_loss: 10.6756 - lr: 1.0000e-04\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9399 - regression_loss: 8.7954 - val_loss: 14.1894 - val_regression_loss: 10.3031 - lr: 1.0000e-04\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0670 - regression_loss: 8.7235 - val_loss: 14.3078 - val_regression_loss: 10.3686 - lr: 1.0000e-04\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7404 - regression_loss: 8.6407 - val_loss: 14.5214 - val_regression_loss: 10.5612 - lr: 1.0000e-04\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0155 - regression_loss: 8.6775 - val_loss: 14.2808 - val_regression_loss: 10.4375 - lr: 1.0000e-04\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9981 - regression_loss: 8.6855 - val_loss: 14.5101 - val_regression_loss: 10.4802 - lr: 1.0000e-04\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9062 - regression_loss: 8.6085 - val_loss: 14.2768 - val_regression_loss: 10.4862 - lr: 1.0000e-04\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.1184 - regression_loss: 8.7949 - val_loss: 15.3207 - val_regression_loss: 11.0100 - lr: 1.0000e-04\n",
      "Epoch 110/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.1957 - regression_loss: 11.4553\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2661 - regression_loss: 8.8873 - val_loss: 14.1851 - val_regression_loss: 10.4311 - lr: 1.0000e-04\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9830 - regression_loss: 8.5238 - val_loss: 14.6907 - val_regression_loss: 10.6084 - lr: 5.0000e-05\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9690 - regression_loss: 8.6328 - val_loss: 14.2877 - val_regression_loss: 10.4158 - lr: 5.0000e-05\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.8957 - regression_loss: 8.6478 - val_loss: 14.2786 - val_regression_loss: 10.3939 - lr: 5.0000e-05\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9247 - regression_loss: 8.4839 - val_loss: 14.6504 - val_regression_loss: 10.6026 - lr: 5.0000e-05\n",
      "Epoch 115/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.0272 - regression_loss: 8.2874\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9609 - regression_loss: 8.5447 - val_loss: 14.2151 - val_regression_loss: 10.3754 - lr: 5.0000e-05\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9832 - regression_loss: 8.5812 - val_loss: 14.2198 - val_regression_loss: 10.3884 - lr: 2.5000e-05\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7886 - regression_loss: 8.4829 - val_loss: 14.5965 - val_regression_loss: 10.5752 - lr: 2.5000e-05\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7376 - regression_loss: 8.4846 - val_loss: 14.3973 - val_regression_loss: 10.4737 - lr: 2.5000e-05\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0169 - regression_loss: 8.4456 - val_loss: 14.3127 - val_regression_loss: 10.4167 - lr: 2.5000e-05\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7952 - regression_loss: 8.4596 - val_loss: 14.3473 - val_regression_loss: 10.4025 - lr: 2.5000e-05\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8624 - regression_loss: 8.4316 - val_loss: 14.3048 - val_regression_loss: 10.3932 - lr: 2.5000e-05\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8140 - regression_loss: 8.4219 - val_loss: 14.3242 - val_regression_loss: 10.4132 - lr: 2.5000e-05\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6773 - regression_loss: 8.4351 - val_loss: 14.4150 - val_regression_loss: 10.4537 - lr: 2.5000e-05\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7997 - regression_loss: 8.4656 - val_loss: 14.3710 - val_regression_loss: 10.4505 - lr: 2.5000e-05\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8027 - regression_loss: 8.4195 - val_loss: 14.2760 - val_regression_loss: 10.4144 - lr: 2.5000e-05\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7328 - regression_loss: 8.4394 - val_loss: 14.3435 - val_regression_loss: 10.4251 - lr: 2.5000e-05\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8284 - regression_loss: 8.4150 - val_loss: 14.4283 - val_regression_loss: 10.4556 - lr: 2.5000e-05\n",
      "Epoch 128/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.2585 - regression_loss: 8.5193\n",
      "Epoch 128: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8856 - regression_loss: 8.4198 - val_loss: 14.3627 - val_regression_loss: 10.4285 - lr: 2.5000e-05\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6074 - regression_loss: 8.3932 - val_loss: 14.3001 - val_regression_loss: 10.4041 - lr: 1.2500e-05\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6295 - regression_loss: 8.4005 - val_loss: 14.2925 - val_regression_loss: 10.4043 - lr: 1.2500e-05\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6691 - regression_loss: 8.3986 - val_loss: 14.3797 - val_regression_loss: 10.4384 - lr: 1.2500e-05\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6035 - regression_loss: 8.3974 - val_loss: 14.4040 - val_regression_loss: 10.4551 - lr: 1.2500e-05\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.4158 - regression_loss: 8.3845 - val_loss: 14.3486 - val_regression_loss: 10.4338 - lr: 1.2500e-05\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6681 - regression_loss: 8.3866 - val_loss: 14.3404 - val_regression_loss: 10.4294 - lr: 1.2500e-05\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7158 - regression_loss: 8.3883 - val_loss: 14.3112 - val_regression_loss: 10.4127 - lr: 1.2500e-05\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8569 - regression_loss: 8.3909 - val_loss: 14.3697 - val_regression_loss: 10.4406 - lr: 1.2500e-05\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7714 - regression_loss: 8.3769 - val_loss: 14.3623 - val_regression_loss: 10.4315 - lr: 1.2500e-05\n",
      "Epoch 138/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.0532 - regression_loss: 7.3143\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.5958 - regression_loss: 8.3756 - val_loss: 14.3288 - val_regression_loss: 10.4110 - lr: 1.2500e-05\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6962 - regression_loss: 8.3686 - val_loss: 14.3306 - val_regression_loss: 10.4117 - lr: 6.2500e-06\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7638 - regression_loss: 8.3769 - val_loss: 14.3533 - val_regression_loss: 10.4237 - lr: 6.2500e-06\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7671 - regression_loss: 8.3667 - val_loss: 14.3315 - val_regression_loss: 10.4158 - lr: 6.2500e-06\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.5737 - regression_loss: 8.3660 - val_loss: 14.3264 - val_regression_loss: 10.4150 - lr: 6.2500e-06\n",
      "Epoch 143/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.1099 - regression_loss: 8.3710\n",
      "Epoch 143: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.7938 - regression_loss: 8.3670 - val_loss: 14.3397 - val_regression_loss: 10.4239 - lr: 6.2500e-06\n",
      "Epoch 144/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6431 - regression_loss: 8.3638 - val_loss: 14.3342 - val_regression_loss: 10.4218 - lr: 3.1250e-06\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6528 - regression_loss: 8.3636 - val_loss: 14.3388 - val_regression_loss: 10.4238 - lr: 3.1250e-06\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.5815 - regression_loss: 8.3632 - val_loss: 14.3527 - val_regression_loss: 10.4327 - lr: 3.1250e-06\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6956 - regression_loss: 8.3648 - val_loss: 14.3465 - val_regression_loss: 10.4310 - lr: 3.1250e-06\n",
      "Epoch 148/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.9571 - regression_loss: 10.2183\n",
      "Epoch 148: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.5037 - regression_loss: 8.3613 - val_loss: 14.3628 - val_regression_loss: 10.4360 - lr: 3.1250e-06\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7784 - regression_loss: 8.3601 - val_loss: 14.3674 - val_regression_loss: 10.4378 - lr: 1.5625e-06\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.4932 - regression_loss: 8.3594 - val_loss: 14.3685 - val_regression_loss: 10.4383 - lr: 1.5625e-06\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 24ms/step - loss: 126.2758 - regression_loss: 114.7672 - val_loss: 75.4150 - val_regression_loss: 56.5149 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 67.4114 - regression_loss: 61.5575 - val_loss: 55.0344 - val_regression_loss: 40.2601 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 48.3874 - regression_loss: 43.5313 - val_loss: 45.8773 - val_regression_loss: 32.4201 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 41.8232 - regression_loss: 36.6920 - val_loss: 42.7207 - val_regression_loss: 30.4888 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 38.5292 - regression_loss: 34.1331 - val_loss: 38.9931 - val_regression_loss: 27.7147 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 35.6843 - regression_loss: 31.9126 - val_loss: 36.1679 - val_regression_loss: 25.7524 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 33.8590 - regression_loss: 29.7224 - val_loss: 34.5968 - val_regression_loss: 24.7398 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.3392 - regression_loss: 27.3470 - val_loss: 31.6875 - val_regression_loss: 22.3820 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.8686 - regression_loss: 25.6694 - val_loss: 30.3884 - val_regression_loss: 21.5586 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.5011 - regression_loss: 23.9009 - val_loss: 28.4277 - val_regression_loss: 20.0439 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.7655 - regression_loss: 22.2151 - val_loss: 26.9752 - val_regression_loss: 19.0123 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.6505 - regression_loss: 21.0836 - val_loss: 25.4097 - val_regression_loss: 17.8564 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.7389 - regression_loss: 19.7241 - val_loss: 24.4203 - val_regression_loss: 17.1895 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.4686 - regression_loss: 18.4623 - val_loss: 23.0137 - val_regression_loss: 16.1401 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.0452 - regression_loss: 17.3942 - val_loss: 21.8725 - val_regression_loss: 15.3234 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.5171 - regression_loss: 16.4776 - val_loss: 20.9233 - val_regression_loss: 14.6519 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.3558 - regression_loss: 15.6187 - val_loss: 19.9635 - val_regression_loss: 13.9114 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4180 - regression_loss: 14.8931 - val_loss: 19.4692 - val_regression_loss: 13.5373 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.1753 - regression_loss: 14.3609 - val_loss: 18.8915 - val_regression_loss: 13.1142 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.6027 - regression_loss: 13.7986 - val_loss: 18.1638 - val_regression_loss: 12.5730 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.0868 - regression_loss: 13.3645 - val_loss: 17.9956 - val_regression_loss: 12.4718 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5997 - regression_loss: 12.9714 - val_loss: 17.3275 - val_regression_loss: 11.9458 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.0504 - regression_loss: 12.6915 - val_loss: 17.2083 - val_regression_loss: 11.8651 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.6862 - regression_loss: 12.3387 - val_loss: 16.6771 - val_regression_loss: 11.4314 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.5840 - regression_loss: 12.1819 - val_loss: 17.2901 - val_regression_loss: 11.9407 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.6280 - regression_loss: 12.0730 - val_loss: 16.5309 - val_regression_loss: 11.3150 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.3880 - regression_loss: 11.7284 - val_loss: 16.2405 - val_regression_loss: 11.0800 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1071 - regression_loss: 11.5921 - val_loss: 16.0545 - val_regression_loss: 10.9362 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.9509 - regression_loss: 11.4140 - val_loss: 15.9134 - val_regression_loss: 10.8193 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8043 - regression_loss: 11.2600 - val_loss: 16.2139 - val_regression_loss: 11.0594 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6471 - regression_loss: 11.2612 - val_loss: 15.7354 - val_regression_loss: 10.6753 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6264 - regression_loss: 11.1207 - val_loss: 16.7019 - val_regression_loss: 11.4516 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7108 - regression_loss: 11.1101 - val_loss: 15.9227 - val_regression_loss: 10.8228 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.0556 - regression_loss: 11.5222 - val_loss: 15.9068 - val_regression_loss: 10.8105 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.3403 - regression_loss: 11.0035 - val_loss: 15.4279 - val_regression_loss: 10.4213 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.3239 - regression_loss: 10.8924 - val_loss: 15.4023 - val_regression_loss: 10.4049 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6641 - regression_loss: 11.3482 - val_loss: 16.2878 - val_regression_loss: 11.1198 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1113 - regression_loss: 10.7045 - val_loss: 15.1559 - val_regression_loss: 10.2013 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9589 - regression_loss: 10.5371 - val_loss: 15.3825 - val_regression_loss: 10.3838 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8909 - regression_loss: 10.4939 - val_loss: 14.8723 - val_regression_loss: 9.9823 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8609 - regression_loss: 10.3034 - val_loss: 14.8181 - val_regression_loss: 9.9423 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7170 - regression_loss: 10.4050 - val_loss: 16.3459 - val_regression_loss: 11.1566 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1656 - regression_loss: 10.8551 - val_loss: 15.6997 - val_regression_loss: 10.6452 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9558 - regression_loss: 10.6862 - val_loss: 15.8889 - val_regression_loss: 10.7688 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.4861 - regression_loss: 11.1049 - val_loss: 14.6968 - val_regression_loss: 9.8419 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7510 - regression_loss: 10.2857 - val_loss: 14.5640 - val_regression_loss: 9.7467 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4507 - regression_loss: 9.9978 - val_loss: 14.8451 - val_regression_loss: 9.9450 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3404 - regression_loss: 10.1181 - val_loss: 14.9823 - val_regression_loss: 10.0587 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5954 - regression_loss: 10.0424 - val_loss: 14.5479 - val_regression_loss: 9.7510 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3781 - regression_loss: 9.9830 - val_loss: 14.7894 - val_regression_loss: 9.9032 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1913 - regression_loss: 9.7698 - val_loss: 14.4559 - val_regression_loss: 9.6769 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3219 - regression_loss: 10.0235 - val_loss: 14.4792 - val_regression_loss: 9.6797 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2927 - regression_loss: 9.8672 - val_loss: 14.6693 - val_regression_loss: 9.8139 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0107 - regression_loss: 9.6634 - val_loss: 14.1788 - val_regression_loss: 9.4529 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7818 - regression_loss: 9.5177 - val_loss: 15.1954 - val_regression_loss: 10.2372 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2060 - regression_loss: 9.8549 - val_loss: 14.2861 - val_regression_loss: 9.5492 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1387 - regression_loss: 9.7254 - val_loss: 14.6890 - val_regression_loss: 9.8495 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7316 - regression_loss: 9.3329 - val_loss: 13.9672 - val_regression_loss: 9.3121 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5998 - regression_loss: 9.3191 - val_loss: 14.0501 - val_regression_loss: 9.3450 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7064 - regression_loss: 9.2582 - val_loss: 13.9151 - val_regression_loss: 9.2665 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7323 - regression_loss: 9.3441 - val_loss: 15.1629 - val_regression_loss: 10.1990 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9904 - regression_loss: 9.4984 - val_loss: 14.0200 - val_regression_loss: 9.3405 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8186 - regression_loss: 9.4567 - val_loss: 13.7890 - val_regression_loss: 9.1490 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5258 - regression_loss: 9.1662 - val_loss: 13.7423 - val_regression_loss: 9.1416 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2926 - regression_loss: 9.1707 - val_loss: 15.3199 - val_regression_loss: 10.3213 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5490 - regression_loss: 9.3375 - val_loss: 14.2337 - val_regression_loss: 9.5424 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5987 - regression_loss: 9.4149 - val_loss: 15.0791 - val_regression_loss: 10.1447 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7444 - regression_loss: 9.3289 - val_loss: 13.9976 - val_regression_loss: 9.3581 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7328 - regression_loss: 9.3466 - val_loss: 14.1218 - val_regression_loss: 9.3776 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.8446 - regression_loss: 9.1174\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8923 - regression_loss: 9.5466 - val_loss: 14.3174 - val_regression_loss: 9.5516 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3826 - regression_loss: 9.1418 - val_loss: 13.6270 - val_regression_loss: 9.0348 - lr: 5.0000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4476 - regression_loss: 9.0455 - val_loss: 13.6336 - val_regression_loss: 9.0309 - lr: 5.0000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.2381 - regression_loss: 8.9628 - val_loss: 13.8525 - val_regression_loss: 9.1900 - lr: 5.0000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0524 - regression_loss: 8.7702 - val_loss: 13.4945 - val_regression_loss: 8.9318 - lr: 5.0000e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1124 - regression_loss: 8.7315 - val_loss: 13.6637 - val_regression_loss: 9.0483 - lr: 5.0000e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1545 - regression_loss: 8.7788 - val_loss: 13.5228 - val_regression_loss: 8.9479 - lr: 5.0000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1148 - regression_loss: 8.7439 - val_loss: 13.5419 - val_regression_loss: 8.9670 - lr: 5.0000e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1849 - regression_loss: 8.7041 - val_loss: 13.4870 - val_regression_loss: 8.9214 - lr: 5.0000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0307 - regression_loss: 8.7098 - val_loss: 13.4856 - val_regression_loss: 8.9209 - lr: 5.0000e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9382 - regression_loss: 8.7062 - val_loss: 13.5738 - val_regression_loss: 8.9907 - lr: 5.0000e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0385 - regression_loss: 8.6288 - val_loss: 13.4452 - val_regression_loss: 8.8955 - lr: 5.0000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7870 - regression_loss: 8.6399 - val_loss: 13.4015 - val_regression_loss: 8.8752 - lr: 5.0000e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.0793 - regression_loss: 8.7067 - val_loss: 13.4787 - val_regression_loss: 8.9213 - lr: 5.0000e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0725 - regression_loss: 8.6105 - val_loss: 13.4056 - val_regression_loss: 8.8637 - lr: 5.0000e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9539 - regression_loss: 8.5799 - val_loss: 13.5390 - val_regression_loss: 8.9550 - lr: 5.0000e-05\n",
      "Epoch 86/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0298 - regression_loss: 8.5692 - val_loss: 13.3872 - val_regression_loss: 8.8545 - lr: 5.0000e-05\n",
      "Epoch 87/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.9875 - regression_loss: 9.2619\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9607 - regression_loss: 8.5518 - val_loss: 13.4835 - val_regression_loss: 8.9219 - lr: 5.0000e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7409 - regression_loss: 8.5137 - val_loss: 13.3740 - val_regression_loss: 8.8427 - lr: 2.5000e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6459 - regression_loss: 8.5055 - val_loss: 13.3443 - val_regression_loss: 8.8257 - lr: 2.5000e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7327 - regression_loss: 8.5077 - val_loss: 13.4219 - val_regression_loss: 8.8739 - lr: 2.5000e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7116 - regression_loss: 8.4923 - val_loss: 13.4070 - val_regression_loss: 8.8638 - lr: 2.5000e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7673 - regression_loss: 8.4905 - val_loss: 13.3558 - val_regression_loss: 8.8314 - lr: 2.5000e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9281 - regression_loss: 8.5186 - val_loss: 13.3632 - val_regression_loss: 8.8402 - lr: 2.5000e-05\n",
      "Epoch 94/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.9469 - regression_loss: 7.2216\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7996 - regression_loss: 8.4803 - val_loss: 13.4824 - val_regression_loss: 8.9206 - lr: 2.5000e-05\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.7794 - regression_loss: 8.4804 - val_loss: 13.3736 - val_regression_loss: 8.8438 - lr: 1.2500e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7847 - regression_loss: 8.4549 - val_loss: 13.3451 - val_regression_loss: 8.8222 - lr: 1.2500e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6996 - regression_loss: 8.4481 - val_loss: 13.3468 - val_regression_loss: 8.8242 - lr: 1.2500e-05\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7111 - regression_loss: 8.4448 - val_loss: 13.3537 - val_regression_loss: 8.8291 - lr: 1.2500e-05\n",
      "Epoch 99/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.2290 - regression_loss: 9.5039\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8028 - regression_loss: 8.4387 - val_loss: 13.3663 - val_regression_loss: 8.8368 - lr: 1.2500e-05\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6621 - regression_loss: 8.4359 - val_loss: 13.3763 - val_regression_loss: 8.8443 - lr: 6.2500e-06\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6613 - regression_loss: 8.4350 - val_loss: 13.3575 - val_regression_loss: 8.8322 - lr: 6.2500e-06\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.7249 - regression_loss: 8.4340 - val_loss: 13.3472 - val_regression_loss: 8.8238 - lr: 6.2500e-06\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.5948 - regression_loss: 8.4284 - val_loss: 13.3555 - val_regression_loss: 8.8296 - lr: 6.2500e-06\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7731 - regression_loss: 8.4268 - val_loss: 13.3585 - val_regression_loss: 8.8319 - lr: 6.2500e-06\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7130 - regression_loss: 8.4272 - val_loss: 13.3589 - val_regression_loss: 8.8329 - lr: 6.2500e-06\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7953 - regression_loss: 8.4237 - val_loss: 13.3544 - val_regression_loss: 8.8302 - lr: 6.2500e-06\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6107 - regression_loss: 8.4250 - val_loss: 13.3602 - val_regression_loss: 8.8338 - lr: 6.2500e-06\n",
      "Epoch 108/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.3674 - regression_loss: 10.6424\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7501 - regression_loss: 8.4263 - val_loss: 13.3293 - val_regression_loss: 8.8126 - lr: 6.2500e-06\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.5736 - regression_loss: 8.4185 - val_loss: 13.3321 - val_regression_loss: 8.8141 - lr: 3.1250e-06\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6019 - regression_loss: 8.4171 - val_loss: 13.3389 - val_regression_loss: 8.8185 - lr: 3.1250e-06\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7268 - regression_loss: 8.4146 - val_loss: 13.3499 - val_regression_loss: 8.8262 - lr: 3.1250e-06\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8262 - regression_loss: 8.4174 - val_loss: 13.3629 - val_regression_loss: 8.8353 - lr: 3.1250e-06\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7431 - regression_loss: 8.4140 - val_loss: 13.3631 - val_regression_loss: 8.8356 - lr: 3.1250e-06\n",
      "Epoch 114/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.4849 - regression_loss: 8.7599\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7065 - regression_loss: 8.4154 - val_loss: 13.3448 - val_regression_loss: 8.8224 - lr: 3.1250e-06\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.7144 - regression_loss: 8.4100 - val_loss: 13.3430 - val_regression_loss: 8.8213 - lr: 1.5625e-06\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6493 - regression_loss: 8.4097 - val_loss: 13.3399 - val_regression_loss: 8.8192 - lr: 1.5625e-06\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6439 - regression_loss: 8.4092 - val_loss: 13.3419 - val_regression_loss: 8.8204 - lr: 1.5625e-06\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8257 - regression_loss: 8.4093 - val_loss: 13.3396 - val_regression_loss: 8.8189 - lr: 1.5625e-06\n",
      "Epoch 119/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.9848 - regression_loss: 9.2599\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8920 - regression_loss: 8.4084 - val_loss: 13.3380 - val_regression_loss: 8.8178 - lr: 1.5625e-06\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.6763 - regression_loss: 8.4090 - val_loss: 13.3429 - val_regression_loss: 8.8212 - lr: 7.8125e-07\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7735 - regression_loss: 8.4074 - val_loss: 13.3424 - val_regression_loss: 8.8209 - lr: 7.8125e-07\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6143 - regression_loss: 8.4122 - val_loss: 13.3359 - val_regression_loss: 8.8163 - lr: 7.8125e-07\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7319 - regression_loss: 8.4069 - val_loss: 13.3376 - val_regression_loss: 8.8174 - lr: 7.8125e-07\n",
      "Epoch 124/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.0470 - regression_loss: 9.3221\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6820 - regression_loss: 8.4066 - val_loss: 13.3400 - val_regression_loss: 8.8191 - lr: 7.8125e-07\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7274 - regression_loss: 8.4064 - val_loss: 13.3420 - val_regression_loss: 8.8204 - lr: 3.9062e-07\n",
      "Epoch 126/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 10.7727 - regression_loss: 8.4065 - val_loss: 13.3430 - val_regression_loss: 8.8211 - lr: 3.9062e-07\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8027 - regression_loss: 8.4060 - val_loss: 13.3420 - val_regression_loss: 8.8205 - lr: 3.9062e-07\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6692 - regression_loss: 8.4063 - val_loss: 13.3430 - val_regression_loss: 8.8212 - lr: 3.9062e-07\n",
      "Epoch 129/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.9747 - regression_loss: 9.2497\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8284 - regression_loss: 8.4063 - val_loss: 13.3442 - val_regression_loss: 8.8220 - lr: 3.9062e-07\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8480 - regression_loss: 8.4053 - val_loss: 13.3433 - val_regression_loss: 8.8214 - lr: 1.9531e-07\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6825 - regression_loss: 8.4057 - val_loss: 13.3436 - val_regression_loss: 8.8216 - lr: 1.9531e-07\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7024 - regression_loss: 8.4054 - val_loss: 13.3425 - val_regression_loss: 8.8208 - lr: 1.9531e-07\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7905 - regression_loss: 8.4053 - val_loss: 13.3424 - val_regression_loss: 8.8207 - lr: 1.9531e-07\n",
      "Epoch 134/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.4418 - regression_loss: 8.7169\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7548 - regression_loss: 8.4054 - val_loss: 13.3419 - val_regression_loss: 8.8204 - lr: 1.9531e-07\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7541 - regression_loss: 8.4050 - val_loss: 13.3419 - val_regression_loss: 8.8204 - lr: 9.7656e-08\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8298 - regression_loss: 8.4050 - val_loss: 13.3421 - val_regression_loss: 8.8205 - lr: 9.7656e-08\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7479 - regression_loss: 8.4050 - val_loss: 13.3421 - val_regression_loss: 8.8205 - lr: 9.7656e-08\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.7246 - regression_loss: 8.4050 - val_loss: 13.3421 - val_regression_loss: 8.8205 - lr: 9.7656e-08\n",
      "Epoch 139/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.6139 - regression_loss: 8.8889\n",
      "Epoch 139: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.6287 - regression_loss: 8.4049 - val_loss: 13.3418 - val_regression_loss: 8.8204 - lr: 9.7656e-08\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7313 - regression_loss: 8.4049 - val_loss: 13.3417 - val_regression_loss: 8.8203 - lr: 4.8828e-08\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8140 - regression_loss: 8.4050 - val_loss: 13.3420 - val_regression_loss: 8.8205 - lr: 4.8828e-08\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8046 - regression_loss: 8.4048 - val_loss: 13.3419 - val_regression_loss: 8.8204 - lr: 4.8828e-08\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7045 - regression_loss: 8.4049 - val_loss: 13.3420 - val_regression_loss: 8.8205 - lr: 4.8828e-08\n",
      "Epoch 144/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.5348 - regression_loss: 11.8099\n",
      "Epoch 144: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7022 - regression_loss: 8.4048 - val_loss: 13.3419 - val_regression_loss: 8.8204 - lr: 4.8828e-08\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7308 - regression_loss: 8.4048 - val_loss: 13.3419 - val_regression_loss: 8.8204 - lr: 2.4414e-08\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6866 - regression_loss: 8.4048 - val_loss: 13.3418 - val_regression_loss: 8.8204 - lr: 2.4414e-08\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7819 - regression_loss: 8.4048 - val_loss: 13.3418 - val_regression_loss: 8.8203 - lr: 2.4414e-08\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6239 - regression_loss: 8.4048 - val_loss: 13.3417 - val_regression_loss: 8.8203 - lr: 2.4414e-08\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 94.6738 - regression_loss: 85.5552 - val_loss: 106.1344 - val_regression_loss: 77.2674 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 76.3429 - regression_loss: 70.2867 - val_loss: 88.7221 - val_regression_loss: 65.1635 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 68.5174 - regression_loss: 61.9291 - val_loss: 75.8000 - val_regression_loss: 55.1632 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 59.8154 - regression_loss: 54.3009 - val_loss: 67.3138 - val_regression_loss: 48.6751 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 51.8090 - regression_loss: 46.4163 - val_loss: 59.8862 - val_regression_loss: 43.4336 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 47.5149 - regression_loss: 42.7474 - val_loss: 52.6227 - val_regression_loss: 37.6213 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 42.3114 - regression_loss: 38.1415 - val_loss: 46.7821 - val_regression_loss: 33.2959 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 38.4901 - regression_loss: 34.2812 - val_loss: 41.7418 - val_regression_loss: 29.5512 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 35.0147 - regression_loss: 30.4782 - val_loss: 36.8625 - val_regression_loss: 25.8645 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.3053 - regression_loss: 27.3519 - val_loss: 32.6265 - val_regression_loss: 22.7970 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.2741 - regression_loss: 24.3595 - val_loss: 28.6064 - val_regression_loss: 19.7413 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.0127 - regression_loss: 22.1319 - val_loss: 25.3868 - val_regression_loss: 17.3922 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.6903 - regression_loss: 19.7093 - val_loss: 22.2946 - val_regression_loss: 15.1351 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7329 - regression_loss: 17.6136 - val_loss: 19.6098 - val_regression_loss: 13.1769 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.0678 - regression_loss: 15.8682 - val_loss: 17.3806 - val_regression_loss: 11.5628 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.4866 - regression_loss: 14.4045 - val_loss: 15.7522 - val_regression_loss: 10.4069 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.7999 - regression_loss: 13.2553 - val_loss: 14.3154 - val_regression_loss: 9.4261 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.9450 - regression_loss: 12.3236 - val_loss: 13.4025 - val_regression_loss: 8.7941 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1663 - regression_loss: 11.6162 - val_loss: 12.5484 - val_regression_loss: 8.2263 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.5513 - regression_loss: 11.0617 - val_loss: 12.0171 - val_regression_loss: 7.8339 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.2116 - regression_loss: 10.7081 - val_loss: 11.7316 - val_regression_loss: 7.6514 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9078 - regression_loss: 10.3291 - val_loss: 11.3710 - val_regression_loss: 7.4624 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6075 - regression_loss: 10.0934 - val_loss: 11.3482 - val_regression_loss: 7.3805 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4298 - regression_loss: 9.9003 - val_loss: 11.0029 - val_regression_loss: 7.1661 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7345 - regression_loss: 9.6410 - val_loss: 10.8965 - val_regression_loss: 7.0433 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6868 - regression_loss: 9.5015 - val_loss: 10.7224 - val_regression_loss: 6.9416 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5943 - regression_loss: 9.3149 - val_loss: 10.8045 - val_regression_loss: 7.0249 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3590 - regression_loss: 9.2916 - val_loss: 10.7768 - val_regression_loss: 7.0480 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3605 - regression_loss: 9.1513 - val_loss: 10.5956 - val_regression_loss: 6.8364 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4389 - regression_loss: 9.1070 - val_loss: 10.6293 - val_regression_loss: 6.8124 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4272 - regression_loss: 9.0393 - val_loss: 10.5909 - val_regression_loss: 6.8532 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3885 - regression_loss: 8.9851 - val_loss: 11.0002 - val_regression_loss: 7.3196 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.9099 - regression_loss: 10.1930\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6248 - regression_loss: 9.1367 - val_loss: 10.9595 - val_regression_loss: 6.9732 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2696 - regression_loss: 9.0039 - val_loss: 10.6994 - val_regression_loss: 7.0333 - lr: 5.0000e-05\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2288 - regression_loss: 8.6786 - val_loss: 10.5471 - val_regression_loss: 6.7311 - lr: 5.0000e-05\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.0222 - regression_loss: 8.6761 - val_loss: 10.4328 - val_regression_loss: 6.7323 - lr: 5.0000e-05\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8263 - regression_loss: 8.6187 - val_loss: 10.3767 - val_regression_loss: 6.7080 - lr: 5.0000e-05\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0501 - regression_loss: 8.7178 - val_loss: 10.3218 - val_regression_loss: 6.6220 - lr: 5.0000e-05\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9210 - regression_loss: 8.5100 - val_loss: 10.3266 - val_regression_loss: 6.6926 - lr: 5.0000e-05\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9191 - regression_loss: 8.6104 - val_loss: 10.3444 - val_regression_loss: 6.6378 - lr: 5.0000e-05\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8182 - regression_loss: 8.4086 - val_loss: 10.3620 - val_regression_loss: 6.7100 - lr: 5.0000e-05\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6728 - regression_loss: 8.3496 - val_loss: 10.3699 - val_regression_loss: 6.6202 - lr: 5.0000e-05\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6064 - regression_loss: 8.3368 - val_loss: 10.1978 - val_regression_loss: 6.5860 - lr: 5.0000e-05\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7190 - regression_loss: 8.3699 - val_loss: 10.1577 - val_regression_loss: 6.4755 - lr: 5.0000e-05\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6315 - regression_loss: 8.3300 - val_loss: 10.1289 - val_regression_loss: 6.5062 - lr: 5.0000e-05\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3999 - regression_loss: 8.2352 - val_loss: 10.1525 - val_regression_loss: 6.5422 - lr: 5.0000e-05\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.1198 - regression_loss: 8.2506 - val_loss: 10.2180 - val_regression_loss: 6.5488 - lr: 5.0000e-05\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7045 - regression_loss: 8.1885 - val_loss: 10.1224 - val_regression_loss: 6.4780 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.4527 - regression_loss: 8.1083 - val_loss: 10.0977 - val_regression_loss: 6.5028 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.1607 - regression_loss: 8.1373 - val_loss: 10.0762 - val_regression_loss: 6.4390 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.4190 - regression_loss: 8.0613 - val_loss: 10.0737 - val_regression_loss: 6.4841 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9760 - regression_loss: 8.0907 - val_loss: 10.0459 - val_regression_loss: 6.4127 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.1554 - regression_loss: 7.9600 - val_loss: 10.1598 - val_regression_loss: 6.5824 - lr: 5.0000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3920 - regression_loss: 8.0707 - val_loss: 10.1871 - val_regression_loss: 6.5031 - lr: 5.0000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.2718 - regression_loss: 8.0422 - val_loss: 10.1313 - val_regression_loss: 6.5327 - lr: 5.0000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.1455 - regression_loss: 7.9840 - val_loss: 9.9799 - val_regression_loss: 6.3679 - lr: 5.0000e-05\n",
      "Epoch 57/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.3186 - regression_loss: 8.6045\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2035 - regression_loss: 7.9471 - val_loss: 10.0576 - val_regression_loss: 6.5272 - lr: 5.0000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3078 - regression_loss: 7.9590 - val_loss: 10.0031 - val_regression_loss: 6.4542 - lr: 2.5000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0748 - regression_loss: 7.8101 - val_loss: 9.9851 - val_regression_loss: 6.3705 - lr: 2.5000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.1704 - regression_loss: 7.8796 - val_loss: 9.9974 - val_regression_loss: 6.4019 - lr: 2.5000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.1509 - regression_loss: 7.8257 - val_loss: 9.9658 - val_regression_loss: 6.3997 - lr: 2.5000e-05\n",
      "Epoch 62/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.4039 - regression_loss: 6.6902\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0978 - regression_loss: 7.7983 - val_loss: 9.9665 - val_regression_loss: 6.3793 - lr: 2.5000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9416 - regression_loss: 7.7890 - val_loss: 9.9608 - val_regression_loss: 6.3720 - lr: 1.2500e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0978 - regression_loss: 7.7728 - val_loss: 9.9583 - val_regression_loss: 6.3918 - lr: 1.2500e-05\n",
      "Epoch 65/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9898 - regression_loss: 7.7796 - val_loss: 9.9507 - val_regression_loss: 6.3859 - lr: 1.2500e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0459 - regression_loss: 7.7662 - val_loss: 9.9419 - val_regression_loss: 6.3583 - lr: 1.2500e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0569 - regression_loss: 7.7621 - val_loss: 9.9287 - val_regression_loss: 6.3609 - lr: 1.2500e-05\n",
      "Epoch 68/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.6899 - regression_loss: 7.9763\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.1744 - regression_loss: 7.7560 - val_loss: 9.9427 - val_regression_loss: 6.3836 - lr: 1.2500e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8725 - regression_loss: 7.7440 - val_loss: 9.9303 - val_regression_loss: 6.3583 - lr: 6.2500e-06\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.1087 - regression_loss: 7.7361 - val_loss: 9.9216 - val_regression_loss: 6.3515 - lr: 6.2500e-06\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8159 - regression_loss: 7.7273 - val_loss: 9.9226 - val_regression_loss: 6.3525 - lr: 6.2500e-06\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0653 - regression_loss: 7.7255 - val_loss: 9.9289 - val_regression_loss: 6.3596 - lr: 6.2500e-06\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9109 - regression_loss: 7.7210 - val_loss: 9.9268 - val_regression_loss: 6.3630 - lr: 6.2500e-06\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9896 - regression_loss: 7.7202 - val_loss: 9.9253 - val_regression_loss: 6.3693 - lr: 6.2500e-06\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9629 - regression_loss: 7.7124 - val_loss: 9.9186 - val_regression_loss: 6.3557 - lr: 6.2500e-06\n",
      "Epoch 76/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.6096 - regression_loss: 10.8962\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8193 - regression_loss: 7.7146 - val_loss: 9.9224 - val_regression_loss: 6.3630 - lr: 6.2500e-06\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9538 - regression_loss: 7.7035 - val_loss: 9.9188 - val_regression_loss: 6.3585 - lr: 3.1250e-06\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9853 - regression_loss: 7.7079 - val_loss: 9.9142 - val_regression_loss: 6.3460 - lr: 3.1250e-06\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8076 - regression_loss: 7.7080 - val_loss: 9.9092 - val_regression_loss: 6.3379 - lr: 3.1250e-06\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0070 - regression_loss: 7.7004 - val_loss: 9.9088 - val_regression_loss: 6.3446 - lr: 3.1250e-06\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0408 - regression_loss: 7.6957 - val_loss: 9.9115 - val_regression_loss: 6.3514 - lr: 3.1250e-06\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7382 - regression_loss: 7.6951 - val_loss: 9.9166 - val_regression_loss: 6.3596 - lr: 3.1250e-06\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7817 - regression_loss: 7.6940 - val_loss: 9.9106 - val_regression_loss: 6.3546 - lr: 3.1250e-06\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8553 - regression_loss: 7.6914 - val_loss: 9.9106 - val_regression_loss: 6.3551 - lr: 3.1250e-06\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9935 - regression_loss: 7.6900 - val_loss: 9.9084 - val_regression_loss: 6.3501 - lr: 3.1250e-06\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9767 - regression_loss: 7.6885 - val_loss: 9.9025 - val_regression_loss: 6.3444 - lr: 3.1250e-06\n",
      "Epoch 87/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.2077 - regression_loss: 8.4944\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9761 - regression_loss: 7.6918 - val_loss: 9.9013 - val_regression_loss: 6.3402 - lr: 3.1250e-06\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0011 - regression_loss: 7.6851 - val_loss: 9.9037 - val_regression_loss: 6.3437 - lr: 1.5625e-06\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0445 - regression_loss: 7.6828 - val_loss: 9.9044 - val_regression_loss: 6.3461 - lr: 1.5625e-06\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0303 - regression_loss: 7.6825 - val_loss: 9.9039 - val_regression_loss: 6.3469 - lr: 1.5625e-06\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0228 - regression_loss: 7.6809 - val_loss: 9.9028 - val_regression_loss: 6.3458 - lr: 1.5625e-06\n",
      "Epoch 92/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.6199 - regression_loss: 8.9066\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0149 - regression_loss: 7.6797 - val_loss: 9.9016 - val_regression_loss: 6.3446 - lr: 1.5625e-06\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9020 - regression_loss: 7.6790 - val_loss: 9.9003 - val_regression_loss: 6.3427 - lr: 7.8125e-07\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6607 - regression_loss: 7.6785 - val_loss: 9.8996 - val_regression_loss: 6.3417 - lr: 7.8125e-07\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9868 - regression_loss: 7.6778 - val_loss: 9.8992 - val_regression_loss: 6.3418 - lr: 7.8125e-07\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0150 - regression_loss: 7.6775 - val_loss: 9.9001 - val_regression_loss: 6.3424 - lr: 7.8125e-07\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.8917 - regression_loss: 7.6794 - val_loss: 9.9015 - val_regression_loss: 6.3456 - lr: 7.8125e-07\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9496 - regression_loss: 7.6766 - val_loss: 9.9002 - val_regression_loss: 6.3435 - lr: 7.8125e-07\n",
      "Epoch 99/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.1696 - regression_loss: 6.4563\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0442 - regression_loss: 7.6758 - val_loss: 9.8994 - val_regression_loss: 6.3427 - lr: 7.8125e-07\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7436 - regression_loss: 7.6754 - val_loss: 9.8990 - val_regression_loss: 6.3418 - lr: 3.9062e-07\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6321 - regression_loss: 7.6750 - val_loss: 9.8990 - val_regression_loss: 6.3422 - lr: 3.9062e-07\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9405 - regression_loss: 7.6746 - val_loss: 9.8990 - val_regression_loss: 6.3422 - lr: 3.9062e-07\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9693 - regression_loss: 7.6744 - val_loss: 9.8992 - val_regression_loss: 6.3425 - lr: 3.9062e-07\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9031 - regression_loss: 7.6741 - val_loss: 9.8989 - val_regression_loss: 6.3422 - lr: 3.9062e-07\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8804 - regression_loss: 7.6741 - val_loss: 9.8988 - val_regression_loss: 6.3425 - lr: 3.9062e-07\n",
      "Epoch 106/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.9510 - regression_loss: 11.2377\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0151 - regression_loss: 7.6738 - val_loss: 9.8992 - val_regression_loss: 6.3429 - lr: 3.9062e-07\n",
      "Epoch 107/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9066 - regression_loss: 7.6735 - val_loss: 9.8987 - val_regression_loss: 6.3422 - lr: 1.9531e-07\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8857 - regression_loss: 7.6732 - val_loss: 9.8987 - val_regression_loss: 6.3422 - lr: 1.9531e-07\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8883 - regression_loss: 7.6733 - val_loss: 9.8984 - val_regression_loss: 6.3417 - lr: 1.9531e-07\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8897 - regression_loss: 7.6731 - val_loss: 9.8985 - val_regression_loss: 6.3417 - lr: 1.9531e-07\n",
      "Epoch 111/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 14.0876 - regression_loss: 12.3743\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8610 - regression_loss: 7.6730 - val_loss: 9.8987 - val_regression_loss: 6.3423 - lr: 1.9531e-07\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0398 - regression_loss: 7.6727 - val_loss: 9.8988 - val_regression_loss: 6.3424 - lr: 9.7656e-08\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9567 - regression_loss: 7.6729 - val_loss: 9.8986 - val_regression_loss: 6.3420 - lr: 9.7656e-08\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0098 - regression_loss: 7.6726 - val_loss: 9.8985 - val_regression_loss: 6.3419 - lr: 9.7656e-08\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8893 - regression_loss: 7.6726 - val_loss: 9.8984 - val_regression_loss: 6.3418 - lr: 9.7656e-08\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5980 - regression_loss: 7.6724 - val_loss: 9.8985 - val_regression_loss: 6.3419 - lr: 9.7656e-08\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9423 - regression_loss: 7.6724 - val_loss: 9.8984 - val_regression_loss: 6.3418 - lr: 9.7656e-08\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0451 - regression_loss: 7.6724 - val_loss: 9.8984 - val_regression_loss: 6.3421 - lr: 9.7656e-08\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0149 - regression_loss: 7.6723 - val_loss: 9.8982 - val_regression_loss: 6.3418 - lr: 9.7656e-08\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0061 - regression_loss: 7.6723 - val_loss: 9.8982 - val_regression_loss: 6.3417 - lr: 9.7656e-08\n",
      "Epoch 121/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.1999 - regression_loss: 7.4866\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9824 - regression_loss: 7.6721 - val_loss: 9.8982 - val_regression_loss: 6.3418 - lr: 9.7656e-08\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9831 - regression_loss: 7.6721 - val_loss: 9.8982 - val_regression_loss: 6.3419 - lr: 4.8828e-08\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9808 - regression_loss: 7.6720 - val_loss: 9.8983 - val_regression_loss: 6.3419 - lr: 4.8828e-08\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9529 - regression_loss: 7.6720 - val_loss: 9.8983 - val_regression_loss: 6.3419 - lr: 4.8828e-08\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9032 - regression_loss: 7.6719 - val_loss: 9.8983 - val_regression_loss: 6.3419 - lr: 4.8828e-08\n",
      "Epoch 126/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.0084 - regression_loss: 8.2951\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0407 - regression_loss: 7.6719 - val_loss: 9.8982 - val_regression_loss: 6.3418 - lr: 4.8828e-08\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8992 - regression_loss: 7.6719 - val_loss: 9.8982 - val_regression_loss: 6.3418 - lr: 2.4414e-08\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9834 - regression_loss: 7.6718 - val_loss: 9.8982 - val_regression_loss: 6.3418 - lr: 2.4414e-08\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8933 - regression_loss: 7.6718 - val_loss: 9.8982 - val_regression_loss: 6.3418 - lr: 2.4414e-08\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0446 - regression_loss: 7.6718 - val_loss: 9.8981 - val_regression_loss: 6.3418 - lr: 2.4414e-08\n",
      "Epoch 131/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.3156 - regression_loss: 8.6023\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7769 - regression_loss: 7.6718 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 2.4414e-08\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9909 - regression_loss: 7.6718 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 1.2207e-08\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9938 - regression_loss: 7.6718 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 1.2207e-08\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8953 - regression_loss: 7.6718 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 1.2207e-08\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9152 - regression_loss: 7.6718 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 1.2207e-08\n",
      "Epoch 136/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.7580 - regression_loss: 7.0447\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9804 - regression_loss: 7.6718 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 1.2207e-08\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9284 - regression_loss: 7.6718 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 6.1035e-09\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8403 - regression_loss: 7.6718 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 6.1035e-09\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7523 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 6.1035e-09\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9835 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 6.1035e-09\n",
      "Epoch 141/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.2886 - regression_loss: 7.5753\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9435 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 6.1035e-09\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7409 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 3.0518e-09\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8319 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 3.0518e-09\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6019 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 3.0518e-09\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9062 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 3.0518e-09\n",
      "Epoch 146/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.1278 - regression_loss: 9.4145\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 1.5258788677030566e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9792 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 3.0518e-09\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0281 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 1.5259e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0145 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 1.5259e-09\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9631 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 1.5259e-09\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0103 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 1.5259e-09\n",
      "Epoch 151/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.5165 - regression_loss: 7.8032\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 7.629394338515283e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0567 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 1.5259e-09\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9365 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 7.6294e-10\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9172 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 7.6294e-10\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9932 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 7.6294e-10\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9671 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 7.6294e-10\n",
      "Epoch 156/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.9231 - regression_loss: 10.2098\n",
      "Epoch 156: ReduceLROnPlateau reducing learning rate to 3.8146971692576415e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0597 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 7.6294e-10\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.0479 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 3.8147e-10\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9562 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 3.8147e-10\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9097 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 3.8147e-10\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8626 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 3.8147e-10\n",
      "Epoch 161/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.1515 - regression_loss: 10.4382\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 1.9073485846288207e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6473 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 3.8147e-10\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8164 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 1.9073e-10\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.1360 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 1.9073e-10\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8645 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 1.9073e-10\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0697 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 1.9073e-10\n",
      "Epoch 166/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.5282 - regression_loss: 6.8149\n",
      "Epoch 166: ReduceLROnPlateau reducing learning rate to 9.536742923144104e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9477 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 1.9073e-10\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9779 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 9.5367e-11\n",
      "Epoch 168/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9201 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 9.5367e-11\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8842 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 9.5367e-11\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8566 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 9.5367e-11\n",
      "Epoch 171/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.9992 - regression_loss: 7.2859\n",
      "Epoch 171: ReduceLROnPlateau reducing learning rate to 4.768371461572052e-11.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.9437 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 9.5367e-11\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7563 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 4.7684e-11\n",
      "Epoch 173/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8712 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 4.7684e-11\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9108 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 4.7684e-11\n",
      "Epoch 175/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0302 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 4.7684e-11\n",
      "Epoch 176/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5862 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 4.7684e-11\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0141 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 4.7684e-11\n",
      "Epoch 178/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8730 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 4.7684e-11\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0387 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 4.7684e-11\n",
      "Epoch 180/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7501 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 4.7684e-11\n",
      "Epoch 181/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.7653 - regression_loss: 8.0520\n",
      "Epoch 181: ReduceLROnPlateau reducing learning rate to 2.384185730786026e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9806 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 4.7684e-11\n",
      "Epoch 182/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9015 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 2.3842e-11\n",
      "Epoch 183/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0501 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 2.3842e-11\n",
      "Epoch 184/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9477 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 2.3842e-11\n",
      "Epoch 185/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0720 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 2.3842e-11\n",
      "Epoch 186/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.4631 - regression_loss: 8.7498\n",
      "Epoch 186: ReduceLROnPlateau reducing learning rate to 1.192092865393013e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9628 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 2.3842e-11\n",
      "Epoch 187/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0197 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 1.1921e-11\n",
      "Epoch 188/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9736 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 1.1921e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9192 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 1.1921e-11\n",
      "Epoch 190/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9944 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 1.1921e-11\n",
      "Epoch 191/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.3679 - regression_loss: 7.6547\n",
      "Epoch 191: ReduceLROnPlateau reducing learning rate to 5.960464326965065e-12.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9070 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 1.1921e-11\n",
      "Epoch 192/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9285 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 5.9605e-12\n",
      "Epoch 193/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0470 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 5.9605e-12\n",
      "Epoch 194/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8489 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 5.9605e-12\n",
      "Epoch 195/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9407 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 5.9605e-12\n",
      "Epoch 196/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.7968 - regression_loss: 7.0835\n",
      "Epoch 196: ReduceLROnPlateau reducing learning rate to 2.9802321634825324e-12.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9654 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 5.9605e-12\n",
      "Epoch 197/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0728 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 2.9802e-12\n",
      "Epoch 198/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9627 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 2.9802e-12\n",
      "Epoch 199/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9500 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 2.9802e-12\n",
      "Epoch 200/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9590 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 2.9802e-12\n",
      "Epoch 201/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.6761 - regression_loss: 9.9628\n",
      "Epoch 201: ReduceLROnPlateau reducing learning rate to 1.4901160817412662e-12.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.9666 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 2.9802e-12\n",
      "Epoch 202/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.0283 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 1.4901e-12\n",
      "Epoch 203/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0300 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 1.4901e-12\n",
      "Epoch 204/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0824 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 1.4901e-12\n",
      "Epoch 205/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9178 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 1.4901e-12\n",
      "Epoch 206/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.3501 - regression_loss: 9.6369\n",
      "Epoch 206: ReduceLROnPlateau reducing learning rate to 7.450580408706331e-13.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0111 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 1.4901e-12\n",
      "Epoch 207/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8390 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 7.4506e-13\n",
      "Epoch 208/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9325 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 7.4506e-13\n",
      "Epoch 209/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9091 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 7.4506e-13\n",
      "Epoch 210/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0559 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 7.4506e-13\n",
      "Epoch 211/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.9237 - regression_loss: 9.2104\n",
      "Epoch 211: ReduceLROnPlateau reducing learning rate to 3.7252902043531655e-13.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9591 - regression_loss: 7.6717 - val_loss: 9.8981 - val_regression_loss: 6.3417 - lr: 7.4506e-13\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 127.2580 - regression_loss: 114.6624 - val_loss: 57.4881 - val_regression_loss: 49.1693 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 88.4913 - regression_loss: 80.9652 - val_loss: 46.1434 - val_regression_loss: 39.3931 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 69.4357 - regression_loss: 62.0675 - val_loss: 38.4889 - val_regression_loss: 32.8215 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 57.2901 - regression_loss: 51.0538 - val_loss: 32.9904 - val_regression_loss: 28.0646 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 48.2701 - regression_loss: 43.0742 - val_loss: 27.9943 - val_regression_loss: 23.8626 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 40.0070 - regression_loss: 37.1877 - val_loss: 24.3918 - val_regression_loss: 20.8067 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 36.6877 - regression_loss: 32.7079 - val_loss: 20.1163 - val_regression_loss: 17.3028 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31.7435 - regression_loss: 28.9160 - val_loss: 19.7188 - val_regression_loss: 16.7911 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.5656 - regression_loss: 26.4752 - val_loss: 15.8463 - val_regression_loss: 13.7069 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.5861 - regression_loss: 23.7372 - val_loss: 15.5043 - val_regression_loss: 13.1920 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.8729 - regression_loss: 21.5913 - val_loss: 13.5855 - val_regression_loss: 11.5684 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.8963 - regression_loss: 19.6359 - val_loss: 12.4597 - val_regression_loss: 10.5385 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.1150 - regression_loss: 17.9463 - val_loss: 11.7263 - val_regression_loss: 9.8456 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.4732 - regression_loss: 16.4146 - val_loss: 10.1324 - val_regression_loss: 8.5171 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.4819 - regression_loss: 15.2380 - val_loss: 9.2486 - val_regression_loss: 7.6947 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.8445 - regression_loss: 13.9842 - val_loss: 8.9890 - val_regression_loss: 7.3835 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8045 - regression_loss: 13.8414 - val_loss: 7.7602 - val_regression_loss: 6.3665 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.0925 - regression_loss: 13.3244 - val_loss: 10.0886 - val_regression_loss: 8.0293 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.9033 - regression_loss: 12.1840 - val_loss: 7.0338 - val_regression_loss: 5.5890 - lr: 1.0000e-04\n",
      "Epoch 20/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 13.2093 - regression_loss: 11.1760 - val_loss: 8.1017 - val_regression_loss: 6.2945 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9900 - regression_loss: 10.4071 - val_loss: 6.2398 - val_regression_loss: 4.7816 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6530 - regression_loss: 10.3387 - val_loss: 7.3342 - val_regression_loss: 5.5201 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1944 - regression_loss: 9.5340 - val_loss: 5.9328 - val_regression_loss: 4.3847 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9085 - regression_loss: 9.1215 - val_loss: 7.3260 - val_regression_loss: 5.4190 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8639 - regression_loss: 8.9611 - val_loss: 5.5433 - val_regression_loss: 3.9801 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9062 - regression_loss: 8.5880 - val_loss: 7.5096 - val_regression_loss: 5.4279 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9443 - regression_loss: 8.3963 - val_loss: 5.3560 - val_regression_loss: 3.7055 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8708 - regression_loss: 7.8929 - val_loss: 6.5142 - val_regression_loss: 4.5355 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6214 - regression_loss: 7.8349 - val_loss: 5.2379 - val_regression_loss: 3.5437 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8465 - regression_loss: 7.6578 - val_loss: 6.5124 - val_regression_loss: 4.4376 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3777 - regression_loss: 7.1398 - val_loss: 4.8221 - val_regression_loss: 3.1175 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0804 - regression_loss: 6.8953 - val_loss: 6.2486 - val_regression_loss: 4.1965 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0931 - regression_loss: 6.8821 - val_loss: 4.5489 - val_regression_loss: 2.8237 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5025 - regression_loss: 6.4176 - val_loss: 5.2851 - val_regression_loss: 3.3417 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6392 - regression_loss: 6.4180 - val_loss: 4.4124 - val_regression_loss: 2.6475 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1805 - regression_loss: 5.9023 - val_loss: 4.2577 - val_regression_loss: 2.5074 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4528 - regression_loss: 5.6624 - val_loss: 4.7667 - val_regression_loss: 2.8668 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6494 - regression_loss: 5.5714 - val_loss: 4.2787 - val_regression_loss: 2.4999 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5247 - regression_loss: 5.6728 - val_loss: 4.6271 - val_regression_loss: 2.7180 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.3713 - regression_loss: 5.1530 - val_loss: 4.1199 - val_regression_loss: 2.3200 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4346 - regression_loss: 5.3290 - val_loss: 4.5562 - val_regression_loss: 2.6247 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9569 - regression_loss: 4.8802 - val_loss: 3.9323 - val_regression_loss: 2.1503 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0960 - regression_loss: 4.9398 - val_loss: 4.4549 - val_regression_loss: 2.5064 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0003 - regression_loss: 4.8564 - val_loss: 3.9507 - val_regression_loss: 2.1096 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7634 - regression_loss: 4.6913 - val_loss: 4.0519 - val_regression_loss: 2.1778 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6542 - regression_loss: 4.5733 - val_loss: 3.6744 - val_regression_loss: 1.8791 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4334 - regression_loss: 4.4314 - val_loss: 3.7922 - val_regression_loss: 1.9530 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3647 - regression_loss: 4.2970 - val_loss: 3.8057 - val_regression_loss: 1.9368 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2317 - regression_loss: 4.1556 - val_loss: 3.6713 - val_regression_loss: 1.8571 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3385 - regression_loss: 4.3031 - val_loss: 3.8818 - val_regression_loss: 1.9794 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.1405 - regression_loss: 4.0951 - val_loss: 3.4954 - val_regression_loss: 1.6634 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.0146 - regression_loss: 3.9995 - val_loss: 3.5256 - val_regression_loss: 1.6781 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.0643 - regression_loss: 4.0201 - val_loss: 4.2357 - val_regression_loss: 2.2436 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9699 - regression_loss: 3.9641 - val_loss: 3.5510 - val_regression_loss: 1.6894 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7196 - regression_loss: 3.7512 - val_loss: 3.4745 - val_regression_loss: 1.6188 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5148 - regression_loss: 3.5043 - val_loss: 3.4020 - val_regression_loss: 1.5720 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.4668 - regression_loss: 3.5338 - val_loss: 3.4482 - val_regression_loss: 1.5807 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2371 - regression_loss: 3.3304 - val_loss: 3.3160 - val_regression_loss: 1.4618 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0405 - regression_loss: 3.2233 - val_loss: 3.5728 - val_regression_loss: 1.6641 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1651 - regression_loss: 3.3537 - val_loss: 3.2608 - val_regression_loss: 1.4112 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9693 - regression_loss: 3.1285 - val_loss: 3.2700 - val_regression_loss: 1.4007 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0025 - regression_loss: 3.0578 - val_loss: 3.3113 - val_regression_loss: 1.4242 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0504 - regression_loss: 3.0909 - val_loss: 3.4477 - val_regression_loss: 1.5407 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0532 - regression_loss: 3.0587 - val_loss: 3.2338 - val_regression_loss: 1.3825 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1002 - regression_loss: 3.1462 - val_loss: 3.1588 - val_regression_loss: 1.2967 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7432 - regression_loss: 2.9241 - val_loss: 3.4323 - val_regression_loss: 1.5315 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9214 - regression_loss: 2.9680 - val_loss: 3.3134 - val_regression_loss: 1.4346 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6806 - regression_loss: 2.8925 - val_loss: 3.4152 - val_regression_loss: 1.5074 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8436 - regression_loss: 2.8982 - val_loss: 3.1768 - val_regression_loss: 1.3022 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6004 - regression_loss: 2.6669 - val_loss: 3.1048 - val_regression_loss: 1.2513 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5072 - regression_loss: 2.6532 - val_loss: 3.1146 - val_regression_loss: 1.2224 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5013 - regression_loss: 2.5925 - val_loss: 3.2252 - val_regression_loss: 1.3326 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3625 - regression_loss: 2.6376 - val_loss: 3.1152 - val_regression_loss: 1.2596 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.4053 - regression_loss: 2.5182 - val_loss: 3.0567 - val_regression_loss: 1.1868 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.1783 - regression_loss: 2.3655 - val_loss: 3.0087 - val_regression_loss: 1.1505 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.1882 - regression_loss: 2.3165 - val_loss: 3.2074 - val_regression_loss: 1.3200 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2759 - regression_loss: 2.3854 - val_loss: 2.9975 - val_regression_loss: 1.1239 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.1607 - regression_loss: 2.2602 - val_loss: 2.9951 - val_regression_loss: 1.1227 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8952 - regression_loss: 2.2170 - val_loss: 2.9874 - val_regression_loss: 1.1229 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0600 - regression_loss: 2.1555 - val_loss: 3.1204 - val_regression_loss: 1.2224 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9750 - regression_loss: 2.2114 - val_loss: 3.1915 - val_regression_loss: 1.2864 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0807 - regression_loss: 2.2424 - val_loss: 2.9133 - val_regression_loss: 1.0569 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9871 - regression_loss: 2.1015 - val_loss: 2.9978 - val_regression_loss: 1.1147 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.9055 - regression_loss: 2.2069\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9243 - regression_loss: 2.0448 - val_loss: 3.0013 - val_regression_loss: 1.1164 - lr: 1.0000e-04\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8540 - regression_loss: 1.9951 - val_loss: 2.9047 - val_regression_loss: 1.0403 - lr: 5.0000e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7816 - regression_loss: 1.9263 - val_loss: 2.9060 - val_regression_loss: 1.0425 - lr: 5.0000e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6932 - regression_loss: 1.9119 - val_loss: 2.8838 - val_regression_loss: 1.0240 - lr: 5.0000e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6870 - regression_loss: 1.8967 - val_loss: 2.8895 - val_regression_loss: 1.0252 - lr: 5.0000e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7858 - regression_loss: 1.9229 - val_loss: 2.8910 - val_regression_loss: 1.0224 - lr: 5.0000e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8211 - regression_loss: 1.9901 - val_loss: 2.9105 - val_regression_loss: 1.0438 - lr: 5.0000e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7365 - regression_loss: 1.9252 - val_loss: 2.8897 - val_regression_loss: 1.0240 - lr: 5.0000e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7194 - regression_loss: 1.8960 - val_loss: 2.8921 - val_regression_loss: 1.0305 - lr: 5.0000e-05\n",
      "Epoch 93/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.8243 - regression_loss: 2.1268\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7529 - regression_loss: 1.8906 - val_loss: 2.8755 - val_regression_loss: 1.0136 - lr: 5.0000e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6434 - regression_loss: 1.8313 - val_loss: 2.8717 - val_regression_loss: 1.0095 - lr: 2.5000e-05\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6203 - regression_loss: 1.7778 - val_loss: 2.8804 - val_regression_loss: 1.0183 - lr: 2.5000e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6834 - regression_loss: 1.8006 - val_loss: 2.8596 - val_regression_loss: 1.0009 - lr: 2.5000e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6163 - regression_loss: 1.7691 - val_loss: 2.8616 - val_regression_loss: 0.9995 - lr: 2.5000e-05\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6482 - regression_loss: 1.7818 - val_loss: 2.8563 - val_regression_loss: 0.9965 - lr: 2.5000e-05\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6408 - regression_loss: 1.7671 - val_loss: 2.8560 - val_regression_loss: 0.9966 - lr: 2.5000e-05\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5736 - regression_loss: 1.7542 - val_loss: 2.8529 - val_regression_loss: 0.9920 - lr: 2.5000e-05\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5470 - regression_loss: 1.7404 - val_loss: 2.8511 - val_regression_loss: 0.9900 - lr: 2.5000e-05\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5901 - regression_loss: 1.7464 - val_loss: 2.8561 - val_regression_loss: 0.9932 - lr: 2.5000e-05\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5250 - regression_loss: 1.7226 - val_loss: 2.8485 - val_regression_loss: 0.9876 - lr: 2.5000e-05\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5628 - regression_loss: 1.7236 - val_loss: 2.8417 - val_regression_loss: 0.9818 - lr: 2.5000e-05\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5668 - regression_loss: 1.7140 - val_loss: 2.8420 - val_regression_loss: 0.9824 - lr: 2.5000e-05\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4274 - regression_loss: 1.7039 - val_loss: 2.8436 - val_regression_loss: 0.9829 - lr: 2.5000e-05\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4987 - regression_loss: 1.7018 - val_loss: 2.8489 - val_regression_loss: 0.9860 - lr: 2.5000e-05\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4897 - regression_loss: 1.6955 - val_loss: 2.8417 - val_regression_loss: 0.9798 - lr: 2.5000e-05\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5045 - regression_loss: 1.6844 - val_loss: 2.8443 - val_regression_loss: 0.9837 - lr: 2.5000e-05\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4881 - regression_loss: 1.6753 - val_loss: 2.8388 - val_regression_loss: 0.9792 - lr: 2.5000e-05\n",
      "Epoch 111/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.0158 - regression_loss: 1.3195\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4976 - regression_loss: 1.6681 - val_loss: 2.8345 - val_regression_loss: 0.9739 - lr: 2.5000e-05\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4219 - regression_loss: 1.6596 - val_loss: 2.8321 - val_regression_loss: 0.9715 - lr: 1.2500e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4919 - regression_loss: 1.6537 - val_loss: 2.8323 - val_regression_loss: 0.9713 - lr: 1.2500e-05\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4831 - regression_loss: 1.6563 - val_loss: 2.8329 - val_regression_loss: 0.9710 - lr: 1.2500e-05\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4453 - regression_loss: 1.6469 - val_loss: 2.8317 - val_regression_loss: 0.9705 - lr: 1.2500e-05\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4168 - regression_loss: 1.6467 - val_loss: 2.8269 - val_regression_loss: 0.9681 - lr: 1.2500e-05\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4151 - regression_loss: 1.6420 - val_loss: 2.8252 - val_regression_loss: 0.9666 - lr: 1.2500e-05\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4153 - regression_loss: 1.6352 - val_loss: 2.8263 - val_regression_loss: 0.9668 - lr: 1.2500e-05\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4152 - regression_loss: 1.6360 - val_loss: 2.8303 - val_regression_loss: 0.9698 - lr: 1.2500e-05\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3750 - regression_loss: 1.6297 - val_loss: 2.8291 - val_regression_loss: 0.9682 - lr: 1.2500e-05\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4497 - regression_loss: 1.6285 - val_loss: 2.8314 - val_regression_loss: 0.9694 - lr: 1.2500e-05\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3909 - regression_loss: 1.6222 - val_loss: 2.8313 - val_regression_loss: 0.9706 - lr: 1.2500e-05\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4397 - regression_loss: 1.6223 - val_loss: 2.8286 - val_regression_loss: 0.9691 - lr: 1.2500e-05\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4299 - regression_loss: 1.6179 - val_loss: 2.8271 - val_regression_loss: 0.9672 - lr: 1.2500e-05\n",
      "Epoch 125/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.3075 - regression_loss: 1.6117\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4406 - regression_loss: 1.6116 - val_loss: 2.8238 - val_regression_loss: 0.9636 - lr: 1.2500e-05\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4028 - regression_loss: 1.6073 - val_loss: 2.8230 - val_regression_loss: 0.9626 - lr: 6.2500e-06\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4640 - regression_loss: 1.6051 - val_loss: 2.8229 - val_regression_loss: 0.9623 - lr: 6.2500e-06\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3648 - regression_loss: 1.6090 - val_loss: 2.8223 - val_regression_loss: 0.9619 - lr: 6.2500e-06\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4266 - regression_loss: 1.6010 - val_loss: 2.8218 - val_regression_loss: 0.9614 - lr: 6.2500e-06\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3843 - regression_loss: 1.5991 - val_loss: 2.8227 - val_regression_loss: 0.9622 - lr: 6.2500e-06\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4116 - regression_loss: 1.6009 - val_loss: 2.8214 - val_regression_loss: 0.9611 - lr: 6.2500e-06\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4430 - regression_loss: 1.6023 - val_loss: 2.8228 - val_regression_loss: 0.9618 - lr: 6.2500e-06\n",
      "Epoch 133/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.8093 - regression_loss: 2.1136\n",
      "Epoch 133: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4010 - regression_loss: 1.5936 - val_loss: 2.8220 - val_regression_loss: 0.9611 - lr: 6.2500e-06\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3733 - regression_loss: 1.5942 - val_loss: 2.8221 - val_regression_loss: 0.9613 - lr: 3.1250e-06\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3021 - regression_loss: 1.5920 - val_loss: 2.8216 - val_regression_loss: 0.9608 - lr: 3.1250e-06\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3478 - regression_loss: 1.5904 - val_loss: 2.8207 - val_regression_loss: 0.9602 - lr: 3.1250e-06\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4021 - regression_loss: 1.5899 - val_loss: 2.8211 - val_regression_loss: 0.9607 - lr: 3.1250e-06\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3993 - regression_loss: 1.5886 - val_loss: 2.8204 - val_regression_loss: 0.9600 - lr: 3.1250e-06\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4212 - regression_loss: 1.5878 - val_loss: 2.8199 - val_regression_loss: 0.9594 - lr: 3.1250e-06\n",
      "Epoch 140/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.0411 - regression_loss: 2.3455\n",
      "Epoch 140: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4270 - regression_loss: 1.5887 - val_loss: 2.8201 - val_regression_loss: 0.9596 - lr: 3.1250e-06\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3969 - regression_loss: 1.5859 - val_loss: 2.8201 - val_regression_loss: 0.9596 - lr: 1.5625e-06\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3739 - regression_loss: 1.5849 - val_loss: 2.8198 - val_regression_loss: 0.9593 - lr: 1.5625e-06\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3947 - regression_loss: 1.5845 - val_loss: 2.8197 - val_regression_loss: 0.9593 - lr: 1.5625e-06\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3809 - regression_loss: 1.5844 - val_loss: 2.8194 - val_regression_loss: 0.9590 - lr: 1.5625e-06\n",
      "Epoch 145/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.5965 - regression_loss: 1.9009\n",
      "Epoch 145: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4237 - regression_loss: 1.5839 - val_loss: 2.8193 - val_regression_loss: 0.9590 - lr: 1.5625e-06\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4005 - regression_loss: 1.5830 - val_loss: 2.8194 - val_regression_loss: 0.9590 - lr: 7.8125e-07\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3975 - regression_loss: 1.5827 - val_loss: 2.8194 - val_regression_loss: 0.9590 - lr: 7.8125e-07\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3948 - regression_loss: 1.5834 - val_loss: 2.8195 - val_regression_loss: 0.9591 - lr: 7.8125e-07\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4215 - regression_loss: 1.5824 - val_loss: 2.8193 - val_regression_loss: 0.9588 - lr: 7.8125e-07\n",
      "Epoch 150/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.5215 - regression_loss: 1.8260\n",
      "Epoch 150: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4100 - regression_loss: 1.5821 - val_loss: 2.8193 - val_regression_loss: 0.9589 - lr: 7.8125e-07\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3625 - regression_loss: 1.5818 - val_loss: 2.8193 - val_regression_loss: 0.9588 - lr: 3.9062e-07\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3998 - regression_loss: 1.5816 - val_loss: 2.8193 - val_regression_loss: 0.9588 - lr: 3.9062e-07\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3256 - regression_loss: 1.5815 - val_loss: 2.8192 - val_regression_loss: 0.9588 - lr: 3.9062e-07\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3367 - regression_loss: 1.5814 - val_loss: 2.8191 - val_regression_loss: 0.9587 - lr: 3.9062e-07\n",
      "Epoch 155/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.4307 - regression_loss: 2.7352\n",
      "Epoch 155: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4104 - regression_loss: 1.5814 - val_loss: 2.8192 - val_regression_loss: 0.9588 - lr: 3.9062e-07\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4226 - regression_loss: 1.5812 - val_loss: 2.8191 - val_regression_loss: 0.9587 - lr: 1.9531e-07\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3943 - regression_loss: 1.5810 - val_loss: 2.8191 - val_regression_loss: 0.9587 - lr: 1.9531e-07\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4229 - regression_loss: 1.5810 - val_loss: 2.8191 - val_regression_loss: 0.9587 - lr: 1.9531e-07\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3551 - regression_loss: 1.5810 - val_loss: 2.8191 - val_regression_loss: 0.9587 - lr: 1.9531e-07\n",
      "Epoch 160/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.6309 - regression_loss: 0.9353\n",
      "Epoch 160: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3359 - regression_loss: 1.5809 - val_loss: 2.8191 - val_regression_loss: 0.9587 - lr: 1.9531e-07\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3517 - regression_loss: 1.5808 - val_loss: 2.8191 - val_regression_loss: 0.9587 - lr: 9.7656e-08\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4113 - regression_loss: 1.5808 - val_loss: 2.8191 - val_regression_loss: 0.9587 - lr: 9.7656e-08\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3290 - regression_loss: 1.5808 - val_loss: 2.8191 - val_regression_loss: 0.9587 - lr: 9.7656e-08\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3902 - regression_loss: 1.5807 - val_loss: 2.8191 - val_regression_loss: 0.9586 - lr: 9.7656e-08\n",
      "Epoch 165/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.6396 - regression_loss: 1.9441\n",
      "Epoch 165: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4036 - regression_loss: 1.5807 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 9.7656e-08\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4191 - regression_loss: 1.5806 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 4.8828e-08\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3883 - regression_loss: 1.5806 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 4.8828e-08\n",
      "Epoch 168/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3714 - regression_loss: 1.5806 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 4.8828e-08\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4341 - regression_loss: 1.5806 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 4.8828e-08\n",
      "Epoch 170/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.1313 - regression_loss: 2.4358\n",
      "Epoch 170: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3906 - regression_loss: 1.5806 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 4.8828e-08\n",
      "Epoch 171/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3316 - regression_loss: 1.5806 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 2.4414e-08\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3266 - regression_loss: 1.5806 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 2.4414e-08\n",
      "Epoch 173/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3075 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 2.4414e-08\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3656 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 2.4414e-08\n",
      "Epoch 175/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.5497 - regression_loss: 1.8542\n",
      "Epoch 175: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4121 - regression_loss: 1.5806 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 2.4414e-08\n",
      "Epoch 176/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4203 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 1.2207e-08\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3713 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 1.2207e-08\n",
      "Epoch 178/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3739 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 1.2207e-08\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3061 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 1.2207e-08\n",
      "Epoch 180/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.4698 - regression_loss: 1.7743\n",
      "Epoch 180: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3958 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 1.2207e-08\n",
      "Epoch 181/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4108 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 6.1035e-09\n",
      "Epoch 182/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4019 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 6.1035e-09\n",
      "Epoch 183/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3423 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 6.1035e-09\n",
      "Epoch 184/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3622 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 6.1035e-09\n",
      "Epoch 185/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.2428 - regression_loss: 1.5473\n",
      "Epoch 185: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3274 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 6.1035e-09\n",
      "Epoch 186/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3974 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 3.0518e-09\n",
      "Epoch 187/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3034 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 3.0518e-09\n",
      "Epoch 188/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4226 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 3.0518e-09\n",
      "Epoch 189/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4079 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 3.0518e-09\n",
      "Epoch 190/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.0022 - regression_loss: 1.3067\n",
      "Epoch 190: ReduceLROnPlateau reducing learning rate to 1.5258788677030566e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3868 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 3.0518e-09\n",
      "Epoch 191/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4104 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 1.5259e-09\n",
      "Epoch 192/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3335 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 1.5259e-09\n",
      "Epoch 193/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3511 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 1.5259e-09\n",
      "Epoch 194/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4073 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 1.5259e-09\n",
      "Epoch 195/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.1340 - regression_loss: 1.4385\n",
      "Epoch 195: ReduceLROnPlateau reducing learning rate to 7.629394338515283e-10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4168 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 1.5259e-09\n",
      "Epoch 196/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3369 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 7.6294e-10\n",
      "Epoch 197/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3940 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 7.6294e-10\n",
      "Epoch 198/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4063 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 7.6294e-10\n",
      "Epoch 199/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4138 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 7.6294e-10\n",
      "Epoch 200/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.4101 - regression_loss: 1.7146\n",
      "Epoch 200: ReduceLROnPlateau reducing learning rate to 3.8146971692576415e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3475 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 7.6294e-10\n",
      "Epoch 201/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3456 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 3.8147e-10\n",
      "Epoch 202/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4154 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 3.8147e-10\n",
      "Epoch 203/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3794 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 3.8147e-10\n",
      "Epoch 204/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4149 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 3.8147e-10\n",
      "Epoch 205/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.4887 - regression_loss: 1.7932\n",
      "Epoch 205: ReduceLROnPlateau reducing learning rate to 1.9073485846288207e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3625 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 3.8147e-10\n",
      "Epoch 206/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3941 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 1.9073e-10\n",
      "Epoch 207/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3758 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 1.9073e-10\n",
      "Epoch 208/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4184 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 1.9073e-10\n",
      "Epoch 209/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4039 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 1.9073e-10\n",
      "Epoch 210/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.5297 - regression_loss: 1.8342\n",
      "Epoch 210: ReduceLROnPlateau reducing learning rate to 9.536742923144104e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3300 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 1.9073e-10\n",
      "Epoch 211/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3979 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 9.5367e-11\n",
      "Epoch 212/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4091 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 9.5367e-11\n",
      "Epoch 213/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4019 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 9.5367e-11\n",
      "Epoch 214/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4005 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 9.5367e-11\n",
      "Epoch 215/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.0888 - regression_loss: 1.3933\n",
      "Epoch 215: ReduceLROnPlateau reducing learning rate to 4.768371461572052e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4219 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 9.5367e-11\n",
      "Epoch 216/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3134 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 4.7684e-11\n",
      "Epoch 217/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3991 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 4.7684e-11\n",
      "Epoch 218/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4188 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 4.7684e-11\n",
      "Epoch 219/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3827 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 4.7684e-11\n",
      "Epoch 220/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.3351 - regression_loss: 2.6396\n",
      "Epoch 220: ReduceLROnPlateau reducing learning rate to 2.384185730786026e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3978 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 4.7684e-11\n",
      "Epoch 221/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3795 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 2.3842e-11\n",
      "Epoch 222/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4005 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 2.3842e-11\n",
      "Epoch 223/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3758 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 2.3842e-11\n",
      "Epoch 224/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3920 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 2.3842e-11\n",
      "Epoch 225/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.7878 - regression_loss: 3.0923\n",
      "Epoch 225: ReduceLROnPlateau reducing learning rate to 1.192092865393013e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3891 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 2.3842e-11\n",
      "Epoch 226/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3624 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 1.1921e-11\n",
      "Epoch 227/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3605 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 1.1921e-11\n",
      "Epoch 228/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4264 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 1.1921e-11\n",
      "Epoch 229/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3171 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 1.1921e-11\n",
      "Epoch 230/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.8031 - regression_loss: 2.1076\n",
      "Epoch 230: ReduceLROnPlateau reducing learning rate to 5.960464326965065e-12.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4132 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 1.1921e-11\n",
      "Epoch 231/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3715 - regression_loss: 1.5805 - val_loss: 2.8190 - val_regression_loss: 0.9586 - lr: 5.9605e-12\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 24ms/step - loss: 93.0854 - regression_loss: 83.4717 - val_loss: 65.7193 - val_regression_loss: 53.8860 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 61.4690 - regression_loss: 55.8789 - val_loss: 51.4043 - val_regression_loss: 39.5758 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 42.9590 - regression_loss: 38.5756 - val_loss: 41.2563 - val_regression_loss: 30.6607 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31.7109 - regression_loss: 28.4924 - val_loss: 33.9195 - val_regression_loss: 24.9228 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.8055 - regression_loss: 22.3102 - val_loss: 30.0108 - val_regression_loss: 21.4404 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.1098 - regression_loss: 18.7108 - val_loss: 25.8545 - val_regression_loss: 18.5281 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.9580 - regression_loss: 16.1504 - val_loss: 22.9721 - val_regression_loss: 16.3440 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2228 - regression_loss: 14.2787 - val_loss: 20.4330 - val_regression_loss: 14.6893 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.1758 - regression_loss: 12.8465 - val_loss: 18.3721 - val_regression_loss: 13.1406 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1392 - regression_loss: 11.5313 - val_loss: 16.6631 - val_regression_loss: 12.0669 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1159 - regression_loss: 10.4289 - val_loss: 15.3762 - val_regression_loss: 11.0419 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0636 - regression_loss: 9.6066 - val_loss: 14.2811 - val_regression_loss: 10.2630 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4172 - regression_loss: 8.8972 - val_loss: 13.4688 - val_regression_loss: 9.6171 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.6955 - regression_loss: 8.2973 - val_loss: 12.8025 - val_regression_loss: 9.1018 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0091 - regression_loss: 7.7360 - val_loss: 12.1006 - val_regression_loss: 8.6019 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3635 - regression_loss: 7.2124 - val_loss: 11.6531 - val_regression_loss: 8.2132 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0535 - regression_loss: 6.7803 - val_loss: 11.0408 - val_regression_loss: 7.8368 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.6242 - regression_loss: 6.5502 - val_loss: 10.6103 - val_regression_loss: 7.4270 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3422 - regression_loss: 6.1518 - val_loss: 10.1163 - val_regression_loss: 7.1085 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8797 - regression_loss: 5.8564 - val_loss: 9.8289 - val_regression_loss: 6.8553 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7271 - regression_loss: 5.5945 - val_loss: 9.5279 - val_regression_loss: 6.6895 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4215 - regression_loss: 5.4301 - val_loss: 9.2204 - val_regression_loss: 6.3908 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.3842 - regression_loss: 5.2468 - val_loss: 8.9754 - val_regression_loss: 6.2056 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.2297 - regression_loss: 5.1022 - val_loss: 8.7709 - val_regression_loss: 6.0715 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9989 - regression_loss: 4.9082 - val_loss: 8.5082 - val_regression_loss: 5.8224 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7484 - regression_loss: 4.6945 - val_loss: 8.3156 - val_regression_loss: 5.7346 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6844 - regression_loss: 4.5783 - val_loss: 8.1196 - val_regression_loss: 5.5148 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4441 - regression_loss: 4.4375 - val_loss: 8.0205 - val_regression_loss: 5.4122 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3064 - regression_loss: 4.2957 - val_loss: 7.9546 - val_regression_loss: 5.4337 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2375 - regression_loss: 4.3284 - val_loss: 7.8943 - val_regression_loss: 5.2461 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.1796 - regression_loss: 4.1632 - val_loss: 7.5709 - val_regression_loss: 5.0934 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8567 - regression_loss: 3.9684 - val_loss: 7.4203 - val_regression_loss: 4.9211 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8266 - regression_loss: 3.9513 - val_loss: 7.3145 - val_regression_loss: 4.8409 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8112 - regression_loss: 3.8517 - val_loss: 7.1927 - val_regression_loss: 4.7826 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6649 - regression_loss: 3.7179 - val_loss: 7.1970 - val_regression_loss: 4.6979 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5357 - regression_loss: 3.7160 - val_loss: 7.2114 - val_regression_loss: 4.8620 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6295 - regression_loss: 3.6681 - val_loss: 7.1960 - val_regression_loss: 4.6539 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.4694 - regression_loss: 3.5439 - val_loss: 7.2265 - val_regression_loss: 4.8535 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7191 - regression_loss: 3.7921 - val_loss: 7.0670 - val_regression_loss: 4.5331 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8266 - regression_loss: 3.7973 - val_loss: 6.5871 - val_regression_loss: 4.2394 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.4224 - regression_loss: 3.4034 - val_loss: 6.5295 - val_regression_loss: 4.2393 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.3086 - regression_loss: 3.3806 - val_loss: 6.5166 - val_regression_loss: 4.1393 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1627 - regression_loss: 3.2319 - val_loss: 6.3866 - val_regression_loss: 4.0925 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9879 - regression_loss: 3.1918 - val_loss: 6.2629 - val_regression_loss: 3.9941 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8806 - regression_loss: 3.0887 - val_loss: 6.2250 - val_regression_loss: 3.9411 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0582 - regression_loss: 3.1308 - val_loss: 6.3381 - val_regression_loss: 3.9916 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0441 - regression_loss: 3.0914 - val_loss: 6.3592 - val_regression_loss: 4.1182 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0025 - regression_loss: 3.0818 - val_loss: 6.4955 - val_regression_loss: 4.0746 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7743 - regression_loss: 3.0702 - val_loss: 6.1868 - val_regression_loss: 4.0010 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9395 - regression_loss: 3.0241 - val_loss: 6.2164 - val_regression_loss: 3.8795 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7428 - regression_loss: 2.8793 - val_loss: 6.1413 - val_regression_loss: 3.9361 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8965 - regression_loss: 2.9706 - val_loss: 5.9532 - val_regression_loss: 3.6741 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6781 - regression_loss: 2.7732 - val_loss: 5.9358 - val_regression_loss: 3.7818 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.7909 - regression_loss: 2.8689 - val_loss: 5.8449 - val_regression_loss: 3.5941 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7380 - regression_loss: 2.8384 - val_loss: 5.6907 - val_regression_loss: 3.5149 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.4644 - regression_loss: 2.7104 - val_loss: 5.6510 - val_regression_loss: 3.4784 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6192 - regression_loss: 2.7198 - val_loss: 5.6895 - val_regression_loss: 3.4856 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5637 - regression_loss: 2.6774 - val_loss: 5.6220 - val_regression_loss: 3.4335 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5789 - regression_loss: 2.7039 - val_loss: 5.5229 - val_regression_loss: 3.4274 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.4374 - regression_loss: 2.5728 - val_loss: 5.5970 - val_regression_loss: 3.4063 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.4605 - regression_loss: 2.6491 - val_loss: 5.5241 - val_regression_loss: 3.3515 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.4041 - regression_loss: 2.6219 - val_loss: 5.6244 - val_regression_loss: 3.4079 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.4056 - regression_loss: 2.5468 - val_loss: 5.4210 - val_regression_loss: 3.3262 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3738 - regression_loss: 2.5184 - val_loss: 5.4069 - val_regression_loss: 3.2768 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3898 - regression_loss: 2.4714 - val_loss: 5.3345 - val_regression_loss: 3.2329 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3401 - regression_loss: 2.4547 - val_loss: 5.2829 - val_regression_loss: 3.1928 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3108 - regression_loss: 2.4600 - val_loss: 5.2624 - val_regression_loss: 3.1691 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2148 - regression_loss: 2.4193 - val_loss: 5.2928 - val_regression_loss: 3.1637 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2014 - regression_loss: 2.4047 - val_loss: 5.1919 - val_regression_loss: 3.1154 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.1952 - regression_loss: 2.3710 - val_loss: 5.2119 - val_regression_loss: 3.1101 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2590 - regression_loss: 2.4125 - val_loss: 5.3086 - val_regression_loss: 3.2333 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3691 - regression_loss: 2.4618 - val_loss: 5.2199 - val_regression_loss: 3.0945 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.1850 - regression_loss: 2.3172 - val_loss: 5.1475 - val_regression_loss: 3.0675 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2195 - regression_loss: 2.3457 - val_loss: 5.0785 - val_regression_loss: 3.0192 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.1531 - regression_loss: 2.3002 - val_loss: 5.1191 - val_regression_loss: 3.0181 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.1342 - regression_loss: 2.2949 - val_loss: 5.0790 - val_regression_loss: 3.0121 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0629 - regression_loss: 2.2468 - val_loss: 5.0400 - val_regression_loss: 2.9619 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0590 - regression_loss: 2.2161 - val_loss: 4.9956 - val_regression_loss: 2.9651 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9980 - regression_loss: 2.2093 - val_loss: 5.1395 - val_regression_loss: 3.0164 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0979 - regression_loss: 2.3075 - val_loss: 5.5836 - val_regression_loss: 3.4828 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3120 - regression_loss: 2.4975 - val_loss: 5.3566 - val_regression_loss: 3.1661 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.1373 - regression_loss: 2.2914 - val_loss: 4.9325 - val_regression_loss: 2.8969 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.1264 - regression_loss: 2.2927 - val_loss: 5.0158 - val_regression_loss: 2.9815 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.4225 - regression_loss: 2.7456\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0966 - regression_loss: 2.2317 - val_loss: 4.9150 - val_regression_loss: 2.8422 - lr: 1.0000e-04\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9711 - regression_loss: 2.1477 - val_loss: 4.8459 - val_regression_loss: 2.8343 - lr: 5.0000e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9514 - regression_loss: 2.1376 - val_loss: 4.8704 - val_regression_loss: 2.8124 - lr: 5.0000e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9706 - regression_loss: 2.1428 - val_loss: 4.8372 - val_regression_loss: 2.8146 - lr: 5.0000e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9565 - regression_loss: 2.1032 - val_loss: 4.8420 - val_regression_loss: 2.7939 - lr: 5.0000e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9236 - regression_loss: 2.0982 - val_loss: 4.8038 - val_regression_loss: 2.7818 - lr: 5.0000e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9166 - regression_loss: 2.0903 - val_loss: 4.8163 - val_regression_loss: 2.7770 - lr: 5.0000e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8624 - regression_loss: 2.0900 - val_loss: 4.8064 - val_regression_loss: 2.7796 - lr: 5.0000e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9044 - regression_loss: 2.0830 - val_loss: 4.7957 - val_regression_loss: 2.7715 - lr: 5.0000e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9226 - regression_loss: 2.0771 - val_loss: 4.7894 - val_regression_loss: 2.7587 - lr: 5.0000e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9046 - regression_loss: 2.0684 - val_loss: 4.7795 - val_regression_loss: 2.7470 - lr: 5.0000e-05\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9169 - regression_loss: 2.0677 - val_loss: 4.7837 - val_regression_loss: 2.7541 - lr: 5.0000e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8563 - regression_loss: 2.0508 - val_loss: 4.7660 - val_regression_loss: 2.7362 - lr: 5.0000e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8891 - regression_loss: 2.0643 - val_loss: 4.7315 - val_regression_loss: 2.7192 - lr: 5.0000e-05\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9005 - regression_loss: 2.0680 - val_loss: 4.7636 - val_regression_loss: 2.7266 - lr: 5.0000e-05\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7829 - regression_loss: 2.0371 - val_loss: 4.7326 - val_regression_loss: 2.7093 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8790 - regression_loss: 2.0262 - val_loss: 4.7265 - val_regression_loss: 2.7096 - lr: 5.0000e-05\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8261 - regression_loss: 2.0232 - val_loss: 4.7353 - val_regression_loss: 2.7022 - lr: 5.0000e-05\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8361 - regression_loss: 2.0343 - val_loss: 4.7407 - val_regression_loss: 2.7250 - lr: 5.0000e-05\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9016 - regression_loss: 2.0521 - val_loss: 4.7432 - val_regression_loss: 2.7004 - lr: 5.0000e-05\n",
      "Epoch 104/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.3111 - regression_loss: 1.6373\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7964 - regression_loss: 2.0268 - val_loss: 4.7014 - val_regression_loss: 2.6963 - lr: 5.0000e-05\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8217 - regression_loss: 1.9940 - val_loss: 4.7081 - val_regression_loss: 2.6822 - lr: 2.5000e-05\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8324 - regression_loss: 2.0087 - val_loss: 4.7056 - val_regression_loss: 2.6807 - lr: 2.5000e-05\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7469 - regression_loss: 1.9848 - val_loss: 4.7050 - val_regression_loss: 2.6918 - lr: 2.5000e-05\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7546 - regression_loss: 1.9905 - val_loss: 4.7111 - val_regression_loss: 2.6819 - lr: 2.5000e-05\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8203 - regression_loss: 1.9858 - val_loss: 4.6948 - val_regression_loss: 2.6744 - lr: 2.5000e-05\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7964 - regression_loss: 1.9840 - val_loss: 4.6737 - val_regression_loss: 2.6638 - lr: 2.5000e-05\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7837 - regression_loss: 1.9820 - val_loss: 4.6826 - val_regression_loss: 2.6584 - lr: 2.5000e-05\n",
      "Epoch 112/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.9297 - regression_loss: 2.2567\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7857 - regression_loss: 1.9746 - val_loss: 4.6673 - val_regression_loss: 2.6545 - lr: 2.5000e-05\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7472 - regression_loss: 1.9689 - val_loss: 4.6751 - val_regression_loss: 2.6544 - lr: 1.2500e-05\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7838 - regression_loss: 1.9639 - val_loss: 4.6777 - val_regression_loss: 2.6553 - lr: 1.2500e-05\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7681 - regression_loss: 1.9743 - val_loss: 4.6664 - val_regression_loss: 2.6541 - lr: 1.2500e-05\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7569 - regression_loss: 1.9644 - val_loss: 4.6724 - val_regression_loss: 2.6496 - lr: 1.2500e-05\n",
      "Epoch 117/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.7310 - regression_loss: 3.0582\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8057 - regression_loss: 1.9679 - val_loss: 4.6666 - val_regression_loss: 2.6461 - lr: 1.2500e-05\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7692 - regression_loss: 1.9590 - val_loss: 4.6592 - val_regression_loss: 2.6436 - lr: 6.2500e-06\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7795 - regression_loss: 1.9593 - val_loss: 4.6561 - val_regression_loss: 2.6447 - lr: 6.2500e-06\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7912 - regression_loss: 1.9576 - val_loss: 4.6568 - val_regression_loss: 2.6422 - lr: 6.2500e-06\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7544 - regression_loss: 1.9566 - val_loss: 4.6580 - val_regression_loss: 2.6416 - lr: 6.2500e-06\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7159 - regression_loss: 1.9537 - val_loss: 4.6611 - val_regression_loss: 2.6422 - lr: 6.2500e-06\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7646 - regression_loss: 1.9599 - val_loss: 4.6675 - val_regression_loss: 2.6438 - lr: 6.2500e-06\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7415 - regression_loss: 1.9547 - val_loss: 4.6597 - val_regression_loss: 2.6408 - lr: 6.2500e-06\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7640 - regression_loss: 1.9540 - val_loss: 4.6508 - val_regression_loss: 2.6378 - lr: 6.2500e-06\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7829 - regression_loss: 1.9564 - val_loss: 4.6500 - val_regression_loss: 2.6378 - lr: 6.2500e-06\n",
      "Epoch 127/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.1491 - regression_loss: 2.4764\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7578 - regression_loss: 1.9514 - val_loss: 4.6520 - val_regression_loss: 2.6360 - lr: 6.2500e-06\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7772 - regression_loss: 1.9509 - val_loss: 4.6520 - val_regression_loss: 2.6355 - lr: 3.1250e-06\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7348 - regression_loss: 1.9515 - val_loss: 4.6543 - val_regression_loss: 2.6359 - lr: 3.1250e-06\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7565 - regression_loss: 1.9505 - val_loss: 4.6546 - val_regression_loss: 2.6364 - lr: 3.1250e-06\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7576 - regression_loss: 1.9487 - val_loss: 4.6533 - val_regression_loss: 2.6359 - lr: 3.1250e-06\n",
      "Epoch 132/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.5354 - regression_loss: 1.8629\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7825 - regression_loss: 1.9485 - val_loss: 4.6501 - val_regression_loss: 2.6347 - lr: 3.1250e-06\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7043 - regression_loss: 1.9482 - val_loss: 4.6492 - val_regression_loss: 2.6347 - lr: 1.5625e-06\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7090 - regression_loss: 1.9483 - val_loss: 4.6495 - val_regression_loss: 2.6343 - lr: 1.5625e-06\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7797 - regression_loss: 1.9487 - val_loss: 4.6490 - val_regression_loss: 2.6344 - lr: 1.5625e-06\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7614 - regression_loss: 1.9483 - val_loss: 4.6489 - val_regression_loss: 2.6344 - lr: 1.5625e-06\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7610 - regression_loss: 1.9474 - val_loss: 4.6478 - val_regression_loss: 2.6332 - lr: 1.5625e-06\n",
      "Epoch 138/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.6840 - regression_loss: 2.0114\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7593 - regression_loss: 1.9478 - val_loss: 4.6497 - val_regression_loss: 2.6332 - lr: 1.5625e-06\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7653 - regression_loss: 1.9470 - val_loss: 4.6498 - val_regression_loss: 2.6330 - lr: 7.8125e-07\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7914 - regression_loss: 1.9468 - val_loss: 4.6493 - val_regression_loss: 2.6327 - lr: 7.8125e-07\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7427 - regression_loss: 1.9468 - val_loss: 4.6487 - val_regression_loss: 2.6326 - lr: 7.8125e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7638 - regression_loss: 1.9468 - val_loss: 4.6487 - val_regression_loss: 2.6327 - lr: 7.8125e-07\n",
      "Epoch 143/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.3614 - regression_loss: 2.6889\n",
      "Epoch 143: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7486 - regression_loss: 1.9464 - val_loss: 4.6489 - val_regression_loss: 2.6326 - lr: 7.8125e-07\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7009 - regression_loss: 1.9462 - val_loss: 4.6485 - val_regression_loss: 2.6324 - lr: 3.9062e-07\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7467 - regression_loss: 1.9460 - val_loss: 4.6487 - val_regression_loss: 2.6324 - lr: 3.9062e-07\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7761 - regression_loss: 1.9460 - val_loss: 4.6485 - val_regression_loss: 2.6324 - lr: 3.9062e-07\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7869 - regression_loss: 1.9460 - val_loss: 4.6483 - val_regression_loss: 2.6322 - lr: 3.9062e-07\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7619 - regression_loss: 1.9459 - val_loss: 4.6484 - val_regression_loss: 2.6322 - lr: 3.9062e-07\n",
      "Epoch 149/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.7317 - regression_loss: 2.0592\n",
      "Epoch 149: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7704 - regression_loss: 1.9461 - val_loss: 4.6479 - val_regression_loss: 2.6321 - lr: 3.9062e-07\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7578 - regression_loss: 1.9459 - val_loss: 4.6478 - val_regression_loss: 2.6320 - lr: 1.9531e-07\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7661 - regression_loss: 1.9458 - val_loss: 4.6478 - val_regression_loss: 2.6320 - lr: 1.9531e-07\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7818 - regression_loss: 1.9458 - val_loss: 4.6479 - val_regression_loss: 2.6320 - lr: 1.9531e-07\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7158 - regression_loss: 1.9458 - val_loss: 4.6480 - val_regression_loss: 2.6320 - lr: 1.9531e-07\n",
      "Epoch 154/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.3779 - regression_loss: 1.7054\n",
      "Epoch 154: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7463 - regression_loss: 1.9458 - val_loss: 4.6477 - val_regression_loss: 2.6319 - lr: 1.9531e-07\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7879 - regression_loss: 1.9456 - val_loss: 4.6477 - val_regression_loss: 2.6319 - lr: 9.7656e-08\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7059 - regression_loss: 1.9456 - val_loss: 4.6477 - val_regression_loss: 2.6319 - lr: 9.7656e-08\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7587 - regression_loss: 1.9456 - val_loss: 4.6476 - val_regression_loss: 2.6319 - lr: 9.7656e-08\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7692 - regression_loss: 1.9456 - val_loss: 4.6476 - val_regression_loss: 2.6319 - lr: 9.7656e-08\n",
      "Epoch 159/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.5846 - regression_loss: 1.9121\n",
      "Epoch 159: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7626 - regression_loss: 1.9456 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 9.7656e-08\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7275 - regression_loss: 1.9456 - val_loss: 4.6476 - val_regression_loss: 2.6319 - lr: 4.8828e-08\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7520 - regression_loss: 1.9456 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 4.8828e-08\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7114 - regression_loss: 1.9456 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 4.8828e-08\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7628 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 4.8828e-08\n",
      "Epoch 164/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.9470 - regression_loss: 2.2745\n",
      "Epoch 164: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7765 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 4.8828e-08\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7254 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 2.4414e-08\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7445 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 2.4414e-08\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7670 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 2.4414e-08\n",
      "Epoch 168/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7299 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 2.4414e-08\n",
      "Epoch 169/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.6836 - regression_loss: 2.0111\n",
      "Epoch 169: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7580 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 2.4414e-08\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7557 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 1.2207e-08\n",
      "Epoch 171/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7436 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 1.2207e-08\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7567 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 1.2207e-08\n",
      "Epoch 173/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7508 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 1.2207e-08\n",
      "Epoch 174/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.6854 - regression_loss: 2.0129\n",
      "Epoch 174: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7389 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 1.2207e-08\n",
      "Epoch 175/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7670 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 6.1035e-09\n",
      "Epoch 176/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7670 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 6.1035e-09\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7660 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 6.1035e-09\n",
      "Epoch 178/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6949 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 6.1035e-09\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7597 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 6.1035e-09\n",
      "Epoch 180/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7635 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 6.1035e-09\n",
      "Epoch 181/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7462 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 6.1035e-09\n",
      "Epoch 182/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7976 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 6.1035e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.4016 - regression_loss: 2.7291\n",
      "Epoch 183: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7098 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 6.1035e-09\n",
      "Epoch 184/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7694 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 3.0518e-09\n",
      "Epoch 185/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7934 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 3.0518e-09\n",
      "Epoch 186/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7272 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 3.0518e-09\n",
      "Epoch 187/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7367 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 3.0518e-09\n",
      "Epoch 188/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.6937 - regression_loss: 2.0212\n",
      "Epoch 188: ReduceLROnPlateau reducing learning rate to 1.5258788677030566e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7425 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 3.0518e-09\n",
      "Epoch 189/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7734 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 1.5259e-09\n",
      "Epoch 190/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7615 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 1.5259e-09\n",
      "Epoch 191/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7667 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 1.5259e-09\n",
      "Epoch 192/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7572 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 1.5259e-09\n",
      "Epoch 193/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6824 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 1.5259e-09\n",
      "Epoch 194/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7423 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 1.5259e-09\n",
      "Epoch 195/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7523 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 1.5259e-09\n",
      "Epoch 196/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7168 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 1.5259e-09\n",
      "Epoch 197/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7142 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 1.5259e-09\n",
      "Epoch 198/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.5068 - regression_loss: 1.8343\n",
      "Epoch 198: ReduceLROnPlateau reducing learning rate to 7.629394338515283e-10.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7577 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 1.5259e-09\n",
      "Epoch 199/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7355 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 7.6294e-10\n",
      "Epoch 200/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7623 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 7.6294e-10\n",
      "Epoch 201/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7323 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 7.6294e-10\n",
      "Epoch 202/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7486 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 7.6294e-10\n",
      "Epoch 203/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.1233 - regression_loss: 1.4508\n",
      "Epoch 203: ReduceLROnPlateau reducing learning rate to 3.8146971692576415e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7573 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 7.6294e-10\n",
      "Epoch 204/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7639 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 3.8147e-10\n",
      "Epoch 205/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7852 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 3.8147e-10\n",
      "Epoch 206/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7645 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 3.8147e-10\n",
      "Epoch 207/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7430 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 3.8147e-10\n",
      "Epoch 208/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.0273 - regression_loss: 2.3548\n",
      "Epoch 208: ReduceLROnPlateau reducing learning rate to 1.9073485846288207e-10.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7700 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 3.8147e-10\n",
      "Epoch 209/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7530 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 1.9073e-10\n",
      "Epoch 210/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7728 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 1.9073e-10\n",
      "Epoch 211/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7713 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 1.9073e-10\n",
      "Epoch 212/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7169 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 1.9073e-10\n",
      "Epoch 213/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.2399 - regression_loss: 2.5674\n",
      "Epoch 213: ReduceLROnPlateau reducing learning rate to 9.536742923144104e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7677 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 1.9073e-10\n",
      "Epoch 214/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7687 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 9.5367e-11\n",
      "Epoch 215/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7012 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 9.5367e-11\n",
      "Epoch 216/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8079 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 9.5367e-11\n",
      "Epoch 217/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7160 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 9.5367e-11\n",
      "Epoch 218/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.4000 - regression_loss: 1.7275\n",
      "Epoch 218: ReduceLROnPlateau reducing learning rate to 4.768371461572052e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7760 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 9.5367e-11\n",
      "Epoch 219/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7339 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 4.7684e-11\n",
      "Epoch 220/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7484 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 4.7684e-11\n",
      "Epoch 221/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7220 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 4.7684e-11\n",
      "Epoch 222/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7438 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 4.7684e-11\n",
      "Epoch 223/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.6392 - regression_loss: 1.9666\n",
      "Epoch 223: ReduceLROnPlateau reducing learning rate to 2.384185730786026e-11.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7553 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 4.7684e-11\n",
      "Epoch 224/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7490 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 2.3842e-11\n",
      "Epoch 225/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7320 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 2.3842e-11\n",
      "Epoch 226/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7698 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 2.3842e-11\n",
      "Epoch 227/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7707 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 2.3842e-11\n",
      "Epoch 228/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.3146 - regression_loss: 1.6420\n",
      "Epoch 228: ReduceLROnPlateau reducing learning rate to 1.192092865393013e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7590 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 2.3842e-11\n",
      "Epoch 229/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7664 - regression_loss: 1.9455 - val_loss: 4.6476 - val_regression_loss: 2.6318 - lr: 1.1921e-11\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 24ms/step - loss: 138.7045 - regression_loss: 126.2835 - val_loss: 61.6427 - val_regression_loss: 49.8153 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 74.2823 - regression_loss: 67.6077 - val_loss: 39.9433 - val_regression_loss: 33.7196 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 52.9353 - regression_loss: 48.3649 - val_loss: 34.4428 - val_regression_loss: 30.4116 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 50.4284 - regression_loss: 45.7548 - val_loss: 32.2556 - val_regression_loss: 28.9236 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 49.9053 - regression_loss: 44.1032 - val_loss: 31.3184 - val_regression_loss: 27.7596 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 45.1006 - regression_loss: 40.4665 - val_loss: 29.7698 - val_regression_loss: 25.9470 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 41.4854 - regression_loss: 37.2914 - val_loss: 28.1813 - val_regression_loss: 24.1153 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 38.9692 - regression_loss: 35.2994 - val_loss: 28.4924 - val_regression_loss: 24.0947 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 38.1856 - regression_loss: 33.9684 - val_loss: 28.1123 - val_regression_loss: 23.7142 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 35.2010 - regression_loss: 32.4767 - val_loss: 25.1859 - val_regression_loss: 21.1902 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 35.1170 - regression_loss: 31.0862 - val_loss: 26.3449 - val_regression_loss: 22.2866 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 33.4804 - regression_loss: 30.0514 - val_loss: 23.5507 - val_regression_loss: 19.8373 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.7913 - regression_loss: 28.9962 - val_loss: 23.4823 - val_regression_loss: 19.7936 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.0528 - regression_loss: 27.8757 - val_loss: 23.0267 - val_regression_loss: 19.3746 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.6283 - regression_loss: 27.0255 - val_loss: 22.6381 - val_regression_loss: 19.0288 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.6777 - regression_loss: 26.1721 - val_loss: 21.9552 - val_regression_loss: 18.3819 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.7373 - regression_loss: 25.4368 - val_loss: 21.2758 - val_regression_loss: 17.7660 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.3855 - regression_loss: 24.6504 - val_loss: 21.7257 - val_regression_loss: 18.2562 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.5034 - regression_loss: 24.1135 - val_loss: 20.8908 - val_regression_loss: 17.4432 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.0256 - regression_loss: 23.5500 - val_loss: 20.9829 - val_regression_loss: 17.5830 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.7028 - regression_loss: 23.4159 - val_loss: 20.0439 - val_regression_loss: 16.6476 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.0494 - regression_loss: 22.6374 - val_loss: 20.6835 - val_regression_loss: 17.3573 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.3901 - regression_loss: 22.0155 - val_loss: 20.2541 - val_regression_loss: 16.9929 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.0298 - regression_loss: 21.7491 - val_loss: 19.8021 - val_regression_loss: 16.5784 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.1097 - regression_loss: 21.1786 - val_loss: 19.6772 - val_regression_loss: 16.5074 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.9600 - regression_loss: 20.6120 - val_loss: 19.1952 - val_regression_loss: 16.0136 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.5810 - regression_loss: 20.2459 - val_loss: 19.9038 - val_regression_loss: 16.7890 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.6859 - regression_loss: 19.9616 - val_loss: 19.0002 - val_regression_loss: 15.8690 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.8148 - regression_loss: 19.9110 - val_loss: 19.7087 - val_regression_loss: 16.6907 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.5688 - regression_loss: 19.5643 - val_loss: 18.7201 - val_regression_loss: 15.6588 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.9315 - regression_loss: 19.2585 - val_loss: 19.5491 - val_regression_loss: 16.5748 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.2175 - regression_loss: 19.3917 - val_loss: 18.2954 - val_regression_loss: 15.1326 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.8275 - regression_loss: 18.8569 - val_loss: 19.0834 - val_regression_loss: 16.1495 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.1755 - regression_loss: 18.5537 - val_loss: 18.7424 - val_regression_loss: 15.7864 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.1353 - regression_loss: 18.3883 - val_loss: 18.3869 - val_regression_loss: 15.3829 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.2975 - regression_loss: 18.1008 - val_loss: 19.0443 - val_regression_loss: 16.1332 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.2020 - regression_loss: 18.0475 - val_loss: 18.3752 - val_regression_loss: 15.4079 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8509 - regression_loss: 17.9543 - val_loss: 18.3013 - val_regression_loss: 15.3611 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7449 - regression_loss: 17.8610 - val_loss: 18.4613 - val_regression_loss: 15.5082 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4487 - regression_loss: 17.6447 - val_loss: 18.4167 - val_regression_loss: 15.4569 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7512 - regression_loss: 17.6753 - val_loss: 18.5278 - val_regression_loss: 15.6246 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.3946 - regression_loss: 17.6281 - val_loss: 18.3377 - val_regression_loss: 15.4031 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.2892 - regression_loss: 17.3137 - val_loss: 18.2565 - val_regression_loss: 15.2379 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.5990 - regression_loss: 17.1606 - val_loss: 19.2154 - val_regression_loss: 16.3958 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.3288 - regression_loss: 17.2283 - val_loss: 18.0693 - val_regression_loss: 14.9939 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.0021 - regression_loss: 17.1649 - val_loss: 19.1469 - val_regression_loss: 16.2870 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.7965 - regression_loss: 17.2952 - val_loss: 18.1085 - val_regression_loss: 15.0116 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.8836 - regression_loss: 17.0915 - val_loss: 19.4538 - val_regression_loss: 16.6085 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 20.0776 - regression_loss: 18.4046\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.7727 - regression_loss: 16.9060 - val_loss: 18.1562 - val_regression_loss: 15.0175 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.6389 - regression_loss: 16.7773 - val_loss: 18.6617 - val_regression_loss: 15.7625 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.4865 - regression_loss: 16.6703 - val_loss: 18.6336 - val_regression_loss: 15.7289 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.5738 - regression_loss: 16.6523 - val_loss: 18.3802 - val_regression_loss: 15.4108 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.7562 - regression_loss: 16.5726 - val_loss: 18.2958 - val_regression_loss: 15.2709 - lr: 5.0000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.4146 - regression_loss: 16.5660 - val_loss: 18.6066 - val_regression_loss: 15.6933 - lr: 5.0000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.0355 - regression_loss: 16.5703 - val_loss: 18.4616 - val_regression_loss: 15.5276 - lr: 5.0000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.4465 - regression_loss: 16.4935 - val_loss: 18.3871 - val_regression_loss: 15.4117 - lr: 5.0000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.2725 - regression_loss: 16.4127 - val_loss: 18.5146 - val_regression_loss: 15.5587 - lr: 5.0000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.2994 - regression_loss: 16.5731 - val_loss: 18.6023 - val_regression_loss: 15.6343 - lr: 5.0000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1822 - regression_loss: 16.3422 - val_loss: 18.3015 - val_regression_loss: 15.2620 - lr: 5.0000e-05\n",
      "Epoch 60/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 24.9710 - regression_loss: 23.2977\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1932 - regression_loss: 16.3599 - val_loss: 18.5423 - val_regression_loss: 15.6039 - lr: 5.0000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.2271 - regression_loss: 16.3095 - val_loss: 18.4971 - val_regression_loss: 15.5384 - lr: 2.5000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.7786 - regression_loss: 16.2746 - val_loss: 18.3693 - val_regression_loss: 15.3594 - lr: 2.5000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.0047 - regression_loss: 16.2640 - val_loss: 18.4351 - val_regression_loss: 15.4468 - lr: 2.5000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1596 - regression_loss: 16.2299 - val_loss: 18.6119 - val_regression_loss: 15.6614 - lr: 2.5000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.9250 - regression_loss: 16.2245 - val_loss: 18.5734 - val_regression_loss: 15.6057 - lr: 2.5000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.9012 - regression_loss: 16.2174 - val_loss: 18.4926 - val_regression_loss: 15.5003 - lr: 2.5000e-05\n",
      "Epoch 67/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 17.8190 - regression_loss: 16.1456\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.0922 - regression_loss: 16.2060 - val_loss: 18.3514 - val_regression_loss: 15.3129 - lr: 2.5000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.2258 - regression_loss: 16.1928 - val_loss: 18.4342 - val_regression_loss: 15.4269 - lr: 1.2500e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1597 - regression_loss: 16.1507 - val_loss: 18.5552 - val_regression_loss: 15.5793 - lr: 1.2500e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.8822 - regression_loss: 16.1801 - val_loss: 18.6210 - val_regression_loss: 15.6649 - lr: 1.2500e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.2565 - regression_loss: 16.1471 - val_loss: 18.5279 - val_regression_loss: 15.5499 - lr: 1.2500e-05\n",
      "Epoch 72/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.7476 - regression_loss: 14.0742\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.0397 - regression_loss: 16.1908 - val_loss: 18.4049 - val_regression_loss: 15.3876 - lr: 1.2500e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1792 - regression_loss: 16.1354 - val_loss: 18.4821 - val_regression_loss: 15.4879 - lr: 6.2500e-06\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.8889 - regression_loss: 16.1168 - val_loss: 18.5263 - val_regression_loss: 15.5415 - lr: 6.2500e-06\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.8513 - regression_loss: 16.1212 - val_loss: 18.5601 - val_regression_loss: 15.5821 - lr: 6.2500e-06\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.0513 - regression_loss: 16.1088 - val_loss: 18.5294 - val_regression_loss: 15.5443 - lr: 6.2500e-06\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.7639 - regression_loss: 16.1055 - val_loss: 18.5318 - val_regression_loss: 15.5492 - lr: 6.2500e-06\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.2063 - regression_loss: 16.1042 - val_loss: 18.5005 - val_regression_loss: 15.5093 - lr: 6.2500e-06\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.8365 - regression_loss: 16.1308 - val_loss: 18.5854 - val_regression_loss: 15.6138 - lr: 6.2500e-06\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.8002 - regression_loss: 16.0964 - val_loss: 18.5192 - val_regression_loss: 15.5321 - lr: 6.2500e-06\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.8194 - regression_loss: 16.0888 - val_loss: 18.5108 - val_regression_loss: 15.5185 - lr: 6.2500e-06\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.7691 - regression_loss: 16.0872 - val_loss: 18.5069 - val_regression_loss: 15.5137 - lr: 6.2500e-06\n",
      "Epoch 83/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 [====>.........................] - ETA: 0s - loss: 15.1161 - regression_loss: 13.4428\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.0654 - regression_loss: 16.0783 - val_loss: 18.5044 - val_regression_loss: 15.5111 - lr: 6.2500e-06\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.9342 - regression_loss: 16.0753 - val_loss: 18.5001 - val_regression_loss: 15.5056 - lr: 3.1250e-06\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.0401 - regression_loss: 16.0718 - val_loss: 18.5113 - val_regression_loss: 15.5190 - lr: 3.1250e-06\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 64.4431 - regression_loss: 57.5711 - val_loss: 42.3243 - val_regression_loss: 32.3437 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 38.9151 - regression_loss: 34.8995 - val_loss: 28.9085 - val_regression_loss: 21.6640 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 33.7541 - regression_loss: 30.1311 - val_loss: 31.0396 - val_regression_loss: 23.4514 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.8152 - regression_loss: 29.2845 - val_loss: 26.6817 - val_regression_loss: 20.0290 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.0316 - regression_loss: 27.4415 - val_loss: 26.5314 - val_regression_loss: 19.8050 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.4709 - regression_loss: 26.3280 - val_loss: 28.4115 - val_regression_loss: 21.1216 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.7036 - regression_loss: 25.3838 - val_loss: 27.3395 - val_regression_loss: 20.0561 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.2069 - regression_loss: 25.0545 - val_loss: 29.3287 - val_regression_loss: 21.5528 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.8343 - regression_loss: 25.1520 - val_loss: 27.8883 - val_regression_loss: 20.4079 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.0982 - regression_loss: 24.5168 - val_loss: 28.4532 - val_regression_loss: 20.7769 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.6698 - regression_loss: 24.1578 - val_loss: 28.3134 - val_regression_loss: 20.6640 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.3090 - regression_loss: 23.8802 - val_loss: 28.1218 - val_regression_loss: 20.5550 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.0554 - regression_loss: 23.7909 - val_loss: 27.1108 - val_regression_loss: 19.7531 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.5427 - regression_loss: 23.7969 - val_loss: 28.6240 - val_regression_loss: 20.8915 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.6212 - regression_loss: 23.5660 - val_loss: 27.0179 - val_regression_loss: 19.6345 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.7726 - regression_loss: 23.7029 - val_loss: 29.2766 - val_regression_loss: 21.4142 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.1189 - regression_loss: 23.4983 - val_loss: 27.4128 - val_regression_loss: 19.8979 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.5336 - regression_loss: 23.2708 - val_loss: 28.3174 - val_regression_loss: 20.5809 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.3801 - regression_loss: 23.5918 - val_loss: 27.7209 - val_regression_loss: 20.0883 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.7936 - regression_loss: 23.7402 - val_loss: 28.7853 - val_regression_loss: 20.9713 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.0539 - regression_loss: 23.4414 - val_loss: 27.3003 - val_regression_loss: 19.8095 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.3634 - regression_loss: 23.0255 - val_loss: 28.1794 - val_regression_loss: 20.4173 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.0228 - regression_loss: 22.8085 - val_loss: 28.5263 - val_regression_loss: 20.6923 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.8948 - regression_loss: 22.9478 - val_loss: 28.4786 - val_regression_loss: 20.6673 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.4131 - regression_loss: 22.7751 - val_loss: 28.0413 - val_regression_loss: 20.3144 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.7311 - regression_loss: 22.6864 - val_loss: 28.3816 - val_regression_loss: 20.5771 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.1774 - regression_loss: 22.5889 - val_loss: 27.5781 - val_regression_loss: 19.9345 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.3350 - regression_loss: 23.0772 - val_loss: 29.6352 - val_regression_loss: 21.5961 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.5888 - regression_loss: 23.0556 - val_loss: 27.4767 - val_regression_loss: 19.8482 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.0644 - regression_loss: 22.7341 - val_loss: 28.5051 - val_regression_loss: 20.6508 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 28.5187 - regression_loss: 26.8510\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.8328 - regression_loss: 22.5507 - val_loss: 27.6983 - val_regression_loss: 20.0325 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.0486 - regression_loss: 22.5511 - val_loss: 27.6134 - val_regression_loss: 19.9582 - lr: 5.0000e-05\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.8364 - regression_loss: 22.6219 - val_loss: 29.0844 - val_regression_loss: 21.0993 - lr: 5.0000e-05\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.3508 - regression_loss: 22.1991 - val_loss: 27.6665 - val_regression_loss: 19.9702 - lr: 5.0000e-05\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.8695 - regression_loss: 22.3567 - val_loss: 27.9224 - val_regression_loss: 20.1840 - lr: 5.0000e-05\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.5799 - regression_loss: 22.2336 - val_loss: 28.5527 - val_regression_loss: 20.6743 - lr: 5.0000e-05\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.7863 - regression_loss: 22.1679 - val_loss: 27.9755 - val_regression_loss: 20.2062 - lr: 5.0000e-05\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.4232 - regression_loss: 22.1887 - val_loss: 28.0773 - val_regression_loss: 20.2816 - lr: 5.0000e-05\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.3302 - regression_loss: 22.1656 - val_loss: 28.2260 - val_regression_loss: 20.3874 - lr: 5.0000e-05\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.2107 - regression_loss: 22.1442 - val_loss: 27.8065 - val_regression_loss: 20.0863 - lr: 5.0000e-05\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.1244 - regression_loss: 22.1188 - val_loss: 28.4477 - val_regression_loss: 20.6063 - lr: 5.0000e-05\n",
      "Epoch 42/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 28.1206 - regression_loss: 26.4540\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 25.6212 - regression_loss: 22.0882 - val_loss: 28.2060 - val_regression_loss: 20.3946 - lr: 5.0000e-05\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.5868 - regression_loss: 22.0666 - val_loss: 27.7987 - val_regression_loss: 20.0670 - lr: 2.5000e-05\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.8632 - regression_loss: 22.0253 - val_loss: 28.2570 - val_regression_loss: 20.4380 - lr: 2.5000e-05\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.8279 - regression_loss: 22.0386 - val_loss: 28.5066 - val_regression_loss: 20.6321 - lr: 2.5000e-05\n",
      "3/3 [==============================] - 0s 972us/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 24ms/step - loss: 46.0003 - regression_loss: 41.0241 - val_loss: 30.1156 - val_regression_loss: 24.9331 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 35.0175 - regression_loss: 30.7775 - val_loss: 24.1443 - val_regression_loss: 19.6593 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.6150 - regression_loss: 25.4624 - val_loss: 20.7378 - val_regression_loss: 16.5786 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.5223 - regression_loss: 23.5389 - val_loss: 19.9247 - val_regression_loss: 15.6258 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.1441 - regression_loss: 23.1187 - val_loss: 19.5676 - val_regression_loss: 15.1843 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.6724 - regression_loss: 22.6407 - val_loss: 18.5239 - val_regression_loss: 14.3764 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.3575 - regression_loss: 22.1512 - val_loss: 18.5600 - val_regression_loss: 14.3931 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.0702 - regression_loss: 21.6993 - val_loss: 18.6541 - val_regression_loss: 14.5107 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.8178 - regression_loss: 21.3824 - val_loss: 18.1128 - val_regression_loss: 14.1232 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.3432 - regression_loss: 21.2263 - val_loss: 18.6382 - val_regression_loss: 14.5766 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.4932 - regression_loss: 21.3229 - val_loss: 18.0219 - val_regression_loss: 13.9241 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.0764 - regression_loss: 20.8630 - val_loss: 18.4556 - val_regression_loss: 14.3746 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.8289 - regression_loss: 20.6631 - val_loss: 18.1570 - val_regression_loss: 14.0355 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.1430 - regression_loss: 20.3943 - val_loss: 18.3368 - val_regression_loss: 14.2366 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.4656 - regression_loss: 20.3303 - val_loss: 18.1123 - val_regression_loss: 14.0174 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.0804 - regression_loss: 20.0948 - val_loss: 18.1125 - val_regression_loss: 13.9942 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.0308 - regression_loss: 20.0347 - val_loss: 18.0739 - val_regression_loss: 14.0370 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.7773 - regression_loss: 19.8103 - val_loss: 18.0868 - val_regression_loss: 13.9349 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.8378 - regression_loss: 19.8580 - val_loss: 18.7929 - val_regression_loss: 14.6229 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.0521 - regression_loss: 19.9670 - val_loss: 17.7073 - val_regression_loss: 13.6597 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.1757 - regression_loss: 19.4579 - val_loss: 19.4195 - val_regression_loss: 15.1897 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.5717 - regression_loss: 19.7629 - val_loss: 18.7918 - val_regression_loss: 14.4450 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.7645 - regression_loss: 20.4027 - val_loss: 18.9234 - val_regression_loss: 14.8189 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.1893 - regression_loss: 19.3275 - val_loss: 18.4851 - val_regression_loss: 14.2080 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.3576 - regression_loss: 19.3340 - val_loss: 19.3487 - val_regression_loss: 15.2027 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 20.8344 - regression_loss: 19.1736\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.5499 - regression_loss: 19.3091 - val_loss: 18.1541 - val_regression_loss: 14.0018 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.4278 - regression_loss: 19.2798 - val_loss: 18.2483 - val_regression_loss: 14.2002 - lr: 5.0000e-05\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.1486 - regression_loss: 19.1367 - val_loss: 18.5088 - val_regression_loss: 14.3659 - lr: 5.0000e-05\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.6636 - regression_loss: 18.8525 - val_loss: 18.0106 - val_regression_loss: 13.9305 - lr: 5.0000e-05\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.8048 - regression_loss: 18.8904 - val_loss: 18.0100 - val_regression_loss: 14.0197 - lr: 5.0000e-05\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.3903 - regression_loss: 18.9328 - val_loss: 18.0864 - val_regression_loss: 14.1082 - lr: 5.0000e-05\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.9756 - regression_loss: 18.9195 - val_loss: 18.0830 - val_regression_loss: 13.9886 - lr: 5.0000e-05\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.4353 - regression_loss: 18.7298 - val_loss: 18.2985 - val_regression_loss: 14.2245 - lr: 5.0000e-05\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.4993 - regression_loss: 18.9190 - val_loss: 18.2506 - val_regression_loss: 14.2123 - lr: 5.0000e-05\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.6006 - regression_loss: 18.7649 - val_loss: 18.0765 - val_regression_loss: 14.0143 - lr: 5.0000e-05\n",
      "Epoch 36/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 22.9893 - regression_loss: 21.3295\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.7420 - regression_loss: 18.8231 - val_loss: 18.2973 - val_regression_loss: 14.2997 - lr: 5.0000e-05\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.7230 - regression_loss: 18.6268 - val_loss: 18.2273 - val_regression_loss: 14.1806 - lr: 2.5000e-05\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.6732 - regression_loss: 18.6519 - val_loss: 18.0981 - val_regression_loss: 14.0656 - lr: 2.5000e-05\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.6822 - regression_loss: 18.5798 - val_loss: 18.1815 - val_regression_loss: 14.1409 - lr: 2.5000e-05\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.8337 - regression_loss: 18.7560 - val_loss: 18.2540 - val_regression_loss: 14.2328 - lr: 2.5000e-05\n",
      "Epoch 41/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 20.2223 - regression_loss: 18.5627\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 21.5950 - regression_loss: 18.5467 - val_loss: 18.0421 - val_regression_loss: 14.0184 - lr: 2.5000e-05\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.6675 - regression_loss: 18.5694 - val_loss: 18.0420 - val_regression_loss: 14.0195 - lr: 1.2500e-05\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.1883 - regression_loss: 18.5027 - val_loss: 18.1233 - val_regression_loss: 14.1186 - lr: 1.2500e-05\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.7385 - regression_loss: 18.4941 - val_loss: 18.2472 - val_regression_loss: 14.2325 - lr: 1.2500e-05\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.2448 - regression_loss: 18.5029 - val_loss: 18.2114 - val_regression_loss: 14.1921 - lr: 1.2500e-05\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.5680 - regression_loss: 18.4885 - val_loss: 18.0956 - val_regression_loss: 14.0825 - lr: 1.2500e-05\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.3160 - regression_loss: 18.4713 - val_loss: 18.1186 - val_regression_loss: 14.1026 - lr: 1.2500e-05\n",
      "Epoch 48/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 17.9726 - regression_loss: 16.3132\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.3345 - regression_loss: 18.4598 - val_loss: 18.1293 - val_regression_loss: 14.1170 - lr: 1.2500e-05\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.4215 - regression_loss: 18.4655 - val_loss: 18.1132 - val_regression_loss: 14.1169 - lr: 6.2500e-06\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.7295 - regression_loss: 18.4504 - val_loss: 18.1340 - val_regression_loss: 14.1321 - lr: 6.2500e-06\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.5137 - regression_loss: 18.4394 - val_loss: 18.1079 - val_regression_loss: 14.1112 - lr: 6.2500e-06\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.5296 - regression_loss: 18.4364 - val_loss: 18.1085 - val_regression_loss: 14.1058 - lr: 6.2500e-06\n",
      "Epoch 53/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 20.5559 - regression_loss: 18.8966\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.4780 - regression_loss: 18.4409 - val_loss: 18.1013 - val_regression_loss: 14.1025 - lr: 6.2500e-06\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9420 - regression_loss: 18.4320 - val_loss: 18.1108 - val_regression_loss: 14.1072 - lr: 3.1250e-06\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.0783 - regression_loss: 18.4268 - val_loss: 18.1052 - val_regression_loss: 14.1025 - lr: 3.1250e-06\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.6806 - regression_loss: 18.4256 - val_loss: 18.1118 - val_regression_loss: 14.1098 - lr: 3.1250e-06\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.0915 - regression_loss: 18.4253 - val_loss: 18.1097 - val_regression_loss: 14.1109 - lr: 3.1250e-06\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.4837 - regression_loss: 18.4237 - val_loss: 18.1289 - val_regression_loss: 14.1286 - lr: 3.1250e-06\n",
      "Epoch 59/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 21.3420 - regression_loss: 19.6827\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.3632 - regression_loss: 18.4205 - val_loss: 18.1265 - val_regression_loss: 14.1281 - lr: 3.1250e-06\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.1362 - regression_loss: 18.4152 - val_loss: 18.1221 - val_regression_loss: 14.1245 - lr: 1.5625e-06\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 26ms/step - loss: 94.1735 - regression_loss: 86.4455 - val_loss: 60.9359 - val_regression_loss: 46.4102 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 71.7468 - regression_loss: 65.2554 - val_loss: 52.3949 - val_regression_loss: 39.3742 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 59.3974 - regression_loss: 53.8774 - val_loss: 46.8592 - val_regression_loss: 34.8411 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 51.1131 - regression_loss: 46.2097 - val_loss: 42.4094 - val_regression_loss: 31.2526 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 45.6725 - regression_loss: 40.9292 - val_loss: 39.4697 - val_regression_loss: 28.8958 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 41.2769 - regression_loss: 37.0203 - val_loss: 38.4516 - val_regression_loss: 27.9802 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 39.2566 - regression_loss: 34.7368 - val_loss: 35.9773 - val_regression_loss: 26.2822 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 36.6146 - regression_loss: 32.2910 - val_loss: 34.8714 - val_regression_loss: 25.2145 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 33.5735 - regression_loss: 29.8552 - val_loss: 32.2045 - val_regression_loss: 23.2908 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.5146 - regression_loss: 27.6749 - val_loss: 30.1446 - val_regression_loss: 21.6887 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.7274 - regression_loss: 25.4002 - val_loss: 27.7860 - val_regression_loss: 19.9609 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.4629 - regression_loss: 23.6960 - val_loss: 26.0580 - val_regression_loss: 18.6269 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.0035 - regression_loss: 21.7028 - val_loss: 24.0668 - val_regression_loss: 17.2306 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.3695 - regression_loss: 19.5293 - val_loss: 22.6256 - val_regression_loss: 16.1512 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7133 - regression_loss: 17.9907 - val_loss: 21.3990 - val_regression_loss: 15.2519 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.3756 - regression_loss: 16.7315 - val_loss: 20.2839 - val_regression_loss: 14.4590 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.5623 - regression_loss: 15.6554 - val_loss: 19.3147 - val_regression_loss: 13.7981 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.1872 - regression_loss: 14.6010 - val_loss: 18.6829 - val_regression_loss: 13.2961 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7913 - regression_loss: 14.0796 - val_loss: 18.3405 - val_regression_loss: 13.1862 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.1331 - regression_loss: 13.4236 - val_loss: 17.8685 - val_regression_loss: 12.7482 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.6946 - regression_loss: 13.0032 - val_loss: 17.4463 - val_regression_loss: 12.5267 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.4317 - regression_loss: 12.7581 - val_loss: 17.3277 - val_regression_loss: 12.3911 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.8916 - regression_loss: 12.3400 - val_loss: 16.6986 - val_regression_loss: 11.9612 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.4444 - regression_loss: 12.0044 - val_loss: 16.8877 - val_regression_loss: 12.0955 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.4069 - regression_loss: 11.8383 - val_loss: 16.0727 - val_regression_loss: 11.4593 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.8232 - regression_loss: 11.5297 - val_loss: 16.0014 - val_regression_loss: 11.3975 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1855 - regression_loss: 11.0076 - val_loss: 15.8480 - val_regression_loss: 11.2838 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6598 - regression_loss: 11.1445 - val_loss: 15.5752 - val_regression_loss: 11.0590 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.2213 - regression_loss: 10.7346 - val_loss: 14.5968 - val_regression_loss: 10.2477 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0565 - regression_loss: 10.6319 - val_loss: 13.8485 - val_regression_loss: 9.6502 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3911 - regression_loss: 9.8435 - val_loss: 13.7142 - val_regression_loss: 9.5296 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0379 - regression_loss: 9.6325 - val_loss: 13.5019 - val_regression_loss: 9.3531 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9755 - regression_loss: 9.5832 - val_loss: 13.4931 - val_regression_loss: 9.3538 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6421 - regression_loss: 9.4945 - val_loss: 13.3364 - val_regression_loss: 9.2125 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6839 - regression_loss: 9.2577 - val_loss: 13.1357 - val_regression_loss: 9.0505 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4147 - regression_loss: 9.1245 - val_loss: 12.9749 - val_regression_loss: 8.9049 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1616 - regression_loss: 8.9359 - val_loss: 12.7216 - val_regression_loss: 8.6932 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9992 - regression_loss: 8.7793 - val_loss: 12.6933 - val_regression_loss: 8.6680 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8560 - regression_loss: 8.7706 - val_loss: 12.7362 - val_regression_loss: 8.7220 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9013 - regression_loss: 8.6643 - val_loss: 12.3808 - val_regression_loss: 8.4087 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6969 - regression_loss: 8.3870 - val_loss: 12.2662 - val_regression_loss: 8.3280 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.5616 - regression_loss: 8.3393 - val_loss: 12.1667 - val_regression_loss: 8.2461 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.5655 - regression_loss: 8.2652 - val_loss: 12.0409 - val_regression_loss: 8.1461 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3155 - regression_loss: 8.1444 - val_loss: 12.0580 - val_regression_loss: 8.1351 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2616 - regression_loss: 8.0283 - val_loss: 11.8790 - val_regression_loss: 8.0111 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2154 - regression_loss: 7.9269 - val_loss: 11.9423 - val_regression_loss: 8.0273 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3316 - regression_loss: 7.9655 - val_loss: 11.8559 - val_regression_loss: 7.9734 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9819 - regression_loss: 7.7489 - val_loss: 11.7089 - val_regression_loss: 7.8618 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8384 - regression_loss: 7.7441 - val_loss: 11.6766 - val_regression_loss: 7.7958 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9255 - regression_loss: 7.8017 - val_loss: 11.4584 - val_regression_loss: 7.6572 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6194 - regression_loss: 7.5147 - val_loss: 11.5514 - val_regression_loss: 7.7566 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6090 - regression_loss: 7.4548 - val_loss: 11.3265 - val_regression_loss: 7.5416 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5591 - regression_loss: 7.5361 - val_loss: 11.7018 - val_regression_loss: 7.8079 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7437 - regression_loss: 7.6516 - val_loss: 11.3825 - val_regression_loss: 7.6107 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4107 - regression_loss: 7.3238 - val_loss: 11.3014 - val_regression_loss: 7.5310 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3562 - regression_loss: 7.2166 - val_loss: 11.0968 - val_regression_loss: 7.3465 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2649 - regression_loss: 7.1406 - val_loss: 11.7558 - val_regression_loss: 7.9429 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4076 - regression_loss: 7.2226 - val_loss: 11.3628 - val_regression_loss: 7.5291 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6077 - regression_loss: 7.4600 - val_loss: 10.9953 - val_regression_loss: 7.2580 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2407 - regression_loss: 7.0815 - val_loss: 10.8318 - val_regression_loss: 7.1638 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1223 - regression_loss: 6.9750 - val_loss: 11.3592 - val_regression_loss: 7.6319 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2986 - regression_loss: 7.3611 - val_loss: 12.0482 - val_regression_loss: 8.0716 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6421 - regression_loss: 7.4185 - val_loss: 11.2463 - val_regression_loss: 7.5206 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0042 - regression_loss: 6.8025 - val_loss: 10.6588 - val_regression_loss: 7.0088 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1171 - regression_loss: 6.9771 - val_loss: 10.9530 - val_regression_loss: 7.2105 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0402 - regression_loss: 6.9364 - val_loss: 11.2836 - val_regression_loss: 7.5759 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8512 - regression_loss: 6.8068 - val_loss: 10.7984 - val_regression_loss: 7.1240 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8334 - regression_loss: 6.7212 - val_loss: 10.5989 - val_regression_loss: 6.9561 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5657 - regression_loss: 6.6342 - val_loss: 10.6810 - val_regression_loss: 7.0583 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9461 - regression_loss: 6.7246 - val_loss: 11.2383 - val_regression_loss: 7.4251 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8742 - regression_loss: 6.8359 - val_loss: 11.1906 - val_regression_loss: 7.5298 - lr: 1.0000e-04\n",
      "Epoch 72/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9500 - regression_loss: 6.7624 - val_loss: 10.5719 - val_regression_loss: 6.9444 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5419 - regression_loss: 6.5543 - val_loss: 10.6922 - val_regression_loss: 7.0251 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6531 - regression_loss: 6.5053 - val_loss: 10.6274 - val_regression_loss: 7.0228 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6560 - regression_loss: 6.5119 - val_loss: 10.6859 - val_regression_loss: 7.0205 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7460 - regression_loss: 6.7001 - val_loss: 10.6658 - val_regression_loss: 7.0703 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6909 - regression_loss: 6.5913 - val_loss: 10.8339 - val_regression_loss: 7.2209 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.3953 - regression_loss: 6.7431\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7895 - regression_loss: 6.7228 - val_loss: 10.9845 - val_regression_loss: 7.2371 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6333 - regression_loss: 6.5797 - val_loss: 10.8747 - val_regression_loss: 7.2638 - lr: 5.0000e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4708 - regression_loss: 6.3929 - val_loss: 10.6113 - val_regression_loss: 6.9560 - lr: 5.0000e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2662 - regression_loss: 6.3021 - val_loss: 10.6446 - val_regression_loss: 7.0635 - lr: 5.0000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5261 - regression_loss: 6.4928 - val_loss: 10.4648 - val_regression_loss: 6.8542 - lr: 5.0000e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4324 - regression_loss: 6.3773 - val_loss: 10.4315 - val_regression_loss: 6.8817 - lr: 5.0000e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3676 - regression_loss: 6.3002 - val_loss: 10.4581 - val_regression_loss: 6.8752 - lr: 5.0000e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3147 - regression_loss: 6.2983 - val_loss: 10.5566 - val_regression_loss: 6.9452 - lr: 5.0000e-05\n",
      "Epoch 86/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.3067 - regression_loss: 9.6549\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4241 - regression_loss: 6.3001 - val_loss: 10.4564 - val_regression_loss: 6.9017 - lr: 5.0000e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2923 - regression_loss: 6.2805 - val_loss: 10.3529 - val_regression_loss: 6.7992 - lr: 2.5000e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3426 - regression_loss: 6.2993 - val_loss: 10.3964 - val_regression_loss: 6.8213 - lr: 2.5000e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2624 - regression_loss: 6.2340 - val_loss: 10.4109 - val_regression_loss: 6.8604 - lr: 2.5000e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3774 - regression_loss: 6.2069 - val_loss: 10.3347 - val_regression_loss: 6.7820 - lr: 2.5000e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2468 - regression_loss: 6.2293 - val_loss: 10.3859 - val_regression_loss: 6.8161 - lr: 2.5000e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3139 - regression_loss: 6.2228 - val_loss: 10.4615 - val_regression_loss: 6.8995 - lr: 2.5000e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2616 - regression_loss: 6.2110 - val_loss: 10.3763 - val_regression_loss: 6.8137 - lr: 2.5000e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4307 - regression_loss: 6.2087 - val_loss: 10.3824 - val_regression_loss: 6.8265 - lr: 2.5000e-05\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2926 - regression_loss: 6.1967 - val_loss: 10.4044 - val_regression_loss: 6.8378 - lr: 2.5000e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2105 - regression_loss: 6.1895 - val_loss: 10.3704 - val_regression_loss: 6.8279 - lr: 2.5000e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2594 - regression_loss: 6.1965 - val_loss: 10.3332 - val_regression_loss: 6.7778 - lr: 2.5000e-05\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2544 - regression_loss: 6.1853 - val_loss: 10.4219 - val_regression_loss: 6.8723 - lr: 2.5000e-05\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1926 - regression_loss: 6.1829 - val_loss: 10.3604 - val_regression_loss: 6.8066 - lr: 2.5000e-05\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2620 - regression_loss: 6.1989 - val_loss: 10.3583 - val_regression_loss: 6.8175 - lr: 2.5000e-05\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1953 - regression_loss: 6.1528 - val_loss: 10.3583 - val_regression_loss: 6.8110 - lr: 2.5000e-05\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1533 - regression_loss: 6.1525 - val_loss: 10.3642 - val_regression_loss: 6.8182 - lr: 2.5000e-05\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2818 - regression_loss: 6.1466 - val_loss: 10.3729 - val_regression_loss: 6.8142 - lr: 2.5000e-05\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1424 - regression_loss: 6.1655 - val_loss: 10.3180 - val_regression_loss: 6.7768 - lr: 2.5000e-05\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2873 - regression_loss: 6.1658 - val_loss: 10.3227 - val_regression_loss: 6.7970 - lr: 2.5000e-05\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2355 - regression_loss: 6.1351 - val_loss: 10.3259 - val_regression_loss: 6.7725 - lr: 2.5000e-05\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3329 - regression_loss: 6.1423 - val_loss: 10.3614 - val_regression_loss: 6.8202 - lr: 2.5000e-05\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2388 - regression_loss: 6.1323 - val_loss: 10.3532 - val_regression_loss: 6.8127 - lr: 2.5000e-05\n",
      "Epoch 109/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.2619 - regression_loss: 6.6108\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1693 - regression_loss: 6.1382 - val_loss: 10.3236 - val_regression_loss: 6.7883 - lr: 2.5000e-05\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1432 - regression_loss: 6.1055 - val_loss: 10.3071 - val_regression_loss: 6.7672 - lr: 1.2500e-05\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2316 - regression_loss: 6.1058 - val_loss: 10.3246 - val_regression_loss: 6.7867 - lr: 1.2500e-05\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.1509 - regression_loss: 6.1261 - val_loss: 10.3617 - val_regression_loss: 6.8267 - lr: 1.2500e-05\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1544 - regression_loss: 6.1136 - val_loss: 10.3196 - val_regression_loss: 6.7732 - lr: 1.2500e-05\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1203 - regression_loss: 6.1060 - val_loss: 10.3216 - val_regression_loss: 6.7815 - lr: 1.2500e-05\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0037 - regression_loss: 6.0876 - val_loss: 10.3127 - val_regression_loss: 6.7785 - lr: 1.2500e-05\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1665 - regression_loss: 6.0976 - val_loss: 10.3227 - val_regression_loss: 6.7922 - lr: 1.2500e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1833 - regression_loss: 6.0941 - val_loss: 10.3105 - val_regression_loss: 6.7776 - lr: 1.2500e-05\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2295 - regression_loss: 6.0966 - val_loss: 10.2888 - val_regression_loss: 6.7513 - lr: 1.2500e-05\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1644 - regression_loss: 6.0976 - val_loss: 10.3137 - val_regression_loss: 6.7834 - lr: 1.2500e-05\n",
      "Epoch 120/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.6899 - regression_loss: 7.0391\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1616 - regression_loss: 6.0811 - val_loss: 10.3242 - val_regression_loss: 6.7883 - lr: 1.2500e-05\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1601 - regression_loss: 6.0741 - val_loss: 10.3121 - val_regression_loss: 6.7752 - lr: 6.2500e-06\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1085 - regression_loss: 6.0800 - val_loss: 10.3167 - val_regression_loss: 6.7807 - lr: 6.2500e-06\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2205 - regression_loss: 6.0756 - val_loss: 10.3069 - val_regression_loss: 6.7719 - lr: 6.2500e-06\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1010 - regression_loss: 6.0794 - val_loss: 10.3198 - val_regression_loss: 6.7799 - lr: 6.2500e-06\n",
      "Epoch 125/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.1732 - regression_loss: 7.5224\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1219 - regression_loss: 6.0793 - val_loss: 10.3211 - val_regression_loss: 6.7882 - lr: 6.2500e-06\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1251 - regression_loss: 6.0700 - val_loss: 10.3150 - val_regression_loss: 6.7846 - lr: 3.1250e-06\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1327 - regression_loss: 6.0650 - val_loss: 10.3058 - val_regression_loss: 6.7742 - lr: 3.1250e-06\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1600 - regression_loss: 6.0674 - val_loss: 10.3026 - val_regression_loss: 6.7673 - lr: 3.1250e-06\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0884 - regression_loss: 6.0677 - val_loss: 10.2991 - val_regression_loss: 6.7661 - lr: 3.1250e-06\n",
      "Epoch 130/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.8501 - regression_loss: 5.1993\n",
      "Epoch 130: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2294 - regression_loss: 6.0665 - val_loss: 10.3015 - val_regression_loss: 6.7673 - lr: 3.1250e-06\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1906 - regression_loss: 6.0631 - val_loss: 10.3037 - val_regression_loss: 6.7706 - lr: 1.5625e-06\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1291 - regression_loss: 6.0633 - val_loss: 10.3054 - val_regression_loss: 6.7734 - lr: 1.5625e-06\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.0119 - regression_loss: 6.0647 - val_loss: 10.3016 - val_regression_loss: 6.7693 - lr: 1.5625e-06\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.0807 - regression_loss: 6.0624 - val_loss: 10.3058 - val_regression_loss: 6.7743 - lr: 1.5625e-06\n",
      "Epoch 135/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.9354 - regression_loss: 5.2846\n",
      "Epoch 135: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2024 - regression_loss: 6.0613 - val_loss: 10.3025 - val_regression_loss: 6.7711 - lr: 1.5625e-06\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2222 - regression_loss: 6.0602 - val_loss: 10.3029 - val_regression_loss: 6.7715 - lr: 7.8125e-07\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2047 - regression_loss: 6.0606 - val_loss: 10.3033 - val_regression_loss: 6.7718 - lr: 7.8125e-07\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2273 - regression_loss: 6.0600 - val_loss: 10.3033 - val_regression_loss: 6.7719 - lr: 7.8125e-07\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.0463 - regression_loss: 6.0602 - val_loss: 10.3032 - val_regression_loss: 6.7722 - lr: 7.8125e-07\n",
      "Epoch 140/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.5446 - regression_loss: 7.8938\n",
      "Epoch 140: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.1264 - regression_loss: 6.0598 - val_loss: 10.3037 - val_regression_loss: 6.7728 - lr: 7.8125e-07\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1878 - regression_loss: 6.0597 - val_loss: 10.3039 - val_regression_loss: 6.7731 - lr: 3.9062e-07\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1669 - regression_loss: 6.0597 - val_loss: 10.3028 - val_regression_loss: 6.7717 - lr: 3.9062e-07\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1226 - regression_loss: 6.0592 - val_loss: 10.3023 - val_regression_loss: 6.7712 - lr: 3.9062e-07\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.0688 - regression_loss: 6.0595 - val_loss: 10.3021 - val_regression_loss: 6.7712 - lr: 3.9062e-07\n",
      "Epoch 145/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.9811 - regression_loss: 6.3303\n",
      "Epoch 145: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0333 - regression_loss: 6.0594 - val_loss: 10.3020 - val_regression_loss: 6.7709 - lr: 3.9062e-07\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1774 - regression_loss: 6.0590 - val_loss: 10.3023 - val_regression_loss: 6.7712 - lr: 1.9531e-07\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1136 - regression_loss: 6.0589 - val_loss: 10.3022 - val_regression_loss: 6.7712 - lr: 1.9531e-07\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.1391 - regression_loss: 6.0593 - val_loss: 10.3020 - val_regression_loss: 6.7708 - lr: 1.9531e-07\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1574 - regression_loss: 6.0588 - val_loss: 10.3021 - val_regression_loss: 6.7710 - lr: 1.9531e-07\n",
      "Epoch 150/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.0245 - regression_loss: 7.3738\n",
      "Epoch 150: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1830 - regression_loss: 6.0589 - val_loss: 10.3019 - val_regression_loss: 6.7708 - lr: 1.9531e-07\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1436 - regression_loss: 6.0587 - val_loss: 10.3020 - val_regression_loss: 6.7709 - lr: 9.7656e-08\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.1896 - regression_loss: 6.0586 - val_loss: 10.3018 - val_regression_loss: 6.7707 - lr: 9.7656e-08\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0553 - regression_loss: 6.0587 - val_loss: 10.3019 - val_regression_loss: 6.7709 - lr: 9.7656e-08\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2156 - regression_loss: 6.0587 - val_loss: 10.3020 - val_regression_loss: 6.7710 - lr: 9.7656e-08\n",
      "Epoch 155/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.9453 - regression_loss: 7.2945\n",
      "Epoch 155: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2064 - regression_loss: 6.0586 - val_loss: 10.3020 - val_regression_loss: 6.7709 - lr: 9.7656e-08\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0147 - regression_loss: 6.0585 - val_loss: 10.3019 - val_regression_loss: 6.7709 - lr: 4.8828e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2247 - regression_loss: 6.0585 - val_loss: 10.3020 - val_regression_loss: 6.7709 - lr: 4.8828e-08\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1221 - regression_loss: 6.0585 - val_loss: 10.3020 - val_regression_loss: 6.7709 - lr: 4.8828e-08\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 26ms/step - loss: 79.0940 - regression_loss: 71.7998 - val_loss: 56.5785 - val_regression_loss: 42.7470 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 53.8003 - regression_loss: 48.0168 - val_loss: 44.7150 - val_regression_loss: 33.7293 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 43.6842 - regression_loss: 39.0985 - val_loss: 36.6520 - val_regression_loss: 27.3202 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 39.4761 - regression_loss: 35.5555 - val_loss: 32.7700 - val_regression_loss: 24.6099 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 36.5370 - regression_loss: 32.6775 - val_loss: 30.5835 - val_regression_loss: 23.1518 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 33.7330 - regression_loss: 29.9291 - val_loss: 29.2955 - val_regression_loss: 22.0785 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.1640 - regression_loss: 27.6002 - val_loss: 28.3525 - val_regression_loss: 21.5739 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.0824 - regression_loss: 26.2520 - val_loss: 26.5613 - val_regression_loss: 19.8559 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.9764 - regression_loss: 25.5353 - val_loss: 25.8351 - val_regression_loss: 19.3948 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.9305 - regression_loss: 24.3410 - val_loss: 24.9760 - val_regression_loss: 18.6789 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.3153 - regression_loss: 23.2827 - val_loss: 24.3550 - val_regression_loss: 18.0405 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.7716 - regression_loss: 22.6014 - val_loss: 24.0353 - val_regression_loss: 17.8859 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.9690 - regression_loss: 21.8829 - val_loss: 23.3004 - val_regression_loss: 17.1589 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.6427 - regression_loss: 21.4812 - val_loss: 22.8866 - val_regression_loss: 16.9324 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.0173 - regression_loss: 20.9453 - val_loss: 22.4298 - val_regression_loss: 16.5083 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.4620 - regression_loss: 20.5590 - val_loss: 22.1998 - val_regression_loss: 16.3393 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.1349 - regression_loss: 19.9760 - val_loss: 22.0264 - val_regression_loss: 16.1515 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.7478 - regression_loss: 19.6517 - val_loss: 21.9193 - val_regression_loss: 16.0710 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.1351 - regression_loss: 19.3208 - val_loss: 21.5063 - val_regression_loss: 15.7474 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.1973 - regression_loss: 19.0246 - val_loss: 21.2604 - val_regression_loss: 15.5080 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.3037 - regression_loss: 18.8132 - val_loss: 20.9571 - val_regression_loss: 15.2413 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.4503 - regression_loss: 18.7474 - val_loss: 20.9401 - val_regression_loss: 15.2831 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.8197 - regression_loss: 18.5948 - val_loss: 20.8852 - val_regression_loss: 15.1683 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8383 - regression_loss: 18.2796 - val_loss: 20.8284 - val_regression_loss: 15.1300 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9241 - regression_loss: 17.9968 - val_loss: 20.7199 - val_regression_loss: 15.0635 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6035 - regression_loss: 17.7783 - val_loss: 20.5352 - val_regression_loss: 14.8600 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.0592 - regression_loss: 17.5816 - val_loss: 20.4262 - val_regression_loss: 14.8335 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.3071 - regression_loss: 17.4719 - val_loss: 20.4413 - val_regression_loss: 14.7378 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4299 - regression_loss: 17.5219 - val_loss: 20.5232 - val_regression_loss: 14.9210 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.3793 - regression_loss: 17.3029 - val_loss: 20.4276 - val_regression_loss: 14.6802 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.1088 - regression_loss: 17.2071 - val_loss: 20.4187 - val_regression_loss: 14.8594 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 20.2204 - regression_loss: 18.5718\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.0868 - regression_loss: 17.2678 - val_loss: 20.0815 - val_regression_loss: 14.4952 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.6741 - regression_loss: 16.9387 - val_loss: 20.0824 - val_regression_loss: 14.4883 - lr: 5.0000e-05\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.9154 - regression_loss: 16.8617 - val_loss: 19.9866 - val_regression_loss: 14.4117 - lr: 5.0000e-05\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.4261 - regression_loss: 16.8122 - val_loss: 19.9117 - val_regression_loss: 14.3298 - lr: 5.0000e-05\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.4093 - regression_loss: 16.8511 - val_loss: 19.8970 - val_regression_loss: 14.3381 - lr: 5.0000e-05\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.5153 - regression_loss: 16.7529 - val_loss: 20.0052 - val_regression_loss: 14.4320 - lr: 5.0000e-05\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.4158 - regression_loss: 16.7575 - val_loss: 19.9420 - val_regression_loss: 14.3513 - lr: 5.0000e-05\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.5482 - regression_loss: 16.7056 - val_loss: 19.9584 - val_regression_loss: 14.3235 - lr: 5.0000e-05\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.5247 - regression_loss: 16.6926 - val_loss: 19.8999 - val_regression_loss: 14.3401 - lr: 5.0000e-05\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.3614 - regression_loss: 16.6893 - val_loss: 19.9150 - val_regression_loss: 14.3382 - lr: 5.0000e-05\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.3843 - regression_loss: 16.5951 - val_loss: 19.8359 - val_regression_loss: 14.2852 - lr: 5.0000e-05\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1915 - regression_loss: 16.5509 - val_loss: 19.9058 - val_regression_loss: 14.3009 - lr: 5.0000e-05\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.4320 - regression_loss: 16.6305 - val_loss: 19.9422 - val_regression_loss: 14.3221 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.5815 - regression_loss: 16.5923 - val_loss: 19.8412 - val_regression_loss: 14.3021 - lr: 5.0000e-05\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.3262 - regression_loss: 16.5398 - val_loss: 19.7648 - val_regression_loss: 14.1666 - lr: 5.0000e-05\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.3009 - regression_loss: 16.4711 - val_loss: 19.8448 - val_regression_loss: 14.3106 - lr: 5.0000e-05\n",
      "Epoch 48/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 16.2985 - regression_loss: 14.6508\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.5068 - regression_loss: 16.4827 - val_loss: 19.7573 - val_regression_loss: 14.1725 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.3783 - regression_loss: 16.3657 - val_loss: 19.7632 - val_regression_loss: 14.1850 - lr: 2.5000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.4234 - regression_loss: 16.3652 - val_loss: 19.7680 - val_regression_loss: 14.1975 - lr: 2.5000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.6681 - regression_loss: 16.3304 - val_loss: 19.6843 - val_regression_loss: 14.1431 - lr: 2.5000e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.3524 - regression_loss: 16.3128 - val_loss: 19.6933 - val_regression_loss: 14.1320 - lr: 2.5000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.9197 - regression_loss: 16.3032 - val_loss: 19.6741 - val_regression_loss: 14.1034 - lr: 2.5000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.2475 - regression_loss: 16.2808 - val_loss: 19.6979 - val_regression_loss: 14.1284 - lr: 2.5000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.9309 - regression_loss: 16.2833 - val_loss: 19.6604 - val_regression_loss: 14.1109 - lr: 2.5000e-05\n",
      "Epoch 56/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 20.5091 - regression_loss: 18.8616\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1856 - regression_loss: 16.2616 - val_loss: 19.6483 - val_regression_loss: 14.0781 - lr: 2.5000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.0683 - regression_loss: 16.2499 - val_loss: 19.6436 - val_regression_loss: 14.0737 - lr: 1.2500e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.7052 - regression_loss: 16.2492 - val_loss: 19.6314 - val_regression_loss: 14.0913 - lr: 1.2500e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.9051 - regression_loss: 16.2386 - val_loss: 19.6329 - val_regression_loss: 14.0853 - lr: 1.2500e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1022 - regression_loss: 16.2247 - val_loss: 19.6300 - val_regression_loss: 14.0742 - lr: 1.2500e-05\n",
      "Epoch 61/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 21.6377 - regression_loss: 19.9904\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.8972 - regression_loss: 16.3051 - val_loss: 19.6355 - val_regression_loss: 14.0565 - lr: 1.2500e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.7805 - regression_loss: 16.2140 - val_loss: 19.6151 - val_regression_loss: 14.0574 - lr: 6.2500e-06\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1170 - regression_loss: 16.2051 - val_loss: 19.6265 - val_regression_loss: 14.0804 - lr: 6.2500e-06\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1506 - regression_loss: 16.2029 - val_loss: 19.6280 - val_regression_loss: 14.0811 - lr: 6.2500e-06\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1451 - regression_loss: 16.2009 - val_loss: 19.6311 - val_regression_loss: 14.0757 - lr: 6.2500e-06\n",
      "Epoch 66/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 22.6791 - regression_loss: 21.0318\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.0761 - regression_loss: 16.1918 - val_loss: 19.6351 - val_regression_loss: 14.0736 - lr: 6.2500e-06\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.8878 - regression_loss: 16.1944 - val_loss: 19.6347 - val_regression_loss: 14.0682 - lr: 3.1250e-06\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.8453 - regression_loss: 16.1847 - val_loss: 19.6367 - val_regression_loss: 14.0722 - lr: 3.1250e-06\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.9993 - regression_loss: 16.1838 - val_loss: 19.6349 - val_regression_loss: 14.0750 - lr: 3.1250e-06\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.8745 - regression_loss: 16.1831 - val_loss: 19.6339 - val_regression_loss: 14.0715 - lr: 3.1250e-06\n",
      "Epoch 71/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 19.8224 - regression_loss: 18.1751\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.8084 - regression_loss: 16.1791 - val_loss: 19.6316 - val_regression_loss: 14.0722 - lr: 3.1250e-06\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1242 - regression_loss: 16.1760 - val_loss: 19.6269 - val_regression_loss: 14.0687 - lr: 1.5625e-06\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.9606 - regression_loss: 16.1748 - val_loss: 19.6276 - val_regression_loss: 14.0688 - lr: 1.5625e-06\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1876 - regression_loss: 16.1736 - val_loss: 19.6276 - val_regression_loss: 14.0683 - lr: 1.5625e-06\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1009 - regression_loss: 16.1800 - val_loss: 19.6289 - val_regression_loss: 14.0722 - lr: 1.5625e-06\n",
      "Epoch 76/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 17.2802 - regression_loss: 15.6329\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.8079 - regression_loss: 16.1729 - val_loss: 19.6270 - val_regression_loss: 14.0682 - lr: 1.5625e-06\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1117 - regression_loss: 16.1720 - val_loss: 19.6259 - val_regression_loss: 14.0678 - lr: 7.8125e-07\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.0335 - regression_loss: 16.1709 - val_loss: 19.6253 - val_regression_loss: 14.0666 - lr: 7.8125e-07\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.9143 - regression_loss: 16.1712 - val_loss: 19.6251 - val_regression_loss: 14.0650 - lr: 7.8125e-07\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.1041 - regression_loss: 16.1697 - val_loss: 19.6242 - val_regression_loss: 14.0642 - lr: 7.8125e-07\n",
      "Epoch 81/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 22.0906 - regression_loss: 20.4433\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.0483 - regression_loss: 16.1697 - val_loss: 19.6250 - val_regression_loss: 14.0649 - lr: 7.8125e-07\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.7516 - regression_loss: 16.1683 - val_loss: 19.6245 - val_regression_loss: 14.0647 - lr: 3.9062e-07\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.6514 - regression_loss: 16.1682 - val_loss: 19.6243 - val_regression_loss: 14.0647 - lr: 3.9062e-07\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.9863 - regression_loss: 16.1679 - val_loss: 19.6232 - val_regression_loss: 14.0636 - lr: 3.9062e-07\n",
      "Epoch 85/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 18.7101 - regression_loss: 16.1678 - val_loss: 19.6230 - val_regression_loss: 14.0633 - lr: 3.9062e-07\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.9042 - regression_loss: 16.1675 - val_loss: 19.6226 - val_regression_loss: 14.0626 - lr: 3.9062e-07\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.0766 - regression_loss: 16.1677 - val_loss: 19.6225 - val_regression_loss: 14.0623 - lr: 3.9062e-07\n",
      "Epoch 88/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 21.1022 - regression_loss: 19.4549\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.8925 - regression_loss: 16.1671 - val_loss: 19.6234 - val_regression_loss: 14.0631 - lr: 3.9062e-07\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.8772 - regression_loss: 16.1669 - val_loss: 19.6233 - val_regression_loss: 14.0633 - lr: 1.9531e-07\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.9276 - regression_loss: 16.1667 - val_loss: 19.6232 - val_regression_loss: 14.0631 - lr: 1.9531e-07\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1453 - regression_loss: 16.1670 - val_loss: 19.6235 - val_regression_loss: 14.0632 - lr: 1.9531e-07\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.8014 - regression_loss: 16.1667 - val_loss: 19.6230 - val_regression_loss: 14.0627 - lr: 1.9531e-07\n",
      "Epoch 93/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 23.9579 - regression_loss: 22.3107\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.9608 - regression_loss: 16.1663 - val_loss: 19.6229 - val_regression_loss: 14.0629 - lr: 1.9531e-07\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.3556 - regression_loss: 16.1660 - val_loss: 19.6230 - val_regression_loss: 14.0630 - lr: 9.7656e-08\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.8216 - regression_loss: 16.1660 - val_loss: 19.6227 - val_regression_loss: 14.0627 - lr: 9.7656e-08\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.0243 - regression_loss: 16.1658 - val_loss: 19.6225 - val_regression_loss: 14.0625 - lr: 9.7656e-08\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.0078 - regression_loss: 16.1660 - val_loss: 19.6226 - val_regression_loss: 14.0625 - lr: 9.7656e-08\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1902 - regression_loss: 16.1659 - val_loss: 19.6226 - val_regression_loss: 14.0626 - lr: 9.7656e-08\n",
      "Epoch 99/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.4439 - regression_loss: 16.7966\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.0861 - regression_loss: 16.1659 - val_loss: 19.6228 - val_regression_loss: 14.0628 - lr: 9.7656e-08\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.9833 - regression_loss: 16.1656 - val_loss: 19.6227 - val_regression_loss: 14.0627 - lr: 4.8828e-08\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.9074 - regression_loss: 16.1656 - val_loss: 19.6227 - val_regression_loss: 14.0628 - lr: 4.8828e-08\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.9824 - regression_loss: 16.1656 - val_loss: 19.6226 - val_regression_loss: 14.0627 - lr: 4.8828e-08\n",
      "3/3 [==============================] - 0s 998us/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 24ms/step - loss: 140.7179 - regression_loss: 127.7219 - val_loss: 88.4696 - val_regression_loss: 66.0897 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 92.3749 - regression_loss: 85.2338 - val_loss: 63.5246 - val_regression_loss: 47.4755 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 70.9792 - regression_loss: 64.1563 - val_loss: 55.5779 - val_regression_loss: 41.0695 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 58.4045 - regression_loss: 52.0790 - val_loss: 50.2632 - val_regression_loss: 37.4946 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 50.5411 - regression_loss: 44.6925 - val_loss: 45.5550 - val_regression_loss: 34.4581 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 43.8318 - regression_loss: 39.1030 - val_loss: 41.8277 - val_regression_loss: 31.8276 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 39.7764 - regression_loss: 35.1467 - val_loss: 37.7290 - val_regression_loss: 28.9323 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 36.0213 - regression_loss: 31.8999 - val_loss: 34.1287 - val_regression_loss: 26.1840 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31.8461 - regression_loss: 28.1325 - val_loss: 29.7427 - val_regression_loss: 22.8302 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.6424 - regression_loss: 25.3885 - val_loss: 26.7634 - val_regression_loss: 20.4524 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.3743 - regression_loss: 22.8976 - val_loss: 23.6455 - val_regression_loss: 18.0183 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.2994 - regression_loss: 20.6339 - val_loss: 20.8144 - val_regression_loss: 15.8506 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.9784 - regression_loss: 19.1866 - val_loss: 19.2204 - val_regression_loss: 14.6370 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.2321 - regression_loss: 17.0677 - val_loss: 17.1650 - val_regression_loss: 13.0167 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.1335 - regression_loss: 15.5870 - val_loss: 16.4447 - val_regression_loss: 12.4508 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2270 - regression_loss: 14.6619 - val_loss: 15.1953 - val_regression_loss: 11.4545 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.7590 - regression_loss: 13.0885 - val_loss: 13.8586 - val_regression_loss: 10.3778 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.7988 - regression_loss: 12.0203 - val_loss: 12.8212 - val_regression_loss: 9.5633 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2981 - regression_loss: 11.0280 - val_loss: 12.1807 - val_regression_loss: 9.0312 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3249 - regression_loss: 10.3244 - val_loss: 11.4859 - val_regression_loss: 8.4602 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9078 - regression_loss: 9.4849 - val_loss: 10.6522 - val_regression_loss: 7.7808 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9449 - regression_loss: 8.7908 - val_loss: 10.4162 - val_regression_loss: 7.6415 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8305 - regression_loss: 8.4517 - val_loss: 9.5730 - val_regression_loss: 6.9170 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7184 - regression_loss: 8.5144 - val_loss: 9.5969 - val_regression_loss: 7.0019 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5891 - regression_loss: 7.3491 - val_loss: 8.5235 - val_regression_loss: 6.0105 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7324 - regression_loss: 6.6688 - val_loss: 8.4166 - val_regression_loss: 5.9496 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6514 - regression_loss: 6.3353 - val_loss: 7.9906 - val_regression_loss: 5.5871 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1873 - regression_loss: 6.0783 - val_loss: 8.4814 - val_regression_loss: 6.0713 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8486 - regression_loss: 6.2841 - val_loss: 7.1307 - val_regression_loss: 4.8479 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4994 - regression_loss: 5.3649 - val_loss: 6.9995 - val_regression_loss: 4.8206 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0245 - regression_loss: 5.0083 - val_loss: 6.8482 - val_regression_loss: 4.7232 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7488 - regression_loss: 4.7086 - val_loss: 6.5719 - val_regression_loss: 4.4077 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5990 - regression_loss: 4.7218 - val_loss: 6.7690 - val_regression_loss: 4.6368 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3419 - regression_loss: 4.3023 - val_loss: 6.1408 - val_regression_loss: 4.0297 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.1395 - regression_loss: 4.1257 - val_loss: 6.1539 - val_regression_loss: 4.1409 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7925 - regression_loss: 3.8145 - val_loss: 5.8689 - val_regression_loss: 3.8217 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6272 - regression_loss: 3.8066 - val_loss: 5.7079 - val_regression_loss: 3.7203 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.4342 - regression_loss: 3.4840 - val_loss: 5.4750 - val_regression_loss: 3.4881 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9573 - regression_loss: 3.3563 - val_loss: 5.3843 - val_regression_loss: 3.4321 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2810 - regression_loss: 3.3513 - val_loss: 5.6387 - val_regression_loss: 3.6774 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0616 - regression_loss: 3.2260 - val_loss: 5.6816 - val_regression_loss: 3.6110 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0619 - regression_loss: 3.1684 - val_loss: 5.5990 - val_regression_loss: 3.6168 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8046 - regression_loss: 3.1279 - val_loss: 5.0900 - val_regression_loss: 3.1212 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8964 - regression_loss: 3.0507 - val_loss: 5.0510 - val_regression_loss: 3.1401 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7361 - regression_loss: 2.9032 - val_loss: 5.2031 - val_regression_loss: 3.2663 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5201 - regression_loss: 2.7126 - val_loss: 4.9898 - val_regression_loss: 3.0254 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.4396 - regression_loss: 2.6540 - val_loss: 4.9051 - val_regression_loss: 2.9978 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3454 - regression_loss: 2.5082 - val_loss: 4.6913 - val_regression_loss: 2.7906 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2386 - regression_loss: 2.3904 - val_loss: 4.7107 - val_regression_loss: 2.8302 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.1018 - regression_loss: 2.3284 - val_loss: 4.8193 - val_regression_loss: 2.8603 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2214 - regression_loss: 2.3806 - val_loss: 4.7716 - val_regression_loss: 2.8821 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2793 - regression_loss: 2.4643 - val_loss: 4.5928 - val_regression_loss: 2.6856 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0965 - regression_loss: 2.2724 - val_loss: 4.3501 - val_regression_loss: 2.4972 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8693 - regression_loss: 2.1423 - val_loss: 4.3611 - val_regression_loss: 2.5201 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8871 - regression_loss: 2.0591 - val_loss: 4.2906 - val_regression_loss: 2.4312 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8205 - regression_loss: 2.0559 - val_loss: 4.3290 - val_regression_loss: 2.4352 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7533 - regression_loss: 2.0605 - val_loss: 4.1304 - val_regression_loss: 2.2973 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7585 - regression_loss: 1.9504 - val_loss: 4.2191 - val_regression_loss: 2.3856 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7011 - regression_loss: 1.9242 - val_loss: 4.5881 - val_regression_loss: 2.6236 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8544 - regression_loss: 2.0356 - val_loss: 4.6478 - val_regression_loss: 2.7370 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8781 - regression_loss: 2.0762 - val_loss: 4.1563 - val_regression_loss: 2.2725 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5487 - regression_loss: 1.7951 - val_loss: 3.8974 - val_regression_loss: 2.0747 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4927 - regression_loss: 1.7135 - val_loss: 3.8650 - val_regression_loss: 2.0477 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4716 - regression_loss: 1.6974 - val_loss: 3.8262 - val_regression_loss: 2.0037 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4149 - regression_loss: 1.6439 - val_loss: 3.8838 - val_regression_loss: 2.0665 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4100 - regression_loss: 1.6530 - val_loss: 3.8520 - val_regression_loss: 2.0105 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2342 - regression_loss: 1.5683 - val_loss: 3.7054 - val_regression_loss: 1.9065 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2765 - regression_loss: 1.5309 - val_loss: 3.7984 - val_regression_loss: 1.9606 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2961 - regression_loss: 1.5645 - val_loss: 3.6639 - val_regression_loss: 1.8558 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2462 - regression_loss: 1.5074 - val_loss: 3.6431 - val_regression_loss: 1.8335 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2790 - regression_loss: 1.5282 - val_loss: 3.6274 - val_regression_loss: 1.8018 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1814 - regression_loss: 1.4378 - val_loss: 3.5853 - val_regression_loss: 1.7652 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1582 - regression_loss: 1.4023 - val_loss: 3.5952 - val_regression_loss: 1.7874 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1755 - regression_loss: 1.4254 - val_loss: 3.5830 - val_regression_loss: 1.7565 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1132 - regression_loss: 1.3970 - val_loss: 3.4965 - val_regression_loss: 1.6859 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1384 - regression_loss: 1.3780 - val_loss: 3.4440 - val_regression_loss: 1.6464 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0819 - regression_loss: 1.3448 - val_loss: 3.4577 - val_regression_loss: 1.6454 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0454 - regression_loss: 1.3128 - val_loss: 3.4931 - val_regression_loss: 1.6687 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0475 - regression_loss: 1.3264 - val_loss: 3.6182 - val_regression_loss: 1.7893 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2552 - regression_loss: 1.5031 - val_loss: 3.3960 - val_regression_loss: 1.5852 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0157 - regression_loss: 1.2811 - val_loss: 3.4149 - val_regression_loss: 1.6010 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9573 - regression_loss: 1.2377 - val_loss: 3.3274 - val_regression_loss: 1.5242 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9568 - regression_loss: 1.2215 - val_loss: 3.2733 - val_regression_loss: 1.4785 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9681 - regression_loss: 1.2258 - val_loss: 3.2888 - val_regression_loss: 1.4923 - lr: 1.0000e-04\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9662 - regression_loss: 1.2174 - val_loss: 3.2619 - val_regression_loss: 1.4714 - lr: 1.0000e-04\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8737 - regression_loss: 1.1999 - val_loss: 3.2209 - val_regression_loss: 1.4227 - lr: 1.0000e-04\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8574 - regression_loss: 1.1552 - val_loss: 3.2238 - val_regression_loss: 1.4238 - lr: 1.0000e-04\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8627 - regression_loss: 1.1465 - val_loss: 3.2539 - val_regression_loss: 1.4484 - lr: 1.0000e-04\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8901 - regression_loss: 1.1645 - val_loss: 3.2269 - val_regression_loss: 1.4300 - lr: 1.0000e-04\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8984 - regression_loss: 1.1739 - val_loss: 3.1806 - val_regression_loss: 1.3889 - lr: 1.0000e-04\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8215 - regression_loss: 1.1039 - val_loss: 3.2502 - val_regression_loss: 1.4347 - lr: 1.0000e-04\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8501 - regression_loss: 1.1568 - val_loss: 3.1965 - val_regression_loss: 1.3880 - lr: 1.0000e-04\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7922 - regression_loss: 1.0917 - val_loss: 3.1410 - val_regression_loss: 1.3455 - lr: 1.0000e-04\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7546 - regression_loss: 1.0727 - val_loss: 3.1463 - val_regression_loss: 1.3472 - lr: 1.0000e-04\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7542 - regression_loss: 1.0612 - val_loss: 3.1109 - val_regression_loss: 1.3206 - lr: 1.0000e-04\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7419 - regression_loss: 1.0437 - val_loss: 3.1900 - val_regression_loss: 1.3696 - lr: 1.0000e-04\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8334 - regression_loss: 1.0973 - val_loss: 3.1479 - val_regression_loss: 1.3342 - lr: 1.0000e-04\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.7502 - regression_loss: 1.0724 - val_loss: 3.0810 - val_regression_loss: 1.2890 - lr: 1.0000e-04\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6755 - regression_loss: 1.0274 - val_loss: 3.0701 - val_regression_loss: 1.2790 - lr: 1.0000e-04\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7583 - regression_loss: 1.0381 - val_loss: 3.1075 - val_regression_loss: 1.3018 - lr: 1.0000e-04\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6793 - regression_loss: 0.9974 - val_loss: 3.0237 - val_regression_loss: 1.2336 - lr: 1.0000e-04\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7445 - regression_loss: 1.0535 - val_loss: 3.0994 - val_regression_loss: 1.2869 - lr: 1.0000e-04\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6613 - regression_loss: 0.9724 - val_loss: 3.0938 - val_regression_loss: 1.2887 - lr: 1.0000e-04\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7183 - regression_loss: 1.0230 - val_loss: 3.0188 - val_regression_loss: 1.2224 - lr: 1.0000e-04\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6761 - regression_loss: 0.9941 - val_loss: 3.0472 - val_regression_loss: 1.2452 - lr: 1.0000e-04\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6675 - regression_loss: 0.9904 - val_loss: 3.0783 - val_regression_loss: 1.2675 - lr: 1.0000e-04\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6722 - regression_loss: 0.9885 - val_loss: 2.9642 - val_regression_loss: 1.1754 - lr: 1.0000e-04\n",
      "Epoch 108/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.0378 - regression_loss: 1.4034\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6837 - regression_loss: 0.9880 - val_loss: 3.1480 - val_regression_loss: 1.3094 - lr: 1.0000e-04\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7185 - regression_loss: 1.0208 - val_loss: 2.9988 - val_regression_loss: 1.1984 - lr: 5.0000e-05\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6479 - regression_loss: 0.9523 - val_loss: 3.0067 - val_regression_loss: 1.2006 - lr: 5.0000e-05\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6379 - regression_loss: 0.9521 - val_loss: 3.0175 - val_regression_loss: 1.2098 - lr: 5.0000e-05\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6439 - regression_loss: 0.9406 - val_loss: 2.9600 - val_regression_loss: 1.1679 - lr: 5.0000e-05\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6446 - regression_loss: 0.9524 - val_loss: 3.0685 - val_regression_loss: 1.2492 - lr: 5.0000e-05\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6901 - regression_loss: 0.9740 - val_loss: 2.9926 - val_regression_loss: 1.1927 - lr: 5.0000e-05\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6150 - regression_loss: 0.9165 - val_loss: 3.0063 - val_regression_loss: 1.1978 - lr: 5.0000e-05\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6044 - regression_loss: 0.9183 - val_loss: 2.9371 - val_regression_loss: 1.1467 - lr: 5.0000e-05\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6062 - regression_loss: 0.9290 - val_loss: 3.0260 - val_regression_loss: 1.2132 - lr: 5.0000e-05\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6257 - regression_loss: 0.9310 - val_loss: 2.9661 - val_regression_loss: 1.1716 - lr: 5.0000e-05\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6091 - regression_loss: 0.9243 - val_loss: 2.9930 - val_regression_loss: 1.1878 - lr: 5.0000e-05\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6220 - regression_loss: 0.9176 - val_loss: 2.9585 - val_regression_loss: 1.1642 - lr: 5.0000e-05\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6032 - regression_loss: 0.9262 - val_loss: 2.9719 - val_regression_loss: 1.1723 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5936 - regression_loss: 0.9094 - val_loss: 2.9356 - val_regression_loss: 1.1428 - lr: 5.0000e-05\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5978 - regression_loss: 0.8948 - val_loss: 2.9371 - val_regression_loss: 1.1420 - lr: 5.0000e-05\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6035 - regression_loss: 0.8986 - val_loss: 2.9461 - val_regression_loss: 1.1471 - lr: 5.0000e-05\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6015 - regression_loss: 0.8997 - val_loss: 2.9247 - val_regression_loss: 1.1297 - lr: 5.0000e-05\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5614 - regression_loss: 0.8942 - val_loss: 2.9199 - val_regression_loss: 1.1244 - lr: 5.0000e-05\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5838 - regression_loss: 0.8898 - val_loss: 2.9161 - val_regression_loss: 1.1244 - lr: 5.0000e-05\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6086 - regression_loss: 0.9077 - val_loss: 2.9382 - val_regression_loss: 1.1414 - lr: 5.0000e-05\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5966 - regression_loss: 0.8945 - val_loss: 2.9308 - val_regression_loss: 1.1327 - lr: 5.0000e-05\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5851 - regression_loss: 0.8866 - val_loss: 2.9069 - val_regression_loss: 1.1151 - lr: 5.0000e-05\n",
      "Epoch 131/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.4343 - regression_loss: 0.8024\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5718 - regression_loss: 0.8827 - val_loss: 2.9200 - val_regression_loss: 1.1236 - lr: 5.0000e-05\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5746 - regression_loss: 0.8735 - val_loss: 2.9146 - val_regression_loss: 1.1201 - lr: 2.5000e-05\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5488 - regression_loss: 0.8692 - val_loss: 2.9202 - val_regression_loss: 1.1238 - lr: 2.5000e-05\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5545 - regression_loss: 0.8684 - val_loss: 2.9064 - val_regression_loss: 1.1129 - lr: 2.5000e-05\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5478 - regression_loss: 0.8684 - val_loss: 2.9079 - val_regression_loss: 1.1137 - lr: 2.5000e-05\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5706 - regression_loss: 0.8676 - val_loss: 2.9273 - val_regression_loss: 1.1286 - lr: 2.5000e-05\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5589 - regression_loss: 0.8685 - val_loss: 2.9090 - val_regression_loss: 1.1143 - lr: 2.5000e-05\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5620 - regression_loss: 0.8644 - val_loss: 2.9065 - val_regression_loss: 1.1110 - lr: 2.5000e-05\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5447 - regression_loss: 0.8604 - val_loss: 2.8972 - val_regression_loss: 1.1047 - lr: 2.5000e-05\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5461 - regression_loss: 0.8674 - val_loss: 2.9054 - val_regression_loss: 1.1100 - lr: 2.5000e-05\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5564 - regression_loss: 0.8567 - val_loss: 2.8949 - val_regression_loss: 1.1027 - lr: 2.5000e-05\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5667 - regression_loss: 0.8620 - val_loss: 2.9146 - val_regression_loss: 1.1168 - lr: 2.5000e-05\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5418 - regression_loss: 0.8633 - val_loss: 2.9010 - val_regression_loss: 1.1071 - lr: 2.5000e-05\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5531 - regression_loss: 0.8632 - val_loss: 2.8875 - val_regression_loss: 1.0961 - lr: 2.5000e-05\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5563 - regression_loss: 0.8535 - val_loss: 2.9138 - val_regression_loss: 1.1159 - lr: 2.5000e-05\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5589 - regression_loss: 0.8608 - val_loss: 2.8877 - val_regression_loss: 1.0963 - lr: 2.5000e-05\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5556 - regression_loss: 0.8601 - val_loss: 2.8970 - val_regression_loss: 1.1029 - lr: 2.5000e-05\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5301 - regression_loss: 0.8501 - val_loss: 2.8996 - val_regression_loss: 1.1056 - lr: 2.5000e-05\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5135 - regression_loss: 0.8525 - val_loss: 2.8983 - val_regression_loss: 1.1034 - lr: 2.5000e-05\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5441 - regression_loss: 0.8527 - val_loss: 2.8860 - val_regression_loss: 1.0931 - lr: 2.5000e-05\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5462 - regression_loss: 0.8507 - val_loss: 2.8995 - val_regression_loss: 1.1031 - lr: 2.5000e-05\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5404 - regression_loss: 0.8476 - val_loss: 2.8892 - val_regression_loss: 1.0955 - lr: 2.5000e-05\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5360 - regression_loss: 0.8467 - val_loss: 2.8867 - val_regression_loss: 1.0926 - lr: 2.5000e-05\n",
      "Epoch 154/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.4967 - regression_loss: 0.8662\n",
      "Epoch 154: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5241 - regression_loss: 0.8443 - val_loss: 2.8817 - val_regression_loss: 1.0879 - lr: 2.5000e-05\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5431 - regression_loss: 0.8414 - val_loss: 2.8831 - val_regression_loss: 1.0897 - lr: 1.2500e-05\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5240 - regression_loss: 0.8404 - val_loss: 2.8794 - val_regression_loss: 1.0868 - lr: 1.2500e-05\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5338 - regression_loss: 0.8418 - val_loss: 2.8836 - val_regression_loss: 1.0898 - lr: 1.2500e-05\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5251 - regression_loss: 0.8418 - val_loss: 2.8936 - val_regression_loss: 1.0975 - lr: 1.2500e-05\n",
      "Epoch 159/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.4863 - regression_loss: 0.8560\n",
      "Epoch 159: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5357 - regression_loss: 0.8412 - val_loss: 2.8756 - val_regression_loss: 1.0841 - lr: 1.2500e-05\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5352 - regression_loss: 0.8382 - val_loss: 2.8787 - val_regression_loss: 1.0864 - lr: 6.2500e-06\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5249 - regression_loss: 0.8376 - val_loss: 2.8839 - val_regression_loss: 1.0902 - lr: 6.2500e-06\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5207 - regression_loss: 0.8371 - val_loss: 2.8822 - val_regression_loss: 1.0888 - lr: 6.2500e-06\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5049 - regression_loss: 0.8368 - val_loss: 2.8796 - val_regression_loss: 1.0869 - lr: 6.2500e-06\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5126 - regression_loss: 0.8377 - val_loss: 2.8757 - val_regression_loss: 1.0839 - lr: 6.2500e-06\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5286 - regression_loss: 0.8365 - val_loss: 2.8820 - val_regression_loss: 1.0886 - lr: 6.2500e-06\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5090 - regression_loss: 0.8364 - val_loss: 2.8811 - val_regression_loss: 1.0877 - lr: 6.2500e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5298 - regression_loss: 0.8371 - val_loss: 2.8762 - val_regression_loss: 1.0843 - lr: 6.2500e-06\n",
      "Epoch 168/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.6491 - regression_loss: 1.0190\n",
      "Epoch 168: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5256 - regression_loss: 0.8359 - val_loss: 2.8815 - val_regression_loss: 1.0880 - lr: 6.2500e-06\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5375 - regression_loss: 0.8340 - val_loss: 2.8818 - val_regression_loss: 1.0883 - lr: 3.1250e-06\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5032 - regression_loss: 0.8348 - val_loss: 2.8791 - val_regression_loss: 1.0861 - lr: 3.1250e-06\n",
      "Epoch 171/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5184 - regression_loss: 0.8339 - val_loss: 2.8807 - val_regression_loss: 1.0872 - lr: 3.1250e-06\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5226 - regression_loss: 0.8339 - val_loss: 2.8807 - val_regression_loss: 1.0872 - lr: 3.1250e-06\n",
      "Epoch 173/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5241 - regression_loss: 0.8336 - val_loss: 2.8796 - val_regression_loss: 1.0863 - lr: 3.1250e-06\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5368 - regression_loss: 0.8339 - val_loss: 2.8790 - val_regression_loss: 1.0859 - lr: 3.1250e-06\n",
      "Epoch 175/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5005 - regression_loss: 0.8334 - val_loss: 2.8778 - val_regression_loss: 1.0849 - lr: 3.1250e-06\n",
      "Epoch 176/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5244 - regression_loss: 0.8332 - val_loss: 2.8786 - val_regression_loss: 1.0856 - lr: 3.1250e-06\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5227 - regression_loss: 0.8328 - val_loss: 2.8790 - val_regression_loss: 1.0859 - lr: 3.1250e-06\n",
      "Epoch 178/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5088 - regression_loss: 0.8332 - val_loss: 2.8801 - val_regression_loss: 1.0868 - lr: 3.1250e-06\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5245 - regression_loss: 0.8330 - val_loss: 2.8779 - val_regression_loss: 1.0850 - lr: 3.1250e-06\n",
      "Epoch 180/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.5573 - regression_loss: 0.9273\n",
      "Epoch 180: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5053 - regression_loss: 0.8328 - val_loss: 2.8784 - val_regression_loss: 1.0853 - lr: 3.1250e-06\n",
      "Epoch 181/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5244 - regression_loss: 0.8325 - val_loss: 2.8772 - val_regression_loss: 1.0843 - lr: 1.5625e-06\n",
      "Epoch 182/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5193 - regression_loss: 0.8325 - val_loss: 2.8765 - val_regression_loss: 1.0839 - lr: 1.5625e-06\n",
      "Epoch 183/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5208 - regression_loss: 0.8322 - val_loss: 2.8759 - val_regression_loss: 1.0833 - lr: 1.5625e-06\n",
      "Epoch 184/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5225 - regression_loss: 0.8321 - val_loss: 2.8758 - val_regression_loss: 1.0832 - lr: 1.5625e-06\n",
      "Epoch 185/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.5429 - regression_loss: 0.9129\n",
      "Epoch 185: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5177 - regression_loss: 0.8321 - val_loss: 2.8759 - val_regression_loss: 1.0833 - lr: 1.5625e-06\n",
      "Epoch 186/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5242 - regression_loss: 0.8318 - val_loss: 2.8762 - val_regression_loss: 1.0835 - lr: 7.8125e-07\n",
      "Epoch 187/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5195 - regression_loss: 0.8318 - val_loss: 2.8763 - val_regression_loss: 1.0836 - lr: 7.8125e-07\n",
      "Epoch 188/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5302 - regression_loss: 0.8321 - val_loss: 2.8770 - val_regression_loss: 1.0841 - lr: 7.8125e-07\n",
      "Epoch 189/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5224 - regression_loss: 0.8318 - val_loss: 2.8769 - val_regression_loss: 1.0840 - lr: 7.8125e-07\n",
      "Epoch 190/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.7457 - regression_loss: 1.1157\n",
      "Epoch 190: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5228 - regression_loss: 0.8319 - val_loss: 2.8763 - val_regression_loss: 1.0835 - lr: 7.8125e-07\n",
      "Epoch 191/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5124 - regression_loss: 0.8316 - val_loss: 2.8762 - val_regression_loss: 1.0835 - lr: 3.9062e-07\n",
      "Epoch 192/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4978 - regression_loss: 0.8315 - val_loss: 2.8762 - val_regression_loss: 1.0835 - lr: 3.9062e-07\n",
      "Epoch 193/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5223 - regression_loss: 0.8315 - val_loss: 2.8761 - val_regression_loss: 1.0834 - lr: 3.9062e-07\n",
      "Epoch 194/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5275 - regression_loss: 0.8315 - val_loss: 2.8761 - val_regression_loss: 1.0833 - lr: 3.9062e-07\n",
      "Epoch 195/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5175 - regression_loss: 0.8315 - val_loss: 2.8761 - val_regression_loss: 1.0834 - lr: 3.9062e-07\n",
      "Epoch 196/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5019 - regression_loss: 0.8316 - val_loss: 2.8763 - val_regression_loss: 1.0835 - lr: 3.9062e-07\n",
      "Epoch 197/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.5490 - regression_loss: 0.9190\n",
      "Epoch 197: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5323 - regression_loss: 0.8316 - val_loss: 2.8764 - val_regression_loss: 1.0836 - lr: 3.9062e-07\n",
      "Epoch 198/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5126 - regression_loss: 0.8314 - val_loss: 2.8763 - val_regression_loss: 1.0836 - lr: 1.9531e-07\n",
      "Epoch 199/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5295 - regression_loss: 0.8314 - val_loss: 2.8764 - val_regression_loss: 1.0836 - lr: 1.9531e-07\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 26ms/step - loss: 132.0133 - regression_loss: 119.9632 - val_loss: 68.9259 - val_regression_loss: 56.3690 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 69.5282 - regression_loss: 64.2046 - val_loss: 45.1620 - val_regression_loss: 35.3331 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 49.4345 - regression_loss: 44.3540 - val_loss: 32.5796 - val_regression_loss: 25.2259 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 40.7933 - regression_loss: 36.0281 - val_loss: 27.8317 - val_regression_loss: 21.0581 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 36.8245 - regression_loss: 32.4263 - val_loss: 26.0933 - val_regression_loss: 19.2828 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 34.3523 - regression_loss: 30.6833 - val_loss: 24.0257 - val_regression_loss: 18.1214 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.8417 - regression_loss: 28.8959 - val_loss: 23.1360 - val_regression_loss: 17.2368 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.0824 - regression_loss: 27.4941 - val_loss: 21.9356 - val_regression_loss: 16.7497 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.1962 - regression_loss: 26.3969 - val_loss: 21.4945 - val_regression_loss: 16.3737 - lr: 1.0000e-04\n",
      "Epoch 10/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 29.3841 - regression_loss: 25.6803 - val_loss: 21.1005 - val_regression_loss: 16.1667 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.8748 - regression_loss: 24.9645 - val_loss: 20.8190 - val_regression_loss: 15.9382 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.7588 - regression_loss: 24.4636 - val_loss: 20.5912 - val_regression_loss: 15.6943 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.4477 - regression_loss: 23.8899 - val_loss: 20.3747 - val_regression_loss: 15.5685 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.6834 - regression_loss: 23.5459 - val_loss: 20.2958 - val_regression_loss: 15.3486 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.3538 - regression_loss: 23.1802 - val_loss: 20.1115 - val_regression_loss: 15.4693 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.3220 - regression_loss: 23.3830 - val_loss: 20.2168 - val_regression_loss: 15.2268 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.4030 - regression_loss: 22.9237 - val_loss: 19.7361 - val_regression_loss: 15.2141 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.7993 - regression_loss: 22.6017 - val_loss: 19.5384 - val_regression_loss: 14.8672 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.1447 - regression_loss: 22.2961 - val_loss: 19.4976 - val_regression_loss: 14.9606 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.8817 - regression_loss: 21.9232 - val_loss: 19.5538 - val_regression_loss: 14.8458 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.2561 - regression_loss: 21.9302 - val_loss: 19.6440 - val_regression_loss: 15.2206 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.6123 - regression_loss: 21.5882 - val_loss: 19.4530 - val_regression_loss: 14.7518 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.6923 - regression_loss: 21.5483 - val_loss: 19.5625 - val_regression_loss: 15.1494 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.3524 - regression_loss: 21.4831 - val_loss: 19.4519 - val_regression_loss: 14.7371 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.4624 - regression_loss: 21.4610 - val_loss: 19.5261 - val_regression_loss: 15.2097 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.2843 - regression_loss: 21.2790 - val_loss: 19.6805 - val_regression_loss: 14.8070 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.3625 - regression_loss: 21.8640 - val_loss: 20.4051 - val_regression_loss: 16.1416 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.0571 - regression_loss: 22.3130 - val_loss: 19.9974 - val_regression_loss: 14.9796 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.5475 - regression_loss: 21.3185 - val_loss: 19.5153 - val_regression_loss: 15.2044 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.9247 - regression_loss: 20.8689 - val_loss: 19.2150 - val_regression_loss: 14.5667 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.9142 - regression_loss: 20.7329 - val_loss: 19.4962 - val_regression_loss: 15.2336 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.0111 - regression_loss: 20.7584 - val_loss: 19.0430 - val_regression_loss: 14.4210 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.9019 - regression_loss: 20.7197 - val_loss: 19.1887 - val_regression_loss: 14.8609 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.7162 - regression_loss: 20.3856 - val_loss: 19.1880 - val_regression_loss: 14.5053 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.0317 - regression_loss: 21.0213 - val_loss: 19.6038 - val_regression_loss: 15.3599 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.5723 - regression_loss: 20.5403 - val_loss: 19.3869 - val_regression_loss: 14.5739 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.8891 - regression_loss: 20.6783 - val_loss: 20.3937 - val_regression_loss: 16.1996 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.9620 - regression_loss: 20.8948 - val_loss: 19.8545 - val_regression_loss: 14.8823 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.6130 - regression_loss: 21.3435 - val_loss: 21.7056 - val_regression_loss: 17.4404 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.1220 - regression_loss: 21.0482 - val_loss: 20.1892 - val_regression_loss: 15.0462 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 28.1090 - regression_loss: 26.4805\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.1366 - regression_loss: 20.9108 - val_loss: 20.5470 - val_regression_loss: 16.3343 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.2930 - regression_loss: 21.0439 - val_loss: 18.9392 - val_regression_loss: 14.3998 - lr: 5.0000e-05\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.3565 - regression_loss: 20.3927 - val_loss: 18.9410 - val_regression_loss: 14.4341 - lr: 5.0000e-05\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.3569 - regression_loss: 20.1817 - val_loss: 19.3003 - val_regression_loss: 15.0768 - lr: 5.0000e-05\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.8721 - regression_loss: 19.8667 - val_loss: 19.0022 - val_regression_loss: 14.4989 - lr: 5.0000e-05\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.8150 - regression_loss: 19.8861 - val_loss: 18.9648 - val_regression_loss: 14.5372 - lr: 5.0000e-05\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.4561 - regression_loss: 19.8332 - val_loss: 18.9343 - val_regression_loss: 14.5969 - lr: 5.0000e-05\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.9102 - regression_loss: 19.7629 - val_loss: 18.9573 - val_regression_loss: 14.6280 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.7572 - regression_loss: 19.8630 - val_loss: 18.8934 - val_regression_loss: 14.4825 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.6329 - regression_loss: 19.7243 - val_loss: 18.9660 - val_regression_loss: 14.6451 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.7166 - regression_loss: 19.6869 - val_loss: 18.9666 - val_regression_loss: 14.6523 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 26.7568 - regression_loss: 25.1284\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.8321 - regression_loss: 19.6810 - val_loss: 18.9220 - val_regression_loss: 14.5568 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.1493 - regression_loss: 19.6101 - val_loss: 18.9653 - val_regression_loss: 14.6397 - lr: 2.5000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.5248 - regression_loss: 19.6009 - val_loss: 18.9958 - val_regression_loss: 14.6862 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.6544 - regression_loss: 19.5952 - val_loss: 18.9682 - val_regression_loss: 14.5940 - lr: 2.5000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.4906 - regression_loss: 19.5735 - val_loss: 18.9731 - val_regression_loss: 14.5835 - lr: 2.5000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.4820 - regression_loss: 19.6213 - val_loss: 19.0031 - val_regression_loss: 14.6675 - lr: 2.5000e-05\n",
      "Epoch 58/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 24.2642 - regression_loss: 22.6359\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.5149 - regression_loss: 19.5517 - val_loss: 18.9878 - val_regression_loss: 14.5805 - lr: 2.5000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.2012 - regression_loss: 19.5323 - val_loss: 18.9892 - val_regression_loss: 14.5900 - lr: 1.2500e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.6221 - regression_loss: 19.5289 - val_loss: 19.0076 - val_regression_loss: 14.6520 - lr: 1.2500e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.6940 - regression_loss: 19.5166 - val_loss: 19.0220 - val_regression_loss: 14.6808 - lr: 1.2500e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.5076 - regression_loss: 19.5405 - val_loss: 19.0045 - val_regression_loss: 14.6248 - lr: 1.2500e-05\n",
      "Epoch 63/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 24.4221 - regression_loss: 22.7938\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.4096 - regression_loss: 19.5166 - val_loss: 18.9984 - val_regression_loss: 14.6327 - lr: 1.2500e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.4713 - regression_loss: 19.4910 - val_loss: 18.9977 - val_regression_loss: 14.6317 - lr: 6.2500e-06\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.4258 - regression_loss: 19.4878 - val_loss: 19.0052 - val_regression_loss: 14.6512 - lr: 6.2500e-06\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.0999 - regression_loss: 19.4857 - val_loss: 18.9965 - val_regression_loss: 14.6343 - lr: 6.2500e-06\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.2873 - regression_loss: 19.4826 - val_loss: 19.0047 - val_regression_loss: 14.6496 - lr: 6.2500e-06\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.5969 - regression_loss: 19.4772 - val_loss: 19.0032 - val_regression_loss: 14.6371 - lr: 6.2500e-06\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.0866 - regression_loss: 19.4847 - val_loss: 19.0104 - val_regression_loss: 14.6523 - lr: 6.2500e-06\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.4637 - regression_loss: 19.4802 - val_loss: 18.9960 - val_regression_loss: 14.6210 - lr: 6.2500e-06\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.3705 - regression_loss: 19.4705 - val_loss: 18.9990 - val_regression_loss: 14.6319 - lr: 6.2500e-06\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.4933 - regression_loss: 19.4687 - val_loss: 18.9972 - val_regression_loss: 14.6302 - lr: 6.2500e-06\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.6009 - regression_loss: 19.4662 - val_loss: 19.0146 - val_regression_loss: 14.6625 - lr: 6.2500e-06\n",
      "Epoch 74/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 25.6555 - regression_loss: 24.0273\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.3620 - regression_loss: 19.4621 - val_loss: 19.0072 - val_regression_loss: 14.6432 - lr: 6.2500e-06\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.4107 - regression_loss: 19.4634 - val_loss: 19.0029 - val_regression_loss: 14.6287 - lr: 3.1250e-06\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.5104 - regression_loss: 19.4525 - val_loss: 19.0053 - val_regression_loss: 14.6424 - lr: 3.1250e-06\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.5773 - regression_loss: 19.4487 - val_loss: 19.0106 - val_regression_loss: 14.6524 - lr: 3.1250e-06\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.0307 - regression_loss: 19.4469 - val_loss: 19.0107 - val_regression_loss: 14.6510 - lr: 3.1250e-06\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.2669 - regression_loss: 19.4470 - val_loss: 19.0114 - val_regression_loss: 14.6543 - lr: 3.1250e-06\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.3038 - regression_loss: 19.4470 - val_loss: 19.0073 - val_regression_loss: 14.6422 - lr: 3.1250e-06\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.2212 - regression_loss: 19.4423 - val_loss: 19.0075 - val_regression_loss: 14.6434 - lr: 3.1250e-06\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.5172 - regression_loss: 19.4416 - val_loss: 19.0116 - val_regression_loss: 14.6489 - lr: 3.1250e-06\n",
      "Epoch 83/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 24.8980 - regression_loss: 23.2697\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.4084 - regression_loss: 19.4398 - val_loss: 19.0178 - val_regression_loss: 14.6568 - lr: 3.1250e-06\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.5286 - regression_loss: 19.4363 - val_loss: 19.0152 - val_regression_loss: 14.6523 - lr: 1.5625e-06\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.6861 - regression_loss: 19.4351 - val_loss: 19.0159 - val_regression_loss: 14.6535 - lr: 1.5625e-06\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.4523 - regression_loss: 19.4344 - val_loss: 19.0146 - val_regression_loss: 14.6504 - lr: 1.5625e-06\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.0155 - regression_loss: 19.4333 - val_loss: 19.0142 - val_regression_loss: 14.6495 - lr: 1.5625e-06\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.2798 - regression_loss: 19.4336 - val_loss: 19.0175 - val_regression_loss: 14.6555 - lr: 1.5625e-06\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.6264 - regression_loss: 19.4324 - val_loss: 19.0155 - val_regression_loss: 14.6518 - lr: 1.5625e-06\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 86.3142 - regression_loss: 78.5842 - val_loss: 52.7183 - val_regression_loss: 44.4655 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 57.0475 - regression_loss: 53.9514 - val_loss: 40.9037 - val_regression_loss: 33.6055 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 47.6191 - regression_loss: 42.9218 - val_loss: 33.1790 - val_regression_loss: 26.5442 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 38.0839 - regression_loss: 35.2228 - val_loss: 28.9868 - val_regression_loss: 22.9135 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 35.3368 - regression_loss: 31.0736 - val_loss: 26.1615 - val_regression_loss: 20.4684 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.4016 - regression_loss: 28.6003 - val_loss: 25.8705 - val_regression_loss: 20.4261 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.6052 - regression_loss: 26.5168 - val_loss: 23.3865 - val_regression_loss: 18.1859 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.3270 - regression_loss: 24.7445 - val_loss: 23.8615 - val_regression_loss: 18.8658 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.0377 - regression_loss: 23.3529 - val_loss: 21.6277 - val_regression_loss: 16.9108 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.7326 - regression_loss: 22.3016 - val_loss: 21.4240 - val_regression_loss: 16.9185 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.8486 - regression_loss: 21.5921 - val_loss: 20.0029 - val_regression_loss: 15.6111 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.9529 - regression_loss: 20.4088 - val_loss: 19.9922 - val_regression_loss: 15.7834 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.8225 - regression_loss: 19.4241 - val_loss: 19.6075 - val_regression_loss: 15.5101 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.7137 - regression_loss: 18.7532 - val_loss: 18.7838 - val_regression_loss: 14.7517 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.1704 - regression_loss: 18.1594 - val_loss: 18.8227 - val_regression_loss: 14.9096 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4501 - regression_loss: 17.7173 - val_loss: 17.9739 - val_regression_loss: 14.0835 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.4289 - regression_loss: 17.4137 - val_loss: 18.5041 - val_regression_loss: 14.7518 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.3092 - regression_loss: 16.6835 - val_loss: 17.5444 - val_regression_loss: 13.7978 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1675 - regression_loss: 16.2874 - val_loss: 17.6249 - val_regression_loss: 13.9900 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.6657 - regression_loss: 15.8641 - val_loss: 17.0411 - val_regression_loss: 13.3685 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.2353 - regression_loss: 15.3898 - val_loss: 17.0179 - val_regression_loss: 13.4414 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5646 - regression_loss: 15.0927 - val_loss: 16.7141 - val_regression_loss: 13.1680 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.0283 - regression_loss: 14.6811 - val_loss: 16.5390 - val_regression_loss: 13.0032 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.6734 - regression_loss: 14.5704 - val_loss: 16.6469 - val_regression_loss: 13.1435 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.1114 - regression_loss: 14.3694 - val_loss: 16.3413 - val_regression_loss: 12.7517 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5720 - regression_loss: 14.0382 - val_loss: 16.3660 - val_regression_loss: 12.8958 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.3200 - regression_loss: 13.8495 - val_loss: 16.0094 - val_regression_loss: 12.4924 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.2157 - regression_loss: 13.5121 - val_loss: 16.1885 - val_regression_loss: 12.7754 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.9409 - regression_loss: 13.3501 - val_loss: 15.9464 - val_regression_loss: 12.4732 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.6799 - regression_loss: 13.1919 - val_loss: 15.8528 - val_regression_loss: 12.4158 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5662 - regression_loss: 12.9808 - val_loss: 15.6979 - val_regression_loss: 12.1801 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.3089 - regression_loss: 12.8298 - val_loss: 16.0202 - val_regression_loss: 12.6783 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.4904 - regression_loss: 13.0001 - val_loss: 15.7813 - val_regression_loss: 12.1585 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.3585 - regression_loss: 12.9603 - val_loss: 15.9547 - val_regression_loss: 12.6295 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.0032 - regression_loss: 12.4732 - val_loss: 15.5189 - val_regression_loss: 12.0256 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.1434 - regression_loss: 12.6001 - val_loss: 15.5667 - val_regression_loss: 12.2156 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.0094 - regression_loss: 12.3826 - val_loss: 15.4896 - val_regression_loss: 12.1245 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.5274 - regression_loss: 12.1675 - val_loss: 15.5854 - val_regression_loss: 12.2214 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.3954 - regression_loss: 12.0245 - val_loss: 15.4068 - val_regression_loss: 12.0185 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.4785 - regression_loss: 11.8886 - val_loss: 15.3313 - val_regression_loss: 11.9269 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.2242 - regression_loss: 11.8284 - val_loss: 15.3290 - val_regression_loss: 11.9710 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.2502 - regression_loss: 11.7689 - val_loss: 15.2433 - val_regression_loss: 11.8068 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1768 - regression_loss: 11.8776 - val_loss: 16.0731 - val_regression_loss: 12.8525 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.3522 - regression_loss: 11.8755 - val_loss: 15.2072 - val_regression_loss: 11.6942 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.1299 - regression_loss: 11.7573 - val_loss: 16.0228 - val_regression_loss: 12.7842 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.4610 - regression_loss: 12.0373 - val_loss: 15.6092 - val_regression_loss: 11.8574 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.3623 - regression_loss: 11.8859 - val_loss: 15.6323 - val_regression_loss: 12.3703 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8048 - regression_loss: 11.4977 - val_loss: 15.1967 - val_regression_loss: 11.6757 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.9073 - regression_loss: 11.4454 - val_loss: 15.4619 - val_regression_loss: 12.2110 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.9618 - regression_loss: 11.4407 - val_loss: 15.0736 - val_regression_loss: 11.6426 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.6399 - regression_loss: 11.2169 - val_loss: 15.2109 - val_regression_loss: 11.9246 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.4862 - regression_loss: 11.1837 - val_loss: 15.1009 - val_regression_loss: 11.7662 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.2935 - regression_loss: 11.1370 - val_loss: 15.0640 - val_regression_loss: 11.6986 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6283 - regression_loss: 11.0600 - val_loss: 15.1439 - val_regression_loss: 11.8488 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.5434 - regression_loss: 11.1393 - val_loss: 14.9709 - val_regression_loss: 11.5870 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1487 - regression_loss: 11.0173 - val_loss: 15.0120 - val_regression_loss: 11.6698 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.3939 - regression_loss: 11.0534 - val_loss: 15.2714 - val_regression_loss: 11.9936 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.4642 - regression_loss: 11.0460 - val_loss: 15.0439 - val_regression_loss: 11.6933 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1674 - regression_loss: 10.8437 - val_loss: 15.0140 - val_regression_loss: 11.6152 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.4107 - regression_loss: 10.8855 - val_loss: 14.9905 - val_regression_loss: 11.5843 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.1549 - regression_loss: 11.5327\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.2997 - regression_loss: 10.8683 - val_loss: 15.3333 - val_regression_loss: 12.0395 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.4107 - regression_loss: 10.8094 - val_loss: 14.9528 - val_regression_loss: 11.4927 - lr: 5.0000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1369 - regression_loss: 10.7847 - val_loss: 15.0015 - val_regression_loss: 11.6565 - lr: 5.0000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9957 - regression_loss: 10.6958 - val_loss: 15.0257 - val_regression_loss: 11.6937 - lr: 5.0000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.2038 - regression_loss: 10.6785 - val_loss: 14.9657 - val_regression_loss: 11.6017 - lr: 5.0000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.0198 - regression_loss: 10.6653 - val_loss: 14.9109 - val_regression_loss: 11.4911 - lr: 5.0000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.0639 - regression_loss: 10.6829 - val_loss: 15.0611 - val_regression_loss: 11.7310 - lr: 5.0000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9253 - regression_loss: 10.6124 - val_loss: 14.9148 - val_regression_loss: 11.4914 - lr: 5.0000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0272 - regression_loss: 10.6003 - val_loss: 15.0156 - val_regression_loss: 11.6863 - lr: 5.0000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1049 - regression_loss: 10.6432 - val_loss: 14.9719 - val_regression_loss: 11.6275 - lr: 5.0000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0378 - regression_loss: 10.5677 - val_loss: 14.9069 - val_regression_loss: 11.4847 - lr: 5.0000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0900 - regression_loss: 10.5956 - val_loss: 15.0996 - val_regression_loss: 11.7681 - lr: 5.0000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8087 - regression_loss: 10.5408 - val_loss: 14.9132 - val_regression_loss: 11.5022 - lr: 5.0000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8390 - regression_loss: 10.5371 - val_loss: 15.0714 - val_regression_loss: 11.7270 - lr: 5.0000e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8542 - regression_loss: 10.5342 - val_loss: 14.9005 - val_regression_loss: 11.4815 - lr: 5.0000e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8358 - regression_loss: 10.4813 - val_loss: 14.9922 - val_regression_loss: 11.6410 - lr: 5.0000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7953 - regression_loss: 10.5147 - val_loss: 15.0170 - val_regression_loss: 11.6710 - lr: 5.0000e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9184 - regression_loss: 10.4373 - val_loss: 14.8958 - val_regression_loss: 11.4486 - lr: 5.0000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7227 - regression_loss: 10.4988 - val_loss: 15.1024 - val_regression_loss: 11.7726 - lr: 5.0000e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7624 - regression_loss: 10.4940 - val_loss: 14.9120 - val_regression_loss: 11.5093 - lr: 5.0000e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5799 - regression_loss: 10.4086 - val_loss: 15.1559 - val_regression_loss: 11.8580 - lr: 5.0000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7224 - regression_loss: 10.4177 - val_loss: 14.9102 - val_regression_loss: 11.4773 - lr: 5.0000e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7421 - regression_loss: 10.4142 - val_loss: 15.1510 - val_regression_loss: 11.8189 - lr: 5.0000e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8712 - regression_loss: 10.4059 - val_loss: 14.9315 - val_regression_loss: 11.5158 - lr: 5.0000e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5826 - regression_loss: 10.3294 - val_loss: 14.9308 - val_regression_loss: 11.5426 - lr: 5.0000e-05\n",
      "Epoch 86/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.8978 - regression_loss: 12.2771\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6955 - regression_loss: 10.3221 - val_loss: 15.0129 - val_regression_loss: 11.6667 - lr: 5.0000e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6053 - regression_loss: 10.2936 - val_loss: 14.8909 - val_regression_loss: 11.4870 - lr: 2.5000e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7012 - regression_loss: 10.2582 - val_loss: 14.9297 - val_regression_loss: 11.5600 - lr: 2.5000e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5448 - regression_loss: 10.2539 - val_loss: 14.9294 - val_regression_loss: 11.5563 - lr: 2.5000e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6245 - regression_loss: 10.2314 - val_loss: 14.9437 - val_regression_loss: 11.5760 - lr: 2.5000e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6770 - regression_loss: 10.2308 - val_loss: 14.9228 - val_regression_loss: 11.5489 - lr: 2.5000e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4533 - regression_loss: 10.2287 - val_loss: 14.8940 - val_regression_loss: 11.4935 - lr: 2.5000e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5107 - regression_loss: 10.2474 - val_loss: 14.9588 - val_regression_loss: 11.5808 - lr: 2.5000e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6468 - regression_loss: 10.2114 - val_loss: 15.0054 - val_regression_loss: 11.6481 - lr: 2.5000e-05\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4923 - regression_loss: 10.2032 - val_loss: 14.9157 - val_regression_loss: 11.5346 - lr: 2.5000e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5031 - regression_loss: 10.2072 - val_loss: 14.9012 - val_regression_loss: 11.5047 - lr: 2.5000e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4266 - regression_loss: 10.1826 - val_loss: 14.9869 - val_regression_loss: 11.6275 - lr: 2.5000e-05\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4614 - regression_loss: 10.1855 - val_loss: 14.9316 - val_regression_loss: 11.5619 - lr: 2.5000e-05\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4958 - regression_loss: 10.1696 - val_loss: 14.8944 - val_regression_loss: 11.5082 - lr: 2.5000e-05\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4408 - regression_loss: 10.1645 - val_loss: 14.9524 - val_regression_loss: 11.5865 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5229 - regression_loss: 10.1577 - val_loss: 14.9345 - val_regression_loss: 11.5583 - lr: 2.5000e-05\n",
      "Epoch 102/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.9062 - regression_loss: 12.2860\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5470 - regression_loss: 10.1522 - val_loss: 14.9124 - val_regression_loss: 11.5285 - lr: 2.5000e-05\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2281 - regression_loss: 10.1341 - val_loss: 14.9081 - val_regression_loss: 11.5228 - lr: 1.2500e-05\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4312 - regression_loss: 10.1360 - val_loss: 14.9624 - val_regression_loss: 11.5956 - lr: 1.2500e-05\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1474 - regression_loss: 10.1238 - val_loss: 14.9439 - val_regression_loss: 11.5694 - lr: 1.2500e-05\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6187 - regression_loss: 10.1192 - val_loss: 14.8981 - val_regression_loss: 11.5065 - lr: 1.2500e-05\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3431 - regression_loss: 10.1207 - val_loss: 14.8926 - val_regression_loss: 11.4966 - lr: 1.2500e-05\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4772 - regression_loss: 10.1134 - val_loss: 14.9370 - val_regression_loss: 11.5662 - lr: 1.2500e-05\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5506 - regression_loss: 10.1191 - val_loss: 14.9664 - val_regression_loss: 11.6022 - lr: 1.2500e-05\n",
      "Epoch 110/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.0796 - regression_loss: 13.4595\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3234 - regression_loss: 10.1047 - val_loss: 14.9258 - val_regression_loss: 11.5448 - lr: 1.2500e-05\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3425 - regression_loss: 10.1011 - val_loss: 14.9135 - val_regression_loss: 11.5244 - lr: 6.2500e-06\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5420 - regression_loss: 10.0994 - val_loss: 14.8984 - val_regression_loss: 11.5043 - lr: 6.2500e-06\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3899 - regression_loss: 10.0970 - val_loss: 14.9326 - val_regression_loss: 11.5541 - lr: 6.2500e-06\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3650 - regression_loss: 10.0869 - val_loss: 14.9372 - val_regression_loss: 11.5586 - lr: 6.2500e-06\n",
      "Epoch 115/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.9283 - regression_loss: 12.3082\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.5899 - regression_loss: 10.0910 - val_loss: 14.9377 - val_regression_loss: 11.5606 - lr: 6.2500e-06\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5661 - regression_loss: 10.0829 - val_loss: 14.9294 - val_regression_loss: 11.5501 - lr: 3.1250e-06\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4824 - regression_loss: 10.0835 - val_loss: 14.9132 - val_regression_loss: 11.5276 - lr: 3.1250e-06\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4405 - regression_loss: 10.0812 - val_loss: 14.9117 - val_regression_loss: 11.5246 - lr: 3.1250e-06\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6706 - regression_loss: 10.0818 - val_loss: 14.9071 - val_regression_loss: 11.5175 - lr: 3.1250e-06\n",
      "Epoch 120/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.3072 - regression_loss: 11.6872\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4870 - regression_loss: 10.0788 - val_loss: 14.9176 - val_regression_loss: 11.5310 - lr: 3.1250e-06\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4317 - regression_loss: 10.0760 - val_loss: 14.9188 - val_regression_loss: 11.5331 - lr: 1.5625e-06\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.3265 - regression_loss: 10.0804 - val_loss: 14.9283 - val_regression_loss: 11.5471 - lr: 1.5625e-06\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3755 - regression_loss: 10.0767 - val_loss: 14.9233 - val_regression_loss: 11.5406 - lr: 1.5625e-06\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2013 - regression_loss: 10.0751 - val_loss: 14.9213 - val_regression_loss: 11.5369 - lr: 1.5625e-06\n",
      "Epoch 125/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.4858 - regression_loss: 10.8658\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4733 - regression_loss: 10.0745 - val_loss: 14.9208 - val_regression_loss: 11.5369 - lr: 1.5625e-06\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5744 - regression_loss: 10.0736 - val_loss: 14.9196 - val_regression_loss: 11.5351 - lr: 7.8125e-07\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3745 - regression_loss: 10.0729 - val_loss: 14.9206 - val_regression_loss: 11.5368 - lr: 7.8125e-07\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 28ms/step - loss: 94.2776 - regression_loss: 84.8703 - val_loss: 71.1340 - val_regression_loss: 57.1841 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 59.3507 - regression_loss: 54.3379 - val_loss: 52.9227 - val_regression_loss: 42.0252 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 44.3123 - regression_loss: 39.5431 - val_loss: 41.8827 - val_regression_loss: 32.9468 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 35.7855 - regression_loss: 31.4824 - val_loss: 32.9434 - val_regression_loss: 25.7236 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.2668 - regression_loss: 23.7734 - val_loss: 26.6014 - val_regression_loss: 20.6519 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.7751 - regression_loss: 19.6801 - val_loss: 23.4271 - val_regression_loss: 18.0673 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8576 - regression_loss: 17.7665 - val_loss: 21.3969 - val_regression_loss: 16.4702 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.3487 - regression_loss: 16.4017 - val_loss: 20.2037 - val_regression_loss: 15.4718 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6982 - regression_loss: 15.0583 - val_loss: 19.0122 - val_regression_loss: 14.5146 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.3884 - regression_loss: 13.8728 - val_loss: 18.3124 - val_regression_loss: 13.8809 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.5196 - regression_loss: 13.1582 - val_loss: 17.3526 - val_regression_loss: 13.1182 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.6954 - regression_loss: 12.3000 - val_loss: 16.7492 - val_regression_loss: 12.5882 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7739 - regression_loss: 11.5661 - val_loss: 16.2294 - val_regression_loss: 12.1527 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.2064 - regression_loss: 11.0250 - val_loss: 15.8020 - val_regression_loss: 11.7919 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.0318 - regression_loss: 10.6300 - val_loss: 15.5734 - val_regression_loss: 11.5443 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6960 - regression_loss: 10.3081 - val_loss: 15.0338 - val_regression_loss: 11.1402 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3559 - regression_loss: 9.8985 - val_loss: 15.0031 - val_regression_loss: 11.0288 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9964 - regression_loss: 9.6907 - val_loss: 14.4825 - val_regression_loss: 10.6673 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8786 - regression_loss: 9.4306 - val_loss: 14.2004 - val_regression_loss: 10.3713 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3530 - regression_loss: 9.1708 - val_loss: 13.9458 - val_regression_loss: 10.1351 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1104 - regression_loss: 8.8870 - val_loss: 13.6177 - val_regression_loss: 9.8704 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8327 - regression_loss: 8.7077 - val_loss: 13.3775 - val_regression_loss: 9.6642 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7283 - regression_loss: 8.6048 - val_loss: 13.3768 - val_regression_loss: 9.7205 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7851 - regression_loss: 8.6548 - val_loss: 13.5022 - val_regression_loss: 9.6993 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3858 - regression_loss: 8.3393 - val_loss: 13.0537 - val_regression_loss: 9.4493 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3058 - regression_loss: 8.1898 - val_loss: 12.7672 - val_regression_loss: 9.1255 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.1697 - regression_loss: 7.9576 - val_loss: 12.3732 - val_regression_loss: 8.8288 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9382 - regression_loss: 7.8193 - val_loss: 12.2878 - val_regression_loss: 8.7385 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.1811 - regression_loss: 7.8360 - val_loss: 12.1004 - val_regression_loss: 8.5898 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8772 - regression_loss: 7.7032 - val_loss: 11.9541 - val_regression_loss: 8.4910 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9925 - regression_loss: 7.8204 - val_loss: 12.2702 - val_regression_loss: 8.6917 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9519 - regression_loss: 7.7951 - val_loss: 11.8636 - val_regression_loss: 8.4353 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6242 - regression_loss: 7.5524 - val_loss: 12.0589 - val_regression_loss: 8.5033 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6078 - regression_loss: 7.6131 - val_loss: 11.6786 - val_regression_loss: 8.2863 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4791 - regression_loss: 7.3517 - val_loss: 11.5475 - val_regression_loss: 8.1333 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3342 - regression_loss: 7.2051 - val_loss: 11.4994 - val_regression_loss: 8.1424 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0653 - regression_loss: 7.2483 - val_loss: 11.5571 - val_regression_loss: 8.1199 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3998 - regression_loss: 7.3824 - val_loss: 11.4101 - val_regression_loss: 8.0747 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1865 - regression_loss: 7.2565 - val_loss: 11.4855 - val_regression_loss: 8.0748 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0115 - regression_loss: 7.1617 - val_loss: 11.2030 - val_regression_loss: 7.8972 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0631 - regression_loss: 7.0500 - val_loss: 11.5150 - val_regression_loss: 8.0956 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5081 - regression_loss: 7.3335 - val_loss: 11.7773 - val_regression_loss: 8.4177 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4365 - regression_loss: 7.2232 - val_loss: 12.0605 - val_regression_loss: 8.5295 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2114 - regression_loss: 7.3253 - val_loss: 11.9294 - val_regression_loss: 8.5510 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.0749 - regression_loss: 10.4616\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8219 - regression_loss: 7.6135 - val_loss: 11.3383 - val_regression_loss: 7.9470 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0513 - regression_loss: 6.9633 - val_loss: 11.0656 - val_regression_loss: 7.7969 - lr: 5.0000e-05\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3656 - regression_loss: 7.2364 - val_loss: 10.7530 - val_regression_loss: 7.5179 - lr: 5.0000e-05\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1646 - regression_loss: 6.9198 - val_loss: 10.8848 - val_regression_loss: 7.6233 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9029 - regression_loss: 6.8354 - val_loss: 10.8674 - val_regression_loss: 7.6454 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8583 - regression_loss: 6.7998 - val_loss: 10.7951 - val_regression_loss: 7.5559 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9369 - regression_loss: 6.8231 - val_loss: 10.7205 - val_regression_loss: 7.5099 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8244 - regression_loss: 6.7934 - val_loss: 10.6820 - val_regression_loss: 7.4725 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6895 - regression_loss: 6.6962 - val_loss: 10.6730 - val_regression_loss: 7.4721 - lr: 5.0000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.7482 - regression_loss: 6.7085 - val_loss: 10.6816 - val_regression_loss: 7.4845 - lr: 5.0000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8712 - regression_loss: 6.6870 - val_loss: 10.6362 - val_regression_loss: 7.4451 - lr: 5.0000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7160 - regression_loss: 6.6871 - val_loss: 10.6014 - val_regression_loss: 7.4170 - lr: 5.0000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5906 - regression_loss: 6.6586 - val_loss: 10.5919 - val_regression_loss: 7.4072 - lr: 5.0000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5786 - regression_loss: 6.6361 - val_loss: 10.6054 - val_regression_loss: 7.4155 - lr: 5.0000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7088 - regression_loss: 6.6321 - val_loss: 10.6219 - val_regression_loss: 7.4480 - lr: 5.0000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6874 - regression_loss: 6.6522 - val_loss: 10.5849 - val_regression_loss: 7.3952 - lr: 5.0000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.7381 - regression_loss: 6.6099 - val_loss: 10.6043 - val_regression_loss: 7.4301 - lr: 5.0000e-05\n",
      "Epoch 62/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 8.7748 - regression_loss: 6.6974 - val_loss: 10.5481 - val_regression_loss: 7.3768 - lr: 5.0000e-05\n",
      "Epoch 63/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.3073 - regression_loss: 6.6955\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.6360 - regression_loss: 6.6220 - val_loss: 10.5287 - val_regression_loss: 7.3779 - lr: 5.0000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5796 - regression_loss: 6.5469 - val_loss: 10.5148 - val_regression_loss: 7.3597 - lr: 2.5000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6732 - regression_loss: 6.5335 - val_loss: 10.5176 - val_regression_loss: 7.3632 - lr: 2.5000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6132 - regression_loss: 6.5286 - val_loss: 10.5112 - val_regression_loss: 7.3555 - lr: 2.5000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6307 - regression_loss: 6.5265 - val_loss: 10.5103 - val_regression_loss: 7.3591 - lr: 2.5000e-05\n",
      "Epoch 68/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.1557 - regression_loss: 7.5442\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6139 - regression_loss: 6.5182 - val_loss: 10.5067 - val_regression_loss: 7.3552 - lr: 2.5000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6494 - regression_loss: 6.4996 - val_loss: 10.5039 - val_regression_loss: 7.3511 - lr: 1.2500e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6880 - regression_loss: 6.5023 - val_loss: 10.4983 - val_regression_loss: 7.3468 - lr: 1.2500e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3923 - regression_loss: 6.5013 - val_loss: 10.4942 - val_regression_loss: 7.3455 - lr: 1.2500e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5496 - regression_loss: 6.5006 - val_loss: 10.4892 - val_regression_loss: 7.3409 - lr: 1.2500e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5708 - regression_loss: 6.4938 - val_loss: 10.4853 - val_regression_loss: 7.3381 - lr: 1.2500e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5442 - regression_loss: 6.4919 - val_loss: 10.4852 - val_regression_loss: 7.3404 - lr: 1.2500e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5968 - regression_loss: 6.4883 - val_loss: 10.4800 - val_regression_loss: 7.3342 - lr: 1.2500e-05\n",
      "Epoch 76/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.5593 - regression_loss: 5.9479\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5006 - regression_loss: 6.4853 - val_loss: 10.4752 - val_regression_loss: 7.3326 - lr: 1.2500e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5772 - regression_loss: 6.4750 - val_loss: 10.4774 - val_regression_loss: 7.3342 - lr: 6.2500e-06\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4906 - regression_loss: 6.4739 - val_loss: 10.4787 - val_regression_loss: 7.3357 - lr: 6.2500e-06\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6386 - regression_loss: 6.4720 - val_loss: 10.4778 - val_regression_loss: 7.3351 - lr: 6.2500e-06\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5326 - regression_loss: 6.4720 - val_loss: 10.4770 - val_regression_loss: 7.3343 - lr: 6.2500e-06\n",
      "Epoch 81/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.8965 - regression_loss: 8.2853\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5188 - regression_loss: 6.4718 - val_loss: 10.4759 - val_regression_loss: 7.3342 - lr: 6.2500e-06\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4849 - regression_loss: 6.4685 - val_loss: 10.4746 - val_regression_loss: 7.3321 - lr: 3.1250e-06\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5375 - regression_loss: 6.4673 - val_loss: 10.4740 - val_regression_loss: 7.3316 - lr: 3.1250e-06\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.4991 - regression_loss: 6.4663 - val_loss: 10.4718 - val_regression_loss: 7.3303 - lr: 3.1250e-06\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4939 - regression_loss: 6.4626 - val_loss: 10.4713 - val_regression_loss: 7.3295 - lr: 3.1250e-06\n",
      "Epoch 86/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.1154 - regression_loss: 6.5041\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5587 - regression_loss: 6.4624 - val_loss: 10.4715 - val_regression_loss: 7.3301 - lr: 3.1250e-06\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4734 - regression_loss: 6.4609 - val_loss: 10.4715 - val_regression_loss: 7.3303 - lr: 1.5625e-06\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4446 - regression_loss: 6.4605 - val_loss: 10.4710 - val_regression_loss: 7.3296 - lr: 1.5625e-06\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5677 - regression_loss: 6.4598 - val_loss: 10.4705 - val_regression_loss: 7.3293 - lr: 1.5625e-06\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5470 - regression_loss: 6.4594 - val_loss: 10.4700 - val_regression_loss: 7.3288 - lr: 1.5625e-06\n",
      "Epoch 91/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.0919 - regression_loss: 8.4807\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4965 - regression_loss: 6.4592 - val_loss: 10.4698 - val_regression_loss: 7.3284 - lr: 1.5625e-06\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5385 - regression_loss: 6.4584 - val_loss: 10.4698 - val_regression_loss: 7.3284 - lr: 7.8125e-07\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5475 - regression_loss: 6.4578 - val_loss: 10.4699 - val_regression_loss: 7.3286 - lr: 7.8125e-07\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6070 - regression_loss: 6.4587 - val_loss: 10.4700 - val_regression_loss: 7.3287 - lr: 7.8125e-07\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5634 - regression_loss: 6.4579 - val_loss: 10.4704 - val_regression_loss: 7.3294 - lr: 7.8125e-07\n",
      "Epoch 96/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.3380 - regression_loss: 7.7268\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.4827 - regression_loss: 6.4570 - val_loss: 10.4700 - val_regression_loss: 7.3291 - lr: 7.8125e-07\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6020 - regression_loss: 6.4569 - val_loss: 10.4699 - val_regression_loss: 7.3291 - lr: 3.9062e-07\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6148 - regression_loss: 6.4570 - val_loss: 10.4697 - val_regression_loss: 7.3289 - lr: 3.9062e-07\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4194 - regression_loss: 6.4567 - val_loss: 10.4698 - val_regression_loss: 7.3291 - lr: 3.9062e-07\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6831 - regression_loss: 6.4566 - val_loss: 10.4695 - val_regression_loss: 7.3287 - lr: 3.9062e-07\n",
      "Epoch 101/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.2424 - regression_loss: 5.6312\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4897 - regression_loss: 6.4565 - val_loss: 10.4695 - val_regression_loss: 7.3287 - lr: 3.9062e-07\n",
      "Epoch 102/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4718 - regression_loss: 6.4563 - val_loss: 10.4695 - val_regression_loss: 7.3287 - lr: 1.9531e-07\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5671 - regression_loss: 6.4562 - val_loss: 10.4695 - val_regression_loss: 7.3287 - lr: 1.9531e-07\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.4990 - regression_loss: 6.4562 - val_loss: 10.4696 - val_regression_loss: 7.3289 - lr: 1.9531e-07\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5972 - regression_loss: 6.4562 - val_loss: 10.4696 - val_regression_loss: 7.3288 - lr: 1.9531e-07\n",
      "Epoch 106/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.1475 - regression_loss: 7.5363\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4927 - regression_loss: 6.4561 - val_loss: 10.4695 - val_regression_loss: 7.3288 - lr: 1.9531e-07\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5429 - regression_loss: 6.4559 - val_loss: 10.4695 - val_regression_loss: 7.3288 - lr: 9.7656e-08\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4773 - regression_loss: 6.4559 - val_loss: 10.4694 - val_regression_loss: 7.3287 - lr: 9.7656e-08\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5702 - regression_loss: 6.4559 - val_loss: 10.4694 - val_regression_loss: 7.3288 - lr: 9.7656e-08\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5549 - regression_loss: 6.4559 - val_loss: 10.4694 - val_regression_loss: 7.3288 - lr: 9.7656e-08\n",
      "Epoch 111/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.6646 - regression_loss: 8.0534\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4117 - regression_loss: 6.4559 - val_loss: 10.4694 - val_regression_loss: 7.3287 - lr: 9.7656e-08\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4874 - regression_loss: 6.4558 - val_loss: 10.4694 - val_regression_loss: 7.3287 - lr: 4.8828e-08\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4784 - regression_loss: 6.4558 - val_loss: 10.4694 - val_regression_loss: 7.3287 - lr: 4.8828e-08\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5755 - regression_loss: 6.4558 - val_loss: 10.4694 - val_regression_loss: 7.3288 - lr: 4.8828e-08\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4666 - regression_loss: 6.4557 - val_loss: 10.4694 - val_regression_loss: 7.3287 - lr: 4.8828e-08\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3318 - regression_loss: 6.4558 - val_loss: 10.4694 - val_regression_loss: 7.3287 - lr: 4.8828e-08\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5065 - regression_loss: 6.4557 - val_loss: 10.4694 - val_regression_loss: 7.3287 - lr: 4.8828e-08\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.5156 - regression_loss: 6.4557 - val_loss: 10.4694 - val_regression_loss: 7.3287 - lr: 4.8828e-08\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5392 - regression_loss: 6.4557 - val_loss: 10.4694 - val_regression_loss: 7.3287 - lr: 4.8828e-08\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4722 - regression_loss: 6.4557 - val_loss: 10.4694 - val_regression_loss: 7.3287 - lr: 4.8828e-08\n",
      "Epoch 121/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.5327 - regression_loss: 8.9215\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4855 - regression_loss: 6.4557 - val_loss: 10.4694 - val_regression_loss: 7.3287 - lr: 4.8828e-08\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4555 - regression_loss: 6.4556 - val_loss: 10.4694 - val_regression_loss: 7.3287 - lr: 2.4414e-08\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4574 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 2.4414e-08\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4974 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 2.4414e-08\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4468 - regression_loss: 6.4556 - val_loss: 10.4694 - val_regression_loss: 7.3287 - lr: 2.4414e-08\n",
      "Epoch 126/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.8109 - regression_loss: 7.1997\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4968 - regression_loss: 6.4556 - val_loss: 10.4694 - val_regression_loss: 7.3287 - lr: 2.4414e-08\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4891 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 1.2207e-08\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5636 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 1.2207e-08\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4583 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 1.2207e-08\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5521 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 1.2207e-08\n",
      "Epoch 131/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.4337 - regression_loss: 5.8225\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6149 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 1.2207e-08\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4989 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 6.1035e-09\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5929 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 6.1035e-09\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4655 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 6.1035e-09\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6344 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 6.1035e-09\n",
      "Epoch 136/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.5653 - regression_loss: 7.9540\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6486 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 6.1035e-09\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5422 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 3.0518e-09\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3544 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 3.0518e-09\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5180 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 3.0518e-09\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5449 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 3.0518e-09\n",
      "Epoch 141/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.2416 - regression_loss: 8.6304\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 1.5258788677030566e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6593 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 3.0518e-09\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5061 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 1.5259e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5930 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 1.5259e-09\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5901 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 1.5259e-09\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4427 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 1.5259e-09\n",
      "Epoch 146/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.4599 - regression_loss: 7.8487\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 7.629394338515283e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4622 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 1.5259e-09\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5638 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 7.6294e-10\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5272 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 7.6294e-10\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5347 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 7.6294e-10\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5496 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 7.6294e-10\n",
      "Epoch 151/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.7179 - regression_loss: 7.1067\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 3.8146971692576415e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4392 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 7.6294e-10\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4959 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 3.8147e-10\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4655 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 3.8147e-10\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5288 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 3.8147e-10\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6733 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 3.8147e-10\n",
      "Epoch 156/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.2447 - regression_loss: 5.6335\n",
      "Epoch 156: ReduceLROnPlateau reducing learning rate to 1.9073485846288207e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6335 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 3.8147e-10\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6063 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 1.9073e-10\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4605 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 1.9073e-10\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3966 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 1.9073e-10\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4488 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 1.9073e-10\n",
      "Epoch 161/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.3541 - regression_loss: 8.7429\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 9.536742923144104e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4761 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 1.9073e-10\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6002 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 9.5367e-11\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5213 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 9.5367e-11\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4779 - regression_loss: 6.4556 - val_loss: 10.4693 - val_regression_loss: 7.3287 - lr: 9.5367e-11\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 26ms/step - loss: 88.0299 - regression_loss: 81.2175 - val_loss: 82.2587 - val_regression_loss: 61.0472 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 68.8215 - regression_loss: 61.8884 - val_loss: 72.7450 - val_regression_loss: 53.5209 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 58.3246 - regression_loss: 51.5258 - val_loss: 65.2648 - val_regression_loss: 47.7211 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 47.0678 - regression_loss: 43.4870 - val_loss: 57.9605 - val_regression_loss: 42.8119 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 43.1472 - regression_loss: 38.7324 - val_loss: 51.7145 - val_regression_loss: 37.5326 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 38.9708 - regression_loss: 34.4242 - val_loss: 46.4631 - val_regression_loss: 33.7671 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 34.3316 - regression_loss: 30.4346 - val_loss: 43.1766 - val_regression_loss: 31.0758 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.2731 - regression_loss: 27.5974 - val_loss: 40.4323 - val_regression_loss: 29.2188 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.6880 - regression_loss: 25.1237 - val_loss: 38.8501 - val_regression_loss: 27.8782 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.6080 - regression_loss: 23.1649 - val_loss: 37.2236 - val_regression_loss: 26.6153 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.3554 - regression_loss: 21.4052 - val_loss: 36.0548 - val_regression_loss: 25.7363 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.0017 - regression_loss: 19.9807 - val_loss: 34.0697 - val_regression_loss: 24.3920 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.8437 - regression_loss: 18.7349 - val_loss: 33.0984 - val_regression_loss: 23.6131 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.3325 - regression_loss: 17.5071 - val_loss: 32.0364 - val_regression_loss: 22.8956 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.4041 - regression_loss: 16.5695 - val_loss: 31.0887 - val_regression_loss: 22.0412 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.5528 - regression_loss: 15.6956 - val_loss: 29.8020 - val_regression_loss: 21.2429 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.8169 - regression_loss: 15.1519 - val_loss: 29.6352 - val_regression_loss: 20.9031 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3181 - regression_loss: 14.5951 - val_loss: 27.8399 - val_regression_loss: 19.7609 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.6126 - regression_loss: 13.8913 - val_loss: 27.4180 - val_regression_loss: 19.3376 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.0749 - regression_loss: 13.6984 - val_loss: 26.3227 - val_regression_loss: 18.8022 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.9662 - regression_loss: 13.4905 - val_loss: 26.8481 - val_regression_loss: 18.8338 - lr: 1.0000e-04\n",
      "Epoch 22/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 15.3366 - regression_loss: 12.8771 - val_loss: 25.0890 - val_regression_loss: 17.9210 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.1896 - regression_loss: 12.8007 - val_loss: 25.5764 - val_regression_loss: 17.8734 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.4949 - regression_loss: 12.1169 - val_loss: 24.0688 - val_regression_loss: 17.0735 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.2590 - regression_loss: 11.8943 - val_loss: 23.7915 - val_regression_loss: 16.6097 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1033 - regression_loss: 11.6841 - val_loss: 22.8687 - val_regression_loss: 16.2155 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.0279 - regression_loss: 11.5971 - val_loss: 23.1332 - val_regression_loss: 16.1250 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7118 - regression_loss: 11.2511 - val_loss: 21.9656 - val_regression_loss: 15.4790 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.3324 - regression_loss: 10.9893 - val_loss: 22.2318 - val_regression_loss: 15.4457 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6344 - regression_loss: 11.1555 - val_loss: 21.5889 - val_regression_loss: 15.3258 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6914 - regression_loss: 11.2412 - val_loss: 22.1230 - val_regression_loss: 15.3278 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.5158 - regression_loss: 11.2600 - val_loss: 21.0275 - val_regression_loss: 14.8751 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1821 - regression_loss: 10.7472 - val_loss: 20.8873 - val_regression_loss: 14.5087 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1740 - regression_loss: 10.6729 - val_loss: 20.1324 - val_regression_loss: 14.0398 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5501 - regression_loss: 10.2183 - val_loss: 20.1815 - val_regression_loss: 14.0019 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3279 - regression_loss: 10.1195 - val_loss: 19.6691 - val_regression_loss: 13.7427 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9069 - regression_loss: 10.3316 - val_loss: 19.5924 - val_regression_loss: 13.6612 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3890 - regression_loss: 10.1234 - val_loss: 19.7062 - val_regression_loss: 13.6401 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3389 - regression_loss: 10.0638 - val_loss: 19.2103 - val_regression_loss: 13.3444 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1590 - regression_loss: 9.9236 - val_loss: 19.0987 - val_regression_loss: 13.3757 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3647 - regression_loss: 9.9486 - val_loss: 19.5348 - val_regression_loss: 13.4906 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4239 - regression_loss: 9.9697 - val_loss: 18.6511 - val_regression_loss: 13.0283 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9562 - regression_loss: 9.6140 - val_loss: 18.8385 - val_regression_loss: 13.0681 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9762 - regression_loss: 9.6947 - val_loss: 18.5392 - val_regression_loss: 13.0562 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2432 - regression_loss: 9.8211 - val_loss: 18.8311 - val_regression_loss: 12.9988 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4677 - regression_loss: 10.0019 - val_loss: 18.6362 - val_regression_loss: 13.2383 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3001 - regression_loss: 10.0374 - val_loss: 19.1862 - val_regression_loss: 13.2476 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 16.1340 - regression_loss: 14.5262\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5074 - regression_loss: 10.1509 - val_loss: 17.9136 - val_regression_loss: 12.6312 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8001 - regression_loss: 9.7484 - val_loss: 18.4830 - val_regression_loss: 12.7894 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6919 - regression_loss: 9.4125 - val_loss: 17.6963 - val_regression_loss: 12.3576 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8491 - regression_loss: 9.5567 - val_loss: 17.5597 - val_regression_loss: 12.1582 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6073 - regression_loss: 9.2871 - val_loss: 17.6311 - val_regression_loss: 12.1918 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7579 - regression_loss: 9.4211 - val_loss: 17.4267 - val_regression_loss: 12.1469 - lr: 5.0000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3829 - regression_loss: 9.0955 - val_loss: 17.7182 - val_regression_loss: 12.2583 - lr: 5.0000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5652 - regression_loss: 9.2343 - val_loss: 17.4438 - val_regression_loss: 12.1333 - lr: 5.0000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4352 - regression_loss: 9.1117 - val_loss: 17.3224 - val_regression_loss: 12.0351 - lr: 5.0000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4332 - regression_loss: 9.1559 - val_loss: 17.3513 - val_regression_loss: 12.0039 - lr: 5.0000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1750 - regression_loss: 9.0853 - val_loss: 17.1686 - val_regression_loss: 11.9348 - lr: 5.0000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0665 - regression_loss: 9.0695 - val_loss: 17.4755 - val_regression_loss: 12.0768 - lr: 5.0000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3444 - regression_loss: 9.1159 - val_loss: 17.2927 - val_regression_loss: 12.0190 - lr: 5.0000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2693 - regression_loss: 8.9907 - val_loss: 17.2479 - val_regression_loss: 11.9386 - lr: 5.0000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2350 - regression_loss: 9.0037 - val_loss: 17.0137 - val_regression_loss: 11.8324 - lr: 5.0000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0435 - regression_loss: 9.0222 - val_loss: 17.0454 - val_regression_loss: 11.8097 - lr: 5.0000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2456 - regression_loss: 8.9734 - val_loss: 16.9636 - val_regression_loss: 11.7658 - lr: 5.0000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1937 - regression_loss: 8.9132 - val_loss: 17.0629 - val_regression_loss: 11.8011 - lr: 5.0000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2962 - regression_loss: 8.9755 - val_loss: 17.0090 - val_regression_loss: 11.8350 - lr: 5.0000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1828 - regression_loss: 8.8845 - val_loss: 17.0310 - val_regression_loss: 11.7624 - lr: 5.0000e-05\n",
      "Epoch 68/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 [====>.........................] - ETA: 0s - loss: 11.0733 - regression_loss: 9.4664\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1579 - regression_loss: 8.8544 - val_loss: 16.8396 - val_regression_loss: 11.6908 - lr: 5.0000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9591 - regression_loss: 8.8439 - val_loss: 16.8305 - val_regression_loss: 11.6886 - lr: 2.5000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9866 - regression_loss: 8.7817 - val_loss: 16.8039 - val_regression_loss: 11.6348 - lr: 2.5000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1337 - regression_loss: 8.8121 - val_loss: 16.7741 - val_regression_loss: 11.6105 - lr: 2.5000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0664 - regression_loss: 8.7948 - val_loss: 16.8079 - val_regression_loss: 11.6335 - lr: 2.5000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0308 - regression_loss: 8.7722 - val_loss: 16.6821 - val_regression_loss: 11.5909 - lr: 2.5000e-05\n",
      "Epoch 74/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.7361 - regression_loss: 9.1293\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0932 - regression_loss: 8.7708 - val_loss: 16.7535 - val_regression_loss: 11.6163 - lr: 2.5000e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0510 - regression_loss: 8.7336 - val_loss: 16.7375 - val_regression_loss: 11.5922 - lr: 1.2500e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7385 - regression_loss: 8.7462 - val_loss: 16.7313 - val_regression_loss: 11.5795 - lr: 1.2500e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0220 - regression_loss: 8.7288 - val_loss: 16.7017 - val_regression_loss: 11.5806 - lr: 1.2500e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0085 - regression_loss: 8.7193 - val_loss: 16.6700 - val_regression_loss: 11.5581 - lr: 1.2500e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7783 - regression_loss: 8.7154 - val_loss: 16.6464 - val_regression_loss: 11.5428 - lr: 1.2500e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.8892 - regression_loss: 8.7180 - val_loss: 16.6509 - val_regression_loss: 11.5507 - lr: 1.2500e-05\n",
      "Epoch 81/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.9600 - regression_loss: 8.3533\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7873 - regression_loss: 8.7018 - val_loss: 16.6552 - val_regression_loss: 11.5393 - lr: 1.2500e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9294 - regression_loss: 8.7074 - val_loss: 16.6743 - val_regression_loss: 11.5460 - lr: 6.2500e-06\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9807 - regression_loss: 8.7025 - val_loss: 16.6415 - val_regression_loss: 11.5350 - lr: 6.2500e-06\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8646 - regression_loss: 8.6972 - val_loss: 16.6149 - val_regression_loss: 11.5257 - lr: 6.2500e-06\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9236 - regression_loss: 8.6947 - val_loss: 16.6283 - val_regression_loss: 11.5268 - lr: 6.2500e-06\n",
      "Epoch 86/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.9152 - regression_loss: 9.3086\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0341 - regression_loss: 8.6897 - val_loss: 16.6260 - val_regression_loss: 11.5240 - lr: 6.2500e-06\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8832 - regression_loss: 8.6829 - val_loss: 16.6163 - val_regression_loss: 11.5191 - lr: 3.1250e-06\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8374 - regression_loss: 8.6840 - val_loss: 16.6238 - val_regression_loss: 11.5218 - lr: 3.1250e-06\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0075 - regression_loss: 8.6795 - val_loss: 16.6134 - val_regression_loss: 11.5160 - lr: 3.1250e-06\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8084 - regression_loss: 8.6787 - val_loss: 16.6077 - val_regression_loss: 11.5109 - lr: 3.1250e-06\n",
      "Epoch 91/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.5493 - regression_loss: 8.9427\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8081 - regression_loss: 8.6793 - val_loss: 16.6049 - val_regression_loss: 11.5108 - lr: 3.1250e-06\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9175 - regression_loss: 8.6757 - val_loss: 16.6020 - val_regression_loss: 11.5084 - lr: 1.5625e-06\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0215 - regression_loss: 8.6748 - val_loss: 16.6033 - val_regression_loss: 11.5086 - lr: 1.5625e-06\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7274 - regression_loss: 8.6745 - val_loss: 16.5998 - val_regression_loss: 11.5066 - lr: 1.5625e-06\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9618 - regression_loss: 8.6739 - val_loss: 16.5989 - val_regression_loss: 11.5065 - lr: 1.5625e-06\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8374 - regression_loss: 8.6737 - val_loss: 16.5998 - val_regression_loss: 11.5073 - lr: 1.5625e-06\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8557 - regression_loss: 8.6759 - val_loss: 16.5964 - val_regression_loss: 11.5062 - lr: 1.5625e-06\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9991 - regression_loss: 8.6720 - val_loss: 16.5998 - val_regression_loss: 11.5061 - lr: 1.5625e-06\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7221 - regression_loss: 8.6718 - val_loss: 16.5988 - val_regression_loss: 11.5039 - lr: 1.5625e-06\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8618 - regression_loss: 8.6706 - val_loss: 16.6001 - val_regression_loss: 11.5042 - lr: 1.5625e-06\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8695 - regression_loss: 8.6703 - val_loss: 16.6030 - val_regression_loss: 11.5052 - lr: 1.5625e-06\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9962 - regression_loss: 8.6700 - val_loss: 16.5990 - val_regression_loss: 11.5027 - lr: 1.5625e-06\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9215 - regression_loss: 8.6710 - val_loss: 16.5923 - val_regression_loss: 11.5006 - lr: 1.5625e-06\n",
      "Epoch 104/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.6763 - regression_loss: 8.0697\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8055 - regression_loss: 8.6680 - val_loss: 16.5938 - val_regression_loss: 11.5012 - lr: 1.5625e-06\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0140 - regression_loss: 8.6673 - val_loss: 16.5944 - val_regression_loss: 11.5014 - lr: 7.8125e-07\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0246 - regression_loss: 8.6669 - val_loss: 16.5915 - val_regression_loss: 11.4993 - lr: 7.8125e-07\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9572 - regression_loss: 8.6671 - val_loss: 16.5901 - val_regression_loss: 11.4992 - lr: 7.8125e-07\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8566 - regression_loss: 8.6661 - val_loss: 16.5894 - val_regression_loss: 11.4983 - lr: 7.8125e-07\n",
      "Epoch 109/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6676 - regression_loss: 8.6662 - val_loss: 16.5867 - val_regression_loss: 11.4961 - lr: 7.8125e-07\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9434 - regression_loss: 8.6655 - val_loss: 16.5847 - val_regression_loss: 11.4952 - lr: 7.8125e-07\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0050 - regression_loss: 8.6665 - val_loss: 16.5876 - val_regression_loss: 11.4963 - lr: 7.8125e-07\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7798 - regression_loss: 8.6654 - val_loss: 16.5882 - val_regression_loss: 11.4976 - lr: 7.8125e-07\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9421 - regression_loss: 8.6644 - val_loss: 16.5865 - val_regression_loss: 11.4962 - lr: 7.8125e-07\n",
      "Epoch 114/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.7957 - regression_loss: 7.1891\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9611 - regression_loss: 8.6644 - val_loss: 16.5859 - val_regression_loss: 11.4960 - lr: 7.8125e-07\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9527 - regression_loss: 8.6636 - val_loss: 16.5843 - val_regression_loss: 11.4951 - lr: 3.9062e-07\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8722 - regression_loss: 8.6636 - val_loss: 16.5830 - val_regression_loss: 11.4943 - lr: 3.9062e-07\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8994 - regression_loss: 8.6636 - val_loss: 16.5838 - val_regression_loss: 11.4950 - lr: 3.9062e-07\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.5401 - regression_loss: 8.6635 - val_loss: 16.5845 - val_regression_loss: 11.4951 - lr: 3.9062e-07\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8527 - regression_loss: 8.6631 - val_loss: 16.5847 - val_regression_loss: 11.4949 - lr: 3.9062e-07\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8924 - regression_loss: 8.6634 - val_loss: 16.5845 - val_regression_loss: 11.4944 - lr: 3.9062e-07\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9967 - regression_loss: 8.6628 - val_loss: 16.5835 - val_regression_loss: 11.4939 - lr: 3.9062e-07\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.8856 - regression_loss: 8.6626 - val_loss: 16.5825 - val_regression_loss: 11.4935 - lr: 3.9062e-07\n",
      "Epoch 123/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.6989 - regression_loss: 14.0923\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9935 - regression_loss: 8.6624 - val_loss: 16.5821 - val_regression_loss: 11.4935 - lr: 3.9062e-07\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9156 - regression_loss: 8.6624 - val_loss: 16.5812 - val_regression_loss: 11.4931 - lr: 1.9531e-07\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9265 - regression_loss: 8.6621 - val_loss: 16.5815 - val_regression_loss: 11.4932 - lr: 1.9531e-07\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.5150 - regression_loss: 8.6622 - val_loss: 16.5824 - val_regression_loss: 11.4935 - lr: 1.9531e-07\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7888 - regression_loss: 8.6621 - val_loss: 16.5814 - val_regression_loss: 11.4931 - lr: 1.9531e-07\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8728 - regression_loss: 8.6618 - val_loss: 16.5819 - val_regression_loss: 11.4934 - lr: 1.9531e-07\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9872 - regression_loss: 8.6619 - val_loss: 16.5816 - val_regression_loss: 11.4934 - lr: 1.9531e-07\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7239 - regression_loss: 8.6617 - val_loss: 16.5815 - val_regression_loss: 11.4930 - lr: 1.9531e-07\n",
      "Epoch 131/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.3053 - regression_loss: 8.6987\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9614 - regression_loss: 8.6615 - val_loss: 16.5811 - val_regression_loss: 11.4926 - lr: 1.9531e-07\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9075 - regression_loss: 8.6614 - val_loss: 16.5812 - val_regression_loss: 11.4926 - lr: 9.7656e-08\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9085 - regression_loss: 8.6616 - val_loss: 16.5808 - val_regression_loss: 11.4925 - lr: 9.7656e-08\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9062 - regression_loss: 8.6613 - val_loss: 16.5812 - val_regression_loss: 11.4927 - lr: 9.7656e-08\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8594 - regression_loss: 8.6613 - val_loss: 16.5806 - val_regression_loss: 11.4924 - lr: 9.7656e-08\n",
      "Epoch 136/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.2073 - regression_loss: 10.6006\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9766 - regression_loss: 8.6615 - val_loss: 16.5809 - val_regression_loss: 11.4924 - lr: 9.7656e-08\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7885 - regression_loss: 8.6611 - val_loss: 16.5806 - val_regression_loss: 11.4922 - lr: 4.8828e-08\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.8341 - regression_loss: 8.6613 - val_loss: 16.5808 - val_regression_loss: 11.4923 - lr: 4.8828e-08\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8075 - regression_loss: 8.6611 - val_loss: 16.5806 - val_regression_loss: 11.4922 - lr: 4.8828e-08\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7498 - regression_loss: 8.6612 - val_loss: 16.5807 - val_regression_loss: 11.4922 - lr: 4.8828e-08\n",
      "Epoch 141/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.0654 - regression_loss: 13.4588\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9985 - regression_loss: 8.6611 - val_loss: 16.5804 - val_regression_loss: 11.4920 - lr: 4.8828e-08\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6978 - regression_loss: 8.6610 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 2.4414e-08\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8909 - regression_loss: 8.6610 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 2.4414e-08\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9551 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 2.4414e-08\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9508 - regression_loss: 8.6610 - val_loss: 16.5804 - val_regression_loss: 11.4921 - lr: 2.4414e-08\n",
      "Epoch 146/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.4019 - regression_loss: 7.7953\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.8331 - regression_loss: 8.6609 - val_loss: 16.5804 - val_regression_loss: 11.4921 - lr: 2.4414e-08\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9848 - regression_loss: 8.6609 - val_loss: 16.5804 - val_regression_loss: 11.4920 - lr: 1.2207e-08\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.9256 - regression_loss: 8.6609 - val_loss: 16.5804 - val_regression_loss: 11.4920 - lr: 1.2207e-08\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7367 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 1.2207e-08\n",
      "Epoch 150/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 10.8835 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 1.2207e-08\n",
      "Epoch 151/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.0707 - regression_loss: 13.4641\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.9311 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 1.2207e-08\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.9637 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 6.1035e-09\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.6625 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 6.1035e-09\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.8482 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 6.1035e-09\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.9614 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 6.1035e-09\n",
      "Epoch 156/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.8864 - regression_loss: 9.2798\n",
      "Epoch 156: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.8953 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 6.1035e-09\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.9769 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 3.0518e-09\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.9622 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 3.0518e-09\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.8481 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 3.0518e-09\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.7309 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 3.0518e-09\n",
      "Epoch 161/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.4938 - regression_loss: 10.8872\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 1.5258788677030566e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.6570 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 3.0518e-09\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.8911 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 1.5259e-09\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.9158 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 1.5259e-09\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.7837 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 1.5259e-09\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0232 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 1.5259e-09\n",
      "Epoch 166/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.1743 - regression_loss: 7.5677\n",
      "Epoch 166: ReduceLROnPlateau reducing learning rate to 7.629394338515283e-10.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.7441 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 1.5259e-09\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9446 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 7.6294e-10\n",
      "Epoch 168/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7306 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 7.6294e-10\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9629 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 7.6294e-10\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8929 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 7.6294e-10\n",
      "Epoch 171/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 14.0693 - regression_loss: 12.4626\n",
      "Epoch 171: ReduceLROnPlateau reducing learning rate to 3.8146971692576415e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9880 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 7.6294e-10\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8423 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 3.8147e-10\n",
      "Epoch 173/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8336 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 3.8147e-10\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7159 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 3.8147e-10\n",
      "Epoch 175/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9214 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 3.8147e-10\n",
      "Epoch 176/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.8619 - regression_loss: 10.2553\n",
      "Epoch 176: ReduceLROnPlateau reducing learning rate to 1.9073485846288207e-10.\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 11.0479 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 3.8147e-10\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.9395 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 1.9073e-10\n",
      "Epoch 178/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 10.8367 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 1.9073e-10\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.9805 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 1.9073e-10\n",
      "Epoch 180/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.6852 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 1.9073e-10\n",
      "Epoch 181/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.4282 - regression_loss: 10.8215\n",
      "Epoch 181: ReduceLROnPlateau reducing learning rate to 9.536742923144104e-11.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.0135 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 1.9073e-10\n",
      "Epoch 182/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.9529 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 9.5367e-11\n",
      "Epoch 183/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.9152 - regression_loss: 8.6609 - val_loss: 16.5803 - val_regression_loss: 11.4920 - lr: 9.5367e-11\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 26ms/step - loss: 74.5991 - regression_loss: 69.3249 - val_loss: 51.3086 - val_regression_loss: 41.2235 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 55.2430 - regression_loss: 49.7352 - val_loss: 39.4527 - val_regression_loss: 31.9130 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 44.5763 - regression_loss: 39.8823 - val_loss: 31.3332 - val_regression_loss: 25.7354 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 37.9127 - regression_loss: 33.6521 - val_loss: 27.4746 - val_regression_loss: 22.1388 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 32.1426 - regression_loss: 28.3545 - val_loss: 24.7245 - val_regression_loss: 19.7718 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.4829 - regression_loss: 24.8249 - val_loss: 22.5059 - val_regression_loss: 17.9750 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.6322 - regression_loss: 22.3624 - val_loss: 20.4329 - val_regression_loss: 16.2332 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.2155 - regression_loss: 19.7212 - val_loss: 18.5477 - val_regression_loss: 14.7615 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.6101 - regression_loss: 17.7436 - val_loss: 16.8009 - val_regression_loss: 13.3792 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.6998 - regression_loss: 16.0100 - val_loss: 15.5360 - val_regression_loss: 12.2963 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.6370 - regression_loss: 14.7632 - val_loss: 14.4797 - val_regression_loss: 11.3986 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.1670 - regression_loss: 13.6910 - val_loss: 13.4859 - val_regression_loss: 10.6114 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.6035 - regression_loss: 12.3059 - val_loss: 12.6759 - val_regression_loss: 9.9313 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.7842 - regression_loss: 11.3028 - val_loss: 12.0919 - val_regression_loss: 9.4272 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.0028 - regression_loss: 10.5326 - val_loss: 11.6505 - val_regression_loss: 9.0209 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.3180 - regression_loss: 9.2750 - val_loss: 11.1960 - val_regression_loss: 8.6437 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.9721 - regression_loss: 8.7101 - val_loss: 10.7584 - val_regression_loss: 8.2797 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.3511 - regression_loss: 8.1463 - val_loss: 10.4422 - val_regression_loss: 8.0137 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.9406 - regression_loss: 7.7632 - val_loss: 10.1242 - val_regression_loss: 7.7567 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.6486 - regression_loss: 7.4489 - val_loss: 9.9013 - val_regression_loss: 7.5524 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1473 - regression_loss: 7.1246 - val_loss: 9.7071 - val_regression_loss: 7.4431 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.1957 - regression_loss: 7.0087 - val_loss: 9.5775 - val_regression_loss: 7.2964 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.6429 - regression_loss: 6.6349 - val_loss: 9.3187 - val_regression_loss: 7.1135 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.5202 - regression_loss: 6.5086 - val_loss: 9.2246 - val_regression_loss: 7.0035 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3226 - regression_loss: 6.3628 - val_loss: 8.9747 - val_regression_loss: 6.8502 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.0131 - regression_loss: 6.1947 - val_loss: 8.9718 - val_regression_loss: 6.7920 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.0481 - regression_loss: 6.0890 - val_loss: 8.7034 - val_regression_loss: 6.6415 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8877 - regression_loss: 5.9902 - val_loss: 8.7624 - val_regression_loss: 6.5985 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.9203 - regression_loss: 5.9326 - val_loss: 8.3974 - val_regression_loss: 6.3537 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6591 - regression_loss: 5.7385 - val_loss: 8.4296 - val_regression_loss: 6.4131 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7237 - regression_loss: 5.6729 - val_loss: 8.4641 - val_regression_loss: 6.3623 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.4277 - regression_loss: 5.5443 - val_loss: 8.2165 - val_regression_loss: 6.2509 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5230 - regression_loss: 5.5436 - val_loss: 8.1487 - val_regression_loss: 6.1047 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.2527 - regression_loss: 5.3902 - val_loss: 8.0120 - val_regression_loss: 6.0371 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.2317 - regression_loss: 5.2837 - val_loss: 7.9172 - val_regression_loss: 5.9402 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.4164 - regression_loss: 5.3756 - val_loss: 7.8422 - val_regression_loss: 5.8658 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.2814 - regression_loss: 5.2908 - val_loss: 7.7854 - val_regression_loss: 5.8518 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.3236 - regression_loss: 5.2604 - val_loss: 7.7691 - val_regression_loss: 5.7917 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0758 - regression_loss: 5.0901 - val_loss: 7.6109 - val_regression_loss: 5.7333 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0489 - regression_loss: 5.0433 - val_loss: 7.8371 - val_regression_loss: 5.8000 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9726 - regression_loss: 5.0961 - val_loss: 7.7197 - val_regression_loss: 5.8570 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.9424 - regression_loss: 5.0339 - val_loss: 8.1047 - val_regression_loss: 5.9695 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.9953 - regression_loss: 5.0919 - val_loss: 7.8463 - val_regression_loss: 5.9554 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1602 - regression_loss: 5.2584 - val_loss: 7.5619 - val_regression_loss: 5.5954 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7455 - regression_loss: 4.8614 - val_loss: 7.3401 - val_regression_loss: 5.4739 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.6981 - regression_loss: 4.7642 - val_loss: 7.3593 - val_regression_loss: 5.4118 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.8073 - regression_loss: 4.8306 - val_loss: 7.2688 - val_regression_loss: 5.3556 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6405 - regression_loss: 4.7007 - val_loss: 7.0972 - val_regression_loss: 5.2604 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.4853 - regression_loss: 4.6688 - val_loss: 7.2726 - val_regression_loss: 5.3286 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5185 - regression_loss: 4.6781 - val_loss: 7.2936 - val_regression_loss: 5.4767 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.7354 - regression_loss: 4.8861 - val_loss: 7.0401 - val_regression_loss: 5.1335 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5802 - regression_loss: 4.5847 - val_loss: 6.9714 - val_regression_loss: 5.1127 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4326 - regression_loss: 4.5340 - val_loss: 6.9978 - val_regression_loss: 5.1595 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.4468 - regression_loss: 4.5899 - val_loss: 6.9339 - val_regression_loss: 5.0907 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.3782 - regression_loss: 4.4412 - val_loss: 6.8174 - val_regression_loss: 4.9958 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.3452 - regression_loss: 4.4298 - val_loss: 6.8915 - val_regression_loss: 5.0194 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.2766 - regression_loss: 4.4098 - val_loss: 6.9071 - val_regression_loss: 5.1120 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.3744 - regression_loss: 4.4166 - val_loss: 6.8879 - val_regression_loss: 4.9862 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.2529 - regression_loss: 4.4500 - val_loss: 6.7128 - val_regression_loss: 4.9361 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.3709 - regression_loss: 4.4678 - val_loss: 6.7118 - val_regression_loss: 4.9071 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.2905 - regression_loss: 4.3466 - val_loss: 6.8670 - val_regression_loss: 4.9496 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.2963 - regression_loss: 4.3796 - val_loss: 6.6238 - val_regression_loss: 4.8225 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.1341 - regression_loss: 4.2210 - val_loss: 6.7110 - val_regression_loss: 4.8354 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.0146 - regression_loss: 4.2374 - val_loss: 6.5327 - val_regression_loss: 4.7377 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.0616 - regression_loss: 4.2230 - val_loss: 6.4717 - val_regression_loss: 4.6836 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.0390 - regression_loss: 4.2094 - val_loss: 6.4896 - val_regression_loss: 4.6925 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.1169 - regression_loss: 4.2478 - val_loss: 6.5266 - val_regression_loss: 4.6931 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.0516 - regression_loss: 4.1841 - val_loss: 6.4888 - val_regression_loss: 4.6970 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.1606 - regression_loss: 3.5683\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.1626 - regression_loss: 4.2640 - val_loss: 6.4457 - val_regression_loss: 4.6715 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.0267 - regression_loss: 4.0965 - val_loss: 6.4490 - val_regression_loss: 4.6236 - lr: 5.0000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9603 - regression_loss: 4.0819 - val_loss: 6.3644 - val_regression_loss: 4.6152 - lr: 5.0000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9845 - regression_loss: 4.1219 - val_loss: 6.3619 - val_regression_loss: 4.5700 - lr: 5.0000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.9476 - regression_loss: 4.0531 - val_loss: 6.3679 - val_regression_loss: 4.5646 - lr: 5.0000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9212 - regression_loss: 4.0271 - val_loss: 6.3458 - val_regression_loss: 4.5576 - lr: 5.0000e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.8001 - regression_loss: 4.0276 - val_loss: 6.3208 - val_regression_loss: 4.5401 - lr: 5.0000e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7899 - regression_loss: 4.0146 - val_loss: 6.3379 - val_regression_loss: 4.5249 - lr: 5.0000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.9052 - regression_loss: 4.0004 - val_loss: 6.2949 - val_regression_loss: 4.5231 - lr: 5.0000e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.9404 - regression_loss: 4.0224 - val_loss: 6.2806 - val_regression_loss: 4.5186 - lr: 5.0000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8918 - regression_loss: 3.9782 - val_loss: 6.2753 - val_regression_loss: 4.4909 - lr: 5.0000e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.9858 - regression_loss: 4.0215 - val_loss: 6.3255 - val_regression_loss: 4.5371 - lr: 5.0000e-05\n",
      "Epoch 81/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.9176 - regression_loss: 5.3268\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.8403 - regression_loss: 4.0227 - val_loss: 6.2816 - val_regression_loss: 4.5100 - lr: 5.0000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.8588 - regression_loss: 3.9710 - val_loss: 6.2513 - val_regression_loss: 4.4702 - lr: 2.5000e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7911 - regression_loss: 3.9633 - val_loss: 6.2683 - val_regression_loss: 4.4761 - lr: 2.5000e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6989 - regression_loss: 3.9622 - val_loss: 6.2517 - val_regression_loss: 4.4789 - lr: 2.5000e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7319 - regression_loss: 3.9306 - val_loss: 6.2665 - val_regression_loss: 4.4677 - lr: 2.5000e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.8053 - regression_loss: 3.9273 - val_loss: 6.2410 - val_regression_loss: 4.4576 - lr: 2.5000e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7182 - regression_loss: 3.9147 - val_loss: 6.2244 - val_regression_loss: 4.4557 - lr: 2.5000e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7468 - regression_loss: 3.9161 - val_loss: 6.2393 - val_regression_loss: 4.4497 - lr: 2.5000e-05\n",
      "Epoch 89/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.5999 - regression_loss: 4.0095\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7877 - regression_loss: 3.9228 - val_loss: 6.2344 - val_regression_loss: 4.4435 - lr: 2.5000e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.8172 - regression_loss: 3.9034 - val_loss: 6.2191 - val_regression_loss: 4.4419 - lr: 1.2500e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.8061 - regression_loss: 3.9026 - val_loss: 6.2191 - val_regression_loss: 4.4448 - lr: 1.2500e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7701 - regression_loss: 3.9017 - val_loss: 6.2280 - val_regression_loss: 4.4411 - lr: 1.2500e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7459 - regression_loss: 3.8999 - val_loss: 6.2243 - val_regression_loss: 4.4376 - lr: 1.2500e-05\n",
      "Epoch 94/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.8163 - regression_loss: 4.2261\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7950 - regression_loss: 3.8927 - val_loss: 6.2176 - val_regression_loss: 4.4403 - lr: 1.2500e-05\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7823 - regression_loss: 3.8888 - val_loss: 6.2195 - val_regression_loss: 4.4390 - lr: 6.2500e-06\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7398 - regression_loss: 3.8873 - val_loss: 6.2157 - val_regression_loss: 4.4342 - lr: 6.2500e-06\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7522 - regression_loss: 3.8858 - val_loss: 6.2135 - val_regression_loss: 4.4304 - lr: 6.2500e-06\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7205 - regression_loss: 3.8855 - val_loss: 6.2100 - val_regression_loss: 4.4291 - lr: 6.2500e-06\n",
      "Epoch 99/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 [====>.........................] - ETA: 0s - loss: 6.0755 - regression_loss: 4.4853\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7789 - regression_loss: 3.8844 - val_loss: 6.2063 - val_regression_loss: 4.4281 - lr: 6.2500e-06\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7660 - regression_loss: 3.8811 - val_loss: 6.2085 - val_regression_loss: 4.4277 - lr: 3.1250e-06\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.8131 - regression_loss: 3.8829 - val_loss: 6.2087 - val_regression_loss: 4.4289 - lr: 3.1250e-06\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7760 - regression_loss: 3.8801 - val_loss: 6.2074 - val_regression_loss: 4.4268 - lr: 3.1250e-06\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7219 - regression_loss: 3.8797 - val_loss: 6.2096 - val_regression_loss: 4.4264 - lr: 3.1250e-06\n",
      "Epoch 104/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.7881 - regression_loss: 5.1980\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7238 - regression_loss: 3.8795 - val_loss: 6.2086 - val_regression_loss: 4.4253 - lr: 3.1250e-06\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7648 - regression_loss: 3.8782 - val_loss: 6.2073 - val_regression_loss: 4.4251 - lr: 1.5625e-06\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7465 - regression_loss: 3.8778 - val_loss: 6.2072 - val_regression_loss: 4.4254 - lr: 1.5625e-06\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7364 - regression_loss: 3.8782 - val_loss: 6.2075 - val_regression_loss: 4.4251 - lr: 1.5625e-06\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7037 - regression_loss: 3.8775 - val_loss: 6.2077 - val_regression_loss: 4.4252 - lr: 1.5625e-06\n",
      "Epoch 109/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.6373 - regression_loss: 3.0473\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7471 - regression_loss: 3.8768 - val_loss: 6.2075 - val_regression_loss: 4.4257 - lr: 1.5625e-06\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7431 - regression_loss: 3.8768 - val_loss: 6.2067 - val_regression_loss: 4.4257 - lr: 7.8125e-07\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6767 - regression_loss: 3.8772 - val_loss: 6.2068 - val_regression_loss: 4.4250 - lr: 7.8125e-07\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7752 - regression_loss: 3.8764 - val_loss: 6.2067 - val_regression_loss: 4.4249 - lr: 7.8125e-07\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7429 - regression_loss: 3.8763 - val_loss: 6.2058 - val_regression_loss: 4.4249 - lr: 7.8125e-07\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7422 - regression_loss: 3.8760 - val_loss: 6.2054 - val_regression_loss: 4.4243 - lr: 7.8125e-07\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7254 - regression_loss: 3.8757 - val_loss: 6.2058 - val_regression_loss: 4.4242 - lr: 7.8125e-07\n",
      "Epoch 116/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.6250 - regression_loss: 4.0349\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7180 - regression_loss: 3.8754 - val_loss: 6.2059 - val_regression_loss: 4.4243 - lr: 7.8125e-07\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7230 - regression_loss: 3.8753 - val_loss: 6.2058 - val_regression_loss: 4.4243 - lr: 3.9062e-07\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7027 - regression_loss: 3.8750 - val_loss: 6.2057 - val_regression_loss: 4.4241 - lr: 3.9062e-07\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6970 - regression_loss: 3.8754 - val_loss: 6.2055 - val_regression_loss: 4.4242 - lr: 3.9062e-07\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6667 - regression_loss: 3.8749 - val_loss: 6.2056 - val_regression_loss: 4.4240 - lr: 3.9062e-07\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6936 - regression_loss: 3.8749 - val_loss: 6.2055 - val_regression_loss: 4.4237 - lr: 3.9062e-07\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6931 - regression_loss: 3.8748 - val_loss: 6.2055 - val_regression_loss: 4.4236 - lr: 3.9062e-07\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.8027 - regression_loss: 3.8748 - val_loss: 6.2057 - val_regression_loss: 4.4238 - lr: 3.9062e-07\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7984 - regression_loss: 3.8747 - val_loss: 6.2054 - val_regression_loss: 4.4238 - lr: 3.9062e-07\n",
      "Epoch 125/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.8020 - regression_loss: 4.2119\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7809 - regression_loss: 3.8749 - val_loss: 6.2056 - val_regression_loss: 4.4236 - lr: 3.9062e-07\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7443 - regression_loss: 3.8744 - val_loss: 6.2054 - val_regression_loss: 4.4236 - lr: 1.9531e-07\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6982 - regression_loss: 3.8745 - val_loss: 6.2054 - val_regression_loss: 4.4237 - lr: 1.9531e-07\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7719 - regression_loss: 3.8744 - val_loss: 6.2053 - val_regression_loss: 4.4235 - lr: 1.9531e-07\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7437 - regression_loss: 3.8743 - val_loss: 6.2054 - val_regression_loss: 4.4235 - lr: 1.9531e-07\n",
      "Epoch 130/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.2434 - regression_loss: 4.6534\n",
      "Epoch 130: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7427 - regression_loss: 3.8744 - val_loss: 6.2053 - val_regression_loss: 4.4235 - lr: 1.9531e-07\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6453 - regression_loss: 3.8743 - val_loss: 6.2051 - val_regression_loss: 4.4234 - lr: 9.7656e-08\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7599 - regression_loss: 3.8742 - val_loss: 6.2051 - val_regression_loss: 4.4234 - lr: 9.7656e-08\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7553 - regression_loss: 3.8742 - val_loss: 6.2051 - val_regression_loss: 4.4234 - lr: 9.7656e-08\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.8224 - regression_loss: 3.8741 - val_loss: 6.2051 - val_regression_loss: 4.4234 - lr: 9.7656e-08\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7258 - regression_loss: 3.8741 - val_loss: 6.2051 - val_regression_loss: 4.4233 - lr: 9.7656e-08\n",
      "Epoch 136/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.9314 - regression_loss: 3.3414\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6808 - regression_loss: 3.8741 - val_loss: 6.2050 - val_regression_loss: 4.4233 - lr: 9.7656e-08\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7792 - regression_loss: 3.8741 - val_loss: 6.2050 - val_regression_loss: 4.4232 - lr: 4.8828e-08\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6926 - regression_loss: 3.8740 - val_loss: 6.2050 - val_regression_loss: 4.4232 - lr: 4.8828e-08\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6343 - regression_loss: 3.8741 - val_loss: 6.2050 - val_regression_loss: 4.4233 - lr: 4.8828e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7320 - regression_loss: 3.8740 - val_loss: 6.2050 - val_regression_loss: 4.4233 - lr: 4.8828e-08\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7624 - regression_loss: 3.8740 - val_loss: 6.2050 - val_regression_loss: 4.4233 - lr: 4.8828e-08\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7207 - regression_loss: 3.8740 - val_loss: 6.2050 - val_regression_loss: 4.4232 - lr: 4.8828e-08\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7604 - regression_loss: 3.8740 - val_loss: 6.2050 - val_regression_loss: 4.4232 - lr: 4.8828e-08\n",
      "Epoch 144/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.1743 - regression_loss: 4.5843\n",
      "Epoch 144: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7930 - regression_loss: 3.8740 - val_loss: 6.2050 - val_regression_loss: 4.4232 - lr: 4.8828e-08\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.8005 - regression_loss: 3.8740 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 2.4414e-08\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7409 - regression_loss: 3.8740 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 2.4414e-08\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7458 - regression_loss: 3.8740 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 2.4414e-08\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.5871 - regression_loss: 3.8740 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 2.4414e-08\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.8151 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 2.4414e-08\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7389 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 2.4414e-08\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6814 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 2.4414e-08\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7663 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 2.4414e-08\n",
      "Epoch 153/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.6308 - regression_loss: 4.0408\n",
      "Epoch 153: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6858 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 2.4414e-08\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6641 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 1.2207e-08\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7545 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 1.2207e-08\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7173 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 1.2207e-08\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7504 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 1.2207e-08\n",
      "Epoch 158/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.6889 - regression_loss: 5.0989\n",
      "Epoch 158: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7483 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 1.2207e-08\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7730 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 6.1035e-09\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6806 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 6.1035e-09\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7742 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 6.1035e-09\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7288 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 6.1035e-09\n",
      "Epoch 163/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.5832 - regression_loss: 2.9932\n",
      "Epoch 163: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7345 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 6.1035e-09\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7756 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 3.0518e-09\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6756 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 3.0518e-09\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7548 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 3.0518e-09\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6619 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 3.0518e-09\n",
      "Epoch 168/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.9988 - regression_loss: 4.4087\n",
      "Epoch 168: ReduceLROnPlateau reducing learning rate to 1.5258788677030566e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6760 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 3.0518e-09\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7466 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 1.5259e-09\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7212 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 1.5259e-09\n",
      "Epoch 171/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7450 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 1.5259e-09\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6234 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 1.5259e-09\n",
      "Epoch 173/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.5926 - regression_loss: 5.0026\n",
      "Epoch 173: ReduceLROnPlateau reducing learning rate to 7.629394338515283e-10.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7232 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 1.5259e-09\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7670 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 7.6294e-10\n",
      "Epoch 175/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7982 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 7.6294e-10\n",
      "Epoch 176/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7211 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 7.6294e-10\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7108 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 7.6294e-10\n",
      "Epoch 178/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.3809 - regression_loss: 4.7908\n",
      "Epoch 178: ReduceLROnPlateau reducing learning rate to 3.8146971692576415e-10.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6825 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 7.6294e-10\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7416 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 3.8147e-10\n",
      "Epoch 180/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7673 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 3.8147e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7462 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 3.8147e-10\n",
      "Epoch 182/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7323 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 3.8147e-10\n",
      "Epoch 183/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.1231 - regression_loss: 4.5331\n",
      "Epoch 183: ReduceLROnPlateau reducing learning rate to 1.9073485846288207e-10.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6550 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 3.8147e-10\n",
      "Epoch 184/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6748 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 1.9073e-10\n",
      "Epoch 185/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6941 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 1.9073e-10\n",
      "Epoch 186/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7633 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 1.9073e-10\n",
      "Epoch 187/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7676 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 1.9073e-10\n",
      "Epoch 188/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.3634 - regression_loss: 3.7734\n",
      "Epoch 188: ReduceLROnPlateau reducing learning rate to 9.536742923144104e-11.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7315 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 1.9073e-10\n",
      "Epoch 189/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6717 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 9.5367e-11\n",
      "Epoch 190/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6687 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 9.5367e-11\n",
      "Epoch 191/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7235 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 9.5367e-11\n",
      "Epoch 192/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6189 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 9.5367e-11\n",
      "Epoch 193/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.2501 - regression_loss: 4.6600\n",
      "Epoch 193: ReduceLROnPlateau reducing learning rate to 4.768371461572052e-11.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7606 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 9.5367e-11\n",
      "Epoch 194/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7583 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 4.7684e-11\n",
      "Epoch 195/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7174 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 4.7684e-11\n",
      "Epoch 196/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6821 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 4.7684e-11\n",
      "Epoch 197/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7296 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 4.7684e-11\n",
      "Epoch 198/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.5763 - regression_loss: 3.9863\n",
      "Epoch 198: ReduceLROnPlateau reducing learning rate to 2.384185730786026e-11.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7423 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 4.7684e-11\n",
      "Epoch 199/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7460 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 2.3842e-11\n",
      "Epoch 200/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6798 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 2.3842e-11\n",
      "Epoch 201/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7264 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 2.3842e-11\n",
      "Epoch 202/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7251 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 2.3842e-11\n",
      "Epoch 203/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.2281 - regression_loss: 4.6380\n",
      "Epoch 203: ReduceLROnPlateau reducing learning rate to 1.192092865393013e-11.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7975 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 2.3842e-11\n",
      "Epoch 204/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7749 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 1.1921e-11\n",
      "Epoch 205/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7459 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 1.1921e-11\n",
      "Epoch 206/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6625 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 1.1921e-11\n",
      "Epoch 207/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7046 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 1.1921e-11\n",
      "Epoch 208/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.5660 - regression_loss: 3.9759\n",
      "Epoch 208: ReduceLROnPlateau reducing learning rate to 5.960464326965065e-12.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7154 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 1.1921e-11\n",
      "Epoch 209/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7245 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 5.9605e-12\n",
      "Epoch 210/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6397 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 5.9605e-12\n",
      "Epoch 211/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6852 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 5.9605e-12\n",
      "Epoch 212/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6866 - regression_loss: 3.8739 - val_loss: 6.2049 - val_regression_loss: 4.4232 - lr: 5.9605e-12\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 24ms/step - loss: 114.4422 - regression_loss: 103.6922 - val_loss: 82.4677 - val_regression_loss: 59.1333 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 62.4713 - regression_loss: 57.8603 - val_loss: 51.9523 - val_regression_loss: 37.7022 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 43.5035 - regression_loss: 38.7826 - val_loss: 36.2433 - val_regression_loss: 26.8514 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 34.0544 - regression_loss: 29.9215 - val_loss: 32.2696 - val_regression_loss: 24.4803 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.8861 - regression_loss: 26.2822 - val_loss: 29.6322 - val_regression_loss: 22.7266 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.4929 - regression_loss: 24.0567 - val_loss: 27.3773 - val_regression_loss: 21.0026 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.9452 - regression_loss: 23.0077 - val_loss: 26.4915 - val_regression_loss: 20.3103 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.5487 - regression_loss: 21.6042 - val_loss: 25.4010 - val_regression_loss: 19.3682 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.8356 - regression_loss: 20.6343 - val_loss: 23.1037 - val_regression_loss: 17.4437 - lr: 1.0000e-04\n",
      "Epoch 10/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 23.2149 - regression_loss: 19.8143 - val_loss: 24.8096 - val_regression_loss: 18.7571 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.5764 - regression_loss: 19.7676 - val_loss: 22.0563 - val_regression_loss: 16.4928 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.7762 - regression_loss: 19.0527 - val_loss: 23.1889 - val_regression_loss: 17.3651 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.4640 - regression_loss: 18.5403 - val_loss: 21.2543 - val_regression_loss: 15.7890 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.1140 - regression_loss: 18.3114 - val_loss: 21.7421 - val_regression_loss: 16.1630 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.3846 - regression_loss: 17.8246 - val_loss: 21.6690 - val_regression_loss: 16.1106 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.2873 - regression_loss: 17.4940 - val_loss: 21.1218 - val_regression_loss: 15.6755 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.2009 - regression_loss: 17.2168 - val_loss: 21.2147 - val_regression_loss: 15.7453 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.5119 - regression_loss: 17.0551 - val_loss: 20.8395 - val_regression_loss: 15.4203 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.8495 - regression_loss: 16.9809 - val_loss: 21.9513 - val_regression_loss: 16.2652 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.7128 - regression_loss: 16.6812 - val_loss: 20.2600 - val_regression_loss: 14.9282 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.3759 - regression_loss: 16.5149 - val_loss: 21.7762 - val_regression_loss: 16.1258 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 18.7583 - regression_loss: 16.1165 - val_loss: 19.9718 - val_regression_loss: 14.6850 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.8861 - regression_loss: 16.0015 - val_loss: 21.1994 - val_regression_loss: 15.6537 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.0415 - regression_loss: 15.7036 - val_loss: 20.4051 - val_regression_loss: 15.0160 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.0144 - regression_loss: 15.4696 - val_loss: 20.1265 - val_regression_loss: 14.7841 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.0910 - regression_loss: 15.4507 - val_loss: 20.4567 - val_regression_loss: 15.0363 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.9866 - regression_loss: 15.3112 - val_loss: 20.0317 - val_regression_loss: 14.7129 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.8714 - regression_loss: 15.1329 - val_loss: 20.1591 - val_regression_loss: 14.8363 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6813 - regression_loss: 15.0191 - val_loss: 20.0891 - val_regression_loss: 14.7710 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.1758 - regression_loss: 14.8928 - val_loss: 19.4343 - val_regression_loss: 14.2022 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.4982 - regression_loss: 14.8709 - val_loss: 20.6511 - val_regression_loss: 15.1843 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.3368 - regression_loss: 14.6924 - val_loss: 19.5737 - val_regression_loss: 14.3196 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.0337 - regression_loss: 14.7367 - val_loss: 19.5926 - val_regression_loss: 14.3488 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.4697 - regression_loss: 14.6455 - val_loss: 20.5961 - val_regression_loss: 15.1426 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.7674 - regression_loss: 14.4778 - val_loss: 19.1726 - val_regression_loss: 14.0094 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.2390 - regression_loss: 14.4394 - val_loss: 20.7444 - val_regression_loss: 15.2688 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.0230 - regression_loss: 14.3453 - val_loss: 18.9332 - val_regression_loss: 13.8393 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.8539 - regression_loss: 14.2367 - val_loss: 22.0386 - val_regression_loss: 16.2895 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2775 - regression_loss: 14.5749 - val_loss: 18.9070 - val_regression_loss: 13.7913 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.6490 - regression_loss: 14.0622 - val_loss: 20.6692 - val_regression_loss: 15.1819 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.2168 - regression_loss: 13.8715 - val_loss: 19.0409 - val_regression_loss: 13.8901 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.3266 - regression_loss: 13.7044 - val_loss: 20.2549 - val_regression_loss: 14.8812 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.2356 - regression_loss: 13.6603 - val_loss: 19.2887 - val_regression_loss: 14.0911 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.3453 - regression_loss: 13.5913 - val_loss: 19.8159 - val_regression_loss: 14.5180 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.0586 - regression_loss: 13.8846 - val_loss: 18.8461 - val_regression_loss: 13.7536 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5740 - regression_loss: 13.9422 - val_loss: 21.0143 - val_regression_loss: 15.4395 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.6131 - regression_loss: 14.0400 - val_loss: 18.5403 - val_regression_loss: 13.4845 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.0112 - regression_loss: 13.6484 - val_loss: 20.5829 - val_regression_loss: 15.1168 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.1368 - regression_loss: 13.5668 - val_loss: 18.7446 - val_regression_loss: 13.6645 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.5942 - regression_loss: 13.3378 - val_loss: 20.3890 - val_regression_loss: 14.9645 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5411 - regression_loss: 13.1930 - val_loss: 18.7452 - val_regression_loss: 13.6509 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.6152 - regression_loss: 13.2812 - val_loss: 19.3203 - val_regression_loss: 14.0797 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.1736 - regression_loss: 12.9424 - val_loss: 19.0440 - val_regression_loss: 13.8903 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.5731 - regression_loss: 13.0338 - val_loss: 19.2597 - val_regression_loss: 14.0602 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.3226 - regression_loss: 12.9565 - val_loss: 19.3599 - val_regression_loss: 14.1149 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.1852 - regression_loss: 12.7719 - val_loss: 19.1850 - val_regression_loss: 13.9737 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.7911 - regression_loss: 12.6791 - val_loss: 19.2801 - val_regression_loss: 14.0467 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.0205 - regression_loss: 12.6734 - val_loss: 19.4572 - val_regression_loss: 14.1802 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.2469 - regression_loss: 12.6900 - val_loss: 18.7226 - val_regression_loss: 13.5918 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.1202 - regression_loss: 12.6272 - val_loss: 19.1950 - val_regression_loss: 13.9789 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.1089 - regression_loss: 12.6007 - val_loss: 18.7717 - val_regression_loss: 13.6346 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 14.3262 - regression_loss: 12.7414\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.0042 - regression_loss: 12.6700 - val_loss: 19.1034 - val_regression_loss: 13.8967 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.9378 - regression_loss: 12.4113 - val_loss: 19.0512 - val_regression_loss: 13.8381 - lr: 5.0000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.8049 - regression_loss: 12.3535 - val_loss: 19.3773 - val_regression_loss: 14.0893 - lr: 5.0000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.9418 - regression_loss: 12.4250 - val_loss: 19.0072 - val_regression_loss: 13.7974 - lr: 5.0000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.8669 - regression_loss: 12.3232 - val_loss: 18.7907 - val_regression_loss: 13.6493 - lr: 5.0000e-05\n",
      "Epoch 67/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.0249 - regression_loss: 11.4403\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.8414 - regression_loss: 12.4051 - val_loss: 19.4101 - val_regression_loss: 14.1361 - lr: 5.0000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.5652 - regression_loss: 12.2936 - val_loss: 18.9986 - val_regression_loss: 13.8070 - lr: 2.5000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.5159 - regression_loss: 12.2483 - val_loss: 19.0685 - val_regression_loss: 13.8605 - lr: 2.5000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.4942 - regression_loss: 12.2311 - val_loss: 19.3116 - val_regression_loss: 14.0547 - lr: 2.5000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.4696 - regression_loss: 12.2081 - val_loss: 19.1236 - val_regression_loss: 13.9002 - lr: 2.5000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.5691 - regression_loss: 12.2054 - val_loss: 19.0303 - val_regression_loss: 13.8152 - lr: 2.5000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.8947 - regression_loss: 12.2033 - val_loss: 19.1269 - val_regression_loss: 13.8917 - lr: 2.5000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.7711 - regression_loss: 12.1720 - val_loss: 19.1239 - val_regression_loss: 13.8915 - lr: 2.5000e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.7546 - regression_loss: 12.1703 - val_loss: 19.0888 - val_regression_loss: 13.8717 - lr: 2.5000e-05\n",
      "Epoch 76/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.7575 - regression_loss: 14.1730\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.5547 - regression_loss: 12.1736 - val_loss: 19.0921 - val_regression_loss: 13.8731 - lr: 2.5000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.5903 - regression_loss: 12.1570 - val_loss: 19.2646 - val_regression_loss: 14.0070 - lr: 1.2500e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.3704 - regression_loss: 12.1423 - val_loss: 19.1397 - val_regression_loss: 13.9060 - lr: 1.2500e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.5702 - regression_loss: 12.1307 - val_loss: 19.1674 - val_regression_loss: 13.9281 - lr: 1.2500e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.6173 - regression_loss: 12.1322 - val_loss: 19.0546 - val_regression_loss: 13.8379 - lr: 1.2500e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.0754 - regression_loss: 12.1263 - val_loss: 19.1602 - val_regression_loss: 13.9228 - lr: 1.2500e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.3475 - regression_loss: 12.1136 - val_loss: 19.1867 - val_regression_loss: 13.9414 - lr: 1.2500e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.4796 - regression_loss: 12.1129 - val_loss: 19.1143 - val_regression_loss: 13.8837 - lr: 1.2500e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.4373 - regression_loss: 12.1080 - val_loss: 19.0752 - val_regression_loss: 13.8510 - lr: 1.2500e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.6610 - regression_loss: 12.0926 - val_loss: 19.1588 - val_regression_loss: 13.9163 - lr: 1.2500e-05\n",
      "Epoch 86/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.4785 - regression_loss: 7.8942\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.3926 - regression_loss: 12.1003 - val_loss: 19.2442 - val_regression_loss: 13.9824 - lr: 1.2500e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.5408 - regression_loss: 12.0837 - val_loss: 19.1790 - val_regression_loss: 13.9324 - lr: 6.2500e-06\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 56.1421 - regression_loss: 50.1488 - val_loss: 30.5222 - val_regression_loss: 24.0927 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 43.4729 - regression_loss: 39.1032 - val_loss: 28.0667 - val_regression_loss: 21.9933 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 40.1661 - regression_loss: 35.5053 - val_loss: 26.4219 - val_regression_loss: 20.6881 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 36.8126 - regression_loss: 32.9743 - val_loss: 25.2558 - val_regression_loss: 19.7226 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 34.8131 - regression_loss: 31.2330 - val_loss: 24.5977 - val_regression_loss: 19.1210 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 33.2182 - regression_loss: 29.7972 - val_loss: 23.3359 - val_regression_loss: 18.1032 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31.9772 - regression_loss: 28.2613 - val_loss: 22.8084 - val_regression_loss: 17.6148 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.9518 - regression_loss: 26.9625 - val_loss: 22.3865 - val_regression_loss: 17.1801 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.4411 - regression_loss: 26.1058 - val_loss: 21.3782 - val_regression_loss: 16.3557 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.3692 - regression_loss: 25.0330 - val_loss: 21.2769 - val_regression_loss: 16.1025 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.5937 - regression_loss: 24.4259 - val_loss: 20.3469 - val_regression_loss: 15.3176 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.6686 - regression_loss: 23.4226 - val_loss: 20.6226 - val_regression_loss: 15.4277 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.8959 - regression_loss: 22.8263 - val_loss: 19.4892 - val_regression_loss: 14.5321 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.0287 - regression_loss: 22.3632 - val_loss: 19.6267 - val_regression_loss: 14.5142 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.7540 - regression_loss: 21.8390 - val_loss: 19.2682 - val_regression_loss: 14.2261 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.9325 - regression_loss: 21.3132 - val_loss: 20.2692 - val_regression_loss: 14.8846 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.3098 - regression_loss: 21.1302 - val_loss: 19.3181 - val_regression_loss: 14.1332 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.5458 - regression_loss: 20.6460 - val_loss: 20.2051 - val_regression_loss: 14.7164 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.5342 - regression_loss: 20.7239 - val_loss: 19.3623 - val_regression_loss: 14.0258 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.5071 - regression_loss: 20.1725 - val_loss: 19.3681 - val_regression_loss: 14.0043 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.7749 - regression_loss: 19.9427 - val_loss: 19.1420 - val_regression_loss: 13.7776 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.6079 - regression_loss: 19.7292 - val_loss: 19.1977 - val_regression_loss: 13.7928 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.3620 - regression_loss: 19.6728 - val_loss: 20.1621 - val_regression_loss: 14.5162 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.7636 - regression_loss: 19.5277 - val_loss: 19.4804 - val_regression_loss: 13.9549 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.1296 - regression_loss: 19.3801 - val_loss: 19.2040 - val_regression_loss: 13.7236 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.9677 - regression_loss: 19.2422 - val_loss: 19.9228 - val_regression_loss: 14.2744 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.2128 - regression_loss: 19.0620 - val_loss: 19.8365 - val_regression_loss: 14.2049 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.1601 - regression_loss: 19.0618 - val_loss: 19.2327 - val_regression_loss: 13.7289 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.1065 - regression_loss: 18.9802 - val_loss: 21.9029 - val_regression_loss: 15.8061 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.1132 - regression_loss: 19.3465 - val_loss: 19.6225 - val_regression_loss: 14.0256 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.3458 - regression_loss: 18.6854 - val_loss: 20.4205 - val_regression_loss: 14.6014 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.7306 - regression_loss: 18.5889 - val_loss: 19.5240 - val_regression_loss: 13.9141 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.2084 - regression_loss: 18.4907 - val_loss: 20.0947 - val_regression_loss: 14.3638 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.4798 - regression_loss: 18.4191 - val_loss: 20.6692 - val_regression_loss: 14.8064 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.1958 - regression_loss: 18.3382 - val_loss: 19.5205 - val_regression_loss: 13.8958 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.5963 - regression_loss: 18.4372 - val_loss: 20.9312 - val_regression_loss: 14.9961 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.0849 - regression_loss: 18.1129 - val_loss: 19.4282 - val_regression_loss: 13.8208 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.3290 - regression_loss: 18.2400 - val_loss: 20.8131 - val_regression_loss: 14.8960 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.1061 - regression_loss: 18.1420 - val_loss: 20.0161 - val_regression_loss: 14.2790 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.7760 - regression_loss: 18.0212 - val_loss: 20.1407 - val_regression_loss: 14.3645 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.1726 - regression_loss: 17.8767 - val_loss: 20.3584 - val_regression_loss: 14.5451 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.3724 - regression_loss: 17.8053 - val_loss: 19.8587 - val_regression_loss: 14.1461 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.6040 - regression_loss: 17.7397 - val_loss: 20.1762 - val_regression_loss: 14.4050 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.3900 - regression_loss: 17.7307 - val_loss: 20.6816 - val_regression_loss: 14.7939 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.3923 - regression_loss: 17.6032 - val_loss: 20.1024 - val_regression_loss: 14.3694 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 22.1850 - regression_loss: 20.6021\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.5605 - regression_loss: 17.6014 - val_loss: 20.5500 - val_regression_loss: 14.6948 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.9707 - regression_loss: 17.4129 - val_loss: 20.1369 - val_regression_loss: 14.3743 - lr: 5.0000e-05\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.0073 - regression_loss: 17.4395 - val_loss: 20.3663 - val_regression_loss: 14.5531 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.2695 - regression_loss: 17.3876 - val_loss: 20.7862 - val_regression_loss: 14.8934 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.9891 - regression_loss: 17.4947 - val_loss: 20.2016 - val_regression_loss: 14.4372 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.1974 - regression_loss: 17.3571 - val_loss: 20.8448 - val_regression_loss: 14.9345 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.9199 - regression_loss: 17.2622 - val_loss: 20.3283 - val_regression_loss: 14.5385 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 20.0864 - regression_loss: 17.2852 - val_loss: 20.6683 - val_regression_loss: 14.8005 - lr: 5.0000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.1201 - regression_loss: 17.1966 - val_loss: 20.6879 - val_regression_loss: 14.8196 - lr: 5.0000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.7377 - regression_loss: 17.6820 - val_loss: 20.4886 - val_regression_loss: 14.6553 - lr: 5.0000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.2134 - regression_loss: 17.2556 - val_loss: 21.1478 - val_regression_loss: 15.1780 - lr: 5.0000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.8790 - regression_loss: 17.1229 - val_loss: 20.1663 - val_regression_loss: 14.4377 - lr: 5.0000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.8492 - regression_loss: 17.1746 - val_loss: 20.5680 - val_regression_loss: 14.7428 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.8568 - regression_loss: 17.0968 - val_loss: 20.8106 - val_regression_loss: 14.9249 - lr: 5.0000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.2939 - regression_loss: 17.2683 - val_loss: 20.3345 - val_regression_loss: 14.5468 - lr: 5.0000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.9517 - regression_loss: 17.0578 - val_loss: 21.1597 - val_regression_loss: 15.2022 - lr: 5.0000e-05\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 118.5175 - regression_loss: 109.6472 - val_loss: 86.7642 - val_regression_loss: 73.6658 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 84.3016 - regression_loss: 75.8865 - val_loss: 69.0829 - val_regression_loss: 59.8075 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 68.0510 - regression_loss: 60.7910 - val_loss: 60.5257 - val_regression_loss: 52.6196 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 58.1936 - regression_loss: 52.2326 - val_loss: 52.8751 - val_regression_loss: 46.1194 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 50.4953 - regression_loss: 45.0957 - val_loss: 46.4845 - val_regression_loss: 40.6471 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 42.2182 - regression_loss: 38.7053 - val_loss: 40.9904 - val_regression_loss: 35.7770 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 37.5781 - regression_loss: 33.6968 - val_loss: 36.8998 - val_regression_loss: 31.9229 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 33.2091 - regression_loss: 29.3662 - val_loss: 32.8171 - val_regression_loss: 27.9564 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.7589 - regression_loss: 26.0764 - val_loss: 29.1165 - val_regression_loss: 24.5310 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.0096 - regression_loss: 22.2401 - val_loss: 26.4315 - val_regression_loss: 22.0460 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.8058 - regression_loss: 19.5371 - val_loss: 23.3241 - val_regression_loss: 19.0217 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.5471 - regression_loss: 17.6345 - val_loss: 20.9919 - val_regression_loss: 17.0038 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.1375 - regression_loss: 15.1180 - val_loss: 18.6209 - val_regression_loss: 14.7829 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.7552 - regression_loss: 13.4693 - val_loss: 16.8615 - val_regression_loss: 13.2130 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 14.5643 - regression_loss: 12.0334 - val_loss: 15.3071 - val_regression_loss: 11.7860 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.0839 - regression_loss: 10.7047 - val_loss: 14.1649 - val_regression_loss: 10.8179 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.7412 - regression_loss: 9.6498 - val_loss: 13.0745 - val_regression_loss: 9.7903 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.6674 - regression_loss: 8.7309 - val_loss: 12.1305 - val_regression_loss: 8.9563 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.1795 - regression_loss: 7.9995 - val_loss: 11.5473 - val_regression_loss: 8.4926 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.6125 - regression_loss: 7.3522 - val_loss: 10.9936 - val_regression_loss: 7.9233 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.0268 - regression_loss: 6.8970 - val_loss: 10.7101 - val_regression_loss: 7.7171 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3456 - regression_loss: 6.4746 - val_loss: 9.9046 - val_regression_loss: 7.0063 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7437 - regression_loss: 5.9561 - val_loss: 9.7753 - val_regression_loss: 6.8870 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.9609 - regression_loss: 5.8017 - val_loss: 9.3579 - val_regression_loss: 6.4952 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.3939 - regression_loss: 5.5293 - val_loss: 9.4177 - val_regression_loss: 6.5661 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1752 - regression_loss: 5.1535 - val_loss: 8.7317 - val_regression_loss: 5.9674 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0086 - regression_loss: 4.9862 - val_loss: 8.9356 - val_regression_loss: 6.1221 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.7313 - regression_loss: 4.7373 - val_loss: 8.2516 - val_regression_loss: 5.5705 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.2956 - regression_loss: 4.5740 - val_loss: 8.3284 - val_regression_loss: 5.6059 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.1963 - regression_loss: 4.3255 - val_loss: 7.7212 - val_regression_loss: 5.1281 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7245 - regression_loss: 4.0619 - val_loss: 7.8225 - val_regression_loss: 5.1684 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6112 - regression_loss: 3.9324 - val_loss: 7.3229 - val_regression_loss: 4.7847 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7044 - regression_loss: 3.7864 - val_loss: 7.4742 - val_regression_loss: 4.9034 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.4822 - regression_loss: 3.7304 - val_loss: 7.0923 - val_regression_loss: 4.5968 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.5837 - regression_loss: 3.6507 - val_loss: 7.0906 - val_regression_loss: 4.5635 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.1618 - regression_loss: 3.3831 - val_loss: 6.8016 - val_regression_loss: 4.3372 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.2620 - regression_loss: 3.3882 - val_loss: 6.8247 - val_regression_loss: 4.3461 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8466 - regression_loss: 3.3491 - val_loss: 7.0669 - val_regression_loss: 4.5347 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.0943 - regression_loss: 3.2044 - val_loss: 6.4773 - val_regression_loss: 4.0620 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8135 - regression_loss: 2.9578 - val_loss: 6.4608 - val_regression_loss: 4.0304 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.7093 - regression_loss: 2.8722 - val_loss: 6.3326 - val_regression_loss: 3.9449 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.5727 - regression_loss: 2.8123 - val_loss: 6.5402 - val_regression_loss: 4.1086 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.5228 - regression_loss: 2.6994 - val_loss: 6.0675 - val_regression_loss: 3.7238 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.5056 - regression_loss: 2.7618 - val_loss: 6.0297 - val_regression_loss: 3.6721 - lr: 1.0000e-04\n",
      "Epoch 45/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3454 - regression_loss: 2.5270 - val_loss: 6.1367 - val_regression_loss: 3.7697 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.2960 - regression_loss: 2.5709 - val_loss: 5.6801 - val_regression_loss: 3.4108 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0935 - regression_loss: 2.3671 - val_loss: 5.7644 - val_regression_loss: 3.4662 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.8495 - regression_loss: 2.3053 - val_loss: 5.8599 - val_regression_loss: 3.5354 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0777 - regression_loss: 2.3366 - val_loss: 5.7548 - val_regression_loss: 3.4541 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9844 - regression_loss: 2.2120 - val_loss: 5.4415 - val_regression_loss: 3.1920 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9066 - regression_loss: 2.1376 - val_loss: 6.0439 - val_regression_loss: 3.6799 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9338 - regression_loss: 2.2712 - val_loss: 5.4876 - val_regression_loss: 3.2364 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.6233 - regression_loss: 3.0521\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0890 - regression_loss: 2.2995 - val_loss: 5.4021 - val_regression_loss: 3.1371 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7082 - regression_loss: 1.9467 - val_loss: 5.3477 - val_regression_loss: 3.0965 - lr: 5.0000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5642 - regression_loss: 1.8894 - val_loss: 5.3173 - val_regression_loss: 3.0835 - lr: 5.0000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5899 - regression_loss: 1.8631 - val_loss: 5.1728 - val_regression_loss: 2.9649 - lr: 5.0000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5438 - regression_loss: 1.8489 - val_loss: 5.1574 - val_regression_loss: 2.9563 - lr: 5.0000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5314 - regression_loss: 1.8112 - val_loss: 5.1403 - val_regression_loss: 2.9374 - lr: 5.0000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4842 - regression_loss: 1.7833 - val_loss: 5.0735 - val_regression_loss: 2.8830 - lr: 5.0000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4839 - regression_loss: 1.7648 - val_loss: 5.0400 - val_regression_loss: 2.8557 - lr: 5.0000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4940 - regression_loss: 1.7661 - val_loss: 5.0252 - val_regression_loss: 2.8520 - lr: 5.0000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4502 - regression_loss: 1.7337 - val_loss: 4.9200 - val_regression_loss: 2.7564 - lr: 5.0000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3688 - regression_loss: 1.6835 - val_loss: 4.9388 - val_regression_loss: 2.7673 - lr: 5.0000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3672 - regression_loss: 1.6660 - val_loss: 4.9751 - val_regression_loss: 2.7970 - lr: 5.0000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.2238 - regression_loss: 1.6448 - val_loss: 4.9004 - val_regression_loss: 2.7311 - lr: 5.0000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3029 - regression_loss: 1.6273 - val_loss: 4.8284 - val_regression_loss: 2.6744 - lr: 5.0000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.2342 - regression_loss: 1.6142 - val_loss: 4.7605 - val_regression_loss: 2.6188 - lr: 5.0000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2725 - regression_loss: 1.5799 - val_loss: 4.8644 - val_regression_loss: 2.6940 - lr: 5.0000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.2831 - regression_loss: 1.5586 - val_loss: 4.7216 - val_regression_loss: 2.5790 - lr: 5.0000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.2041 - regression_loss: 1.5648 - val_loss: 4.6745 - val_regression_loss: 2.5547 - lr: 5.0000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.2233 - regression_loss: 1.5064 - val_loss: 4.6478 - val_regression_loss: 2.5259 - lr: 5.0000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1976 - regression_loss: 1.4905 - val_loss: 4.6576 - val_regression_loss: 2.5270 - lr: 5.0000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0962 - regression_loss: 1.4669 - val_loss: 4.6582 - val_regression_loss: 2.5215 - lr: 5.0000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1483 - regression_loss: 1.4520 - val_loss: 4.6051 - val_regression_loss: 2.4778 - lr: 5.0000e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1182 - regression_loss: 1.4363 - val_loss: 4.6250 - val_regression_loss: 2.5064 - lr: 5.0000e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0978 - regression_loss: 1.4158 - val_loss: 4.4720 - val_regression_loss: 2.3781 - lr: 5.0000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1408 - regression_loss: 1.4375 - val_loss: 4.5478 - val_regression_loss: 2.4219 - lr: 5.0000e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0274 - regression_loss: 1.3913 - val_loss: 4.5668 - val_regression_loss: 2.4448 - lr: 5.0000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9682 - regression_loss: 1.3605 - val_loss: 4.3699 - val_regression_loss: 2.2899 - lr: 5.0000e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0552 - regression_loss: 1.3534 - val_loss: 4.4898 - val_regression_loss: 2.3827 - lr: 5.0000e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9382 - regression_loss: 1.3537 - val_loss: 4.2794 - val_regression_loss: 2.2156 - lr: 5.0000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9473 - regression_loss: 1.2959 - val_loss: 4.5549 - val_regression_loss: 2.4270 - lr: 5.0000e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9938 - regression_loss: 1.3205 - val_loss: 4.3282 - val_regression_loss: 2.2432 - lr: 5.0000e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9716 - regression_loss: 1.2933 - val_loss: 4.3890 - val_regression_loss: 2.2951 - lr: 5.0000e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9193 - regression_loss: 1.2423 - val_loss: 4.2380 - val_regression_loss: 2.1729 - lr: 5.0000e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9196 - regression_loss: 1.2431 - val_loss: 4.2741 - val_regression_loss: 2.2011 - lr: 5.0000e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9079 - regression_loss: 1.2224 - val_loss: 4.2425 - val_regression_loss: 2.1621 - lr: 5.0000e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8437 - regression_loss: 1.2190 - val_loss: 4.3485 - val_regression_loss: 2.2548 - lr: 5.0000e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8931 - regression_loss: 1.2593 - val_loss: 4.1041 - val_regression_loss: 2.0747 - lr: 5.0000e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9004 - regression_loss: 1.2366 - val_loss: 4.1263 - val_regression_loss: 2.0752 - lr: 5.0000e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8676 - regression_loss: 1.1996 - val_loss: 4.3105 - val_regression_loss: 2.2224 - lr: 5.0000e-05\n",
      "Epoch 92/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7711 - regression_loss: 1.1497 - val_loss: 4.0451 - val_regression_loss: 2.0092 - lr: 5.0000e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8239 - regression_loss: 1.1483 - val_loss: 4.2744 - val_regression_loss: 2.1898 - lr: 5.0000e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7867 - regression_loss: 1.1295 - val_loss: 4.0506 - val_regression_loss: 2.0053 - lr: 5.0000e-05\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7885 - regression_loss: 1.1217 - val_loss: 4.2481 - val_regression_loss: 2.1705 - lr: 5.0000e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8048 - regression_loss: 1.1352 - val_loss: 3.9907 - val_regression_loss: 1.9695 - lr: 5.0000e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6796 - regression_loss: 1.0723 - val_loss: 4.0208 - val_regression_loss: 1.9861 - lr: 5.0000e-05\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7013 - regression_loss: 1.0629 - val_loss: 4.0624 - val_regression_loss: 2.0098 - lr: 5.0000e-05\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6809 - regression_loss: 1.0393 - val_loss: 3.9623 - val_regression_loss: 1.9311 - lr: 5.0000e-05\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6642 - regression_loss: 1.0328 - val_loss: 3.9912 - val_regression_loss: 1.9606 - lr: 5.0000e-05\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6761 - regression_loss: 1.0199 - val_loss: 3.9143 - val_regression_loss: 1.8887 - lr: 5.0000e-05\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6467 - regression_loss: 1.0095 - val_loss: 4.0081 - val_regression_loss: 1.9645 - lr: 5.0000e-05\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6317 - regression_loss: 0.9966 - val_loss: 3.8954 - val_regression_loss: 1.8735 - lr: 5.0000e-05\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6380 - regression_loss: 0.9773 - val_loss: 3.8768 - val_regression_loss: 1.8638 - lr: 5.0000e-05\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6288 - regression_loss: 0.9772 - val_loss: 3.9686 - val_regression_loss: 1.9365 - lr: 5.0000e-05\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6451 - regression_loss: 0.9897 - val_loss: 3.8035 - val_regression_loss: 1.7994 - lr: 5.0000e-05\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5780 - regression_loss: 0.9527 - val_loss: 3.8980 - val_regression_loss: 1.8759 - lr: 5.0000e-05\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5237 - regression_loss: 0.9459 - val_loss: 3.7301 - val_regression_loss: 1.7428 - lr: 5.0000e-05\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5885 - regression_loss: 0.9478 - val_loss: 3.9196 - val_regression_loss: 1.8927 - lr: 5.0000e-05\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5561 - regression_loss: 0.9315 - val_loss: 3.8125 - val_regression_loss: 1.7930 - lr: 5.0000e-05\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5452 - regression_loss: 0.9073 - val_loss: 3.7633 - val_regression_loss: 1.7628 - lr: 5.0000e-05\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5524 - regression_loss: 0.9061 - val_loss: 3.8761 - val_regression_loss: 1.8581 - lr: 5.0000e-05\n",
      "Epoch 113/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.1402 - regression_loss: 0.5740\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5592 - regression_loss: 0.9281 - val_loss: 3.6672 - val_regression_loss: 1.6840 - lr: 5.0000e-05\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5702 - regression_loss: 0.9219 - val_loss: 3.8759 - val_regression_loss: 1.8498 - lr: 2.5000e-05\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5438 - regression_loss: 0.9031 - val_loss: 3.7161 - val_regression_loss: 1.7214 - lr: 2.5000e-05\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5267 - regression_loss: 0.8810 - val_loss: 3.6927 - val_regression_loss: 1.7022 - lr: 2.5000e-05\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4359 - regression_loss: 0.8864 - val_loss: 3.7599 - val_regression_loss: 1.7555 - lr: 2.5000e-05\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4804 - regression_loss: 0.8575 - val_loss: 3.6493 - val_regression_loss: 1.6675 - lr: 2.5000e-05\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5078 - regression_loss: 0.8627 - val_loss: 3.7056 - val_regression_loss: 1.7115 - lr: 2.5000e-05\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4730 - regression_loss: 0.8491 - val_loss: 3.6593 - val_regression_loss: 1.6726 - lr: 2.5000e-05\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4823 - regression_loss: 0.8394 - val_loss: 3.7142 - val_regression_loss: 1.7170 - lr: 2.5000e-05\n",
      "Epoch 122/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.6649 - regression_loss: 1.0992\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4745 - regression_loss: 0.8400 - val_loss: 3.6803 - val_regression_loss: 1.6905 - lr: 2.5000e-05\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4450 - regression_loss: 0.8283 - val_loss: 3.6565 - val_regression_loss: 1.6702 - lr: 1.2500e-05\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4622 - regression_loss: 0.8263 - val_loss: 3.6536 - val_regression_loss: 1.6682 - lr: 1.2500e-05\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4551 - regression_loss: 0.8249 - val_loss: 3.6524 - val_regression_loss: 1.6674 - lr: 1.2500e-05\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4623 - regression_loss: 0.8222 - val_loss: 3.6596 - val_regression_loss: 1.6721 - lr: 1.2500e-05\n",
      "Epoch 127/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.5385 - regression_loss: 0.9729\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4627 - regression_loss: 0.8201 - val_loss: 3.6509 - val_regression_loss: 1.6637 - lr: 1.2500e-05\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4524 - regression_loss: 0.8174 - val_loss: 3.6422 - val_regression_loss: 1.6568 - lr: 6.2500e-06\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4345 - regression_loss: 0.8177 - val_loss: 3.6303 - val_regression_loss: 1.6481 - lr: 6.2500e-06\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4495 - regression_loss: 0.8149 - val_loss: 3.6598 - val_regression_loss: 1.6717 - lr: 6.2500e-06\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4328 - regression_loss: 0.8143 - val_loss: 3.6461 - val_regression_loss: 1.6612 - lr: 6.2500e-06\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4529 - regression_loss: 0.8165 - val_loss: 3.6207 - val_regression_loss: 1.6415 - lr: 6.2500e-06\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4529 - regression_loss: 0.8108 - val_loss: 3.6328 - val_regression_loss: 1.6510 - lr: 6.2500e-06\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4544 - regression_loss: 0.8142 - val_loss: 3.6554 - val_regression_loss: 1.6683 - lr: 6.2500e-06\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4089 - regression_loss: 0.8092 - val_loss: 3.6381 - val_regression_loss: 1.6544 - lr: 6.2500e-06\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4451 - regression_loss: 0.8081 - val_loss: 3.6259 - val_regression_loss: 1.6435 - lr: 6.2500e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4351 - regression_loss: 0.8091 - val_loss: 3.6206 - val_regression_loss: 1.6402 - lr: 6.2500e-06\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4489 - regression_loss: 0.8066 - val_loss: 3.6317 - val_regression_loss: 1.6487 - lr: 6.2500e-06\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4397 - regression_loss: 0.8051 - val_loss: 3.6276 - val_regression_loss: 1.6452 - lr: 6.2500e-06\n",
      "Epoch 140/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.0658 - regression_loss: 0.5004\n",
      "Epoch 140: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4199 - regression_loss: 0.8047 - val_loss: 3.6308 - val_regression_loss: 1.6470 - lr: 6.2500e-06\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4007 - regression_loss: 0.8026 - val_loss: 3.6224 - val_regression_loss: 1.6407 - lr: 3.1250e-06\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4307 - regression_loss: 0.8028 - val_loss: 3.6300 - val_regression_loss: 1.6464 - lr: 3.1250e-06\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3953 - regression_loss: 0.8016 - val_loss: 3.6197 - val_regression_loss: 1.6382 - lr: 3.1250e-06\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4016 - regression_loss: 0.8020 - val_loss: 3.6252 - val_regression_loss: 1.6428 - lr: 3.1250e-06\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4178 - regression_loss: 0.8003 - val_loss: 3.6225 - val_regression_loss: 1.6405 - lr: 3.1250e-06\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4302 - regression_loss: 0.7998 - val_loss: 3.6128 - val_regression_loss: 1.6330 - lr: 3.1250e-06\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4263 - regression_loss: 0.7992 - val_loss: 3.6171 - val_regression_loss: 1.6365 - lr: 3.1250e-06\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3900 - regression_loss: 0.7999 - val_loss: 3.6063 - val_regression_loss: 1.6286 - lr: 3.1250e-06\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3915 - regression_loss: 0.7979 - val_loss: 3.6081 - val_regression_loss: 1.6298 - lr: 3.1250e-06\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4451 - regression_loss: 0.7985 - val_loss: 3.6204 - val_regression_loss: 1.6390 - lr: 3.1250e-06\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4375 - regression_loss: 0.7970 - val_loss: 3.6146 - val_regression_loss: 1.6347 - lr: 3.1250e-06\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4173 - regression_loss: 0.7972 - val_loss: 3.6153 - val_regression_loss: 1.6349 - lr: 3.1250e-06\n",
      "Epoch 153/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.4983 - regression_loss: 0.9331\n",
      "Epoch 153: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4231 - regression_loss: 0.7957 - val_loss: 3.6040 - val_regression_loss: 1.6259 - lr: 3.1250e-06\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3549 - regression_loss: 0.7950 - val_loss: 3.6049 - val_regression_loss: 1.6265 - lr: 1.5625e-06\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3498 - regression_loss: 0.7951 - val_loss: 3.6077 - val_regression_loss: 1.6289 - lr: 1.5625e-06\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.4317 - regression_loss: 0.7943 - val_loss: 3.6080 - val_regression_loss: 1.6290 - lr: 1.5625e-06\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4328 - regression_loss: 0.7942 - val_loss: 3.6091 - val_regression_loss: 1.6298 - lr: 1.5625e-06\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4322 - regression_loss: 0.7955 - val_loss: 3.6035 - val_regression_loss: 1.6253 - lr: 1.5625e-06\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3443 - regression_loss: 0.7934 - val_loss: 3.6055 - val_regression_loss: 1.6270 - lr: 1.5625e-06\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4280 - regression_loss: 0.7933 - val_loss: 3.6105 - val_regression_loss: 1.6311 - lr: 1.5625e-06\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4288 - regression_loss: 0.7928 - val_loss: 3.6099 - val_regression_loss: 1.6305 - lr: 1.5625e-06\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4230 - regression_loss: 0.7926 - val_loss: 3.6086 - val_regression_loss: 1.6294 - lr: 1.5625e-06\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4320 - regression_loss: 0.7935 - val_loss: 3.6029 - val_regression_loss: 1.6246 - lr: 1.5625e-06\n",
      "Epoch 164/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.4023 - regression_loss: 0.8370\n",
      "Epoch 164: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4291 - regression_loss: 0.7928 - val_loss: 3.6109 - val_regression_loss: 1.6309 - lr: 1.5625e-06\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3591 - regression_loss: 0.7917 - val_loss: 3.6110 - val_regression_loss: 1.6309 - lr: 7.8125e-07\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3987 - regression_loss: 0.7916 - val_loss: 3.6107 - val_regression_loss: 1.6307 - lr: 7.8125e-07\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4138 - regression_loss: 0.7914 - val_loss: 3.6086 - val_regression_loss: 1.6291 - lr: 7.8125e-07\n",
      "Epoch 168/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3884 - regression_loss: 0.7914 - val_loss: 3.6065 - val_regression_loss: 1.6273 - lr: 7.8125e-07\n",
      "Epoch 169/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.3797 - regression_loss: 0.8144\n",
      "Epoch 169: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4290 - regression_loss: 0.7911 - val_loss: 3.6061 - val_regression_loss: 1.6270 - lr: 7.8125e-07\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4338 - regression_loss: 0.7910 - val_loss: 3.6045 - val_regression_loss: 1.6257 - lr: 3.9062e-07\n",
      "Epoch 171/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4330 - regression_loss: 0.7909 - val_loss: 3.6038 - val_regression_loss: 1.6252 - lr: 3.9062e-07\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4228 - regression_loss: 0.7907 - val_loss: 3.6042 - val_regression_loss: 1.6255 - lr: 3.9062e-07\n",
      "Epoch 173/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4218 - regression_loss: 0.7907 - val_loss: 3.6046 - val_regression_loss: 1.6258 - lr: 3.9062e-07\n",
      "Epoch 174/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.9534 - regression_loss: 1.3882\n",
      "Epoch 174: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.3896 - regression_loss: 0.7906 - val_loss: 3.6038 - val_regression_loss: 1.6252 - lr: 3.9062e-07\n",
      "Epoch 175/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3887 - regression_loss: 0.7905 - val_loss: 3.6044 - val_regression_loss: 1.6256 - lr: 1.9531e-07\n",
      "Epoch 176/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4235 - regression_loss: 0.7905 - val_loss: 3.6042 - val_regression_loss: 1.6255 - lr: 1.9531e-07\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4246 - regression_loss: 0.7904 - val_loss: 3.6043 - val_regression_loss: 1.6256 - lr: 1.9531e-07\n",
      "Epoch 178/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4321 - regression_loss: 0.7904 - val_loss: 3.6043 - val_regression_loss: 1.6256 - lr: 1.9531e-07\n",
      "Epoch 179/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.9362 - regression_loss: 1.3710\n",
      "Epoch 179: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4324 - regression_loss: 0.7904 - val_loss: 3.6037 - val_regression_loss: 1.6251 - lr: 1.9531e-07\n",
      "Epoch 180/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3918 - regression_loss: 0.7904 - val_loss: 3.6044 - val_regression_loss: 1.6256 - lr: 9.7656e-08\n",
      "Epoch 181/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4300 - regression_loss: 0.7903 - val_loss: 3.6042 - val_regression_loss: 1.6255 - lr: 9.7656e-08\n",
      "Epoch 182/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4193 - regression_loss: 0.7903 - val_loss: 3.6039 - val_regression_loss: 1.6252 - lr: 9.7656e-08\n",
      "Epoch 183/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4248 - regression_loss: 0.7902 - val_loss: 3.6038 - val_regression_loss: 1.6252 - lr: 9.7656e-08\n",
      "Epoch 184/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.0954 - regression_loss: 0.5302\n",
      "Epoch 184: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4153 - regression_loss: 0.7902 - val_loss: 3.6039 - val_regression_loss: 1.6253 - lr: 9.7656e-08\n",
      "Epoch 185/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3814 - regression_loss: 0.7902 - val_loss: 3.6041 - val_regression_loss: 1.6254 - lr: 4.8828e-08\n",
      "Epoch 186/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4023 - regression_loss: 0.7902 - val_loss: 3.6040 - val_regression_loss: 1.6254 - lr: 4.8828e-08\n",
      "Epoch 187/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4172 - regression_loss: 0.7902 - val_loss: 3.6041 - val_regression_loss: 1.6254 - lr: 4.8828e-08\n",
      "Epoch 188/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4286 - regression_loss: 0.7902 - val_loss: 3.6041 - val_regression_loss: 1.6254 - lr: 4.8828e-08\n",
      "Epoch 189/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.7493 - regression_loss: 1.1840\n",
      "Epoch 189: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4173 - regression_loss: 0.7902 - val_loss: 3.6038 - val_regression_loss: 1.6252 - lr: 4.8828e-08\n",
      "Epoch 190/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.3652 - regression_loss: 0.7901 - val_loss: 3.6038 - val_regression_loss: 1.6252 - lr: 2.4414e-08\n",
      "Epoch 191/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4188 - regression_loss: 0.7901 - val_loss: 3.6038 - val_regression_loss: 1.6252 - lr: 2.4414e-08\n",
      "Epoch 192/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3748 - regression_loss: 0.7901 - val_loss: 3.6038 - val_regression_loss: 1.6252 - lr: 2.4414e-08\n",
      "Epoch 193/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4352 - regression_loss: 0.7901 - val_loss: 3.6038 - val_regression_loss: 1.6252 - lr: 2.4414e-08\n",
      "Epoch 194/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.5512 - regression_loss: 0.9859\n",
      "Epoch 194: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4179 - regression_loss: 0.7901 - val_loss: 3.6038 - val_regression_loss: 1.6252 - lr: 2.4414e-08\n",
      "Epoch 195/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4223 - regression_loss: 0.7901 - val_loss: 3.6038 - val_regression_loss: 1.6252 - lr: 1.2207e-08\n",
      "Epoch 196/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4056 - regression_loss: 0.7901 - val_loss: 3.6038 - val_regression_loss: 1.6252 - lr: 1.2207e-08\n",
      "Epoch 197/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.4268 - regression_loss: 0.7901 - val_loss: 3.6038 - val_regression_loss: 1.6252 - lr: 1.2207e-08\n",
      "Epoch 198/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3534 - regression_loss: 0.7901 - val_loss: 3.6038 - val_regression_loss: 1.6251 - lr: 1.2207e-08\n",
      "Epoch 199/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.4806 - regression_loss: 0.9154\n",
      "Epoch 199: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4120 - regression_loss: 0.7901 - val_loss: 3.6038 - val_regression_loss: 1.6252 - lr: 1.2207e-08\n",
      "Epoch 200/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4248 - regression_loss: 0.7901 - val_loss: 3.6038 - val_regression_loss: 1.6252 - lr: 6.1035e-09\n",
      "Epoch 201/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4143 - regression_loss: 0.7901 - val_loss: 3.6038 - val_regression_loss: 1.6252 - lr: 6.1035e-09\n",
      "Epoch 202/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4046 - regression_loss: 0.7901 - val_loss: 3.6038 - val_regression_loss: 1.6252 - lr: 6.1035e-09\n",
      "Epoch 203/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3970 - regression_loss: 0.7901 - val_loss: 3.6038 - val_regression_loss: 1.6252 - lr: 6.1035e-09\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 86.5983 - regression_loss: 78.1331 - val_loss: 38.8121 - val_regression_loss: 34.3363 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 50.5887 - regression_loss: 45.4682 - val_loss: 26.7871 - val_regression_loss: 23.7692 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 37.1574 - regression_loss: 33.1278 - val_loss: 20.4876 - val_regression_loss: 18.3135 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30.6943 - regression_loss: 27.0857 - val_loss: 20.3662 - val_regression_loss: 17.2523 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.5942 - regression_loss: 23.4830 - val_loss: 19.1411 - val_regression_loss: 16.1487 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.6412 - regression_loss: 21.3741 - val_loss: 20.1598 - val_regression_loss: 16.4708 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.8020 - regression_loss: 20.2060 - val_loss: 20.1824 - val_regression_loss: 16.4381 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.1294 - regression_loss: 20.2485 - val_loss: 20.7657 - val_regression_loss: 16.6680 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.6518 - regression_loss: 19.5980 - val_loss: 20.8609 - val_regression_loss: 16.6348 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.2259 - regression_loss: 18.6080 - val_loss: 20.1484 - val_regression_loss: 16.2901 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.9508 - regression_loss: 18.2843 - val_loss: 19.9655 - val_regression_loss: 15.9596 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8355 - regression_loss: 17.9660 - val_loss: 19.4525 - val_regression_loss: 15.7049 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.3916 - regression_loss: 17.6168 - val_loss: 19.4346 - val_regression_loss: 15.6108 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.2703 - regression_loss: 17.5035 - val_loss: 18.9289 - val_regression_loss: 15.4581 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.0060 - regression_loss: 17.3348 - val_loss: 18.9643 - val_regression_loss: 15.2692 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4874 - regression_loss: 17.5133 - val_loss: 18.6397 - val_regression_loss: 15.2673 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.0189 - regression_loss: 17.1059 - val_loss: 18.5049 - val_regression_loss: 15.0372 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.8438 - regression_loss: 16.7696 - val_loss: 18.4801 - val_regression_loss: 14.9574 - lr: 1.0000e-04\n",
      "Epoch 19/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 19.2023 - regression_loss: 16.8038 - val_loss: 18.5555 - val_regression_loss: 15.0003 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.6315 - regression_loss: 16.7007 - val_loss: 18.5811 - val_regression_loss: 15.2135 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.6727 - regression_loss: 17.0099 - val_loss: 18.8379 - val_regression_loss: 15.1105 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.7305 - regression_loss: 17.1033 - val_loss: 18.5110 - val_regression_loss: 15.2092 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.7289 - regression_loss: 16.6197 - val_loss: 18.4377 - val_regression_loss: 14.8536 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 17.7111 - regression_loss: 16.1472\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.2074 - regression_loss: 16.2963 - val_loss: 18.3085 - val_regression_loss: 14.9060 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.8562 - regression_loss: 16.0798 - val_loss: 18.3126 - val_regression_loss: 14.8955 - lr: 5.0000e-05\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.5677 - regression_loss: 15.9383 - val_loss: 18.3681 - val_regression_loss: 14.8168 - lr: 5.0000e-05\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.5442 - regression_loss: 15.9343 - val_loss: 18.2317 - val_regression_loss: 14.8395 - lr: 5.0000e-05\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.5673 - regression_loss: 15.8229 - val_loss: 18.2292 - val_regression_loss: 14.7960 - lr: 5.0000e-05\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.4521 - regression_loss: 15.8228 - val_loss: 18.2986 - val_regression_loss: 14.8798 - lr: 5.0000e-05\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.5241 - regression_loss: 15.8192 - val_loss: 18.3153 - val_regression_loss: 14.9083 - lr: 5.0000e-05\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.3842 - regression_loss: 15.6995 - val_loss: 18.3125 - val_regression_loss: 14.7705 - lr: 5.0000e-05\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.3168 - regression_loss: 15.8100 - val_loss: 18.2400 - val_regression_loss: 14.8279 - lr: 5.0000e-05\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.1963 - regression_loss: 15.7029 - val_loss: 18.2216 - val_regression_loss: 14.8407 - lr: 5.0000e-05\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.2839 - regression_loss: 15.5898 - val_loss: 18.2389 - val_regression_loss: 14.7722 - lr: 5.0000e-05\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.0984 - regression_loss: 15.5497 - val_loss: 18.2041 - val_regression_loss: 14.7965 - lr: 5.0000e-05\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.1541 - regression_loss: 15.5572 - val_loss: 18.1490 - val_regression_loss: 14.7957 - lr: 5.0000e-05\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.2270 - regression_loss: 15.4766 - val_loss: 18.2660 - val_regression_loss: 14.7938 - lr: 5.0000e-05\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.2397 - regression_loss: 15.4711 - val_loss: 18.2844 - val_regression_loss: 14.8591 - lr: 5.0000e-05\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.0121 - regression_loss: 15.4351 - val_loss: 18.2310 - val_regression_loss: 14.7803 - lr: 5.0000e-05\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.9763 - regression_loss: 15.4273 - val_loss: 18.3088 - val_regression_loss: 14.8603 - lr: 5.0000e-05\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.0899 - regression_loss: 15.3403 - val_loss: 18.2655 - val_regression_loss: 14.8788 - lr: 5.0000e-05\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.8890 - regression_loss: 15.3156 - val_loss: 18.2815 - val_regression_loss: 14.8118 - lr: 5.0000e-05\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.0054 - regression_loss: 15.2884 - val_loss: 18.2423 - val_regression_loss: 14.7885 - lr: 5.0000e-05\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6356 - regression_loss: 15.2831 - val_loss: 18.3391 - val_regression_loss: 14.8845 - lr: 5.0000e-05\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.8977 - regression_loss: 15.2141 - val_loss: 18.2843 - val_regression_loss: 14.8392 - lr: 5.0000e-05\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.8354 - regression_loss: 15.1860 - val_loss: 18.2999 - val_regression_loss: 14.8295 - lr: 5.0000e-05\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.8027 - regression_loss: 15.1311 - val_loss: 18.4575 - val_regression_loss: 14.9994 - lr: 5.0000e-05\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.0259 - regression_loss: 15.1150 - val_loss: 18.4690 - val_regression_loss: 14.9756 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 17.7292 - regression_loss: 16.1661\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.7407 - regression_loss: 15.0716 - val_loss: 18.4425 - val_regression_loss: 14.9203 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.8322 - regression_loss: 15.0345 - val_loss: 18.4163 - val_regression_loss: 14.9337 - lr: 2.5000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6524 - regression_loss: 15.0338 - val_loss: 18.4293 - val_regression_loss: 14.9550 - lr: 2.5000e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5001 - regression_loss: 15.0277 - val_loss: 18.4778 - val_regression_loss: 14.9362 - lr: 2.5000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5940 - regression_loss: 15.0119 - val_loss: 18.4635 - val_regression_loss: 14.9725 - lr: 2.5000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5575 - regression_loss: 15.0241 - val_loss: 18.4990 - val_regression_loss: 15.0286 - lr: 2.5000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.7475 - regression_loss: 14.9576 - val_loss: 18.5156 - val_regression_loss: 15.0031 - lr: 2.5000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4558 - regression_loss: 14.9592 - val_loss: 18.5702 - val_regression_loss: 14.9963 - lr: 2.5000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5695 - regression_loss: 14.9565 - val_loss: 18.5389 - val_regression_loss: 15.0575 - lr: 2.5000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.7388 - regression_loss: 14.9517 - val_loss: 18.5467 - val_regression_loss: 15.0389 - lr: 2.5000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2812 - regression_loss: 14.8961 - val_loss: 18.5673 - val_regression_loss: 15.0395 - lr: 2.5000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6753 - regression_loss: 14.8834 - val_loss: 18.6238 - val_regression_loss: 15.0703 - lr: 2.5000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5548 - regression_loss: 14.8678 - val_loss: 18.6254 - val_regression_loss: 15.0940 - lr: 2.5000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3515 - regression_loss: 14.8548 - val_loss: 18.6585 - val_regression_loss: 15.1353 - lr: 2.5000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3114 - regression_loss: 14.8584 - val_loss: 18.5922 - val_regression_loss: 15.0716 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.4604 - regression_loss: 16.8974\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5386 - regression_loss: 14.8456 - val_loss: 18.6752 - val_regression_loss: 15.1220 - lr: 2.5000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.1831 - regression_loss: 14.7993 - val_loss: 18.6864 - val_regression_loss: 15.1056 - lr: 1.2500e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.7159 - regression_loss: 14.8028 - val_loss: 18.7112 - val_regression_loss: 15.1201 - lr: 1.2500e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2706 - regression_loss: 14.8219 - val_loss: 18.6836 - val_regression_loss: 15.1580 - lr: 1.2500e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2495 - regression_loss: 14.7959 - val_loss: 18.6834 - val_regression_loss: 15.1442 - lr: 1.2500e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.0677 - regression_loss: 14.7804 - val_loss: 18.7329 - val_regression_loss: 15.1313 - lr: 1.2500e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3624 - regression_loss: 14.7802 - val_loss: 18.7525 - val_regression_loss: 15.1398 - lr: 1.2500e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2413 - regression_loss: 14.7639 - val_loss: 18.7270 - val_regression_loss: 15.1596 - lr: 1.2500e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4848 - regression_loss: 14.7595 - val_loss: 18.7517 - val_regression_loss: 15.1987 - lr: 1.2500e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2372 - regression_loss: 14.7486 - val_loss: 18.7437 - val_regression_loss: 15.1780 - lr: 1.2500e-05\n",
      "Epoch 74/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 19.3633 - regression_loss: 17.8004\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3143 - regression_loss: 14.7347 - val_loss: 18.7702 - val_regression_loss: 15.1714 - lr: 1.2500e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3637 - regression_loss: 14.7285 - val_loss: 18.7685 - val_regression_loss: 15.1690 - lr: 6.2500e-06\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3029 - regression_loss: 14.7354 - val_loss: 18.7791 - val_regression_loss: 15.1747 - lr: 6.2500e-06\n",
      "3/3 [==============================] - 1s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 24ms/step - loss: 65.0205 - regression_loss: 58.3335 - val_loss: 38.3409 - val_regression_loss: 28.8150 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 42.1258 - regression_loss: 38.0255 - val_loss: 34.7225 - val_regression_loss: 25.9587 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 36.5245 - regression_loss: 32.4761 - val_loss: 32.2833 - val_regression_loss: 24.3830 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 33.7644 - regression_loss: 29.8332 - val_loss: 31.6001 - val_regression_loss: 23.7928 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.9071 - regression_loss: 28.1853 - val_loss: 29.8827 - val_regression_loss: 22.5984 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.8211 - regression_loss: 27.3230 - val_loss: 29.9995 - val_regression_loss: 22.5397 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.2000 - regression_loss: 26.7072 - val_loss: 30.0892 - val_regression_loss: 22.5211 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.0094 - regression_loss: 26.4998 - val_loss: 29.5570 - val_regression_loss: 22.1449 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.7847 - regression_loss: 26.1006 - val_loss: 28.9682 - val_regression_loss: 21.8588 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.1390 - regression_loss: 25.9557 - val_loss: 29.3892 - val_regression_loss: 22.1971 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.8956 - regression_loss: 25.7393 - val_loss: 28.4694 - val_regression_loss: 21.5215 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.4499 - regression_loss: 25.2502 - val_loss: 28.9976 - val_regression_loss: 21.8422 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.5682 - regression_loss: 25.2300 - val_loss: 27.9956 - val_regression_loss: 21.1175 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.4344 - regression_loss: 25.1713 - val_loss: 28.2364 - val_regression_loss: 21.2921 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.9713 - regression_loss: 24.7947 - val_loss: 28.3030 - val_regression_loss: 21.3237 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.0373 - regression_loss: 24.7248 - val_loss: 28.4048 - val_regression_loss: 21.4095 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.9756 - regression_loss: 24.5739 - val_loss: 27.1173 - val_regression_loss: 20.5517 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.1014 - regression_loss: 24.4127 - val_loss: 28.0708 - val_regression_loss: 21.1718 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.7556 - regression_loss: 24.0707 - val_loss: 26.9124 - val_regression_loss: 20.4172 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.8944 - regression_loss: 24.6398 - val_loss: 28.1879 - val_regression_loss: 21.2544 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.8963 - regression_loss: 24.4162 - val_loss: 27.4376 - val_regression_loss: 20.6679 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.9419 - regression_loss: 23.7567 - val_loss: 26.6618 - val_regression_loss: 20.1667 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.0904 - regression_loss: 23.8605 - val_loss: 27.7566 - val_regression_loss: 21.0111 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.7562 - regression_loss: 23.5153 - val_loss: 26.7067 - val_regression_loss: 20.2669 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.0515 - regression_loss: 23.3461 - val_loss: 26.6808 - val_regression_loss: 20.1755 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.6255 - regression_loss: 23.3521 - val_loss: 26.9934 - val_regression_loss: 20.4127 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.5170 - regression_loss: 23.2992 - val_loss: 26.6711 - val_regression_loss: 20.1534 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.4055 - regression_loss: 23.0695 - val_loss: 26.7966 - val_regression_loss: 20.2585 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.9325 - regression_loss: 22.9757 - val_loss: 26.2132 - val_regression_loss: 19.8642 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.3723 - regression_loss: 22.9026 - val_loss: 26.6211 - val_regression_loss: 20.1916 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.4897 - regression_loss: 22.8203 - val_loss: 26.2898 - val_regression_loss: 19.9493 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.9567 - regression_loss: 22.7211 - val_loss: 26.4282 - val_regression_loss: 19.9662 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.2348 - regression_loss: 23.0659 - val_loss: 26.0938 - val_regression_loss: 19.7093 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.8070 - regression_loss: 22.7160 - val_loss: 26.0824 - val_regression_loss: 19.9021 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.3091 - regression_loss: 22.4599 - val_loss: 26.8906 - val_regression_loss: 20.4641 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.4409 - regression_loss: 22.5137 - val_loss: 25.8458 - val_regression_loss: 19.6215 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.3293 - regression_loss: 22.4972 - val_loss: 26.4463 - val_regression_loss: 20.0270 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.5036 - regression_loss: 22.5089 - val_loss: 26.0347 - val_regression_loss: 19.7827 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.5229 - regression_loss: 22.2402 - val_loss: 26.9652 - val_regression_loss: 20.4852 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.1243 - regression_loss: 22.2228 - val_loss: 25.5142 - val_regression_loss: 19.4487 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.1199 - regression_loss: 22.3697 - val_loss: 26.8289 - val_regression_loss: 20.3596 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.5568 - regression_loss: 22.1669 - val_loss: 25.9405 - val_regression_loss: 19.7123 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.5511 - regression_loss: 22.0342 - val_loss: 26.0312 - val_regression_loss: 19.8336 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.2012 - regression_loss: 22.0703 - val_loss: 25.6603 - val_regression_loss: 19.4892 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.3361 - regression_loss: 22.0687 - val_loss: 26.4268 - val_regression_loss: 20.1205 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.6317 - regression_loss: 22.0601 - val_loss: 25.7896 - val_regression_loss: 19.6090 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 25.7989 - regression_loss: 24.2396\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.7024 - regression_loss: 21.7960 - val_loss: 26.2793 - val_regression_loss: 19.9871 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.8609 - regression_loss: 21.7207 - val_loss: 25.8126 - val_regression_loss: 19.6260 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.9504 - regression_loss: 21.7458 - val_loss: 25.6181 - val_regression_loss: 19.5125 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.7312 - regression_loss: 21.6505 - val_loss: 26.2266 - val_regression_loss: 19.9508 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.1634 - regression_loss: 21.6794 - val_loss: 25.7029 - val_regression_loss: 19.5691 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.4123 - regression_loss: 21.5662 - val_loss: 25.9422 - val_regression_loss: 19.7123 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.4899 - regression_loss: 21.5809 - val_loss: 26.0829 - val_regression_loss: 19.8597 - lr: 5.0000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.4722 - regression_loss: 21.4557 - val_loss: 25.5485 - val_regression_loss: 19.4977 - lr: 5.0000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.7739 - regression_loss: 21.5533 - val_loss: 25.6048 - val_regression_loss: 19.5419 - lr: 5.0000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.7565 - regression_loss: 21.4327 - val_loss: 25.9882 - val_regression_loss: 19.7876 - lr: 5.0000e-05\n",
      "Epoch 57/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 24.1419 - regression_loss: 22.5828\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.6865 - regression_loss: 21.4438 - val_loss: 25.9223 - val_regression_loss: 19.7396 - lr: 5.0000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.2090 - regression_loss: 21.3914 - val_loss: 25.7930 - val_regression_loss: 19.6595 - lr: 2.5000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.3735 - regression_loss: 21.3674 - val_loss: 25.7159 - val_regression_loss: 19.6028 - lr: 2.5000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.3464 - regression_loss: 21.3691 - val_loss: 25.7244 - val_regression_loss: 19.6239 - lr: 2.5000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.0929 - regression_loss: 21.3578 - val_loss: 25.7167 - val_regression_loss: 19.6135 - lr: 2.5000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.6125 - regression_loss: 21.3415 - val_loss: 25.9073 - val_regression_loss: 19.7389 - lr: 2.5000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.8537 - regression_loss: 21.3868 - val_loss: 25.6698 - val_regression_loss: 19.5860 - lr: 2.5000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.4494 - regression_loss: 21.3048 - val_loss: 25.7186 - val_regression_loss: 19.6169 - lr: 2.5000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.8568 - regression_loss: 21.2993 - val_loss: 25.8622 - val_regression_loss: 19.7016 - lr: 2.5000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.1276 - regression_loss: 21.3110 - val_loss: 25.8967 - val_regression_loss: 19.7202 - lr: 2.5000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.6775 - regression_loss: 21.2730 - val_loss: 25.6436 - val_regression_loss: 19.5646 - lr: 2.5000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.6799 - regression_loss: 21.2802 - val_loss: 25.5429 - val_regression_loss: 19.5136 - lr: 2.5000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.5943 - regression_loss: 21.3581 - val_loss: 25.9962 - val_regression_loss: 19.8566 - lr: 2.5000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.5461 - regression_loss: 21.2908 - val_loss: 25.6153 - val_regression_loss: 19.5544 - lr: 2.5000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.9955 - regression_loss: 21.2242 - val_loss: 25.6794 - val_regression_loss: 19.5994 - lr: 2.5000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.1114 - regression_loss: 21.2344 - val_loss: 25.8722 - val_regression_loss: 19.7360 - lr: 2.5000e-05\n",
      "Epoch 73/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 30.5089 - regression_loss: 28.9499\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.3447 - regression_loss: 21.1894 - val_loss: 25.6894 - val_regression_loss: 19.6007 - lr: 2.5000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.3062 - regression_loss: 21.1686 - val_loss: 25.6564 - val_regression_loss: 19.5797 - lr: 1.2500e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.3087 - regression_loss: 21.1919 - val_loss: 25.5807 - val_regression_loss: 19.5244 - lr: 1.2500e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.3712 - regression_loss: 21.1681 - val_loss: 25.7539 - val_regression_loss: 19.6579 - lr: 1.2500e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.3953 - regression_loss: 21.1666 - val_loss: 25.7842 - val_regression_loss: 19.6851 - lr: 1.2500e-05\n",
      "Epoch 78/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 30.6033 - regression_loss: 29.0443\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.3710 - regression_loss: 21.1644 - val_loss: 25.7192 - val_regression_loss: 19.6403 - lr: 1.2500e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.3636 - regression_loss: 21.1322 - val_loss: 25.7341 - val_regression_loss: 19.6506 - lr: 6.2500e-06\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.2977 - regression_loss: 21.1325 - val_loss: 25.7158 - val_regression_loss: 19.6370 - lr: 6.2500e-06\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 24ms/step - loss: 96.4539 - regression_loss: 87.0137 - val_loss: 73.5918 - val_regression_loss: 65.7845 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 65.8872 - regression_loss: 59.2977 - val_loss: 58.2741 - val_regression_loss: 53.5327 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 52.4594 - regression_loss: 48.0027 - val_loss: 49.7481 - val_regression_loss: 45.7072 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 46.2636 - regression_loss: 41.1357 - val_loss: 42.2816 - val_regression_loss: 38.5193 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 41.2301 - regression_loss: 36.1486 - val_loss: 37.5850 - val_regression_loss: 34.0773 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 35.9400 - regression_loss: 31.4455 - val_loss: 33.2196 - val_regression_loss: 29.8822 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30.8211 - regression_loss: 28.0596 - val_loss: 29.2747 - val_regression_loss: 26.1507 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.4792 - regression_loss: 24.9353 - val_loss: 27.9543 - val_regression_loss: 24.9721 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.8810 - regression_loss: 22.4388 - val_loss: 24.3307 - val_regression_loss: 21.7640 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.0674 - regression_loss: 20.2906 - val_loss: 22.3509 - val_regression_loss: 19.9976 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9976 - regression_loss: 18.3258 - val_loss: 20.5032 - val_regression_loss: 18.2359 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.1856 - regression_loss: 16.9166 - val_loss: 19.0976 - val_regression_loss: 16.9048 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.4757 - regression_loss: 15.4900 - val_loss: 18.1321 - val_regression_loss: 15.9600 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9952 - regression_loss: 14.2301 - val_loss: 16.5131 - val_regression_loss: 14.3881 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.9660 - regression_loss: 13.2960 - val_loss: 15.7791 - val_regression_loss: 13.6967 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.6018 - regression_loss: 12.0432 - val_loss: 14.5034 - val_regression_loss: 12.4934 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.5481 - regression_loss: 11.2897 - val_loss: 13.7312 - val_regression_loss: 11.7584 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7791 - regression_loss: 10.7188 - val_loss: 13.3117 - val_regression_loss: 11.3266 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.5612 - regression_loss: 9.9299 - val_loss: 12.1751 - val_regression_loss: 10.2968 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5891 - regression_loss: 9.2698 - val_loss: 11.7723 - val_regression_loss: 9.9144 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.1338 - regression_loss: 8.8178 - val_loss: 11.0337 - val_regression_loss: 9.2224 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5249 - regression_loss: 8.4828 - val_loss: 10.9141 - val_regression_loss: 9.0929 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0214 - regression_loss: 7.8212 - val_loss: 9.9754 - val_regression_loss: 8.1505 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4953 - regression_loss: 7.3171 - val_loss: 9.6556 - val_regression_loss: 7.8727 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9949 - regression_loss: 6.8560 - val_loss: 9.2682 - val_regression_loss: 7.4945 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6975 - regression_loss: 6.5903 - val_loss: 8.8496 - val_regression_loss: 7.0709 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2134 - regression_loss: 6.2534 - val_loss: 8.7423 - val_regression_loss: 7.0026 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8444 - regression_loss: 6.0456 - val_loss: 8.3458 - val_regression_loss: 6.5518 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8801 - regression_loss: 5.8795 - val_loss: 8.1536 - val_regression_loss: 6.4144 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6629 - regression_loss: 5.5287 - val_loss: 7.9463 - val_regression_loss: 6.1887 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.2000 - regression_loss: 5.3818 - val_loss: 7.7288 - val_regression_loss: 5.9803 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.1751 - regression_loss: 5.2155 - val_loss: 7.5351 - val_regression_loss: 5.8032 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8470 - regression_loss: 5.0379 - val_loss: 7.4254 - val_regression_loss: 5.6027 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8101 - regression_loss: 4.9346 - val_loss: 7.4619 - val_regression_loss: 5.7427 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.8541 - regression_loss: 3.3003\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.7870 - regression_loss: 4.8853 - val_loss: 7.0850 - val_regression_loss: 5.1642 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.7347 - regression_loss: 4.7793 - val_loss: 7.0357 - val_regression_loss: 5.3068 - lr: 5.0000e-05\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5448 - regression_loss: 4.6175 - val_loss: 6.9703 - val_regression_loss: 5.2036 - lr: 5.0000e-05\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3756 - regression_loss: 4.4399 - val_loss: 6.9039 - val_regression_loss: 5.0858 - lr: 5.0000e-05\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3477 - regression_loss: 4.3957 - val_loss: 6.8932 - val_regression_loss: 5.1412 - lr: 5.0000e-05\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2323 - regression_loss: 4.3526 - val_loss: 6.6818 - val_regression_loss: 4.8788 - lr: 5.0000e-05\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.2059 - regression_loss: 4.2822 - val_loss: 6.7506 - val_regression_loss: 4.9841 - lr: 5.0000e-05\n",
      "Epoch 42/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 6.1231 - regression_loss: 4.2091 - val_loss: 6.6766 - val_regression_loss: 4.8913 - lr: 5.0000e-05\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.0247 - regression_loss: 4.1549 - val_loss: 6.6236 - val_regression_loss: 4.8412 - lr: 5.0000e-05\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.9251 - regression_loss: 4.0849 - val_loss: 6.5726 - val_regression_loss: 4.8051 - lr: 5.0000e-05\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9216 - regression_loss: 4.0106 - val_loss: 6.4535 - val_regression_loss: 4.6422 - lr: 5.0000e-05\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9436 - regression_loss: 4.0213 - val_loss: 6.4387 - val_regression_loss: 4.6460 - lr: 5.0000e-05\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.8096 - regression_loss: 3.9113 - val_loss: 6.4665 - val_regression_loss: 4.7025 - lr: 5.0000e-05\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7655 - regression_loss: 3.8811 - val_loss: 6.3049 - val_regression_loss: 4.4649 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6974 - regression_loss: 3.8250 - val_loss: 6.3670 - val_regression_loss: 4.6015 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.7091 - regression_loss: 3.7643 - val_loss: 6.2418 - val_regression_loss: 4.4261 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5929 - regression_loss: 3.7082 - val_loss: 6.2253 - val_regression_loss: 4.4207 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.4731 - regression_loss: 3.6594 - val_loss: 6.1480 - val_regression_loss: 4.3511 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.4499 - regression_loss: 3.6018 - val_loss: 6.1932 - val_regression_loss: 4.4034 - lr: 5.0000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.3848 - regression_loss: 3.5479 - val_loss: 6.0635 - val_regression_loss: 4.2488 - lr: 5.0000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.3443 - regression_loss: 3.5016 - val_loss: 6.0875 - val_regression_loss: 4.3011 - lr: 5.0000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.4340 - regression_loss: 3.5333 - val_loss: 6.0497 - val_regression_loss: 4.1840 - lr: 5.0000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2948 - regression_loss: 3.4538 - val_loss: 5.9566 - val_regression_loss: 4.1562 - lr: 5.0000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.2035 - regression_loss: 3.3642 - val_loss: 5.8905 - val_regression_loss: 4.0944 - lr: 5.0000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.1229 - regression_loss: 3.3045 - val_loss: 5.8610 - val_regression_loss: 4.0290 - lr: 5.0000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6076 - regression_loss: 3.2704 - val_loss: 5.8994 - val_regression_loss: 4.1149 - lr: 5.0000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0619 - regression_loss: 3.2757 - val_loss: 5.7934 - val_regression_loss: 3.9821 - lr: 5.0000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9934 - regression_loss: 3.1963 - val_loss: 5.7367 - val_regression_loss: 3.9246 - lr: 5.0000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9413 - regression_loss: 3.1524 - val_loss: 5.7534 - val_regression_loss: 3.9524 - lr: 5.0000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.9261 - regression_loss: 3.1250 - val_loss: 5.7567 - val_regression_loss: 3.9813 - lr: 5.0000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.4033 - regression_loss: 3.0716 - val_loss: 5.7675 - val_regression_loss: 3.9563 - lr: 5.0000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8011 - regression_loss: 3.0606 - val_loss: 5.6446 - val_regression_loss: 3.8320 - lr: 5.0000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.7337 - regression_loss: 3.0092 - val_loss: 5.6634 - val_regression_loss: 3.8881 - lr: 5.0000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8393 - regression_loss: 3.0360 - val_loss: 5.5835 - val_regression_loss: 3.7650 - lr: 5.0000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.7039 - regression_loss: 2.9721 - val_loss: 5.6326 - val_regression_loss: 3.8473 - lr: 5.0000e-05\n",
      "Epoch 70/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.9696 - regression_loss: 2.4184\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6687 - regression_loss: 2.9014 - val_loss: 5.5480 - val_regression_loss: 3.7234 - lr: 5.0000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.5994 - regression_loss: 2.8193 - val_loss: 5.5288 - val_regression_loss: 3.7583 - lr: 2.5000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6178 - regression_loss: 2.8413 - val_loss: 5.5229 - val_regression_loss: 3.7367 - lr: 2.5000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.5378 - regression_loss: 2.7886 - val_loss: 5.4947 - val_regression_loss: 3.6815 - lr: 2.5000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.2364 - regression_loss: 2.7948 - val_loss: 5.4779 - val_regression_loss: 3.6892 - lr: 2.5000e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.5603 - regression_loss: 2.7562 - val_loss: 5.4499 - val_regression_loss: 3.6722 - lr: 2.5000e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.5253 - regression_loss: 2.7500 - val_loss: 5.4397 - val_regression_loss: 3.6408 - lr: 2.5000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.4293 - regression_loss: 2.7269 - val_loss: 5.4280 - val_regression_loss: 3.6351 - lr: 2.5000e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.5166 - regression_loss: 2.7160 - val_loss: 5.3886 - val_regression_loss: 3.5904 - lr: 2.5000e-05\n",
      "Epoch 79/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.2798 - regression_loss: 2.7290\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.4843 - regression_loss: 2.7073 - val_loss: 5.3697 - val_regression_loss: 3.5763 - lr: 2.5000e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3884 - regression_loss: 2.6825 - val_loss: 5.3989 - val_regression_loss: 3.6264 - lr: 1.2500e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.4224 - regression_loss: 2.6667 - val_loss: 5.3808 - val_regression_loss: 3.5916 - lr: 1.2500e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.4241 - regression_loss: 2.6564 - val_loss: 5.3752 - val_regression_loss: 3.5769 - lr: 1.2500e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.4161 - regression_loss: 2.6524 - val_loss: 5.3725 - val_regression_loss: 3.5763 - lr: 1.2500e-05\n",
      "Epoch 84/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.4358 - regression_loss: 4.8851\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.4349 - regression_loss: 2.6414 - val_loss: 5.3676 - val_regression_loss: 3.5883 - lr: 1.2500e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3629 - regression_loss: 2.6342 - val_loss: 5.3597 - val_regression_loss: 3.5800 - lr: 6.2500e-06\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3974 - regression_loss: 2.6289 - val_loss: 5.3505 - val_regression_loss: 3.5624 - lr: 6.2500e-06\n",
      "Epoch 87/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 4.4329 - regression_loss: 2.6257 - val_loss: 5.3548 - val_regression_loss: 3.5711 - lr: 6.2500e-06\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3531 - regression_loss: 2.6210 - val_loss: 5.3468 - val_regression_loss: 3.5553 - lr: 6.2500e-06\n",
      "Epoch 89/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.9755 - regression_loss: 2.4249\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3900 - regression_loss: 2.6178 - val_loss: 5.3417 - val_regression_loss: 3.5551 - lr: 6.2500e-06\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3445 - regression_loss: 2.6111 - val_loss: 5.3403 - val_regression_loss: 3.5525 - lr: 3.1250e-06\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3858 - regression_loss: 2.6085 - val_loss: 5.3387 - val_regression_loss: 3.5521 - lr: 3.1250e-06\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3347 - regression_loss: 2.6066 - val_loss: 5.3391 - val_regression_loss: 3.5518 - lr: 3.1250e-06\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3614 - regression_loss: 2.6074 - val_loss: 5.3404 - val_regression_loss: 3.5573 - lr: 3.1250e-06\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0246 - regression_loss: 2.6025 - val_loss: 5.3426 - val_regression_loss: 3.5598 - lr: 3.1250e-06\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3313 - regression_loss: 2.5998 - val_loss: 5.3390 - val_regression_loss: 3.5536 - lr: 3.1250e-06\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3298 - regression_loss: 2.5992 - val_loss: 5.3315 - val_regression_loss: 3.5431 - lr: 3.1250e-06\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3902 - regression_loss: 2.5963 - val_loss: 5.3328 - val_regression_loss: 3.5457 - lr: 3.1250e-06\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3992 - regression_loss: 2.5956 - val_loss: 5.3355 - val_regression_loss: 3.5477 - lr: 3.1250e-06\n",
      "Epoch 99/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.2847 - regression_loss: 2.7341\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3378 - regression_loss: 2.5934 - val_loss: 5.3301 - val_regression_loss: 3.5423 - lr: 3.1250e-06\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0368 - regression_loss: 2.5915 - val_loss: 5.3323 - val_regression_loss: 3.5482 - lr: 1.5625e-06\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3564 - regression_loss: 2.5886 - val_loss: 5.3303 - val_regression_loss: 3.5458 - lr: 1.5625e-06\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3302 - regression_loss: 2.5879 - val_loss: 5.3271 - val_regression_loss: 3.5429 - lr: 1.5625e-06\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3503 - regression_loss: 2.5864 - val_loss: 5.3274 - val_regression_loss: 3.5434 - lr: 1.5625e-06\n",
      "Epoch 104/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.1306 - regression_loss: 2.5800\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3328 - regression_loss: 2.5854 - val_loss: 5.3262 - val_regression_loss: 3.5424 - lr: 1.5625e-06\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3433 - regression_loss: 2.5846 - val_loss: 5.3253 - val_regression_loss: 3.5406 - lr: 7.8125e-07\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3226 - regression_loss: 2.5838 - val_loss: 5.3242 - val_regression_loss: 3.5397 - lr: 7.8125e-07\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3338 - regression_loss: 2.5833 - val_loss: 5.3250 - val_regression_loss: 3.5410 - lr: 7.8125e-07\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3184 - regression_loss: 2.5827 - val_loss: 5.3233 - val_regression_loss: 3.5392 - lr: 7.8125e-07\n",
      "Epoch 109/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.8792 - regression_loss: 2.3287\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3526 - regression_loss: 2.5822 - val_loss: 5.3238 - val_regression_loss: 3.5403 - lr: 7.8125e-07\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 4.3547 - regression_loss: 2.5816 - val_loss: 5.3239 - val_regression_loss: 3.5404 - lr: 3.9062e-07\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 4.0541 - regression_loss: 2.5814 - val_loss: 5.3232 - val_regression_loss: 3.5394 - lr: 3.9062e-07\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3130 - regression_loss: 2.5813 - val_loss: 5.3226 - val_regression_loss: 3.5386 - lr: 3.9062e-07\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3237 - regression_loss: 2.5809 - val_loss: 5.3224 - val_regression_loss: 3.5388 - lr: 3.9062e-07\n",
      "Epoch 114/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.3423 - regression_loss: 4.7918\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.2841 - regression_loss: 2.5805 - val_loss: 5.3218 - val_regression_loss: 3.5379 - lr: 3.9062e-07\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.3260 - regression_loss: 2.5802 - val_loss: 5.3216 - val_regression_loss: 3.5379 - lr: 1.9531e-07\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3422 - regression_loss: 2.5800 - val_loss: 5.3217 - val_regression_loss: 3.5379 - lr: 1.9531e-07\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3091 - regression_loss: 2.5800 - val_loss: 5.3214 - val_regression_loss: 3.5374 - lr: 1.9531e-07\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3596 - regression_loss: 2.5797 - val_loss: 5.3213 - val_regression_loss: 3.5375 - lr: 1.9531e-07\n",
      "Epoch 119/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.0120 - regression_loss: 2.4615\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.2495 - regression_loss: 2.5796 - val_loss: 5.3216 - val_regression_loss: 3.5378 - lr: 1.9531e-07\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3517 - regression_loss: 2.5795 - val_loss: 5.3216 - val_regression_loss: 3.5379 - lr: 9.7656e-08\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3066 - regression_loss: 2.5794 - val_loss: 5.3214 - val_regression_loss: 3.5376 - lr: 9.7656e-08\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0938 - regression_loss: 2.5793 - val_loss: 5.3213 - val_regression_loss: 3.5376 - lr: 9.7656e-08\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0853 - regression_loss: 2.5793 - val_loss: 5.3213 - val_regression_loss: 3.5377 - lr: 9.7656e-08\n",
      "Epoch 124/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.0672 - regression_loss: 2.5167\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.2958 - regression_loss: 2.5793 - val_loss: 5.3210 - val_regression_loss: 3.5373 - lr: 9.7656e-08\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3226 - regression_loss: 2.5791 - val_loss: 5.3210 - val_regression_loss: 3.5372 - lr: 4.8828e-08\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2811 - regression_loss: 2.5791 - val_loss: 5.3209 - val_regression_loss: 3.5371 - lr: 4.8828e-08\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3604 - regression_loss: 2.5790 - val_loss: 5.3209 - val_regression_loss: 3.5371 - lr: 4.8828e-08\n",
      "Epoch 128/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3223 - regression_loss: 2.5790 - val_loss: 5.3209 - val_regression_loss: 3.5371 - lr: 4.8828e-08\n",
      "Epoch 129/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.5581 - regression_loss: 2.0076\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0292 - regression_loss: 2.5790 - val_loss: 5.3210 - val_regression_loss: 3.5373 - lr: 4.8828e-08\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3446 - regression_loss: 2.5789 - val_loss: 5.3209 - val_regression_loss: 3.5372 - lr: 2.4414e-08\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3279 - regression_loss: 2.5789 - val_loss: 5.3209 - val_regression_loss: 3.5372 - lr: 2.4414e-08\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.0280 - regression_loss: 2.5789 - val_loss: 5.3209 - val_regression_loss: 3.5372 - lr: 2.4414e-08\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3219 - regression_loss: 2.5789 - val_loss: 5.3209 - val_regression_loss: 3.5372 - lr: 2.4414e-08\n",
      "Epoch 134/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.4160 - regression_loss: 2.8655\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3299 - regression_loss: 2.5789 - val_loss: 5.3209 - val_regression_loss: 3.5371 - lr: 2.4414e-08\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.2666 - regression_loss: 2.5788 - val_loss: 5.3209 - val_regression_loss: 3.5371 - lr: 1.2207e-08\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3224 - regression_loss: 2.5788 - val_loss: 5.3209 - val_regression_loss: 3.5371 - lr: 1.2207e-08\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.2605 - regression_loss: 2.5788 - val_loss: 5.3209 - val_regression_loss: 3.5371 - lr: 1.2207e-08\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3257 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 1.2207e-08\n",
      "Epoch 139/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.3715 - regression_loss: 4.8210\n",
      "Epoch 139: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3392 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 1.2207e-08\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3274 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 6.1035e-09\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3421 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 6.1035e-09\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3037 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 6.1035e-09\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.2804 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 6.1035e-09\n",
      "Epoch 144/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.2212 - regression_loss: 2.6707\n",
      "Epoch 144: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-09.\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.3462 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 6.1035e-09\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3314 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 3.0518e-09\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0143 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 3.0518e-09\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.2758 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 3.0518e-09\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3382 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 3.0518e-09\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3420 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 3.0518e-09\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3270 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 3.0518e-09\n",
      "Epoch 151/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.6076 - regression_loss: 2.0571\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 1.5258788677030566e-09.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3229 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 3.0518e-09\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3222 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 1.5259e-09\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3537 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 1.5259e-09\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3104 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 1.5259e-09\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3700 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 1.5259e-09\n",
      "Epoch 156/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.3164 - regression_loss: 2.7659\n",
      "Epoch 156: ReduceLROnPlateau reducing learning rate to 7.629394338515283e-10.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3234 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 1.5259e-09\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.2549 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 7.6294e-10\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3339 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 7.6294e-10\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 4.3431 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 7.6294e-10\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0582 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 7.6294e-10\n",
      "Epoch 161/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.4291 - regression_loss: 1.8786\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 3.8146971692576415e-10.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3618 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 7.6294e-10\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3369 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 3.8147e-10\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3223 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 3.8147e-10\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3321 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 3.8147e-10\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3167 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 3.8147e-10\n",
      "Epoch 166/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.7925 - regression_loss: 2.2420\n",
      "Epoch 166: ReduceLROnPlateau reducing learning rate to 1.9073485846288207e-10.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.2945 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 3.8147e-10\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3153 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 1.9073e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.2520 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 1.9073e-10\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3345 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 1.9073e-10\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3345 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 1.9073e-10\n",
      "Epoch 171/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.3125 - regression_loss: 2.7620\n",
      "Epoch 171: ReduceLROnPlateau reducing learning rate to 9.536742923144104e-11.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.2935 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 1.9073e-10\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3109 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 9.5367e-11\n",
      "Epoch 173/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3531 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 9.5367e-11\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3649 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 9.5367e-11\n",
      "Epoch 175/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.2711 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 9.5367e-11\n",
      "Epoch 176/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.9666 - regression_loss: 4.4161\n",
      "Epoch 176: ReduceLROnPlateau reducing learning rate to 4.768371461572052e-11.\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.3417 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 9.5367e-11\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3317 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 4.7684e-11\n",
      "Epoch 178/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3245 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 4.7684e-11\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.3512 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 4.7684e-11\n",
      "Epoch 180/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.3451 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 4.7684e-11\n",
      "Epoch 181/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.6104 - regression_loss: 2.0599\n",
      "Epoch 181: ReduceLROnPlateau reducing learning rate to 2.384185730786026e-11.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3507 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 4.7684e-11\n",
      "Epoch 182/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3500 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 2.3842e-11\n",
      "Epoch 183/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3509 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 2.3842e-11\n",
      "Epoch 184/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3397 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 2.3842e-11\n",
      "Epoch 185/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.2859 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 2.3842e-11\n",
      "Epoch 186/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.2670 - regression_loss: 2.7165\n",
      "Epoch 186: ReduceLROnPlateau reducing learning rate to 1.192092865393013e-11.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3386 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 2.3842e-11\n",
      "Epoch 187/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3409 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 1.1921e-11\n",
      "Epoch 188/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0718 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 1.1921e-11\n",
      "Epoch 189/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3220 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 1.1921e-11\n",
      "Epoch 190/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3470 - regression_loss: 2.5788 - val_loss: 5.3208 - val_regression_loss: 3.5371 - lr: 1.1921e-11\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 27ms/step - loss: 106.1555 - regression_loss: 95.9379 - val_loss: 47.8304 - val_regression_loss: 38.0382 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 57.8944 - regression_loss: 52.7920 - val_loss: 37.0328 - val_regression_loss: 29.3593 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 44.5281 - regression_loss: 40.0640 - val_loss: 31.4724 - val_regression_loss: 24.7331 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 39.7283 - regression_loss: 35.3381 - val_loss: 30.1218 - val_regression_loss: 23.4090 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 35.2422 - regression_loss: 32.1293 - val_loss: 30.0891 - val_regression_loss: 23.2484 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 35.6537 - regression_loss: 31.5513 - val_loss: 28.9161 - val_regression_loss: 22.1134 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 34.6352 - regression_loss: 30.9475 - val_loss: 28.3859 - val_regression_loss: 21.6468 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 33.7397 - regression_loss: 30.0319 - val_loss: 28.0222 - val_regression_loss: 21.3477 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 32.6620 - regression_loss: 29.3953 - val_loss: 27.8529 - val_regression_loss: 21.1961 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 33.5187 - regression_loss: 29.7262 - val_loss: 27.8368 - val_regression_loss: 21.1039 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.3295 - regression_loss: 28.9842 - val_loss: 28.4004 - val_regression_loss: 21.4604 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 32.0484 - regression_loss: 28.5058 - val_loss: 27.8306 - val_regression_loss: 20.9242 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 32.2544 - regression_loss: 28.6549 - val_loss: 27.9869 - val_regression_loss: 21.0392 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 32.0455 - regression_loss: 28.6239 - val_loss: 28.5255 - val_regression_loss: 21.4123 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31.5498 - regression_loss: 28.1027 - val_loss: 27.7688 - val_regression_loss: 20.8404 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31.0873 - regression_loss: 27.9554 - val_loss: 28.6413 - val_regression_loss: 21.4752 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30.7675 - regression_loss: 27.6060 - val_loss: 27.6085 - val_regression_loss: 20.6825 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31.2329 - regression_loss: 27.5024 - val_loss: 27.4864 - val_regression_loss: 20.5818 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30.0846 - regression_loss: 27.3473 - val_loss: 27.8453 - val_regression_loss: 20.8181 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.5102 - regression_loss: 27.1262 - val_loss: 27.8705 - val_regression_loss: 20.8013 - lr: 1.0000e-04\n",
      "Epoch 21/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 30.8526 - regression_loss: 27.1672 - val_loss: 27.8648 - val_regression_loss: 20.7644 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30.7287 - regression_loss: 27.0110 - val_loss: 27.6455 - val_regression_loss: 20.6223 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30.0057 - regression_loss: 26.9318 - val_loss: 27.8257 - val_regression_loss: 20.7279 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.2211 - regression_loss: 26.7427 - val_loss: 27.6550 - val_regression_loss: 20.6018 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30.0150 - regression_loss: 26.6891 - val_loss: 27.7871 - val_regression_loss: 20.6761 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 29.4610 - regression_loss: 26.5751 - val_loss: 27.9417 - val_regression_loss: 20.7897 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30.1871 - regression_loss: 26.5260 - val_loss: 28.0825 - val_regression_loss: 20.8762 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.8199 - regression_loss: 26.5808 - val_loss: 28.2182 - val_regression_loss: 20.9501 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 36.7517 - regression_loss: 35.2027\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30.0963 - regression_loss: 26.7503 - val_loss: 28.1804 - val_regression_loss: 20.8866 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.3326 - regression_loss: 26.4358 - val_loss: 28.6112 - val_regression_loss: 21.1887 - lr: 5.0000e-05\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.9210 - regression_loss: 26.3172 - val_loss: 27.9489 - val_regression_loss: 20.7186 - lr: 5.0000e-05\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.6282 - regression_loss: 26.4537 - val_loss: 28.0939 - val_regression_loss: 20.8199 - lr: 5.0000e-05\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.6429 - regression_loss: 26.3502 - val_loss: 28.5725 - val_regression_loss: 21.1689 - lr: 5.0000e-05\n",
      "Epoch 34/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 31.9751 - regression_loss: 30.4266\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.7285 - regression_loss: 26.1749 - val_loss: 28.0039 - val_regression_loss: 20.7686 - lr: 5.0000e-05\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.6188 - regression_loss: 26.1882 - val_loss: 28.1227 - val_regression_loss: 20.8492 - lr: 2.5000e-05\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.4226 - regression_loss: 26.0793 - val_loss: 28.2389 - val_regression_loss: 20.9250 - lr: 2.5000e-05\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.8401 - regression_loss: 26.0774 - val_loss: 28.3497 - val_regression_loss: 21.0057 - lr: 2.5000e-05\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.4052 - regression_loss: 26.0598 - val_loss: 28.2690 - val_regression_loss: 20.9493 - lr: 2.5000e-05\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.3050 - regression_loss: 26.0203 - val_loss: 28.1749 - val_regression_loss: 20.8822 - lr: 2.5000e-05\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.5883 - regression_loss: 26.0501 - val_loss: 28.2496 - val_regression_loss: 20.9287 - lr: 2.5000e-05\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.1665 - regression_loss: 26.0298 - val_loss: 28.1733 - val_regression_loss: 20.8752 - lr: 2.5000e-05\n",
      "Epoch 42/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 24.7523 - regression_loss: 23.2039\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 29.7584 - regression_loss: 25.9884 - val_loss: 28.2413 - val_regression_loss: 20.9208 - lr: 2.5000e-05\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.9070 - regression_loss: 25.9659 - val_loss: 28.3348 - val_regression_loss: 20.9824 - lr: 1.2500e-05\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 29.3969 - regression_loss: 25.9511 - val_loss: 28.2795 - val_regression_loss: 20.9450 - lr: 1.2500e-05\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 28.7325 - regression_loss: 25.9374 - val_loss: 28.2813 - val_regression_loss: 20.9429 - lr: 1.2500e-05\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.2207 - regression_loss: 25.9396 - val_loss: 28.2122 - val_regression_loss: 20.8975 - lr: 1.2500e-05\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.8002 - regression_loss: 25.9226 - val_loss: 28.2253 - val_regression_loss: 20.9037 - lr: 1.2500e-05\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.2461 - regression_loss: 25.9221 - val_loss: 28.2787 - val_regression_loss: 20.9388 - lr: 1.2500e-05\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.2351 - regression_loss: 25.9162 - val_loss: 28.3311 - val_regression_loss: 20.9744 - lr: 1.2500e-05\n",
      "Epoch 50/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 35.2242 - regression_loss: 33.6760\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.3211 - regression_loss: 25.9034 - val_loss: 28.3517 - val_regression_loss: 20.9857 - lr: 1.2500e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.0454 - regression_loss: 25.8945 - val_loss: 28.3221 - val_regression_loss: 20.9639 - lr: 6.2500e-06\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.3663 - regression_loss: 25.8854 - val_loss: 28.3318 - val_regression_loss: 20.9695 - lr: 6.2500e-06\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.2208 - regression_loss: 25.8882 - val_loss: 28.3149 - val_regression_loss: 20.9581 - lr: 6.2500e-06\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.3619 - regression_loss: 25.8861 - val_loss: 28.3400 - val_regression_loss: 20.9749 - lr: 6.2500e-06\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.6163 - regression_loss: 25.8762 - val_loss: 28.3342 - val_regression_loss: 20.9708 - lr: 6.2500e-06\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.9009 - regression_loss: 25.8733 - val_loss: 28.3475 - val_regression_loss: 20.9795 - lr: 6.2500e-06\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.2418 - regression_loss: 25.8652 - val_loss: 28.3148 - val_regression_loss: 20.9584 - lr: 6.2500e-06\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.1800 - regression_loss: 25.8677 - val_loss: 28.2974 - val_regression_loss: 20.9476 - lr: 6.2500e-06\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 28ms/step - loss: 57.4472 - regression_loss: 52.2222 - val_loss: 43.8962 - val_regression_loss: 33.0764 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 50.9222 - regression_loss: 46.2681 - val_loss: 42.2493 - val_regression_loss: 32.0658 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 47.2585 - regression_loss: 43.1653 - val_loss: 39.7358 - val_regression_loss: 29.8203 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 43.6354 - regression_loss: 40.0301 - val_loss: 37.7628 - val_regression_loss: 28.3588 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 41.4720 - regression_loss: 37.5823 - val_loss: 36.2545 - val_regression_loss: 27.0991 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 39.6573 - regression_loss: 35.5416 - val_loss: 34.2351 - val_regression_loss: 25.7968 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 38.1205 - regression_loss: 34.1835 - val_loss: 33.9086 - val_regression_loss: 25.2569 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 35.6579 - regression_loss: 32.0542 - val_loss: 30.9895 - val_regression_loss: 23.2919 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 33.7928 - regression_loss: 30.5654 - val_loss: 29.1620 - val_regression_loss: 22.0534 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 32.9215 - regression_loss: 28.9767 - val_loss: 29.1023 - val_regression_loss: 21.8057 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 32.2382 - regression_loss: 28.8500 - val_loss: 27.4117 - val_regression_loss: 20.6511 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31.4760 - regression_loss: 27.4653 - val_loss: 26.2675 - val_regression_loss: 19.7935 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.6976 - regression_loss: 25.6238 - val_loss: 25.1272 - val_regression_loss: 19.1402 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.5835 - regression_loss: 25.2869 - val_loss: 25.3884 - val_regression_loss: 19.1075 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.6807 - regression_loss: 24.5358 - val_loss: 24.3723 - val_regression_loss: 18.4450 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.7995 - regression_loss: 23.5122 - val_loss: 23.9751 - val_regression_loss: 18.1883 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.0230 - regression_loss: 23.1583 - val_loss: 23.5040 - val_regression_loss: 17.8461 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.5699 - regression_loss: 22.4264 - val_loss: 23.7893 - val_regression_loss: 17.9656 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.0154 - regression_loss: 22.0102 - val_loss: 23.0910 - val_regression_loss: 17.5343 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.6440 - regression_loss: 21.7637 - val_loss: 23.1109 - val_regression_loss: 17.5170 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.5732 - regression_loss: 21.3702 - val_loss: 22.3854 - val_regression_loss: 17.0799 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.4436 - regression_loss: 21.3717 - val_loss: 23.5747 - val_regression_loss: 17.8301 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.7403 - regression_loss: 20.9565 - val_loss: 22.7604 - val_regression_loss: 17.3290 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.8459 - regression_loss: 20.7194 - val_loss: 23.5935 - val_regression_loss: 17.8533 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.4160 - regression_loss: 20.4878 - val_loss: 22.5148 - val_regression_loss: 17.1450 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 23.0745 - regression_loss: 20.2561 - val_loss: 22.8083 - val_regression_loss: 17.2987 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.2139 - regression_loss: 20.3295 - val_loss: 22.6904 - val_regression_loss: 17.2099 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.7162 - regression_loss: 19.6999 - val_loss: 22.7157 - val_regression_loss: 17.1977 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 22.4938 - regression_loss: 19.8538 - val_loss: 23.3206 - val_regression_loss: 17.6493 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.5183 - regression_loss: 19.7802 - val_loss: 22.6355 - val_regression_loss: 17.1573 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.0454 - regression_loss: 19.4805 - val_loss: 23.1862 - val_regression_loss: 17.6269 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 22.3695 - regression_loss: 19.4546 - val_loss: 22.7657 - val_regression_loss: 17.2934 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.5027 - regression_loss: 19.6334 - val_loss: 23.7737 - val_regression_loss: 17.8857 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.6693 - regression_loss: 19.5838 - val_loss: 22.5641 - val_regression_loss: 17.1524 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.9624 - regression_loss: 18.9572 - val_loss: 22.7180 - val_regression_loss: 17.2309 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.4156 - regression_loss: 19.0349 - val_loss: 23.1113 - val_regression_loss: 17.4656 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.6001 - regression_loss: 18.7551 - val_loss: 22.7348 - val_regression_loss: 17.2861 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.4013 - regression_loss: 18.6178 - val_loss: 22.7131 - val_regression_loss: 17.2360 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.4630 - regression_loss: 18.5032 - val_loss: 23.1971 - val_regression_loss: 17.4958 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.2886 - regression_loss: 18.5408 - val_loss: 23.2550 - val_regression_loss: 17.5766 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 21.6245 - regression_loss: 18.5408 - val_loss: 22.5728 - val_regression_loss: 17.1847 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 21.3337 - regression_loss: 18.4030 - val_loss: 23.0021 - val_regression_loss: 17.4026 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.9308 - regression_loss: 18.2649 - val_loss: 23.2924 - val_regression_loss: 17.5676 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.7518 - regression_loss: 18.1067 - val_loss: 22.9521 - val_regression_loss: 17.4036 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 21.0868 - regression_loss: 18.1445 - val_loss: 23.1045 - val_regression_loss: 17.4867 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.7941 - regression_loss: 18.1577 - val_loss: 22.9609 - val_regression_loss: 17.4444 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.9147 - regression_loss: 18.3670 - val_loss: 23.7576 - val_regression_loss: 17.9820 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.8318 - regression_loss: 18.1371 - val_loss: 23.2474 - val_regression_loss: 17.6463 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.4380 - regression_loss: 17.9097 - val_loss: 23.7197 - val_regression_loss: 17.9690 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 20.4477 - regression_loss: 17.8431 - val_loss: 23.1766 - val_regression_loss: 17.5568 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 20.8025 - regression_loss: 17.7969 - val_loss: 23.0900 - val_regression_loss: 17.5239 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 20.2829 - regression_loss: 17.6235 - val_loss: 23.3486 - val_regression_loss: 17.6735 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.5023 - regression_loss: 17.5441 - val_loss: 23.4258 - val_regression_loss: 17.7735 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.3101 - regression_loss: 17.7146 - val_loss: 23.2801 - val_regression_loss: 17.6794 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.0046 - regression_loss: 17.2964 - val_loss: 23.8309 - val_regression_loss: 18.0005 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.1940 - regression_loss: 17.3109 - val_loss: 23.4473 - val_regression_loss: 17.8044 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.2758 - regression_loss: 17.3324 - val_loss: 23.4039 - val_regression_loss: 17.7464 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.7723 - regression_loss: 17.1077 - val_loss: 23.2687 - val_regression_loss: 17.6523 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.8904 - regression_loss: 17.0481 - val_loss: 23.4449 - val_regression_loss: 17.7970 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.8295 - regression_loss: 17.0056 - val_loss: 23.5435 - val_regression_loss: 17.8368 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.8443 - regression_loss: 16.9785 - val_loss: 23.8383 - val_regression_loss: 18.0662 - lr: 1.0000e-04\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 30ms/step - loss: 56.5987 - regression_loss: 51.5281 - val_loss: 47.2016 - val_regression_loss: 36.1384 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 44.2445 - regression_loss: 39.3115 - val_loss: 38.6047 - val_regression_loss: 29.1912 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 38.3530 - regression_loss: 33.9401 - val_loss: 32.8473 - val_regression_loss: 24.9526 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 34.4223 - regression_loss: 31.2252 - val_loss: 30.7027 - val_regression_loss: 23.1372 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 33.9696 - regression_loss: 29.7748 - val_loss: 29.0628 - val_regression_loss: 22.0364 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31.8571 - regression_loss: 28.4189 - val_loss: 28.1743 - val_regression_loss: 21.2321 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30.5625 - regression_loss: 27.2308 - val_loss: 27.9566 - val_regression_loss: 21.0967 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.5707 - regression_loss: 26.4759 - val_loss: 27.5603 - val_regression_loss: 20.8115 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.3181 - regression_loss: 25.6955 - val_loss: 27.3635 - val_regression_loss: 20.6058 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.4819 - regression_loss: 25.1669 - val_loss: 27.3722 - val_regression_loss: 20.6817 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.4902 - regression_loss: 25.1698 - val_loss: 27.5450 - val_regression_loss: 20.7389 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.5674 - regression_loss: 25.0211 - val_loss: 27.5442 - val_regression_loss: 20.8759 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.3604 - regression_loss: 24.2678 - val_loss: 27.2734 - val_regression_loss: 20.5519 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.0795 - regression_loss: 23.8978 - val_loss: 27.3852 - val_regression_loss: 20.6947 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.4435 - regression_loss: 23.4486 - val_loss: 27.3739 - val_regression_loss: 20.6802 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.1004 - regression_loss: 23.5267 - val_loss: 27.4505 - val_regression_loss: 20.7271 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.6778 - regression_loss: 23.1605 - val_loss: 27.6519 - val_regression_loss: 20.9119 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.0388 - regression_loss: 22.8287 - val_loss: 27.9753 - val_regression_loss: 21.1682 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.0009 - regression_loss: 22.6156 - val_loss: 27.8875 - val_regression_loss: 21.0631 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.3656 - regression_loss: 22.4457 - val_loss: 27.9069 - val_regression_loss: 21.0501 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.5056 - regression_loss: 22.5817 - val_loss: 27.9140 - val_regression_loss: 21.1064 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.9756 - regression_loss: 22.0950 - val_loss: 28.0446 - val_regression_loss: 21.1793 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.7632 - regression_loss: 22.0301 - val_loss: 28.3663 - val_regression_loss: 21.4672 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.9857 - regression_loss: 21.9568 - val_loss: 28.3116 - val_regression_loss: 21.3658 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.1208 - regression_loss: 21.8163 - val_loss: 28.5953 - val_regression_loss: 21.6412 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 25.1363 - regression_loss: 21.7418 - val_loss: 28.4349 - val_regression_loss: 21.4562 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.8131 - regression_loss: 21.5475 - val_loss: 28.8589 - val_regression_loss: 21.8778 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.5297 - regression_loss: 21.5204 - val_loss: 28.7124 - val_regression_loss: 21.6877 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.3822 - regression_loss: 21.3886 - val_loss: 28.8179 - val_regression_loss: 21.8105 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.6454 - regression_loss: 21.5297 - val_loss: 28.8245 - val_regression_loss: 21.7733 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.6264 - regression_loss: 21.4312 - val_loss: 29.2507 - val_regression_loss: 22.1882 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.3806 - regression_loss: 21.1846 - val_loss: 29.3077 - val_regression_loss: 22.1045 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.6631 - regression_loss: 21.4400 - val_loss: 29.7410 - val_regression_loss: 22.5705 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.3114 - regression_loss: 21.3305 - val_loss: 30.1294 - val_regression_loss: 22.7460 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.5038 - regression_loss: 22.0821 - val_loss: 29.9107 - val_regression_loss: 22.7391 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.6219 - regression_loss: 21.0588 - val_loss: 29.0902 - val_regression_loss: 21.9714 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.0577 - regression_loss: 21.0357 - val_loss: 29.4965 - val_regression_loss: 22.3761 - lr: 1.0000e-04\n",
      "Epoch 38/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 24.1311 - regression_loss: 21.0004 - val_loss: 29.2206 - val_regression_loss: 22.0813 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.1424 - regression_loss: 21.0033 - val_loss: 29.2362 - val_regression_loss: 22.1052 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.3399 - regression_loss: 21.1470 - val_loss: 29.2911 - val_regression_loss: 22.1560 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.9421 - regression_loss: 20.9012 - val_loss: 29.5205 - val_regression_loss: 22.3242 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.0659 - regression_loss: 20.9372 - val_loss: 29.9159 - val_regression_loss: 22.7138 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.8536 - regression_loss: 20.8111 - val_loss: 29.4014 - val_regression_loss: 22.1885 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.1170 - regression_loss: 20.9252 - val_loss: 30.1081 - val_regression_loss: 22.8724 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.7675 - regression_loss: 20.7008 - val_loss: 29.6241 - val_regression_loss: 22.3909 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.5293 - regression_loss: 20.8411 - val_loss: 29.6028 - val_regression_loss: 22.4158 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.3539 - regression_loss: 20.3879 - val_loss: 29.3238 - val_regression_loss: 22.1608 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.3571 - regression_loss: 20.3981 - val_loss: 29.7217 - val_regression_loss: 22.5261 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.2856 - regression_loss: 20.4819 - val_loss: 29.5953 - val_regression_loss: 22.3606 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 23.4177 - regression_loss: 20.4895 - val_loss: 29.5771 - val_regression_loss: 22.3716 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 23.3557 - regression_loss: 20.3435 - val_loss: 29.6222 - val_regression_loss: 22.3813 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 23.2064 - regression_loss: 20.4601 - val_loss: 29.6459 - val_regression_loss: 22.4343 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.9759 - regression_loss: 20.1611 - val_loss: 29.5095 - val_regression_loss: 22.3092 - lr: 1.0000e-04\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 27ms/step - loss: 45.4922 - regression_loss: 40.8838 - val_loss: 36.3686 - val_regression_loss: 28.9529 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 38.9059 - regression_loss: 34.9675 - val_loss: 34.6866 - val_regression_loss: 26.9877 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 35.1647 - regression_loss: 31.8103 - val_loss: 31.2741 - val_regression_loss: 24.6476 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 34.6646 - regression_loss: 30.4917 - val_loss: 31.1466 - val_regression_loss: 24.0549 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 33.0767 - regression_loss: 28.8661 - val_loss: 29.7796 - val_regression_loss: 23.2817 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31.6800 - regression_loss: 27.8570 - val_loss: 29.6017 - val_regression_loss: 23.2609 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30.5355 - regression_loss: 27.1877 - val_loss: 29.4482 - val_regression_loss: 23.1296 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30.0450 - regression_loss: 26.6545 - val_loss: 29.2733 - val_regression_loss: 22.9975 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 30.1384 - regression_loss: 26.3621 - val_loss: 29.2840 - val_regression_loss: 22.9807 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.2327 - regression_loss: 25.8900 - val_loss: 28.6073 - val_regression_loss: 22.5534 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.0991 - regression_loss: 25.7454 - val_loss: 28.8376 - val_regression_loss: 22.7388 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.6415 - regression_loss: 25.3909 - val_loss: 29.1687 - val_regression_loss: 22.9563 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.7653 - regression_loss: 25.4289 - val_loss: 28.7516 - val_regression_loss: 22.7521 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.4513 - regression_loss: 25.2009 - val_loss: 29.3622 - val_regression_loss: 22.9513 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 28.3228 - regression_loss: 25.0330 - val_loss: 28.6850 - val_regression_loss: 22.6102 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.1117 - regression_loss: 24.8761 - val_loss: 29.5337 - val_regression_loss: 23.1652 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.0048 - regression_loss: 24.5566 - val_loss: 28.6935 - val_regression_loss: 22.6997 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.9846 - regression_loss: 24.4501 - val_loss: 29.0168 - val_regression_loss: 22.7946 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.7411 - regression_loss: 24.6121 - val_loss: 28.7656 - val_regression_loss: 22.6198 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.2856 - regression_loss: 24.9864 - val_loss: 28.3949 - val_regression_loss: 22.5570 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.1954 - regression_loss: 24.9471 - val_loss: 29.7140 - val_regression_loss: 23.2490 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.8361 - regression_loss: 24.4809 - val_loss: 28.6153 - val_regression_loss: 22.7235 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.6876 - regression_loss: 24.2964 - val_loss: 29.2317 - val_regression_loss: 22.8909 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.1485 - regression_loss: 24.1711 - val_loss: 28.4996 - val_regression_loss: 22.6078 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.4268 - regression_loss: 24.1292 - val_loss: 29.4326 - val_regression_loss: 23.1495 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.0337 - regression_loss: 23.8377 - val_loss: 28.5559 - val_regression_loss: 22.5899 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.5257 - regression_loss: 23.6473 - val_loss: 28.8053 - val_regression_loss: 22.6505 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.7846 - regression_loss: 23.5447 - val_loss: 29.0206 - val_regression_loss: 22.8597 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.7585 - regression_loss: 23.6592 - val_loss: 29.3955 - val_regression_loss: 23.2117 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.4451 - regression_loss: 23.5406 - val_loss: 29.0543 - val_regression_loss: 22.9437 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.6294 - regression_loss: 23.3710 - val_loss: 28.9342 - val_regression_loss: 22.8911 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.4050 - regression_loss: 23.2829 - val_loss: 29.2184 - val_regression_loss: 23.0634 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.4292 - regression_loss: 23.7401 - val_loss: 29.5223 - val_regression_loss: 23.2032 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.9589 - regression_loss: 23.6379 - val_loss: 28.8911 - val_regression_loss: 22.9302 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.2161 - regression_loss: 23.7147 - val_loss: 29.5220 - val_regression_loss: 23.2453 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.5080 - regression_loss: 23.2406 - val_loss: 29.0614 - val_regression_loss: 23.0106 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.1583 - regression_loss: 23.0438 - val_loss: 29.1256 - val_regression_loss: 23.0795 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.8566 - regression_loss: 22.9194 - val_loss: 29.6625 - val_regression_loss: 23.3828 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.9939 - regression_loss: 22.9033 - val_loss: 29.2991 - val_regression_loss: 23.2218 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.1284 - regression_loss: 23.0938 - val_loss: 29.8890 - val_regression_loss: 23.5137 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.4653 - regression_loss: 23.4967 - val_loss: 29.3408 - val_regression_loss: 23.4276 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 26.8328 - regression_loss: 23.5393 - val_loss: 30.1121 - val_regression_loss: 23.6603 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 28.8022 - regression_loss: 27.2729\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.9354 - regression_loss: 22.9562 - val_loss: 29.6000 - val_regression_loss: 23.4002 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 25.5976 - regression_loss: 22.5479 - val_loss: 29.4142 - val_regression_loss: 23.3470 - lr: 5.0000e-05\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.2394 - regression_loss: 22.4569 - val_loss: 29.6302 - val_regression_loss: 23.4026 - lr: 5.0000e-05\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.8343 - regression_loss: 22.4455 - val_loss: 29.7473 - val_regression_loss: 23.5138 - lr: 5.0000e-05\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.3143 - regression_loss: 22.4164 - val_loss: 29.6143 - val_regression_loss: 23.4890 - lr: 5.0000e-05\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.1810 - regression_loss: 22.4431 - val_loss: 29.7394 - val_regression_loss: 23.4582 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.5469 - regression_loss: 22.4072 - val_loss: 29.6851 - val_regression_loss: 23.5049 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.5088 - regression_loss: 22.3479 - val_loss: 29.6535 - val_regression_loss: 23.4845 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.1483 - regression_loss: 22.3268 - val_loss: 29.6649 - val_regression_loss: 23.5237 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.2386 - regression_loss: 22.2597 - val_loss: 29.8324 - val_regression_loss: 23.5552 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.4057 - regression_loss: 22.2935 - val_loss: 29.7314 - val_regression_loss: 23.4532 - lr: 5.0000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.4603 - regression_loss: 22.3263 - val_loss: 29.7276 - val_regression_loss: 23.5916 - lr: 5.0000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.1545 - regression_loss: 22.2012 - val_loss: 29.9471 - val_regression_loss: 23.6481 - lr: 5.0000e-05\n",
      "Epoch 56/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 25.5493 - regression_loss: 24.0202\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.5709 - regression_loss: 22.3134 - val_loss: 29.7680 - val_regression_loss: 23.5525 - lr: 5.0000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.4437 - regression_loss: 22.2590 - val_loss: 29.6810 - val_regression_loss: 23.5772 - lr: 2.5000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.9468 - regression_loss: 22.1539 - val_loss: 29.8214 - val_regression_loss: 23.5657 - lr: 2.5000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.4446 - regression_loss: 22.1370 - val_loss: 29.9677 - val_regression_loss: 23.6683 - lr: 2.5000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 25.1941 - regression_loss: 22.1059 - val_loss: 29.9339 - val_regression_loss: 23.6776 - lr: 2.5000e-05\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 31ms/step - loss: 101.5216 - regression_loss: 94.3474 - val_loss: 53.0782 - val_regression_loss: 42.8701 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 78.1754 - regression_loss: 72.0342 - val_loss: 43.1051 - val_regression_loss: 34.3621 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 65.2661 - regression_loss: 59.7184 - val_loss: 38.7621 - val_regression_loss: 29.8932 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 57.0425 - regression_loss: 51.2428 - val_loss: 35.5117 - val_regression_loss: 26.8040 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 49.8638 - regression_loss: 43.9486 - val_loss: 31.8448 - val_regression_loss: 23.6764 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 40.5798 - regression_loss: 36.1482 - val_loss: 27.8859 - val_regression_loss: 20.2262 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 34.7734 - regression_loss: 30.3103 - val_loss: 24.8898 - val_regression_loss: 17.7345 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.6418 - regression_loss: 25.1078 - val_loss: 22.5691 - val_regression_loss: 15.9325 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.3316 - regression_loss: 21.0058 - val_loss: 20.4550 - val_regression_loss: 14.3800 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 22.3960 - regression_loss: 18.9882 - val_loss: 18.8053 - val_regression_loss: 13.1421 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 19.2083 - regression_loss: 16.4357 - val_loss: 17.3659 - val_regression_loss: 12.0387 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.0412 - regression_loss: 14.4616 - val_loss: 15.6246 - val_regression_loss: 10.7454 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.3648 - regression_loss: 12.7675 - val_loss: 14.0494 - val_regression_loss: 9.5444 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 13.8577 - regression_loss: 11.4165 - val_loss: 12.3976 - val_regression_loss: 8.3258 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.8172 - regression_loss: 10.4559 - val_loss: 11.3777 - val_regression_loss: 7.5046 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.1350 - regression_loss: 9.9059 - val_loss: 10.9444 - val_regression_loss: 7.1222 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.5310 - regression_loss: 9.2381 - val_loss: 10.1341 - val_regression_loss: 6.6460 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.1570 - regression_loss: 8.8747 - val_loss: 9.5707 - val_regression_loss: 6.0695 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.4085 - regression_loss: 8.1559 - val_loss: 9.0137 - val_regression_loss: 5.7430 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3430 - regression_loss: 7.6710 - val_loss: 8.8079 - val_regression_loss: 5.4879 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.2536 - regression_loss: 7.3906 - val_loss: 8.0750 - val_regression_loss: 5.0153 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.8692 - regression_loss: 6.8382 - val_loss: 7.9102 - val_regression_loss: 4.8048 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.8117 - regression_loss: 6.6764 - val_loss: 7.5245 - val_regression_loss: 4.5482 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.0046 - regression_loss: 6.1923 - val_loss: 7.1545 - val_regression_loss: 4.2666 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8719 - regression_loss: 5.9446 - val_loss: 6.8849 - val_regression_loss: 4.0421 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7421 - regression_loss: 5.8041 - val_loss: 7.0245 - val_regression_loss: 4.2668 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7522 - regression_loss: 5.7784 - val_loss: 7.8505 - val_regression_loss: 4.7979 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6535 - regression_loss: 5.7167 - val_loss: 8.0581 - val_regression_loss: 5.2188 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 7.6865 - regression_loss: 5.7649 - val_loss: 8.2378 - val_regression_loss: 5.1269 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 7.6966 - regression_loss: 5.7878 - val_loss: 7.9337 - val_regression_loss: 5.1493 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7490 - regression_loss: 5.9128 - val_loss: 6.9995 - val_regression_loss: 4.1714 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.6428 - regression_loss: 4.8721 - val_loss: 6.4969 - val_regression_loss: 3.9246 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6.4803 - regression_loss: 4.6813 - val_loss: 5.9249 - val_regression_loss: 3.3538 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6.1022 - regression_loss: 4.2749 - val_loss: 5.9119 - val_regression_loss: 3.4349 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6.0267 - regression_loss: 4.2418 - val_loss: 5.6847 - val_regression_loss: 3.1752 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5.8602 - regression_loss: 4.0664 - val_loss: 5.6566 - val_regression_loss: 3.2266 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.8177 - regression_loss: 3.9522 - val_loss: 5.6059 - val_regression_loss: 3.1423 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5.7730 - regression_loss: 3.9508 - val_loss: 5.6228 - val_regression_loss: 3.2347 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6225 - regression_loss: 3.7934 - val_loss: 5.5509 - val_regression_loss: 3.1033 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5.5577 - regression_loss: 3.7144 - val_loss: 5.7775 - val_regression_loss: 3.3784 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5.6514 - regression_loss: 3.8263 - val_loss: 5.4243 - val_regression_loss: 3.0085 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5.4618 - regression_loss: 3.6203 - val_loss: 5.1243 - val_regression_loss: 2.8138 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.2532 - regression_loss: 3.4511 - val_loss: 5.0524 - val_regression_loss: 2.7478 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.2007 - regression_loss: 3.3821 - val_loss: 5.1219 - val_regression_loss: 2.7999 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5.0500 - regression_loss: 3.2864 - val_loss: 5.0259 - val_regression_loss: 2.7449 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.9607 - regression_loss: 3.1832 - val_loss: 4.9779 - val_regression_loss: 2.7068 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.8096 - regression_loss: 3.1310 - val_loss: 4.9678 - val_regression_loss: 2.6866 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.8432 - regression_loss: 3.0802 - val_loss: 4.9396 - val_regression_loss: 2.6842 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.7553 - regression_loss: 3.0588 - val_loss: 4.8581 - val_regression_loss: 2.6016 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.7290 - regression_loss: 2.9650 - val_loss: 5.0012 - val_regression_loss: 2.7579 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.8128 - regression_loss: 3.0613 - val_loss: 4.7794 - val_regression_loss: 2.5556 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.6903 - regression_loss: 2.9453 - val_loss: 4.7744 - val_regression_loss: 2.5593 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.5675 - regression_loss: 2.8442 - val_loss: 4.8349 - val_regression_loss: 2.6266 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.5992 - regression_loss: 2.9246 - val_loss: 4.7537 - val_regression_loss: 2.5287 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.4733 - regression_loss: 2.7814 - val_loss: 4.8470 - val_regression_loss: 2.6317 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.4268 - regression_loss: 2.6939 - val_loss: 4.7669 - val_regression_loss: 2.5454 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.4259 - regression_loss: 2.7345 - val_loss: 4.6153 - val_regression_loss: 2.4495 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.3810 - regression_loss: 2.6696 - val_loss: 4.6820 - val_regression_loss: 2.5065 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.4071 - regression_loss: 2.6415 - val_loss: 4.6175 - val_regression_loss: 2.4395 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.2939 - regression_loss: 2.5760 - val_loss: 4.5989 - val_regression_loss: 2.4108 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 4.2353 - regression_loss: 2.5648 - val_loss: 4.6562 - val_regression_loss: 2.4788 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.2859 - regression_loss: 2.5425 - val_loss: 4.6108 - val_regression_loss: 2.4566 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.1784 - regression_loss: 2.4964 - val_loss: 4.6042 - val_regression_loss: 2.4293 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.1995 - regression_loss: 2.5033 - val_loss: 4.7106 - val_regression_loss: 2.5392 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0886 - regression_loss: 2.4366 - val_loss: 4.5649 - val_regression_loss: 2.4079 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.1039 - regression_loss: 2.4387 - val_loss: 4.5047 - val_regression_loss: 2.3582 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0511 - regression_loss: 2.3684 - val_loss: 4.4797 - val_regression_loss: 2.3298 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 4.1148 - regression_loss: 2.4169 - val_loss: 4.6545 - val_regression_loss: 2.4959 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 4.0486 - regression_loss: 2.3763 - val_loss: 4.4386 - val_regression_loss: 2.3143 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0790 - regression_loss: 2.3791 - val_loss: 4.4695 - val_regression_loss: 2.3382 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9972 - regression_loss: 2.2978 - val_loss: 4.7723 - val_regression_loss: 2.5982 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.1924 - regression_loss: 2.5011 - val_loss: 4.4523 - val_regression_loss: 2.3183 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9311 - regression_loss: 2.2664 - val_loss: 4.3798 - val_regression_loss: 2.2613 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9068 - regression_loss: 2.2435 - val_loss: 4.5474 - val_regression_loss: 2.4089 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9659 - regression_loss: 2.2856 - val_loss: 4.3821 - val_regression_loss: 2.2608 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.8584 - regression_loss: 2.1966 - val_loss: 4.3595 - val_regression_loss: 2.2448 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.8484 - regression_loss: 2.1658 - val_loss: 4.3799 - val_regression_loss: 2.2656 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.8818 - regression_loss: 2.1830 - val_loss: 4.4012 - val_regression_loss: 2.2924 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.8075 - regression_loss: 2.1416 - val_loss: 4.4245 - val_regression_loss: 2.2969 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9287 - regression_loss: 2.2985 - val_loss: 4.3447 - val_regression_loss: 2.2351 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.8354 - regression_loss: 2.2211 - val_loss: 4.5917 - val_regression_loss: 2.4445 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9600 - regression_loss: 2.2961 - val_loss: 4.4645 - val_regression_loss: 2.3369 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9198 - regression_loss: 2.2363 - val_loss: 4.3841 - val_regression_loss: 2.2648 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7711 - regression_loss: 2.0961 - val_loss: 4.3907 - val_regression_loss: 2.2792 - lr: 1.0000e-04\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7934 - regression_loss: 2.1219 - val_loss: 4.5196 - val_regression_loss: 2.3860 - lr: 1.0000e-04\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7617 - regression_loss: 2.0644 - val_loss: 4.3265 - val_regression_loss: 2.2250 - lr: 1.0000e-04\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6595 - regression_loss: 2.0625 - val_loss: 4.3326 - val_regression_loss: 2.2316 - lr: 1.0000e-04\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7583 - regression_loss: 2.0775 - val_loss: 4.3808 - val_regression_loss: 2.2720 - lr: 1.0000e-04\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7526 - regression_loss: 2.0577 - val_loss: 4.3735 - val_regression_loss: 2.2641 - lr: 1.0000e-04\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6848 - regression_loss: 2.0539 - val_loss: 4.3586 - val_regression_loss: 2.2561 - lr: 1.0000e-04\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6428 - regression_loss: 2.0258 - val_loss: 4.2703 - val_regression_loss: 2.1706 - lr: 1.0000e-04\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6550 - regression_loss: 1.9932 - val_loss: 4.2760 - val_regression_loss: 2.1843 - lr: 1.0000e-04\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6335 - regression_loss: 1.9630 - val_loss: 4.2833 - val_regression_loss: 2.2091 - lr: 1.0000e-04\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6656 - regression_loss: 2.0096 - val_loss: 4.2924 - val_regression_loss: 2.2085 - lr: 1.0000e-04\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6389 - regression_loss: 1.9870 - val_loss: 4.2966 - val_regression_loss: 2.1969 - lr: 1.0000e-04\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6617 - regression_loss: 1.9761 - val_loss: 4.4789 - val_regression_loss: 2.3566 - lr: 1.0000e-04\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6786 - regression_loss: 1.9798 - val_loss: 4.3100 - val_regression_loss: 2.2134 - lr: 1.0000e-04\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6059 - regression_loss: 1.9731 - val_loss: 4.2739 - val_regression_loss: 2.1823 - lr: 1.0000e-04\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5490 - regression_loss: 1.9139 - val_loss: 4.1937 - val_regression_loss: 2.1253 - lr: 1.0000e-04\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6581 - regression_loss: 1.9906 - val_loss: 4.4792 - val_regression_loss: 2.3507 - lr: 1.0000e-04\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7601 - regression_loss: 2.1138 - val_loss: 4.2492 - val_regression_loss: 2.1580 - lr: 1.0000e-04\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5684 - regression_loss: 1.9378 - val_loss: 4.5732 - val_regression_loss: 2.4385 - lr: 1.0000e-04\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 3.7710 - regression_loss: 2.0853 - val_loss: 4.2222 - val_regression_loss: 2.1532 - lr: 1.0000e-04\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5080 - regression_loss: 1.8860 - val_loss: 4.2153 - val_regression_loss: 2.1409 - lr: 1.0000e-04\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5422 - regression_loss: 1.8770 - val_loss: 4.2264 - val_regression_loss: 2.1531 - lr: 1.0000e-04\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5905 - regression_loss: 1.9279 - val_loss: 4.1587 - val_regression_loss: 2.0997 - lr: 1.0000e-04\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5177 - regression_loss: 1.8820 - val_loss: 4.1874 - val_regression_loss: 2.1244 - lr: 1.0000e-04\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5170 - regression_loss: 1.8757 - val_loss: 4.3244 - val_regression_loss: 2.2342 - lr: 1.0000e-04\n",
      "Epoch 109/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.2967 - regression_loss: 1.7839\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5798 - regression_loss: 1.9411 - val_loss: 4.1775 - val_regression_loss: 2.1076 - lr: 1.0000e-04\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4686 - regression_loss: 1.8318 - val_loss: 4.1481 - val_regression_loss: 2.0855 - lr: 5.0000e-05\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4495 - regression_loss: 1.8114 - val_loss: 4.1572 - val_regression_loss: 2.0952 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4798 - regression_loss: 1.8164 - val_loss: 4.1459 - val_regression_loss: 2.0842 - lr: 5.0000e-05\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4706 - regression_loss: 1.8058 - val_loss: 4.1974 - val_regression_loss: 2.1327 - lr: 5.0000e-05\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4448 - regression_loss: 1.8106 - val_loss: 4.1487 - val_regression_loss: 2.0950 - lr: 5.0000e-05\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4566 - regression_loss: 1.8052 - val_loss: 4.1975 - val_regression_loss: 2.1315 - lr: 5.0000e-05\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4441 - regression_loss: 1.8047 - val_loss: 4.1503 - val_regression_loss: 2.0908 - lr: 5.0000e-05\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3945 - regression_loss: 1.7892 - val_loss: 4.1947 - val_regression_loss: 2.1248 - lr: 5.0000e-05\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4279 - regression_loss: 1.7972 - val_loss: 4.1644 - val_regression_loss: 2.1004 - lr: 5.0000e-05\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4235 - regression_loss: 1.7824 - val_loss: 4.1931 - val_regression_loss: 2.1279 - lr: 5.0000e-05\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4323 - regression_loss: 1.7919 - val_loss: 4.1336 - val_regression_loss: 2.0793 - lr: 5.0000e-05\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4053 - regression_loss: 1.7827 - val_loss: 4.1723 - val_regression_loss: 2.1103 - lr: 5.0000e-05\n",
      "Epoch 122/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.5816 - regression_loss: 2.0702\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4184 - regression_loss: 1.7841 - val_loss: 4.1387 - val_regression_loss: 2.0816 - lr: 5.0000e-05\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4191 - regression_loss: 1.7802 - val_loss: 4.1591 - val_regression_loss: 2.1008 - lr: 2.5000e-05\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4149 - regression_loss: 1.7787 - val_loss: 4.1544 - val_regression_loss: 2.0991 - lr: 2.5000e-05\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 3.4236 - regression_loss: 1.7836 - val_loss: 4.1421 - val_regression_loss: 2.0869 - lr: 2.5000e-05\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 3.4078 - regression_loss: 1.7812 - val_loss: 4.1781 - val_regression_loss: 2.1162 - lr: 2.5000e-05\n",
      "Epoch 127/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.2743 - regression_loss: 1.7632\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4169 - regression_loss: 1.7651 - val_loss: 4.1395 - val_regression_loss: 2.0828 - lr: 2.5000e-05\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3890 - regression_loss: 1.7604 - val_loss: 4.1452 - val_regression_loss: 2.0871 - lr: 1.2500e-05\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4129 - regression_loss: 1.7545 - val_loss: 4.1479 - val_regression_loss: 2.0896 - lr: 1.2500e-05\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3970 - regression_loss: 1.7576 - val_loss: 4.1583 - val_regression_loss: 2.0992 - lr: 1.2500e-05\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3982 - regression_loss: 1.7527 - val_loss: 4.1394 - val_regression_loss: 2.0845 - lr: 1.2500e-05\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3845 - regression_loss: 1.7561 - val_loss: 4.1335 - val_regression_loss: 2.0793 - lr: 1.2500e-05\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3626 - regression_loss: 1.7574 - val_loss: 4.1455 - val_regression_loss: 2.0884 - lr: 1.2500e-05\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3628 - regression_loss: 1.7562 - val_loss: 4.1410 - val_regression_loss: 2.0864 - lr: 1.2500e-05\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.4110 - regression_loss: 1.7531 - val_loss: 4.1303 - val_regression_loss: 2.0781 - lr: 1.2500e-05\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4190 - regression_loss: 1.7523 - val_loss: 4.1402 - val_regression_loss: 2.0866 - lr: 1.2500e-05\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4115 - regression_loss: 1.7504 - val_loss: 4.1451 - val_regression_loss: 2.0894 - lr: 1.2500e-05\n",
      "Epoch 138/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.3612 - regression_loss: 1.8504\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3868 - regression_loss: 1.7517 - val_loss: 4.1403 - val_regression_loss: 2.0854 - lr: 1.2500e-05\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3849 - regression_loss: 1.7468 - val_loss: 4.1385 - val_regression_loss: 2.0837 - lr: 6.2500e-06\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3531 - regression_loss: 1.7467 - val_loss: 4.1388 - val_regression_loss: 2.0840 - lr: 6.2500e-06\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.3434 - regression_loss: 1.7466 - val_loss: 4.1356 - val_regression_loss: 2.0810 - lr: 6.2500e-06\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3420 - regression_loss: 1.7453 - val_loss: 4.1370 - val_regression_loss: 2.0827 - lr: 6.2500e-06\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3821 - regression_loss: 1.7475 - val_loss: 4.1360 - val_regression_loss: 2.0818 - lr: 6.2500e-06\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3904 - regression_loss: 1.7441 - val_loss: 4.1395 - val_regression_loss: 2.0845 - lr: 6.2500e-06\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3406 - regression_loss: 1.7445 - val_loss: 4.1458 - val_regression_loss: 2.0897 - lr: 6.2500e-06\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3660 - regression_loss: 1.7473 - val_loss: 4.1343 - val_regression_loss: 2.0807 - lr: 6.2500e-06\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4220 - regression_loss: 1.7446 - val_loss: 4.1340 - val_regression_loss: 2.0803 - lr: 6.2500e-06\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.3524 - regression_loss: 1.7427 - val_loss: 4.1375 - val_regression_loss: 2.0833 - lr: 6.2500e-06\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3861 - regression_loss: 1.7441 - val_loss: 4.1374 - val_regression_loss: 2.0832 - lr: 6.2500e-06\n",
      "Epoch 150/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.1240 - regression_loss: 1.6134\n",
      "Epoch 150: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3733 - regression_loss: 1.7433 - val_loss: 4.1395 - val_regression_loss: 2.0849 - lr: 6.2500e-06\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3594 - regression_loss: 1.7418 - val_loss: 4.1385 - val_regression_loss: 2.0840 - lr: 3.1250e-06\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3961 - regression_loss: 1.7416 - val_loss: 4.1379 - val_regression_loss: 2.0836 - lr: 3.1250e-06\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3708 - regression_loss: 1.7432 - val_loss: 4.1332 - val_regression_loss: 2.0800 - lr: 3.1250e-06\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3645 - regression_loss: 1.7406 - val_loss: 4.1341 - val_regression_loss: 2.0810 - lr: 3.1250e-06\n",
      "Epoch 155/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.5290 - regression_loss: 2.0184\n",
      "Epoch 155: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3469 - regression_loss: 1.7407 - val_loss: 4.1355 - val_regression_loss: 2.0823 - lr: 3.1250e-06\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3568 - regression_loss: 1.7405 - val_loss: 4.1347 - val_regression_loss: 2.0817 - lr: 1.5625e-06\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3856 - regression_loss: 1.7400 - val_loss: 4.1362 - val_regression_loss: 2.0827 - lr: 1.5625e-06\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3710 - regression_loss: 1.7397 - val_loss: 4.1367 - val_regression_loss: 2.0832 - lr: 1.5625e-06\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3605 - regression_loss: 1.7399 - val_loss: 4.1350 - val_regression_loss: 2.0817 - lr: 1.5625e-06\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3244 - regression_loss: 1.7395 - val_loss: 4.1354 - val_regression_loss: 2.0821 - lr: 1.5625e-06\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3556 - regression_loss: 1.7396 - val_loss: 4.1348 - val_regression_loss: 2.0817 - lr: 1.5625e-06\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3694 - regression_loss: 1.7395 - val_loss: 4.1353 - val_regression_loss: 2.0821 - lr: 1.5625e-06\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3663 - regression_loss: 1.7391 - val_loss: 4.1351 - val_regression_loss: 2.0818 - lr: 1.5625e-06\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3779 - regression_loss: 1.7391 - val_loss: 4.1341 - val_regression_loss: 2.0810 - lr: 1.5625e-06\n",
      "Epoch 165/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.0927 - regression_loss: 1.5821\n",
      "Epoch 165: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3916 - regression_loss: 1.7393 - val_loss: 4.1355 - val_regression_loss: 2.0820 - lr: 1.5625e-06\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3654 - regression_loss: 1.7387 - val_loss: 4.1352 - val_regression_loss: 2.0817 - lr: 7.8125e-07\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3616 - regression_loss: 1.7388 - val_loss: 4.1354 - val_regression_loss: 2.0821 - lr: 7.8125e-07\n",
      "Epoch 168/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3863 - regression_loss: 1.7385 - val_loss: 4.1351 - val_regression_loss: 2.0818 - lr: 7.8125e-07\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3565 - regression_loss: 1.7387 - val_loss: 4.1356 - val_regression_loss: 2.0821 - lr: 7.8125e-07\n",
      "Epoch 170/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.1168 - regression_loss: 1.6063\n",
      "Epoch 170: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3499 - regression_loss: 1.7384 - val_loss: 4.1354 - val_regression_loss: 2.0820 - lr: 7.8125e-07\n",
      "Epoch 171/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3520 - regression_loss: 1.7384 - val_loss: 4.1350 - val_regression_loss: 2.0817 - lr: 3.9062e-07\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3350 - regression_loss: 1.7385 - val_loss: 4.1353 - val_regression_loss: 2.0820 - lr: 3.9062e-07\n",
      "Epoch 173/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3810 - regression_loss: 1.7385 - val_loss: 4.1346 - val_regression_loss: 2.0814 - lr: 3.9062e-07\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3607 - regression_loss: 1.7382 - val_loss: 4.1345 - val_regression_loss: 2.0813 - lr: 3.9062e-07\n",
      "Epoch 175/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.5593 - regression_loss: 2.0488\n",
      "Epoch 175: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3670 - regression_loss: 1.7382 - val_loss: 4.1343 - val_regression_loss: 2.0811 - lr: 3.9062e-07\n",
      "3/3 [==============================] - 0s 960us/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 27ms/step - loss: 98.3199 - regression_loss: 89.2417 - val_loss: 92.5783 - val_regression_loss: 68.8830 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 58.8368 - regression_loss: 54.1783 - val_loss: 65.9215 - val_regression_loss: 48.7707 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 43.6321 - regression_loss: 39.1889 - val_loss: 48.0105 - val_regression_loss: 36.2039 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 38.0366 - regression_loss: 34.1507 - val_loss: 44.3464 - val_regression_loss: 33.2354 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 36.6977 - regression_loss: 32.7464 - val_loss: 37.9766 - val_regression_loss: 28.8537 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 33.9284 - regression_loss: 30.4964 - val_loss: 35.0386 - val_regression_loss: 26.7635 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.3009 - regression_loss: 28.7163 - val_loss: 33.9451 - val_regression_loss: 25.8156 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.8832 - regression_loss: 27.4453 - val_loss: 30.8783 - val_regression_loss: 23.6824 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.3904 - regression_loss: 26.0700 - val_loss: 30.7329 - val_regression_loss: 23.4129 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.4129 - regression_loss: 25.1421 - val_loss: 28.7991 - val_regression_loss: 22.0631 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.5407 - regression_loss: 24.6056 - val_loss: 28.4953 - val_regression_loss: 21.7459 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.5841 - regression_loss: 23.6181 - val_loss: 27.6564 - val_regression_loss: 21.1099 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.2354 - regression_loss: 23.1450 - val_loss: 26.6960 - val_regression_loss: 20.4630 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.7350 - regression_loss: 22.6746 - val_loss: 26.4637 - val_regression_loss: 20.2214 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.1091 - regression_loss: 22.2451 - val_loss: 25.7188 - val_regression_loss: 19.7368 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.2152 - regression_loss: 21.9741 - val_loss: 25.4527 - val_regression_loss: 19.5104 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.8738 - regression_loss: 21.7701 - val_loss: 24.8887 - val_regression_loss: 19.1339 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.5969 - regression_loss: 21.4655 - val_loss: 24.7842 - val_regression_loss: 18.9802 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.0221 - regression_loss: 21.1010 - val_loss: 24.2311 - val_regression_loss: 18.6992 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.9472 - regression_loss: 20.9311 - val_loss: 24.4272 - val_regression_loss: 18.6719 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.0715 - regression_loss: 21.0635 - val_loss: 23.8257 - val_regression_loss: 18.3093 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.6403 - regression_loss: 20.5976 - val_loss: 24.0598 - val_regression_loss: 18.3804 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.1096 - regression_loss: 20.3666 - val_loss: 23.5416 - val_regression_loss: 18.0959 - lr: 1.0000e-04\n",
      "Epoch 24/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 22.6568 - regression_loss: 20.2025 - val_loss: 24.2164 - val_regression_loss: 18.4401 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.3556 - regression_loss: 20.2140 - val_loss: 23.2750 - val_regression_loss: 18.0346 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.8466 - regression_loss: 20.1994 - val_loss: 23.6170 - val_regression_loss: 17.9954 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.9328 - regression_loss: 19.9840 - val_loss: 22.8834 - val_regression_loss: 17.5896 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.6690 - regression_loss: 19.6984 - val_loss: 22.9656 - val_regression_loss: 17.5756 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.3440 - regression_loss: 19.5526 - val_loss: 22.4943 - val_regression_loss: 17.3092 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.1689 - regression_loss: 19.5262 - val_loss: 22.4502 - val_regression_loss: 17.2832 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.5161 - regression_loss: 19.4885 - val_loss: 23.1867 - val_regression_loss: 17.6355 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.7459 - regression_loss: 19.5137 - val_loss: 22.6108 - val_regression_loss: 17.6047 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.9693 - regression_loss: 19.8634 - val_loss: 22.2766 - val_regression_loss: 17.0514 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.1417 - regression_loss: 19.1564 - val_loss: 22.2582 - val_regression_loss: 17.0177 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.1784 - regression_loss: 19.2278 - val_loss: 22.4826 - val_regression_loss: 17.1628 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.1735 - regression_loss: 19.5063 - val_loss: 22.3523 - val_regression_loss: 17.4026 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 25.2237 - regression_loss: 23.7139\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.3565 - regression_loss: 19.5029 - val_loss: 23.6280 - val_regression_loss: 17.8644 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.1384 - regression_loss: 19.3285 - val_loss: 21.9709 - val_regression_loss: 17.0331 - lr: 5.0000e-05\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.6468 - regression_loss: 19.1025 - val_loss: 21.8709 - val_regression_loss: 16.7563 - lr: 5.0000e-05\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.5156 - regression_loss: 18.9008 - val_loss: 22.2464 - val_regression_loss: 16.9183 - lr: 5.0000e-05\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.4014 - regression_loss: 18.6971 - val_loss: 21.6944 - val_regression_loss: 16.7759 - lr: 5.0000e-05\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.7597 - regression_loss: 18.6701 - val_loss: 22.0287 - val_regression_loss: 16.8108 - lr: 5.0000e-05\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.1394 - regression_loss: 18.5981 - val_loss: 21.8375 - val_regression_loss: 16.7038 - lr: 5.0000e-05\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9636 - regression_loss: 18.5275 - val_loss: 21.6267 - val_regression_loss: 16.6281 - lr: 5.0000e-05\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.6154 - regression_loss: 18.5521 - val_loss: 21.8722 - val_regression_loss: 16.6942 - lr: 5.0000e-05\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.4328 - regression_loss: 18.4262 - val_loss: 21.5869 - val_regression_loss: 16.5870 - lr: 5.0000e-05\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.0994 - regression_loss: 18.4581 - val_loss: 21.6052 - val_regression_loss: 16.5641 - lr: 5.0000e-05\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9734 - regression_loss: 18.3572 - val_loss: 21.7618 - val_regression_loss: 16.6062 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 21.4370 - regression_loss: 19.9274\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.2062 - regression_loss: 18.3982 - val_loss: 21.4209 - val_regression_loss: 16.4645 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9653 - regression_loss: 18.4336 - val_loss: 21.4239 - val_regression_loss: 16.4799 - lr: 2.5000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9251 - regression_loss: 18.2488 - val_loss: 21.6838 - val_regression_loss: 16.5554 - lr: 2.5000e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9726 - regression_loss: 18.2909 - val_loss: 21.6455 - val_regression_loss: 16.5346 - lr: 2.5000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8519 - regression_loss: 18.2462 - val_loss: 21.4728 - val_regression_loss: 16.4498 - lr: 2.5000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.0167 - regression_loss: 18.2411 - val_loss: 21.3809 - val_regression_loss: 16.4255 - lr: 2.5000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.0966 - regression_loss: 18.2825 - val_loss: 21.5217 - val_regression_loss: 16.4690 - lr: 2.5000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6406 - regression_loss: 18.2120 - val_loss: 21.3714 - val_regression_loss: 16.4104 - lr: 2.5000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9388 - regression_loss: 18.2136 - val_loss: 21.4803 - val_regression_loss: 16.4397 - lr: 2.5000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.0058 - regression_loss: 18.1458 - val_loss: 21.3760 - val_regression_loss: 16.3945 - lr: 2.5000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9506 - regression_loss: 18.1838 - val_loss: 21.3148 - val_regression_loss: 16.3674 - lr: 2.5000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9595 - regression_loss: 18.2258 - val_loss: 21.5703 - val_regression_loss: 16.4568 - lr: 2.5000e-05\n",
      "Epoch 61/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 24.9711 - regression_loss: 23.4614\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6978 - regression_loss: 18.1261 - val_loss: 21.3300 - val_regression_loss: 16.3697 - lr: 2.5000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7581 - regression_loss: 18.0879 - val_loss: 21.2992 - val_regression_loss: 16.3683 - lr: 1.2500e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9516 - regression_loss: 18.0962 - val_loss: 21.3571 - val_regression_loss: 16.3697 - lr: 1.2500e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9661 - regression_loss: 18.0862 - val_loss: 21.3005 - val_regression_loss: 16.3422 - lr: 1.2500e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7540 - regression_loss: 18.0858 - val_loss: 21.3811 - val_regression_loss: 16.3638 - lr: 1.2500e-05\n",
      "Epoch 66/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 20.3054 - regression_loss: 18.7957\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9986 - regression_loss: 18.0494 - val_loss: 21.3058 - val_regression_loss: 16.3292 - lr: 1.2500e-05\n",
      "Epoch 67/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8490 - regression_loss: 18.0410 - val_loss: 21.3220 - val_regression_loss: 16.3330 - lr: 6.2500e-06\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.0385 - regression_loss: 18.0258 - val_loss: 21.2952 - val_regression_loss: 16.3238 - lr: 6.2500e-06\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.0555 - regression_loss: 18.0300 - val_loss: 21.2912 - val_regression_loss: 16.3229 - lr: 6.2500e-06\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9046 - regression_loss: 18.0186 - val_loss: 21.2797 - val_regression_loss: 16.3174 - lr: 6.2500e-06\n",
      "Epoch 71/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 19.8048 - regression_loss: 18.2952\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7033 - regression_loss: 18.0208 - val_loss: 21.2829 - val_regression_loss: 16.3137 - lr: 6.2500e-06\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6709 - regression_loss: 18.0128 - val_loss: 21.2770 - val_regression_loss: 16.3124 - lr: 3.1250e-06\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7284 - regression_loss: 18.0146 - val_loss: 21.2830 - val_regression_loss: 16.3113 - lr: 3.1250e-06\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.0547 - regression_loss: 18.0137 - val_loss: 21.2902 - val_regression_loss: 16.3137 - lr: 3.1250e-06\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7540 - regression_loss: 18.0051 - val_loss: 21.2773 - val_regression_loss: 16.3079 - lr: 3.1250e-06\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4938 - regression_loss: 18.0109 - val_loss: 21.2606 - val_regression_loss: 16.3055 - lr: 3.1250e-06\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7996 - regression_loss: 18.0041 - val_loss: 21.2539 - val_regression_loss: 16.2997 - lr: 3.1250e-06\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4775 - regression_loss: 18.0065 - val_loss: 21.2848 - val_regression_loss: 16.3098 - lr: 3.1250e-06\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6333 - regression_loss: 18.0015 - val_loss: 21.2659 - val_regression_loss: 16.3018 - lr: 3.1250e-06\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5083 - regression_loss: 17.9938 - val_loss: 21.2689 - val_regression_loss: 16.3017 - lr: 3.1250e-06\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8472 - regression_loss: 17.9915 - val_loss: 21.2801 - val_regression_loss: 16.3060 - lr: 3.1250e-06\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7129 - regression_loss: 17.9925 - val_loss: 21.2819 - val_regression_loss: 16.3040 - lr: 3.1250e-06\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4610 - regression_loss: 18.0029 - val_loss: 21.2961 - val_regression_loss: 16.3085 - lr: 3.1250e-06\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6376 - regression_loss: 17.9964 - val_loss: 21.2608 - val_regression_loss: 16.2971 - lr: 3.1250e-06\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7864 - regression_loss: 17.9907 - val_loss: 21.2475 - val_regression_loss: 16.2913 - lr: 3.1250e-06\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5147 - regression_loss: 17.9832 - val_loss: 21.2522 - val_regression_loss: 16.2916 - lr: 3.1250e-06\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7731 - regression_loss: 17.9865 - val_loss: 21.2450 - val_regression_loss: 16.2875 - lr: 3.1250e-06\n",
      "Epoch 88/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 25.7779 - regression_loss: 24.2683\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5223 - regression_loss: 17.9778 - val_loss: 21.2621 - val_regression_loss: 16.2933 - lr: 3.1250e-06\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8167 - regression_loss: 17.9746 - val_loss: 21.2646 - val_regression_loss: 16.2943 - lr: 1.5625e-06\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6612 - regression_loss: 17.9734 - val_loss: 21.2627 - val_regression_loss: 16.2924 - lr: 1.5625e-06\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9469 - regression_loss: 17.9772 - val_loss: 21.2562 - val_regression_loss: 16.2905 - lr: 1.5625e-06\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9345 - regression_loss: 17.9729 - val_loss: 21.2648 - val_regression_loss: 16.2930 - lr: 1.5625e-06\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.3048 - regression_loss: 17.9735 - val_loss: 21.2711 - val_regression_loss: 16.2943 - lr: 1.5625e-06\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9955 - regression_loss: 17.9696 - val_loss: 21.2672 - val_regression_loss: 16.2938 - lr: 1.5625e-06\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.0111 - regression_loss: 17.9701 - val_loss: 21.2643 - val_regression_loss: 16.2922 - lr: 1.5625e-06\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8533 - regression_loss: 17.9690 - val_loss: 21.2516 - val_regression_loss: 16.2872 - lr: 1.5625e-06\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7596 - regression_loss: 17.9667 - val_loss: 21.2429 - val_regression_loss: 16.2829 - lr: 1.5625e-06\n",
      "Epoch 98/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.5915 - regression_loss: 17.0818\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.3625 - regression_loss: 17.9683 - val_loss: 21.2513 - val_regression_loss: 16.2859 - lr: 1.5625e-06\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.3171 - regression_loss: 17.9636 - val_loss: 21.2510 - val_regression_loss: 16.2858 - lr: 7.8125e-07\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6191 - regression_loss: 17.9633 - val_loss: 21.2471 - val_regression_loss: 16.2843 - lr: 7.8125e-07\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6477 - regression_loss: 17.9622 - val_loss: 21.2486 - val_regression_loss: 16.2849 - lr: 7.8125e-07\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7647 - regression_loss: 17.9619 - val_loss: 21.2466 - val_regression_loss: 16.2836 - lr: 7.8125e-07\n",
      "Epoch 103/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 17.6572 - regression_loss: 16.1475\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7735 - regression_loss: 17.9637 - val_loss: 21.2503 - val_regression_loss: 16.2842 - lr: 7.8125e-07\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8565 - regression_loss: 17.9604 - val_loss: 21.2489 - val_regression_loss: 16.2839 - lr: 3.9062e-07\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8147 - regression_loss: 17.9602 - val_loss: 21.2482 - val_regression_loss: 16.2837 - lr: 3.9062e-07\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6224 - regression_loss: 17.9602 - val_loss: 21.2485 - val_regression_loss: 16.2836 - lr: 3.9062e-07\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5465 - regression_loss: 17.9597 - val_loss: 21.2469 - val_regression_loss: 16.2826 - lr: 3.9062e-07\n",
      "Epoch 108/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 23.1171 - regression_loss: 21.6075\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.0294 - regression_loss: 17.9594 - val_loss: 21.2472 - val_regression_loss: 16.2825 - lr: 3.9062e-07\n",
      "Epoch 109/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 20.6936 - regression_loss: 17.9593 - val_loss: 21.2482 - val_regression_loss: 16.2829 - lr: 1.9531e-07\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7717 - regression_loss: 17.9586 - val_loss: 21.2476 - val_regression_loss: 16.2826 - lr: 1.9531e-07\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5484 - regression_loss: 17.9585 - val_loss: 21.2468 - val_regression_loss: 16.2824 - lr: 1.9531e-07\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6965 - regression_loss: 17.9587 - val_loss: 21.2469 - val_regression_loss: 16.2823 - lr: 1.9531e-07\n",
      "Epoch 113/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 20.5768 - regression_loss: 19.0671\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7497 - regression_loss: 17.9583 - val_loss: 21.2459 - val_regression_loss: 16.2818 - lr: 1.9531e-07\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6402 - regression_loss: 17.9580 - val_loss: 21.2455 - val_regression_loss: 16.2817 - lr: 9.7656e-08\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8837 - regression_loss: 17.9580 - val_loss: 21.2454 - val_regression_loss: 16.2817 - lr: 9.7656e-08\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7376 - regression_loss: 17.9579 - val_loss: 21.2454 - val_regression_loss: 16.2817 - lr: 9.7656e-08\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.3334 - regression_loss: 17.9579 - val_loss: 21.2456 - val_regression_loss: 16.2817 - lr: 9.7656e-08\n",
      "Epoch 118/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 20.7040 - regression_loss: 19.1944\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5718 - regression_loss: 17.9582 - val_loss: 21.2446 - val_regression_loss: 16.2813 - lr: 9.7656e-08\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.0234 - regression_loss: 17.9576 - val_loss: 21.2446 - val_regression_loss: 16.2813 - lr: 4.8828e-08\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9664 - regression_loss: 17.9576 - val_loss: 21.2444 - val_regression_loss: 16.2812 - lr: 4.8828e-08\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6428 - regression_loss: 17.9575 - val_loss: 21.2444 - val_regression_loss: 16.2812 - lr: 4.8828e-08\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4982 - regression_loss: 17.9575 - val_loss: 21.2444 - val_regression_loss: 16.2812 - lr: 4.8828e-08\n",
      "Epoch 123/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.3454 - regression_loss: 16.8357\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6268 - regression_loss: 17.9575 - val_loss: 21.2446 - val_regression_loss: 16.2812 - lr: 4.8828e-08\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5752 - regression_loss: 17.9574 - val_loss: 21.2445 - val_regression_loss: 16.2812 - lr: 2.4414e-08\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5594 - regression_loss: 17.9574 - val_loss: 21.2445 - val_regression_loss: 16.2812 - lr: 2.4414e-08\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5970 - regression_loss: 17.9574 - val_loss: 21.2445 - val_regression_loss: 16.2812 - lr: 2.4414e-08\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.2698 - regression_loss: 17.9574 - val_loss: 21.2445 - val_regression_loss: 16.2812 - lr: 2.4414e-08\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9633 - regression_loss: 17.9573 - val_loss: 21.2444 - val_regression_loss: 16.2812 - lr: 2.4414e-08\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6644 - regression_loss: 17.9573 - val_loss: 21.2445 - val_regression_loss: 16.2812 - lr: 2.4414e-08\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6592 - regression_loss: 17.9573 - val_loss: 21.2444 - val_regression_loss: 16.2812 - lr: 2.4414e-08\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6484 - regression_loss: 17.9573 - val_loss: 21.2444 - val_regression_loss: 16.2811 - lr: 2.4414e-08\n",
      "Epoch 132/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 22.7322 - regression_loss: 21.2226\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9984 - regression_loss: 17.9573 - val_loss: 21.2443 - val_regression_loss: 16.2811 - lr: 2.4414e-08\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4156 - regression_loss: 17.9573 - val_loss: 21.2443 - val_regression_loss: 16.2811 - lr: 1.2207e-08\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4500 - regression_loss: 17.9572 - val_loss: 21.2443 - val_regression_loss: 16.2811 - lr: 1.2207e-08\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7332 - regression_loss: 17.9572 - val_loss: 21.2443 - val_regression_loss: 16.2811 - lr: 1.2207e-08\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4389 - regression_loss: 17.9572 - val_loss: 21.2443 - val_regression_loss: 16.2811 - lr: 1.2207e-08\n",
      "Epoch 137/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 21.9819 - regression_loss: 20.4722\n",
      "Epoch 137: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8525 - regression_loss: 17.9572 - val_loss: 21.2443 - val_regression_loss: 16.2811 - lr: 1.2207e-08\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 24ms/step - loss: 50.1173 - regression_loss: 45.7289 - val_loss: 34.4226 - val_regression_loss: 27.8362 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 42.9018 - regression_loss: 38.7235 - val_loss: 32.3135 - val_regression_loss: 25.2237 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 37.5679 - regression_loss: 33.3958 - val_loss: 27.7038 - val_regression_loss: 21.8143 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 34.2650 - regression_loss: 30.5273 - val_loss: 26.5339 - val_regression_loss: 20.6324 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.2010 - regression_loss: 28.1217 - val_loss: 25.5143 - val_regression_loss: 19.7349 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.9565 - regression_loss: 26.7764 - val_loss: 25.0450 - val_regression_loss: 19.2860 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.5572 - regression_loss: 25.6926 - val_loss: 24.3338 - val_regression_loss: 18.7231 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.2947 - regression_loss: 25.0394 - val_loss: 24.0882 - val_regression_loss: 18.4883 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.4244 - regression_loss: 24.8818 - val_loss: 23.9640 - val_regression_loss: 18.3066 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.9744 - regression_loss: 23.8455 - val_loss: 23.8034 - val_regression_loss: 18.1910 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.7971 - regression_loss: 23.4629 - val_loss: 23.9930 - val_regression_loss: 18.3289 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.9096 - regression_loss: 23.0533 - val_loss: 24.0314 - val_regression_loss: 18.2818 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.3377 - regression_loss: 22.9534 - val_loss: 23.6935 - val_regression_loss: 18.0403 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.5562 - regression_loss: 22.3917 - val_loss: 23.6304 - val_regression_loss: 17.9676 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.2204 - regression_loss: 22.4534 - val_loss: 23.8357 - val_regression_loss: 18.0793 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.4168 - regression_loss: 22.2698 - val_loss: 23.8569 - val_regression_loss: 18.1290 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.6178 - regression_loss: 21.9134 - val_loss: 23.9027 - val_regression_loss: 18.1139 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.6818 - regression_loss: 21.8339 - val_loss: 23.8455 - val_regression_loss: 18.0908 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.9480 - regression_loss: 21.8227 - val_loss: 23.8359 - val_regression_loss: 18.0597 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.5170 - regression_loss: 21.6351 - val_loss: 23.8384 - val_regression_loss: 18.0412 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.2126 - regression_loss: 21.3551 - val_loss: 23.8499 - val_regression_loss: 18.1082 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.0693 - regression_loss: 21.3215 - val_loss: 23.8709 - val_regression_loss: 18.0617 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.5688 - regression_loss: 21.2267 - val_loss: 23.9639 - val_regression_loss: 18.1451 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.9595 - regression_loss: 21.2169 - val_loss: 24.0147 - val_regression_loss: 18.1346 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.9217 - regression_loss: 20.9976 - val_loss: 24.0988 - val_regression_loss: 18.2865 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.0935 - regression_loss: 20.9887 - val_loss: 24.0013 - val_regression_loss: 18.1483 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.7015 - regression_loss: 20.8243 - val_loss: 23.8882 - val_regression_loss: 18.1138 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.6101 - regression_loss: 20.6750 - val_loss: 23.9776 - val_regression_loss: 18.1290 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.2919 - regression_loss: 20.6958 - val_loss: 24.2519 - val_regression_loss: 18.3711 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.8081 - regression_loss: 20.8162 - val_loss: 23.9677 - val_regression_loss: 18.1138 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.2860 - regression_loss: 20.6405 - val_loss: 24.6906 - val_regression_loss: 18.7444 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.0139 - regression_loss: 20.7028 - val_loss: 24.8716 - val_regression_loss: 18.7866 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.1708 - regression_loss: 21.1339 - val_loss: 24.6748 - val_regression_loss: 18.7857 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.8826 - regression_loss: 20.7002 - val_loss: 24.6751 - val_regression_loss: 18.6069 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.5718 - regression_loss: 20.2935 - val_loss: 24.4672 - val_regression_loss: 18.5373 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.1282 - regression_loss: 20.4187 - val_loss: 24.3358 - val_regression_loss: 18.3730 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.9972 - regression_loss: 20.2444 - val_loss: 24.4111 - val_regression_loss: 18.4686 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.9691 - regression_loss: 19.9981 - val_loss: 24.3735 - val_regression_loss: 18.3996 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.6208 - regression_loss: 20.5077 - val_loss: 24.3650 - val_regression_loss: 18.4737 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.9401 - regression_loss: 20.3149 - val_loss: 24.2647 - val_regression_loss: 18.3692 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.2226 - regression_loss: 19.9417 - val_loss: 24.3015 - val_regression_loss: 18.3728 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.8798 - regression_loss: 20.0047 - val_loss: 24.2266 - val_regression_loss: 18.3505 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.8031 - regression_loss: 19.7725 - val_loss: 24.4706 - val_regression_loss: 18.5121 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.6862 - regression_loss: 19.7376 - val_loss: 24.5262 - val_regression_loss: 18.5667 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.7005 - regression_loss: 19.6741 - val_loss: 24.5392 - val_regression_loss: 18.5583 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.9355 - regression_loss: 19.7061 - val_loss: 24.6813 - val_regression_loss: 18.6777 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.6512 - regression_loss: 19.5990 - val_loss: 24.8036 - val_regression_loss: 18.7341 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.5340 - regression_loss: 19.5532 - val_loss: 25.2864 - val_regression_loss: 19.1915 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.8661 - regression_loss: 19.9629 - val_loss: 25.0250 - val_regression_loss: 18.9093 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.4080 - regression_loss: 19.5766 - val_loss: 25.0257 - val_regression_loss: 18.9078 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.8319 - regression_loss: 19.4856 - val_loss: 24.8897 - val_regression_loss: 18.8350 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.5364 - regression_loss: 19.4855 - val_loss: 24.8065 - val_regression_loss: 18.7741 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.0930 - regression_loss: 19.2654 - val_loss: 25.0011 - val_regression_loss: 18.8952 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.6982 - regression_loss: 19.2163 - val_loss: 25.1058 - val_regression_loss: 18.9871 - lr: 1.0000e-04\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 24ms/step - loss: 45.2729 - regression_loss: 40.3258 - val_loss: 40.0227 - val_regression_loss: 30.9523 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 36.4608 - regression_loss: 32.4427 - val_loss: 33.2419 - val_regression_loss: 25.3766 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 33.0335 - regression_loss: 29.1731 - val_loss: 32.5238 - val_regression_loss: 24.5251 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.3655 - regression_loss: 27.6645 - val_loss: 31.9036 - val_regression_loss: 23.8317 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.1062 - regression_loss: 26.5904 - val_loss: 30.7420 - val_regression_loss: 22.7856 - lr: 1.0000e-04\n",
      "Epoch 6/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 28.9438 - regression_loss: 25.4835 - val_loss: 30.0644 - val_regression_loss: 22.2650 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.9219 - regression_loss: 24.5592 - val_loss: 29.2118 - val_regression_loss: 21.5112 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.2000 - regression_loss: 23.8505 - val_loss: 28.5684 - val_regression_loss: 20.9957 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.6651 - regression_loss: 23.5097 - val_loss: 28.0017 - val_regression_loss: 20.5721 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.8930 - regression_loss: 22.9705 - val_loss: 27.5644 - val_regression_loss: 20.1425 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.1134 - regression_loss: 22.4976 - val_loss: 27.1007 - val_regression_loss: 19.8346 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.9722 - regression_loss: 22.1129 - val_loss: 26.7699 - val_regression_loss: 19.5054 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.5055 - regression_loss: 21.6441 - val_loss: 26.4507 - val_regression_loss: 19.1976 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.1863 - regression_loss: 21.4218 - val_loss: 25.9844 - val_regression_loss: 18.8193 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.7493 - regression_loss: 21.1070 - val_loss: 25.8666 - val_regression_loss: 18.6860 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.6293 - regression_loss: 20.8052 - val_loss: 25.5841 - val_regression_loss: 18.5191 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.8874 - regression_loss: 20.6429 - val_loss: 25.1978 - val_regression_loss: 18.2002 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.5141 - regression_loss: 20.4280 - val_loss: 25.0874 - val_regression_loss: 18.0864 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.2464 - regression_loss: 20.2180 - val_loss: 24.7254 - val_regression_loss: 17.8290 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.5019 - regression_loss: 20.4357 - val_loss: 24.4319 - val_regression_loss: 17.6326 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.8568 - regression_loss: 20.2160 - val_loss: 24.4750 - val_regression_loss: 17.6104 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.9409 - regression_loss: 19.9253 - val_loss: 24.0652 - val_regression_loss: 17.4072 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.7226 - regression_loss: 19.5708 - val_loss: 23.9131 - val_regression_loss: 17.2011 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.2167 - regression_loss: 19.5178 - val_loss: 23.7690 - val_regression_loss: 17.0712 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.0938 - regression_loss: 19.3592 - val_loss: 23.5437 - val_regression_loss: 16.8976 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.8119 - regression_loss: 19.2664 - val_loss: 23.4438 - val_regression_loss: 16.8844 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.1013 - regression_loss: 19.1447 - val_loss: 23.2934 - val_regression_loss: 16.7939 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.2808 - regression_loss: 19.1487 - val_loss: 23.1618 - val_regression_loss: 16.6473 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.6554 - regression_loss: 19.0738 - val_loss: 22.9674 - val_regression_loss: 16.5025 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.7418 - regression_loss: 18.9650 - val_loss: 22.8216 - val_regression_loss: 16.4734 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.1923 - regression_loss: 18.8756 - val_loss: 22.8111 - val_regression_loss: 16.4208 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.7335 - regression_loss: 18.8538 - val_loss: 22.9317 - val_regression_loss: 16.4939 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.0386 - regression_loss: 18.8094 - val_loss: 23.0070 - val_regression_loss: 16.6501 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 19.8576 - regression_loss: 18.3628\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.8266 - regression_loss: 18.9813 - val_loss: 23.3327 - val_regression_loss: 16.8155 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.0745 - regression_loss: 19.0684 - val_loss: 22.6392 - val_regression_loss: 16.3184 - lr: 5.0000e-05\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.7440 - regression_loss: 18.9172 - val_loss: 22.5396 - val_regression_loss: 16.2280 - lr: 5.0000e-05\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.0132 - regression_loss: 18.4798 - val_loss: 22.5117 - val_regression_loss: 16.1838 - lr: 5.0000e-05\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.3185 - regression_loss: 18.5185 - val_loss: 22.4245 - val_regression_loss: 16.1465 - lr: 5.0000e-05\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.1830 - regression_loss: 18.5860 - val_loss: 22.3603 - val_regression_loss: 16.1428 - lr: 5.0000e-05\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.3128 - regression_loss: 18.5244 - val_loss: 22.4427 - val_regression_loss: 16.1424 - lr: 5.0000e-05\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9410 - regression_loss: 18.3858 - val_loss: 22.4071 - val_regression_loss: 16.1599 - lr: 5.0000e-05\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.1258 - regression_loss: 18.4410 - val_loss: 22.3421 - val_regression_loss: 16.0975 - lr: 5.0000e-05\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.0373 - regression_loss: 18.4367 - val_loss: 22.3279 - val_regression_loss: 16.0406 - lr: 5.0000e-05\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9083 - regression_loss: 18.3689 - val_loss: 22.2758 - val_regression_loss: 16.0658 - lr: 5.0000e-05\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.0559 - regression_loss: 18.3381 - val_loss: 22.2553 - val_regression_loss: 16.0052 - lr: 5.0000e-05\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.0201 - regression_loss: 18.3007 - val_loss: 22.1597 - val_regression_loss: 15.9760 - lr: 5.0000e-05\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6860 - regression_loss: 18.2974 - val_loss: 22.1201 - val_regression_loss: 15.9320 - lr: 5.0000e-05\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.2230 - regression_loss: 18.2723 - val_loss: 22.0942 - val_regression_loss: 15.9093 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.2530 - regression_loss: 18.2727 - val_loss: 22.0721 - val_regression_loss: 15.9210 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9009 - regression_loss: 18.2029 - val_loss: 22.0681 - val_regression_loss: 15.9027 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.2505 - regression_loss: 18.2048 - val_loss: 22.0689 - val_regression_loss: 15.8913 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 21.3483 - regression_loss: 19.8546\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.1234 - regression_loss: 18.2274 - val_loss: 22.0586 - val_regression_loss: 15.8936 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5504 - regression_loss: 18.1036 - val_loss: 22.1030 - val_regression_loss: 15.9360 - lr: 2.5000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8447 - regression_loss: 18.1204 - val_loss: 22.0246 - val_regression_loss: 15.8639 - lr: 2.5000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7509 - regression_loss: 18.0441 - val_loss: 22.0539 - val_regression_loss: 15.8593 - lr: 2.5000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.2884 - regression_loss: 18.1106 - val_loss: 22.0134 - val_regression_loss: 15.8306 - lr: 2.5000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7060 - regression_loss: 18.0261 - val_loss: 22.0219 - val_regression_loss: 15.8563 - lr: 2.5000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.2409 - regression_loss: 18.0791 - val_loss: 21.9914 - val_regression_loss: 15.8662 - lr: 2.5000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.0870 - regression_loss: 18.0147 - val_loss: 21.9785 - val_regression_loss: 15.8413 - lr: 2.5000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5301 - regression_loss: 18.0552 - val_loss: 21.9566 - val_regression_loss: 15.8193 - lr: 2.5000e-05\n",
      "Epoch 61/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 20.8075 - regression_loss: 19.3140\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7193 - regression_loss: 17.9928 - val_loss: 21.9411 - val_regression_loss: 15.8049 - lr: 2.5000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.3547 - regression_loss: 17.9837 - val_loss: 21.9357 - val_regression_loss: 15.8062 - lr: 1.2500e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7741 - regression_loss: 17.9886 - val_loss: 21.9254 - val_regression_loss: 15.7907 - lr: 1.2500e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5832 - regression_loss: 17.9873 - val_loss: 21.9170 - val_regression_loss: 15.7844 - lr: 1.2500e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8458 - regression_loss: 17.9605 - val_loss: 21.9154 - val_regression_loss: 15.7913 - lr: 1.2500e-05\n",
      "Epoch 66/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.8984 - regression_loss: 17.4050\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8813 - regression_loss: 17.9659 - val_loss: 21.9199 - val_regression_loss: 15.7873 - lr: 1.2500e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6099 - regression_loss: 17.9496 - val_loss: 21.9170 - val_regression_loss: 15.7895 - lr: 6.2500e-06\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6234 - regression_loss: 17.9514 - val_loss: 21.9216 - val_regression_loss: 15.7893 - lr: 6.2500e-06\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4766 - regression_loss: 17.9387 - val_loss: 21.9230 - val_regression_loss: 15.7911 - lr: 6.2500e-06\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9531 - regression_loss: 17.9323 - val_loss: 21.9199 - val_regression_loss: 15.7882 - lr: 6.2500e-06\n",
      "Epoch 71/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 22.0791 - regression_loss: 20.5857\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6696 - regression_loss: 17.9316 - val_loss: 21.9174 - val_regression_loss: 15.7883 - lr: 6.2500e-06\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4227 - regression_loss: 17.9308 - val_loss: 21.9182 - val_regression_loss: 15.7871 - lr: 3.1250e-06\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5807 - regression_loss: 17.9242 - val_loss: 21.9138 - val_regression_loss: 15.7851 - lr: 3.1250e-06\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.8222 - regression_loss: 17.9249 - val_loss: 21.9113 - val_regression_loss: 15.7874 - lr: 3.1250e-06\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5920 - regression_loss: 17.9215 - val_loss: 21.9099 - val_regression_loss: 15.7856 - lr: 3.1250e-06\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.3522 - regression_loss: 17.9240 - val_loss: 21.9072 - val_regression_loss: 15.7827 - lr: 3.1250e-06\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9200 - regression_loss: 17.9178 - val_loss: 21.9086 - val_regression_loss: 15.7842 - lr: 3.1250e-06\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7419 - regression_loss: 17.9201 - val_loss: 21.9081 - val_regression_loss: 15.7801 - lr: 3.1250e-06\n",
      "Epoch 79/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.4180 - regression_loss: 16.9247\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.2698 - regression_loss: 17.9143 - val_loss: 21.9058 - val_regression_loss: 15.7773 - lr: 3.1250e-06\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7135 - regression_loss: 17.9136 - val_loss: 21.9053 - val_regression_loss: 15.7778 - lr: 1.5625e-06\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7727 - regression_loss: 17.9177 - val_loss: 21.9050 - val_regression_loss: 15.7764 - lr: 1.5625e-06\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6753 - regression_loss: 17.9126 - val_loss: 21.9039 - val_regression_loss: 15.7772 - lr: 1.5625e-06\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8377 - regression_loss: 17.9098 - val_loss: 21.9025 - val_regression_loss: 15.7770 - lr: 1.5625e-06\n",
      "Epoch 84/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 21.6842 - regression_loss: 20.1908\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8291 - regression_loss: 17.9105 - val_loss: 21.9028 - val_regression_loss: 15.7779 - lr: 1.5625e-06\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5109 - regression_loss: 17.9088 - val_loss: 21.9023 - val_regression_loss: 15.7773 - lr: 7.8125e-07\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7992 - regression_loss: 17.9089 - val_loss: 21.9024 - val_regression_loss: 15.7774 - lr: 7.8125e-07\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.2815 - regression_loss: 17.9075 - val_loss: 21.9023 - val_regression_loss: 15.7775 - lr: 7.8125e-07\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7978 - regression_loss: 17.9093 - val_loss: 21.9019 - val_regression_loss: 15.7778 - lr: 7.8125e-07\n",
      "Epoch 89/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 23.2796 - regression_loss: 21.7862\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9367 - regression_loss: 17.9080 - val_loss: 21.9010 - val_regression_loss: 15.7763 - lr: 7.8125e-07\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.0248 - regression_loss: 17.9062 - val_loss: 21.9009 - val_regression_loss: 15.7762 - lr: 3.9062e-07\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7032 - regression_loss: 17.9058 - val_loss: 21.9010 - val_regression_loss: 15.7763 - lr: 3.9062e-07\n",
      "Epoch 92/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6913 - regression_loss: 17.9075 - val_loss: 21.9007 - val_regression_loss: 15.7759 - lr: 3.9062e-07\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5991 - regression_loss: 17.9055 - val_loss: 21.9003 - val_regression_loss: 15.7760 - lr: 3.9062e-07\n",
      "Epoch 94/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 16.6455 - regression_loss: 15.1522\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4572 - regression_loss: 17.9055 - val_loss: 21.9000 - val_regression_loss: 15.7758 - lr: 3.9062e-07\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5519 - regression_loss: 17.9051 - val_loss: 21.8999 - val_regression_loss: 15.7757 - lr: 1.9531e-07\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6335 - regression_loss: 17.9049 - val_loss: 21.8998 - val_regression_loss: 15.7757 - lr: 1.9531e-07\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6120 - regression_loss: 17.9052 - val_loss: 21.8999 - val_regression_loss: 15.7757 - lr: 1.9531e-07\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5318 - regression_loss: 17.9046 - val_loss: 21.8997 - val_regression_loss: 15.7756 - lr: 1.9531e-07\n",
      "Epoch 99/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 25.2986 - regression_loss: 23.8053\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6335 - regression_loss: 17.9049 - val_loss: 21.8997 - val_regression_loss: 15.7756 - lr: 1.9531e-07\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6257 - regression_loss: 17.9046 - val_loss: 21.8997 - val_regression_loss: 15.7756 - lr: 9.7656e-08\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8022 - regression_loss: 17.9043 - val_loss: 21.8996 - val_regression_loss: 15.7755 - lr: 9.7656e-08\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6512 - regression_loss: 17.9046 - val_loss: 21.8996 - val_regression_loss: 15.7754 - lr: 9.7656e-08\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6606 - regression_loss: 17.9043 - val_loss: 21.8996 - val_regression_loss: 15.7753 - lr: 9.7656e-08\n",
      "Epoch 104/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 23.5554 - regression_loss: 22.0621\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5580 - regression_loss: 17.9043 - val_loss: 21.8996 - val_regression_loss: 15.7753 - lr: 9.7656e-08\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4260 - regression_loss: 17.9041 - val_loss: 21.8996 - val_regression_loss: 15.7753 - lr: 4.8828e-08\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.2568 - regression_loss: 17.9041 - val_loss: 21.8996 - val_regression_loss: 15.7753 - lr: 4.8828e-08\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4933 - regression_loss: 17.9041 - val_loss: 21.8995 - val_regression_loss: 15.7753 - lr: 4.8828e-08\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6156 - regression_loss: 17.9040 - val_loss: 21.8995 - val_regression_loss: 15.7753 - lr: 4.8828e-08\n",
      "Epoch 109/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.9876 - regression_loss: 17.4943\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9393 - regression_loss: 17.9040 - val_loss: 21.8995 - val_regression_loss: 15.7752 - lr: 4.8828e-08\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4636 - regression_loss: 17.9039 - val_loss: 21.8995 - val_regression_loss: 15.7753 - lr: 2.4414e-08\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6922 - regression_loss: 17.9039 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 2.4414e-08\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4903 - regression_loss: 17.9039 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 2.4414e-08\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4017 - regression_loss: 17.9039 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 2.4414e-08\n",
      "Epoch 114/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 16.7280 - regression_loss: 15.2347\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4355 - regression_loss: 17.9039 - val_loss: 21.8994 - val_regression_loss: 15.7753 - lr: 2.4414e-08\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7283 - regression_loss: 17.9039 - val_loss: 21.8994 - val_regression_loss: 15.7753 - lr: 1.2207e-08\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5845 - regression_loss: 17.9039 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 1.2207e-08\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.2303 - regression_loss: 17.9039 - val_loss: 21.8994 - val_regression_loss: 15.7753 - lr: 1.2207e-08\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9457 - regression_loss: 17.9039 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 1.2207e-08\n",
      "Epoch 119/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 21.2532 - regression_loss: 19.7599\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.9467 - regression_loss: 17.9039 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 1.2207e-08\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.3485 - regression_loss: 17.9039 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 6.1035e-09\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6498 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 6.1035e-09\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7075 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 6.1035e-09\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7851 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 6.1035e-09\n",
      "Epoch 124/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 17.4418 - regression_loss: 15.9484\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8574 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 6.1035e-09\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6417 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 3.0518e-09\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6892 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 3.0518e-09\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8235 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 3.0518e-09\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.1929 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 3.0518e-09\n",
      "Epoch 129/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 21.4021 - regression_loss: 19.9088\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 1.5258788677030566e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4273 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 3.0518e-09\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6735 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 1.5259e-09\n",
      "Epoch 131/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7214 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 1.5259e-09\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4462 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 1.5259e-09\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5563 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 1.5259e-09\n",
      "Epoch 134/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.1847 - regression_loss: 16.6914\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 7.629394338515283e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.2828 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 1.5259e-09\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4837 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 7.6294e-10\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7517 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 7.6294e-10\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4872 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 7.6294e-10\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6205 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 7.6294e-10\n",
      "Epoch 139/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 17.4318 - regression_loss: 15.9385\n",
      "Epoch 139: ReduceLROnPlateau reducing learning rate to 3.8146971692576415e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6383 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 7.6294e-10\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.3710 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 3.8147e-10\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4774 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 3.8147e-10\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7062 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 3.8147e-10\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7870 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 3.8147e-10\n",
      "Epoch 144/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 22.3259 - regression_loss: 20.8325\n",
      "Epoch 144: ReduceLROnPlateau reducing learning rate to 1.9073485846288207e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.1653 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 3.8147e-10\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7878 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 1.9073e-10\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4692 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 1.9073e-10\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6489 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 1.9073e-10\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6202 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 1.9073e-10\n",
      "Epoch 149/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 21.5413 - regression_loss: 20.0480\n",
      "Epoch 149: ReduceLROnPlateau reducing learning rate to 9.536742923144104e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5377 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 1.9073e-10\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6891 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 9.5367e-11\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5570 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 9.5367e-11\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.2005 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 9.5367e-11\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8039 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 9.5367e-11\n",
      "Epoch 154/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 22.4253 - regression_loss: 20.9320\n",
      "Epoch 154: ReduceLROnPlateau reducing learning rate to 4.768371461572052e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4819 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 9.5367e-11\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7483 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 4.7684e-11\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4892 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 4.7684e-11\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4217 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 4.7684e-11\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8673 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 4.7684e-11\n",
      "Epoch 159/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 21.3044 - regression_loss: 19.8111\n",
      "Epoch 159: ReduceLROnPlateau reducing learning rate to 2.384185730786026e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5881 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 4.7684e-11\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4438 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 2.3842e-11\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4481 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 2.3842e-11\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.3915 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 2.3842e-11\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7349 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 2.3842e-11\n",
      "Epoch 164/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.9508 - regression_loss: 17.4574\n",
      "Epoch 164: ReduceLROnPlateau reducing learning rate to 1.192092865393013e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4651 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 2.3842e-11\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8386 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 1.1921e-11\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5961 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 1.1921e-11\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.0842 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 1.1921e-11\n",
      "Epoch 168/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4460 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 1.1921e-11\n",
      "Epoch 169/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 25.3916 - regression_loss: 23.8983\n",
      "Epoch 169: ReduceLROnPlateau reducing learning rate to 5.960464326965065e-12.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7580 - regression_loss: 17.9038 - val_loss: 21.8994 - val_regression_loss: 15.7752 - lr: 1.1921e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 965us/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 52.3342 - regression_loss: 46.8980 - val_loss: 34.1724 - val_regression_loss: 25.4973 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 42.0538 - regression_loss: 37.5576 - val_loss: 29.0574 - val_regression_loss: 21.1511 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 38.3834 - regression_loss: 34.3457 - val_loss: 26.3991 - val_regression_loss: 19.1129 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 36.5040 - regression_loss: 32.2603 - val_loss: 25.6144 - val_regression_loss: 18.4863 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 35.0494 - regression_loss: 31.1876 - val_loss: 25.0133 - val_regression_loss: 18.1017 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 33.6011 - regression_loss: 29.6531 - val_loss: 24.5700 - val_regression_loss: 17.8559 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31.7067 - regression_loss: 28.3481 - val_loss: 24.4266 - val_regression_loss: 17.7910 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.9533 - regression_loss: 27.4671 - val_loss: 24.2971 - val_regression_loss: 17.7303 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.6484 - regression_loss: 26.8769 - val_loss: 24.2606 - val_regression_loss: 17.7000 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.8502 - regression_loss: 26.2945 - val_loss: 24.2491 - val_regression_loss: 17.7127 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.5437 - regression_loss: 25.8774 - val_loss: 23.7153 - val_regression_loss: 17.3927 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.7109 - regression_loss: 25.4464 - val_loss: 24.4080 - val_regression_loss: 17.8539 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.2958 - regression_loss: 25.0503 - val_loss: 23.7645 - val_regression_loss: 17.4436 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 27.5697 - regression_loss: 24.6136 - val_loss: 24.2568 - val_regression_loss: 17.8164 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.6830 - regression_loss: 24.4282 - val_loss: 23.7533 - val_regression_loss: 17.5122 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.4859 - regression_loss: 24.1896 - val_loss: 24.0353 - val_regression_loss: 17.7215 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.0249 - regression_loss: 24.0229 - val_loss: 24.4303 - val_regression_loss: 18.0233 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.7980 - regression_loss: 23.9811 - val_loss: 23.8518 - val_regression_loss: 17.6823 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.8480 - regression_loss: 23.8697 - val_loss: 24.7333 - val_regression_loss: 18.3070 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.6997 - regression_loss: 24.3116 - val_loss: 23.9592 - val_regression_loss: 17.8533 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.7811 - regression_loss: 23.7959 - val_loss: 24.4228 - val_regression_loss: 18.1265 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.5634 - regression_loss: 23.4271 - val_loss: 24.5806 - val_regression_loss: 18.3470 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.9825 - regression_loss: 23.5508 - val_loss: 25.9224 - val_regression_loss: 19.2843 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.7327 - regression_loss: 23.3667 - val_loss: 24.4703 - val_regression_loss: 18.2750 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.9976 - regression_loss: 23.5755 - val_loss: 26.1271 - val_regression_loss: 19.4525 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.1930 - regression_loss: 23.8278 - val_loss: 24.3381 - val_regression_loss: 18.2920 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.1134 - regression_loss: 23.6742 - val_loss: 26.2600 - val_regression_loss: 19.5845 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 26.5052 - regression_loss: 25.0183\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.8811 - regression_loss: 23.8950 - val_loss: 24.7046 - val_regression_loss: 18.5188 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.8916 - regression_loss: 22.9343 - val_loss: 24.7953 - val_regression_loss: 18.5872 - lr: 5.0000e-05\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.3296 - regression_loss: 22.7927 - val_loss: 25.6820 - val_regression_loss: 19.2053 - lr: 5.0000e-05\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.7567 - regression_loss: 22.7417 - val_loss: 24.9963 - val_regression_loss: 18.7357 - lr: 5.0000e-05\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.5941 - regression_loss: 22.6672 - val_loss: 24.9165 - val_regression_loss: 18.6910 - lr: 5.0000e-05\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.5190 - regression_loss: 22.6890 - val_loss: 25.2189 - val_regression_loss: 18.9009 - lr: 5.0000e-05\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.9831 - regression_loss: 22.6906 - val_loss: 25.3866 - val_regression_loss: 19.0104 - lr: 5.0000e-05\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.1987 - regression_loss: 22.6027 - val_loss: 24.9517 - val_regression_loss: 18.7316 - lr: 5.0000e-05\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.5354 - regression_loss: 22.6899 - val_loss: 25.2747 - val_regression_loss: 18.9760 - lr: 5.0000e-05\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.9883 - regression_loss: 22.6259 - val_loss: 25.2577 - val_regression_loss: 18.9691 - lr: 5.0000e-05\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.9559 - regression_loss: 22.5536 - val_loss: 25.8322 - val_regression_loss: 19.3777 - lr: 5.0000e-05\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.6360 - regression_loss: 22.5658 - val_loss: 25.1938 - val_regression_loss: 18.9451 - lr: 5.0000e-05\n",
      "Epoch 40/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 25.0550 - regression_loss: 23.5688\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.3903 - regression_loss: 22.5621 - val_loss: 25.2058 - val_regression_loss: 18.9411 - lr: 5.0000e-05\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.0527 - regression_loss: 22.4851 - val_loss: 25.4899 - val_regression_loss: 19.1400 - lr: 2.5000e-05\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.1161 - regression_loss: 22.4402 - val_loss: 25.2597 - val_regression_loss: 18.9832 - lr: 2.5000e-05\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.4218 - regression_loss: 22.4502 - val_loss: 25.2244 - val_regression_loss: 18.9563 - lr: 2.5000e-05\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.3395 - regression_loss: 22.4137 - val_loss: 25.2614 - val_regression_loss: 18.9915 - lr: 2.5000e-05\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.1020 - regression_loss: 22.4151 - val_loss: 25.2689 - val_regression_loss: 19.0114 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 33.0963 - regression_loss: 31.6102\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.7556 - regression_loss: 22.4826 - val_loss: 25.4528 - val_regression_loss: 19.1363 - lr: 2.5000e-05\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.1735 - regression_loss: 22.3697 - val_loss: 25.2881 - val_regression_loss: 19.0271 - lr: 1.2500e-05\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.5434 - regression_loss: 22.3579 - val_loss: 25.2808 - val_regression_loss: 19.0243 - lr: 1.2500e-05\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.3453 - regression_loss: 22.3599 - val_loss: 25.3826 - val_regression_loss: 19.0948 - lr: 1.2500e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.5204 - regression_loss: 22.3473 - val_loss: 25.3745 - val_regression_loss: 19.0912 - lr: 1.2500e-05\n",
      "Epoch 51/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 25.9000 - regression_loss: 24.4140\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.2297 - regression_loss: 22.3574 - val_loss: 25.2655 - val_regression_loss: 19.0164 - lr: 1.2500e-05\n",
      "3/3 [==============================] - 0s 960us/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 115.8878 - regression_loss: 104.8591 - val_loss: 65.2141 - val_regression_loss: 51.4761 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 77.2716 - regression_loss: 74.1837 - val_loss: 43.9489 - val_regression_loss: 35.0340 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 60.2667 - regression_loss: 56.9944 - val_loss: 36.9024 - val_regression_loss: 29.2783 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 52.6876 - regression_loss: 47.3580 - val_loss: 32.5084 - val_regression_loss: 25.8934 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 44.6881 - regression_loss: 41.5239 - val_loss: 27.5304 - val_regression_loss: 22.0035 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 40.8533 - regression_loss: 36.5327 - val_loss: 25.4077 - val_regression_loss: 20.3275 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 36.6694 - regression_loss: 33.1336 - val_loss: 21.3729 - val_regression_loss: 17.1371 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.7020 - regression_loss: 29.2863 - val_loss: 18.1295 - val_regression_loss: 14.5127 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.5403 - regression_loss: 26.1789 - val_loss: 17.6718 - val_regression_loss: 14.1591 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.0887 - regression_loss: 23.8452 - val_loss: 14.6122 - val_regression_loss: 11.5879 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.8744 - regression_loss: 21.3983 - val_loss: 13.7660 - val_regression_loss: 10.8968 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.5876 - regression_loss: 19.2840 - val_loss: 11.9919 - val_regression_loss: 9.3272 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.7209 - regression_loss: 17.2081 - val_loss: 11.2159 - val_regression_loss: 8.6544 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.5025 - regression_loss: 15.5908 - val_loss: 9.7510 - val_regression_loss: 7.3519 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.3483 - regression_loss: 13.9631 - val_loss: 9.1098 - val_regression_loss: 6.6857 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.1321 - regression_loss: 12.6125 - val_loss: 8.1698 - val_regression_loss: 5.8594 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7423 - regression_loss: 11.2342 - val_loss: 7.5400 - val_regression_loss: 5.2926 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7705 - regression_loss: 10.3083 - val_loss: 6.8401 - val_regression_loss: 4.6185 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6662 - regression_loss: 9.4052 - val_loss: 6.8142 - val_regression_loss: 4.5748 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6236 - regression_loss: 8.3435 - val_loss: 5.9106 - val_regression_loss: 3.7084 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5773 - regression_loss: 7.3717 - val_loss: 5.9633 - val_regression_loss: 3.8043 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8954 - regression_loss: 6.8112 - val_loss: 5.4925 - val_regression_loss: 3.3051 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4865 - regression_loss: 6.1519 - val_loss: 5.2774 - val_regression_loss: 3.1644 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8757 - regression_loss: 5.8800 - val_loss: 5.1262 - val_regression_loss: 2.9662 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.1733 - regression_loss: 5.3232 - val_loss: 4.9604 - val_regression_loss: 2.8657 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6057 - regression_loss: 4.7156 - val_loss: 4.7050 - val_regression_loss: 2.6152 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.3287 - regression_loss: 4.4667 - val_loss: 4.5480 - val_regression_loss: 2.4990 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.0812 - regression_loss: 4.2407 - val_loss: 4.5502 - val_regression_loss: 2.4934 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6850 - regression_loss: 3.8906 - val_loss: 4.3940 - val_regression_loss: 2.3670 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9407 - regression_loss: 3.5843 - val_loss: 4.2259 - val_regression_loss: 2.2099 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1981 - regression_loss: 3.4039 - val_loss: 4.2723 - val_regression_loss: 2.2403 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9965 - regression_loss: 3.2665 - val_loss: 4.2451 - val_regression_loss: 2.2585 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8952 - regression_loss: 3.1101 - val_loss: 3.9960 - val_regression_loss: 2.0193 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6392 - regression_loss: 2.8832 - val_loss: 3.9570 - val_regression_loss: 2.0053 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5207 - regression_loss: 2.7485 - val_loss: 3.9023 - val_regression_loss: 1.9313 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0552 - regression_loss: 2.6296 - val_loss: 3.7539 - val_regression_loss: 1.8212 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2386 - regression_loss: 2.5600 - val_loss: 3.7185 - val_regression_loss: 1.7921 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.1162 - regression_loss: 2.4397 - val_loss: 3.7241 - val_regression_loss: 1.7631 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0481 - regression_loss: 2.3825 - val_loss: 3.6774 - val_regression_loss: 1.7650 - lr: 1.0000e-04\n",
      "Epoch 40/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8457 - regression_loss: 2.4116 - val_loss: 3.5417 - val_regression_loss: 1.6095 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8789 - regression_loss: 2.2126 - val_loss: 3.6135 - val_regression_loss: 1.7038 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8009 - regression_loss: 2.1574 - val_loss: 3.4834 - val_regression_loss: 1.5636 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6891 - regression_loss: 2.0361 - val_loss: 3.3533 - val_regression_loss: 1.4763 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5793 - regression_loss: 1.9619 - val_loss: 3.3038 - val_regression_loss: 1.4243 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5232 - regression_loss: 1.9082 - val_loss: 3.2940 - val_regression_loss: 1.4270 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5209 - regression_loss: 1.8704 - val_loss: 3.2898 - val_regression_loss: 1.4001 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4616 - regression_loss: 1.8363 - val_loss: 3.2854 - val_regression_loss: 1.4360 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4636 - regression_loss: 1.8022 - val_loss: 3.1456 - val_regression_loss: 1.2914 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3474 - regression_loss: 1.7391 - val_loss: 3.1746 - val_regression_loss: 1.3289 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2820 - regression_loss: 1.6543 - val_loss: 3.0406 - val_regression_loss: 1.2110 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2383 - regression_loss: 1.6120 - val_loss: 3.0176 - val_regression_loss: 1.1905 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1738 - regression_loss: 1.5608 - val_loss: 2.9759 - val_regression_loss: 1.1632 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1683 - regression_loss: 1.5391 - val_loss: 3.0145 - val_regression_loss: 1.1963 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9733 - regression_loss: 1.4934 - val_loss: 2.9676 - val_regression_loss: 1.1424 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0404 - regression_loss: 1.4609 - val_loss: 2.9347 - val_regression_loss: 1.1335 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9128 - regression_loss: 1.4269 - val_loss: 2.8092 - val_regression_loss: 1.0264 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9265 - regression_loss: 1.3545 - val_loss: 2.8426 - val_regression_loss: 1.0525 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9735 - regression_loss: 1.3651 - val_loss: 2.8413 - val_regression_loss: 1.0604 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9128 - regression_loss: 1.3032 - val_loss: 2.7668 - val_regression_loss: 0.9876 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8630 - regression_loss: 1.2945 - val_loss: 2.7654 - val_regression_loss: 1.0026 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7176 - regression_loss: 1.2471 - val_loss: 2.6857 - val_regression_loss: 0.9227 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6815 - regression_loss: 1.2188 - val_loss: 2.6734 - val_regression_loss: 0.9169 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7302 - regression_loss: 1.1828 - val_loss: 2.6608 - val_regression_loss: 0.9045 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8448 - regression_loss: 1.2672 - val_loss: 2.6376 - val_regression_loss: 0.8979 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6769 - regression_loss: 1.1139 - val_loss: 2.6092 - val_regression_loss: 0.8681 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6729 - regression_loss: 1.0864 - val_loss: 2.6163 - val_regression_loss: 0.8846 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6023 - regression_loss: 1.0709 - val_loss: 2.5338 - val_regression_loss: 0.8118 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6106 - regression_loss: 1.0320 - val_loss: 2.5432 - val_regression_loss: 0.8250 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5238 - regression_loss: 1.0513 - val_loss: 2.4804 - val_regression_loss: 0.7679 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5939 - regression_loss: 1.0335 - val_loss: 2.4971 - val_regression_loss: 0.7790 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5538 - regression_loss: 0.9794 - val_loss: 2.5334 - val_regression_loss: 0.8264 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4598 - regression_loss: 0.9744 - val_loss: 2.4388 - val_regression_loss: 0.7411 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4564 - regression_loss: 0.9102 - val_loss: 2.4560 - val_regression_loss: 0.7612 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4112 - regression_loss: 0.9156 - val_loss: 2.3737 - val_regression_loss: 0.6920 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4460 - regression_loss: 0.8873 - val_loss: 2.3581 - val_regression_loss: 0.6804 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3732 - regression_loss: 0.8679 - val_loss: 2.4199 - val_regression_loss: 0.7401 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4326 - regression_loss: 0.8670 - val_loss: 2.3129 - val_regression_loss: 0.6448 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4079 - regression_loss: 0.8493 - val_loss: 2.3102 - val_regression_loss: 0.6414 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3670 - regression_loss: 0.8495 - val_loss: 2.3349 - val_regression_loss: 0.6722 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3741 - regression_loss: 0.8251 - val_loss: 2.3194 - val_regression_loss: 0.6605 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3068 - regression_loss: 0.7806 - val_loss: 2.2671 - val_regression_loss: 0.6104 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3146 - regression_loss: 0.7815 - val_loss: 2.2457 - val_regression_loss: 0.5960 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3288 - regression_loss: 0.7763 - val_loss: 2.4300 - val_regression_loss: 0.7627 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3581 - regression_loss: 0.8133 - val_loss: 2.2025 - val_regression_loss: 0.5641 - lr: 1.0000e-04\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2518 - regression_loss: 0.7244 - val_loss: 2.1776 - val_regression_loss: 0.5502 - lr: 1.0000e-04\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2024 - regression_loss: 0.7003 - val_loss: 2.1672 - val_regression_loss: 0.5387 - lr: 1.0000e-04\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2283 - regression_loss: 0.7025 - val_loss: 2.1672 - val_regression_loss: 0.5389 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2256 - regression_loss: 0.7112 - val_loss: 2.1549 - val_regression_loss: 0.5349 - lr: 1.0000e-04\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1923 - regression_loss: 0.6584 - val_loss: 2.1439 - val_regression_loss: 0.5267 - lr: 1.0000e-04\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1816 - regression_loss: 0.6537 - val_loss: 2.1451 - val_regression_loss: 0.5304 - lr: 1.0000e-04\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1780 - regression_loss: 0.6499 - val_loss: 2.1254 - val_regression_loss: 0.5151 - lr: 1.0000e-04\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1774 - regression_loss: 0.6606 - val_loss: 2.1131 - val_regression_loss: 0.4967 - lr: 1.0000e-04\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1552 - regression_loss: 0.6470 - val_loss: 2.1861 - val_regression_loss: 0.5767 - lr: 1.0000e-04\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1675 - regression_loss: 0.6368 - val_loss: 2.0708 - val_regression_loss: 0.4697 - lr: 1.0000e-04\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1314 - regression_loss: 0.6068 - val_loss: 2.0613 - val_regression_loss: 0.4620 - lr: 1.0000e-04\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1102 - regression_loss: 0.6078 - val_loss: 2.1795 - val_regression_loss: 0.5748 - lr: 1.0000e-04\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2068 - regression_loss: 0.6683 - val_loss: 2.1643 - val_regression_loss: 0.5604 - lr: 1.0000e-04\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1363 - regression_loss: 0.6360 - val_loss: 2.0477 - val_regression_loss: 0.4497 - lr: 1.0000e-04\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1463 - regression_loss: 0.6424 - val_loss: 2.0914 - val_regression_loss: 0.4846 - lr: 1.0000e-04\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1163 - regression_loss: 0.6034 - val_loss: 2.0293 - val_regression_loss: 0.4483 - lr: 1.0000e-04\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0649 - regression_loss: 0.5525 - val_loss: 2.1299 - val_regression_loss: 0.5403 - lr: 1.0000e-04\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1407 - regression_loss: 0.6074 - val_loss: 1.9991 - val_regression_loss: 0.4211 - lr: 1.0000e-04\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0387 - regression_loss: 0.5226 - val_loss: 2.0087 - val_regression_loss: 0.4321 - lr: 1.0000e-04\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0457 - regression_loss: 0.5239 - val_loss: 2.0170 - val_regression_loss: 0.4399 - lr: 1.0000e-04\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0274 - regression_loss: 0.5129 - val_loss: 1.9992 - val_regression_loss: 0.4221 - lr: 1.0000e-04\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0401 - regression_loss: 0.5187 - val_loss: 1.9900 - val_regression_loss: 0.4215 - lr: 1.0000e-04\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0037 - regression_loss: 0.4920 - val_loss: 1.9721 - val_regression_loss: 0.4027 - lr: 1.0000e-04\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0024 - regression_loss: 0.4916 - val_loss: 1.9986 - val_regression_loss: 0.4317 - lr: 1.0000e-04\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9987 - regression_loss: 0.5032 - val_loss: 1.9828 - val_regression_loss: 0.4193 - lr: 1.0000e-04\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9790 - regression_loss: 0.4705 - val_loss: 1.9496 - val_regression_loss: 0.3878 - lr: 1.0000e-04\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9591 - regression_loss: 0.4815 - val_loss: 1.9458 - val_regression_loss: 0.3866 - lr: 1.0000e-04\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9982 - regression_loss: 0.4818 - val_loss: 1.9497 - val_regression_loss: 0.3837 - lr: 1.0000e-04\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0004 - regression_loss: 0.4897 - val_loss: 1.9593 - val_regression_loss: 0.3928 - lr: 1.0000e-04\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9812 - regression_loss: 0.4893 - val_loss: 1.9896 - val_regression_loss: 0.4331 - lr: 1.0000e-04\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9483 - regression_loss: 0.4557 - val_loss: 1.9216 - val_regression_loss: 0.3633 - lr: 1.0000e-04\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9560 - regression_loss: 0.4628 - val_loss: 1.9112 - val_regression_loss: 0.3607 - lr: 1.0000e-04\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9383 - regression_loss: 0.4451 - val_loss: 1.9138 - val_regression_loss: 0.3648 - lr: 1.0000e-04\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9313 - regression_loss: 0.4365 - val_loss: 1.9255 - val_regression_loss: 0.3713 - lr: 1.0000e-04\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9314 - regression_loss: 0.4382 - val_loss: 1.9355 - val_regression_loss: 0.3896 - lr: 1.0000e-04\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9039 - regression_loss: 0.4234 - val_loss: 1.9175 - val_regression_loss: 0.3628 - lr: 1.0000e-04\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9658 - regression_loss: 0.4597 - val_loss: 1.8990 - val_regression_loss: 0.3560 - lr: 1.0000e-04\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9566 - regression_loss: 0.4441 - val_loss: 2.0605 - val_regression_loss: 0.5055 - lr: 1.0000e-04\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9454 - regression_loss: 0.4409 - val_loss: 1.8999 - val_regression_loss: 0.3508 - lr: 1.0000e-04\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9328 - regression_loss: 0.4304 - val_loss: 1.8973 - val_regression_loss: 0.3526 - lr: 1.0000e-04\n",
      "Epoch 125/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.0622 - regression_loss: 0.5911\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9444 - regression_loss: 0.4377 - val_loss: 1.9248 - val_regression_loss: 0.3881 - lr: 1.0000e-04\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9041 - regression_loss: 0.4042 - val_loss: 1.8788 - val_regression_loss: 0.3400 - lr: 5.0000e-05\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8930 - regression_loss: 0.3891 - val_loss: 1.9127 - val_regression_loss: 0.3779 - lr: 5.0000e-05\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9000 - regression_loss: 0.3956 - val_loss: 1.9023 - val_regression_loss: 0.3544 - lr: 5.0000e-05\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8884 - regression_loss: 0.4029 - val_loss: 1.9532 - val_regression_loss: 0.4155 - lr: 5.0000e-05\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8868 - regression_loss: 0.3973 - val_loss: 1.8976 - val_regression_loss: 0.3525 - lr: 5.0000e-05\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9112 - regression_loss: 0.4073 - val_loss: 1.9300 - val_regression_loss: 0.3943 - lr: 5.0000e-05\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9137 - regression_loss: 0.4082 - val_loss: 1.8711 - val_regression_loss: 0.3349 - lr: 5.0000e-05\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8722 - regression_loss: 0.3712 - val_loss: 1.9057 - val_regression_loss: 0.3729 - lr: 5.0000e-05\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8776 - regression_loss: 0.3841 - val_loss: 1.8641 - val_regression_loss: 0.3288 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8655 - regression_loss: 0.3731 - val_loss: 1.8719 - val_regression_loss: 0.3419 - lr: 5.0000e-05\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8762 - regression_loss: 0.3720 - val_loss: 1.8718 - val_regression_loss: 0.3414 - lr: 5.0000e-05\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8649 - regression_loss: 0.3673 - val_loss: 1.8600 - val_regression_loss: 0.3277 - lr: 5.0000e-05\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8546 - regression_loss: 0.3635 - val_loss: 1.8709 - val_regression_loss: 0.3420 - lr: 5.0000e-05\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8629 - regression_loss: 0.3657 - val_loss: 1.8634 - val_regression_loss: 0.3347 - lr: 5.0000e-05\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8505 - regression_loss: 0.3645 - val_loss: 1.8710 - val_regression_loss: 0.3436 - lr: 5.0000e-05\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8665 - regression_loss: 0.3692 - val_loss: 1.8602 - val_regression_loss: 0.3262 - lr: 5.0000e-05\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8560 - regression_loss: 0.3689 - val_loss: 1.8776 - val_regression_loss: 0.3505 - lr: 5.0000e-05\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8538 - regression_loss: 0.3580 - val_loss: 1.8562 - val_regression_loss: 0.3252 - lr: 5.0000e-05\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8654 - regression_loss: 0.3800 - val_loss: 1.8644 - val_regression_loss: 0.3384 - lr: 5.0000e-05\n",
      "Epoch 145/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.8456 - regression_loss: 0.3770\n",
      "Epoch 145: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8573 - regression_loss: 0.3575 - val_loss: 1.8469 - val_regression_loss: 0.3221 - lr: 5.0000e-05\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8370 - regression_loss: 0.3506 - val_loss: 1.8479 - val_regression_loss: 0.3233 - lr: 2.5000e-05\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8448 - regression_loss: 0.3485 - val_loss: 1.8470 - val_regression_loss: 0.3223 - lr: 2.5000e-05\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8419 - regression_loss: 0.3451 - val_loss: 1.8498 - val_regression_loss: 0.3253 - lr: 2.5000e-05\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8418 - regression_loss: 0.3489 - val_loss: 1.8469 - val_regression_loss: 0.3217 - lr: 2.5000e-05\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8454 - regression_loss: 0.3478 - val_loss: 1.8487 - val_regression_loss: 0.3239 - lr: 2.5000e-05\n",
      "Epoch 151/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.9117 - regression_loss: 0.4435\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8395 - regression_loss: 0.3433 - val_loss: 1.8540 - val_regression_loss: 0.3303 - lr: 2.5000e-05\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8428 - regression_loss: 0.3439 - val_loss: 1.8471 - val_regression_loss: 0.3233 - lr: 1.2500e-05\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8366 - regression_loss: 0.3416 - val_loss: 1.8468 - val_regression_loss: 0.3233 - lr: 1.2500e-05\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8351 - regression_loss: 0.3412 - val_loss: 1.8425 - val_regression_loss: 0.3181 - lr: 1.2500e-05\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8325 - regression_loss: 0.3401 - val_loss: 1.8458 - val_regression_loss: 0.3225 - lr: 1.2500e-05\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8372 - regression_loss: 0.3396 - val_loss: 1.8475 - val_regression_loss: 0.3242 - lr: 1.2500e-05\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8282 - regression_loss: 0.3392 - val_loss: 1.8433 - val_regression_loss: 0.3195 - lr: 1.2500e-05\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8279 - regression_loss: 0.3403 - val_loss: 1.8447 - val_regression_loss: 0.3214 - lr: 1.2500e-05\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8333 - regression_loss: 0.3379 - val_loss: 1.8478 - val_regression_loss: 0.3251 - lr: 1.2500e-05\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8387 - regression_loss: 0.3401 - val_loss: 1.8482 - val_regression_loss: 0.3257 - lr: 1.2500e-05\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8353 - regression_loss: 0.3379 - val_loss: 1.8417 - val_regression_loss: 0.3181 - lr: 1.2500e-05\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8147 - regression_loss: 0.3377 - val_loss: 1.8425 - val_regression_loss: 0.3194 - lr: 1.2500e-05\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8315 - regression_loss: 0.3365 - val_loss: 1.8484 - val_regression_loss: 0.3262 - lr: 1.2500e-05\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8284 - regression_loss: 0.3369 - val_loss: 1.8438 - val_regression_loss: 0.3210 - lr: 1.2500e-05\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8275 - regression_loss: 0.3357 - val_loss: 1.8412 - val_regression_loss: 0.3182 - lr: 1.2500e-05\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8259 - regression_loss: 0.3375 - val_loss: 1.8434 - val_regression_loss: 0.3216 - lr: 1.2500e-05\n",
      "Epoch 167/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.9326 - regression_loss: 0.4650\n",
      "Epoch 167: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8310 - regression_loss: 0.3360 - val_loss: 1.8436 - val_regression_loss: 0.3219 - lr: 1.2500e-05\n",
      "Epoch 168/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8275 - regression_loss: 0.3340 - val_loss: 1.8430 - val_regression_loss: 0.3211 - lr: 6.2500e-06\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8206 - regression_loss: 0.3341 - val_loss: 1.8415 - val_regression_loss: 0.3195 - lr: 6.2500e-06\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8175 - regression_loss: 0.3335 - val_loss: 1.8414 - val_regression_loss: 0.3194 - lr: 6.2500e-06\n",
      "Epoch 171/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8221 - regression_loss: 0.3336 - val_loss: 1.8421 - val_regression_loss: 0.3202 - lr: 6.2500e-06\n",
      "Epoch 172/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.8814 - regression_loss: 0.4139\n",
      "Epoch 172: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8236 - regression_loss: 0.3331 - val_loss: 1.8422 - val_regression_loss: 0.3204 - lr: 6.2500e-06\n",
      "Epoch 173/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8241 - regression_loss: 0.3330 - val_loss: 1.8413 - val_regression_loss: 0.3195 - lr: 3.1250e-06\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8173 - regression_loss: 0.3326 - val_loss: 1.8412 - val_regression_loss: 0.3194 - lr: 3.1250e-06\n",
      "Epoch 175/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8182 - regression_loss: 0.3325 - val_loss: 1.8419 - val_regression_loss: 0.3203 - lr: 3.1250e-06\n",
      "Epoch 176/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8160 - regression_loss: 0.3326 - val_loss: 1.8413 - val_regression_loss: 0.3197 - lr: 3.1250e-06\n",
      "Epoch 177/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.8023 - regression_loss: 0.3348\n",
      "Epoch 177: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8225 - regression_loss: 0.3334 - val_loss: 1.8405 - val_regression_loss: 0.3188 - lr: 3.1250e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8284 - regression_loss: 0.3320 - val_loss: 1.8409 - val_regression_loss: 0.3193 - lr: 1.5625e-06\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8215 - regression_loss: 0.3318 - val_loss: 1.8411 - val_regression_loss: 0.3196 - lr: 1.5625e-06\n",
      "Epoch 180/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8223 - regression_loss: 0.3318 - val_loss: 1.8415 - val_regression_loss: 0.3201 - lr: 1.5625e-06\n",
      "Epoch 181/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8245 - regression_loss: 0.3317 - val_loss: 1.8416 - val_regression_loss: 0.3201 - lr: 1.5625e-06\n",
      "Epoch 182/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.7468 - regression_loss: 0.2794\n",
      "Epoch 182: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8224 - regression_loss: 0.3318 - val_loss: 1.8418 - val_regression_loss: 0.3204 - lr: 1.5625e-06\n",
      "Epoch 183/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8161 - regression_loss: 0.3317 - val_loss: 1.8416 - val_regression_loss: 0.3202 - lr: 7.8125e-07\n",
      "Epoch 184/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8264 - regression_loss: 0.3316 - val_loss: 1.8414 - val_regression_loss: 0.3200 - lr: 7.8125e-07\n",
      "Epoch 185/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8234 - regression_loss: 0.3317 - val_loss: 1.8410 - val_regression_loss: 0.3195 - lr: 7.8125e-07\n",
      "Epoch 186/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8216 - regression_loss: 0.3315 - val_loss: 1.8409 - val_regression_loss: 0.3194 - lr: 7.8125e-07\n",
      "Epoch 187/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8146 - regression_loss: 0.3315 - val_loss: 1.8408 - val_regression_loss: 0.3193 - lr: 7.8125e-07\n",
      "Epoch 188/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8187 - regression_loss: 0.3315 - val_loss: 1.8408 - val_regression_loss: 0.3193 - lr: 7.8125e-07\n",
      "Epoch 189/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8192 - regression_loss: 0.3314 - val_loss: 1.8409 - val_regression_loss: 0.3194 - lr: 7.8125e-07\n",
      "Epoch 190/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8197 - regression_loss: 0.3314 - val_loss: 1.8409 - val_regression_loss: 0.3195 - lr: 7.8125e-07\n",
      "Epoch 191/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8214 - regression_loss: 0.3314 - val_loss: 1.8409 - val_regression_loss: 0.3194 - lr: 7.8125e-07\n",
      "Epoch 192/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.8044 - regression_loss: 0.3369\n",
      "Epoch 192: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8285 - regression_loss: 0.3314 - val_loss: 1.8408 - val_regression_loss: 0.3193 - lr: 7.8125e-07\n",
      "Epoch 193/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8255 - regression_loss: 0.3313 - val_loss: 1.8408 - val_regression_loss: 0.3194 - lr: 3.9062e-07\n",
      "Epoch 194/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8272 - regression_loss: 0.3313 - val_loss: 1.8408 - val_regression_loss: 0.3193 - lr: 3.9062e-07\n",
      "Epoch 195/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8220 - regression_loss: 0.3313 - val_loss: 1.8408 - val_regression_loss: 0.3194 - lr: 3.9062e-07\n",
      "Epoch 196/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8279 - regression_loss: 0.3313 - val_loss: 1.8408 - val_regression_loss: 0.3194 - lr: 3.9062e-07\n",
      "Epoch 197/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.9036 - regression_loss: 0.4362\n",
      "Epoch 197: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8245 - regression_loss: 0.3312 - val_loss: 1.8408 - val_regression_loss: 0.3194 - lr: 3.9062e-07\n",
      "Epoch 198/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8279 - regression_loss: 0.3312 - val_loss: 1.8408 - val_regression_loss: 0.3194 - lr: 1.9531e-07\n",
      "Epoch 199/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8200 - regression_loss: 0.3312 - val_loss: 1.8408 - val_regression_loss: 0.3194 - lr: 1.9531e-07\n",
      "Epoch 200/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.8130 - regression_loss: 0.3312 - val_loss: 1.8407 - val_regression_loss: 0.3193 - lr: 1.9531e-07\n",
      "Epoch 201/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8225 - regression_loss: 0.3312 - val_loss: 1.8408 - val_regression_loss: 0.3194 - lr: 1.9531e-07\n",
      "Epoch 202/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8302 - regression_loss: 0.3312 - val_loss: 1.8408 - val_regression_loss: 0.3194 - lr: 1.9531e-07\n",
      "Epoch 203/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8191 - regression_loss: 0.3312 - val_loss: 1.8408 - val_regression_loss: 0.3194 - lr: 1.9531e-07\n",
      "Epoch 204/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8154 - regression_loss: 0.3312 - val_loss: 1.8408 - val_regression_loss: 0.3194 - lr: 1.9531e-07\n",
      "Epoch 205/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.9079 - regression_loss: 0.4405\n",
      "Epoch 205: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8181 - regression_loss: 0.3312 - val_loss: 1.8407 - val_regression_loss: 0.3194 - lr: 1.9531e-07\n",
      "Epoch 206/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8230 - regression_loss: 0.3312 - val_loss: 1.8408 - val_regression_loss: 0.3194 - lr: 9.7656e-08\n",
      "Epoch 207/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8109 - regression_loss: 0.3312 - val_loss: 1.8407 - val_regression_loss: 0.3193 - lr: 9.7656e-08\n",
      "Epoch 208/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8235 - regression_loss: 0.3311 - val_loss: 1.8407 - val_regression_loss: 0.3193 - lr: 9.7656e-08\n",
      "Epoch 209/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8243 - regression_loss: 0.3311 - val_loss: 1.8407 - val_regression_loss: 0.3193 - lr: 9.7656e-08\n",
      "Epoch 210/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8165 - regression_loss: 0.3311 - val_loss: 1.8407 - val_regression_loss: 0.3194 - lr: 9.7656e-08\n",
      "Epoch 211/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8195 - regression_loss: 0.3311 - val_loss: 1.8407 - val_regression_loss: 0.3193 - lr: 9.7656e-08\n",
      "Epoch 212/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.7969 - regression_loss: 0.3295\n",
      "Epoch 212: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8295 - regression_loss: 0.3311 - val_loss: 1.8407 - val_regression_loss: 0.3194 - lr: 9.7656e-08\n",
      "Epoch 213/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8266 - regression_loss: 0.3311 - val_loss: 1.8407 - val_regression_loss: 0.3194 - lr: 4.8828e-08\n",
      "Epoch 214/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8226 - regression_loss: 0.3311 - val_loss: 1.8407 - val_regression_loss: 0.3194 - lr: 4.8828e-08\n",
      "Epoch 215/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8216 - regression_loss: 0.3311 - val_loss: 1.8407 - val_regression_loss: 0.3193 - lr: 4.8828e-08\n",
      "Epoch 216/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8216 - regression_loss: 0.3311 - val_loss: 1.8407 - val_regression_loss: 0.3193 - lr: 4.8828e-08\n",
      "Epoch 217/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.7389 - regression_loss: 0.2715\n",
      "Epoch 217: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8210 - regression_loss: 0.3311 - val_loss: 1.8407 - val_regression_loss: 0.3193 - lr: 4.8828e-08\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 166.4228 - regression_loss: 154.6838 - val_loss: 138.8033 - val_regression_loss: 102.4633 - lr: 1.0000e-04\n",
      "Epoch 2/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 105.4794 - regression_loss: 97.3301 - val_loss: 101.2080 - val_regression_loss: 73.9030 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 79.0162 - regression_loss: 71.1893 - val_loss: 85.6413 - val_regression_loss: 61.8990 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 56.3056 - regression_loss: 57.4960 - val_loss: 71.3501 - val_regression_loss: 51.4511 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 53.3506 - regression_loss: 48.4797 - val_loss: 61.1629 - val_regression_loss: 43.9537 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 47.6679 - regression_loss: 42.0654 - val_loss: 53.1397 - val_regression_loss: 38.0027 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 40.5317 - regression_loss: 35.4914 - val_loss: 46.9958 - val_regression_loss: 33.5123 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 35.6711 - regression_loss: 31.5362 - val_loss: 40.5446 - val_regression_loss: 29.0019 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.9808 - regression_loss: 27.7091 - val_loss: 37.6993 - val_regression_loss: 26.8938 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.9164 - regression_loss: 25.3963 - val_loss: 33.1475 - val_regression_loss: 23.7604 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.0512 - regression_loss: 23.2676 - val_loss: 30.8780 - val_regression_loss: 22.0336 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.1818 - regression_loss: 21.4300 - val_loss: 28.0372 - val_regression_loss: 19.9418 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.1944 - regression_loss: 19.6811 - val_loss: 25.9099 - val_regression_loss: 18.2963 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.3329 - regression_loss: 18.2828 - val_loss: 24.4677 - val_regression_loss: 17.1020 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.2045 - regression_loss: 17.0520 - val_loss: 22.4739 - val_regression_loss: 15.5885 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.7740 - regression_loss: 15.8926 - val_loss: 21.1305 - val_regression_loss: 14.5307 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.7968 - regression_loss: 14.8028 - val_loss: 19.8998 - val_regression_loss: 13.5768 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.8057 - regression_loss: 13.6840 - val_loss: 18.5907 - val_regression_loss: 12.6190 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5187 - regression_loss: 12.9264 - val_loss: 17.7362 - val_regression_loss: 11.9708 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.9479 - regression_loss: 12.3496 - val_loss: 16.3202 - val_regression_loss: 11.0321 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8951 - regression_loss: 11.7038 - val_loss: 16.3275 - val_regression_loss: 10.9637 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9620 - regression_loss: 11.2665 - val_loss: 15.2130 - val_regression_loss: 10.1821 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8391 - regression_loss: 10.6939 - val_loss: 14.5852 - val_regression_loss: 9.7291 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8416 - regression_loss: 10.4158 - val_loss: 14.3388 - val_regression_loss: 9.5450 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0625 - regression_loss: 9.8822 - val_loss: 13.5828 - val_regression_loss: 9.0363 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9827 - regression_loss: 9.5749 - val_loss: 13.3926 - val_regression_loss: 8.8713 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9255 - regression_loss: 9.2671 - val_loss: 12.4381 - val_regression_loss: 8.2808 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6864 - regression_loss: 9.0047 - val_loss: 12.6074 - val_regression_loss: 8.3122 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6969 - regression_loss: 8.5523 - val_loss: 11.5824 - val_regression_loss: 7.6884 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6677 - regression_loss: 8.4289 - val_loss: 12.0173 - val_regression_loss: 7.9268 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6454 - regression_loss: 8.0361 - val_loss: 11.0677 - val_regression_loss: 7.2738 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8387 - regression_loss: 7.6424 - val_loss: 10.8991 - val_regression_loss: 7.1340 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4946 - regression_loss: 7.3654 - val_loss: 10.3219 - val_regression_loss: 6.7814 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3085 - regression_loss: 7.1001 - val_loss: 9.9625 - val_regression_loss: 6.5291 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9483 - regression_loss: 6.8189 - val_loss: 9.8630 - val_regression_loss: 6.4507 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6845 - regression_loss: 6.4363 - val_loss: 9.4324 - val_regression_loss: 6.1552 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2911 - regression_loss: 6.2341 - val_loss: 9.4597 - val_regression_loss: 6.1536 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9527 - regression_loss: 5.9689 - val_loss: 8.6568 - val_regression_loss: 5.6315 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5055 - regression_loss: 5.6373 - val_loss: 8.7114 - val_regression_loss: 5.6243 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.2116 - regression_loss: 5.2957 - val_loss: 8.0908 - val_regression_loss: 5.2655 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0036 - regression_loss: 5.1452 - val_loss: 8.4265 - val_regression_loss: 5.4618 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7201 - regression_loss: 4.8342 - val_loss: 7.5219 - val_regression_loss: 5.0220 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6374 - regression_loss: 4.7604 - val_loss: 8.1703 - val_regression_loss: 5.2832 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7139 - regression_loss: 4.3814 - val_loss: 7.0867 - val_regression_loss: 4.6230 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7455 - regression_loss: 4.0838 - val_loss: 7.1895 - val_regression_loss: 4.6293 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2884 - regression_loss: 3.9419 - val_loss: 6.6428 - val_regression_loss: 4.3127 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.4201 - regression_loss: 3.6850 - val_loss: 6.5204 - val_regression_loss: 4.2319 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2851 - regression_loss: 3.4623 - val_loss: 6.5111 - val_regression_loss: 4.2158 - lr: 1.0000e-04\n",
      "Epoch 49/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6545 - regression_loss: 3.3120 - val_loss: 6.1196 - val_regression_loss: 3.9742 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9091 - regression_loss: 3.2237 - val_loss: 5.9922 - val_regression_loss: 3.8414 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8687 - regression_loss: 3.1028 - val_loss: 6.2321 - val_regression_loss: 3.9700 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7638 - regression_loss: 3.0036 - val_loss: 5.6590 - val_regression_loss: 3.7200 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6130 - regression_loss: 3.0475 - val_loss: 6.3542 - val_regression_loss: 4.0364 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5148 - regression_loss: 2.8651 - val_loss: 5.5260 - val_regression_loss: 3.6179 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7108 - regression_loss: 3.0084 - val_loss: 5.6835 - val_regression_loss: 3.5147 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3948 - regression_loss: 2.7065 - val_loss: 5.3712 - val_regression_loss: 3.3587 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.1360 - regression_loss: 2.6156 - val_loss: 5.1491 - val_regression_loss: 3.2553 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2328 - regression_loss: 2.5733 - val_loss: 5.7982 - val_regression_loss: 3.5926 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.1845 - regression_loss: 2.5210 - val_loss: 5.0410 - val_regression_loss: 3.1891 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.1142 - regression_loss: 2.4478 - val_loss: 5.2998 - val_regression_loss: 3.1810 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9815 - regression_loss: 2.3071 - val_loss: 4.7680 - val_regression_loss: 2.9110 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8030 - regression_loss: 2.1762 - val_loss: 4.9460 - val_regression_loss: 2.9610 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7976 - regression_loss: 2.1831 - val_loss: 4.7717 - val_regression_loss: 2.8417 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6706 - regression_loss: 2.1544 - val_loss: 4.5647 - val_regression_loss: 2.7237 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7337 - regression_loss: 2.1080 - val_loss: 5.1097 - val_regression_loss: 3.0194 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5923 - regression_loss: 2.0907 - val_loss: 4.5256 - val_regression_loss: 2.7086 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6008 - regression_loss: 2.1175 - val_loss: 4.6199 - val_regression_loss: 2.6575 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5378 - regression_loss: 1.9030 - val_loss: 4.4070 - val_regression_loss: 2.5696 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5234 - regression_loss: 1.9032 - val_loss: 4.3321 - val_regression_loss: 2.4897 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3762 - regression_loss: 1.8213 - val_loss: 4.3167 - val_regression_loss: 2.4588 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3854 - regression_loss: 1.7681 - val_loss: 4.2430 - val_regression_loss: 2.3944 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3872 - regression_loss: 1.7725 - val_loss: 4.1374 - val_regression_loss: 2.3208 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3037 - regression_loss: 1.7258 - val_loss: 4.1645 - val_regression_loss: 2.3237 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2632 - regression_loss: 1.6741 - val_loss: 4.0853 - val_regression_loss: 2.2586 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2638 - regression_loss: 1.6540 - val_loss: 4.0293 - val_regression_loss: 2.2223 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2062 - regression_loss: 1.6169 - val_loss: 4.1098 - val_regression_loss: 2.2505 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1729 - regression_loss: 1.5957 - val_loss: 3.9430 - val_regression_loss: 2.1305 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1339 - regression_loss: 1.5861 - val_loss: 3.8752 - val_regression_loss: 2.0855 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0781 - regression_loss: 1.5375 - val_loss: 3.9383 - val_regression_loss: 2.1285 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1124 - regression_loss: 1.5360 - val_loss: 3.8402 - val_regression_loss: 2.0702 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9565 - regression_loss: 1.4875 - val_loss: 3.9188 - val_regression_loss: 2.0664 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.0106 - regression_loss: 1.4639 - val_loss: 3.8886 - val_regression_loss: 2.1318 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1439 - regression_loss: 1.5850 - val_loss: 4.4373 - val_regression_loss: 2.4419 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2744 - regression_loss: 1.7007 - val_loss: 3.7820 - val_regression_loss: 2.0191 - lr: 1.0000e-04\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1658 - regression_loss: 1.5624 - val_loss: 3.6907 - val_regression_loss: 1.9266 - lr: 1.0000e-04\n",
      "Epoch 86/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.5604 - regression_loss: 1.1076\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9826 - regression_loss: 1.4287 - val_loss: 3.7965 - val_regression_loss: 1.9532 - lr: 1.0000e-04\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9680 - regression_loss: 1.3960 - val_loss: 3.6214 - val_regression_loss: 1.8821 - lr: 5.0000e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9682 - regression_loss: 1.4239 - val_loss: 3.6712 - val_regression_loss: 1.8954 - lr: 5.0000e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9725 - regression_loss: 1.3972 - val_loss: 3.6200 - val_regression_loss: 1.8600 - lr: 5.0000e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9258 - regression_loss: 1.3610 - val_loss: 3.5787 - val_regression_loss: 1.8268 - lr: 5.0000e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8502 - regression_loss: 1.3124 - val_loss: 3.6266 - val_regression_loss: 1.8454 - lr: 5.0000e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8550 - regression_loss: 1.2976 - val_loss: 3.5556 - val_regression_loss: 1.7985 - lr: 5.0000e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8470 - regression_loss: 1.2946 - val_loss: 3.5791 - val_regression_loss: 1.8017 - lr: 5.0000e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8576 - regression_loss: 1.3027 - val_loss: 3.5310 - val_regression_loss: 1.7854 - lr: 5.0000e-05\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8485 - regression_loss: 1.2856 - val_loss: 3.5373 - val_regression_loss: 1.7825 - lr: 5.0000e-05\n",
      "Epoch 96/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7682 - regression_loss: 1.2685 - val_loss: 3.5012 - val_regression_loss: 1.7567 - lr: 5.0000e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7925 - regression_loss: 1.2668 - val_loss: 3.5115 - val_regression_loss: 1.7548 - lr: 5.0000e-05\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8016 - regression_loss: 1.2542 - val_loss: 3.4934 - val_regression_loss: 1.7345 - lr: 5.0000e-05\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7678 - regression_loss: 1.2462 - val_loss: 3.4757 - val_regression_loss: 1.7265 - lr: 5.0000e-05\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8000 - regression_loss: 1.2439 - val_loss: 3.4714 - val_regression_loss: 1.7240 - lr: 5.0000e-05\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7574 - regression_loss: 1.2342 - val_loss: 3.4418 - val_regression_loss: 1.6923 - lr: 5.0000e-05\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7682 - regression_loss: 1.2253 - val_loss: 3.4445 - val_regression_loss: 1.7011 - lr: 5.0000e-05\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7732 - regression_loss: 1.2168 - val_loss: 3.4332 - val_regression_loss: 1.6916 - lr: 5.0000e-05\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7401 - regression_loss: 1.2111 - val_loss: 3.3950 - val_regression_loss: 1.6530 - lr: 5.0000e-05\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7225 - regression_loss: 1.1966 - val_loss: 3.4200 - val_regression_loss: 1.6771 - lr: 5.0000e-05\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7136 - regression_loss: 1.1871 - val_loss: 3.3932 - val_regression_loss: 1.6537 - lr: 5.0000e-05\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6587 - regression_loss: 1.1867 - val_loss: 3.3827 - val_regression_loss: 1.6441 - lr: 5.0000e-05\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7074 - regression_loss: 1.1838 - val_loss: 3.3549 - val_regression_loss: 1.6245 - lr: 5.0000e-05\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7108 - regression_loss: 1.1683 - val_loss: 3.3531 - val_regression_loss: 1.6178 - lr: 5.0000e-05\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7235 - regression_loss: 1.1734 - val_loss: 3.3646 - val_regression_loss: 1.6400 - lr: 5.0000e-05\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6742 - regression_loss: 1.1510 - val_loss: 3.3639 - val_regression_loss: 1.6245 - lr: 5.0000e-05\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6474 - regression_loss: 1.1420 - val_loss: 3.3033 - val_regression_loss: 1.5771 - lr: 5.0000e-05\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6708 - regression_loss: 1.1379 - val_loss: 3.2974 - val_regression_loss: 1.5718 - lr: 5.0000e-05\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6782 - regression_loss: 1.1310 - val_loss: 3.3107 - val_regression_loss: 1.5838 - lr: 5.0000e-05\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6118 - regression_loss: 1.1206 - val_loss: 3.2882 - val_regression_loss: 1.5733 - lr: 5.0000e-05\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6616 - regression_loss: 1.1197 - val_loss: 3.3057 - val_regression_loss: 1.5800 - lr: 5.0000e-05\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6020 - regression_loss: 1.1149 - val_loss: 3.2648 - val_regression_loss: 1.5492 - lr: 5.0000e-05\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6338 - regression_loss: 1.1197 - val_loss: 3.2660 - val_regression_loss: 1.5399 - lr: 5.0000e-05\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6971 - regression_loss: 1.1512 - val_loss: 3.3083 - val_regression_loss: 1.5714 - lr: 5.0000e-05\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6627 - regression_loss: 1.1245 - val_loss: 3.2354 - val_regression_loss: 1.5263 - lr: 5.0000e-05\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6107 - regression_loss: 1.0923 - val_loss: 3.2269 - val_regression_loss: 1.5131 - lr: 5.0000e-05\n",
      "Epoch 122/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.9090 - regression_loss: 1.4613\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6196 - regression_loss: 1.0801 - val_loss: 3.2634 - val_regression_loss: 1.5391 - lr: 5.0000e-05\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5890 - regression_loss: 1.0738 - val_loss: 3.2473 - val_regression_loss: 1.5344 - lr: 2.5000e-05\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5955 - regression_loss: 1.0766 - val_loss: 3.2207 - val_regression_loss: 1.5119 - lr: 2.5000e-05\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5851 - regression_loss: 1.0698 - val_loss: 3.2303 - val_regression_loss: 1.5061 - lr: 2.5000e-05\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6009 - regression_loss: 1.0650 - val_loss: 3.1985 - val_regression_loss: 1.4916 - lr: 2.5000e-05\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5762 - regression_loss: 1.0648 - val_loss: 3.1954 - val_regression_loss: 1.4886 - lr: 2.5000e-05\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5713 - regression_loss: 1.0638 - val_loss: 3.2274 - val_regression_loss: 1.5064 - lr: 2.5000e-05\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5731 - regression_loss: 1.0684 - val_loss: 3.1943 - val_regression_loss: 1.4924 - lr: 2.5000e-05\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5795 - regression_loss: 1.0572 - val_loss: 3.2121 - val_regression_loss: 1.4961 - lr: 2.5000e-05\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5447 - regression_loss: 1.0509 - val_loss: 3.1893 - val_regression_loss: 1.4815 - lr: 2.5000e-05\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5786 - regression_loss: 1.0542 - val_loss: 3.1820 - val_regression_loss: 1.4777 - lr: 2.5000e-05\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5516 - regression_loss: 1.0440 - val_loss: 3.1955 - val_regression_loss: 1.4810 - lr: 2.5000e-05\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5650 - regression_loss: 1.0429 - val_loss: 3.1924 - val_regression_loss: 1.4830 - lr: 2.5000e-05\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5679 - regression_loss: 1.0430 - val_loss: 3.1716 - val_regression_loss: 1.4658 - lr: 2.5000e-05\n",
      "Epoch 136/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.7097 - regression_loss: 1.2632\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5561 - regression_loss: 1.0428 - val_loss: 3.1776 - val_regression_loss: 1.4663 - lr: 2.5000e-05\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5255 - regression_loss: 1.0312 - val_loss: 3.1637 - val_regression_loss: 1.4586 - lr: 1.2500e-05\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5041 - regression_loss: 1.0304 - val_loss: 3.1614 - val_regression_loss: 1.4584 - lr: 1.2500e-05\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5418 - regression_loss: 1.0297 - val_loss: 3.1604 - val_regression_loss: 1.4589 - lr: 1.2500e-05\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5458 - regression_loss: 1.0303 - val_loss: 3.1627 - val_regression_loss: 1.4607 - lr: 1.2500e-05\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5435 - regression_loss: 1.0270 - val_loss: 3.1692 - val_regression_loss: 1.4613 - lr: 1.2500e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5589 - regression_loss: 1.0256 - val_loss: 3.1612 - val_regression_loss: 1.4558 - lr: 1.2500e-05\n",
      "Epoch 143/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.7236 - regression_loss: 1.2774\n",
      "Epoch 143: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5468 - regression_loss: 1.0248 - val_loss: 3.1543 - val_regression_loss: 1.4509 - lr: 1.2500e-05\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5331 - regression_loss: 1.0220 - val_loss: 3.1509 - val_regression_loss: 1.4496 - lr: 6.2500e-06\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5523 - regression_loss: 1.0233 - val_loss: 3.1535 - val_regression_loss: 1.4501 - lr: 6.2500e-06\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5489 - regression_loss: 1.0206 - val_loss: 3.1534 - val_regression_loss: 1.4503 - lr: 6.2500e-06\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5477 - regression_loss: 1.0206 - val_loss: 3.1553 - val_regression_loss: 1.4511 - lr: 6.2500e-06\n",
      "Epoch 148/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.5113 - regression_loss: 1.0652\n",
      "Epoch 148: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5521 - regression_loss: 1.0197 - val_loss: 3.1502 - val_regression_loss: 1.4471 - lr: 6.2500e-06\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5495 - regression_loss: 1.0188 - val_loss: 3.1492 - val_regression_loss: 1.4459 - lr: 3.1250e-06\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5280 - regression_loss: 1.0185 - val_loss: 3.1473 - val_regression_loss: 1.4447 - lr: 3.1250e-06\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5208 - regression_loss: 1.0184 - val_loss: 3.1472 - val_regression_loss: 1.4444 - lr: 3.1250e-06\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5315 - regression_loss: 1.0181 - val_loss: 3.1475 - val_regression_loss: 1.4444 - lr: 3.1250e-06\n",
      "Epoch 153/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.3168 - regression_loss: 0.8707\n",
      "Epoch 153: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5493 - regression_loss: 1.0174 - val_loss: 3.1467 - val_regression_loss: 1.4439 - lr: 3.1250e-06\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5259 - regression_loss: 1.0171 - val_loss: 3.1463 - val_regression_loss: 1.4432 - lr: 1.5625e-06\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5372 - regression_loss: 1.0169 - val_loss: 3.1454 - val_regression_loss: 1.4429 - lr: 1.5625e-06\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5451 - regression_loss: 1.0169 - val_loss: 3.1446 - val_regression_loss: 1.4425 - lr: 1.5625e-06\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5498 - regression_loss: 1.0166 - val_loss: 3.1444 - val_regression_loss: 1.4425 - lr: 1.5625e-06\n",
      "Epoch 158/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.5382 - regression_loss: 1.0921\n",
      "Epoch 158: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5503 - regression_loss: 1.0167 - val_loss: 3.1454 - val_regression_loss: 1.4427 - lr: 1.5625e-06\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5483 - regression_loss: 1.0161 - val_loss: 3.1451 - val_regression_loss: 1.4424 - lr: 7.8125e-07\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5316 - regression_loss: 1.0159 - val_loss: 3.1450 - val_regression_loss: 1.4425 - lr: 7.8125e-07\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5150 - regression_loss: 1.0159 - val_loss: 3.1451 - val_regression_loss: 1.4427 - lr: 7.8125e-07\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5015 - regression_loss: 1.0158 - val_loss: 3.1447 - val_regression_loss: 1.4424 - lr: 7.8125e-07\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5384 - regression_loss: 1.0156 - val_loss: 3.1445 - val_regression_loss: 1.4423 - lr: 7.8125e-07\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5232 - regression_loss: 1.0156 - val_loss: 3.1443 - val_regression_loss: 1.4420 - lr: 7.8125e-07\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5198 - regression_loss: 1.0155 - val_loss: 3.1439 - val_regression_loss: 1.4417 - lr: 7.8125e-07\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5125 - regression_loss: 1.0154 - val_loss: 3.1440 - val_regression_loss: 1.4418 - lr: 7.8125e-07\n",
      "Epoch 167/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.8094 - regression_loss: 1.3634\n",
      "Epoch 167: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5333 - regression_loss: 1.0153 - val_loss: 3.1437 - val_regression_loss: 1.4416 - lr: 7.8125e-07\n",
      "Epoch 168/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5417 - regression_loss: 1.0152 - val_loss: 3.1439 - val_regression_loss: 1.4417 - lr: 3.9062e-07\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5117 - regression_loss: 1.0152 - val_loss: 3.1438 - val_regression_loss: 1.4415 - lr: 3.9062e-07\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4751 - regression_loss: 1.0153 - val_loss: 3.1436 - val_regression_loss: 1.4414 - lr: 3.9062e-07\n",
      "Epoch 171/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5218 - regression_loss: 1.0151 - val_loss: 3.1436 - val_regression_loss: 1.4414 - lr: 3.9062e-07\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5314 - regression_loss: 1.0150 - val_loss: 3.1437 - val_regression_loss: 1.4415 - lr: 3.9062e-07\n",
      "Epoch 173/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5404 - regression_loss: 1.0151 - val_loss: 3.1437 - val_regression_loss: 1.4416 - lr: 3.9062e-07\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5224 - regression_loss: 1.0151 - val_loss: 3.1434 - val_regression_loss: 1.4412 - lr: 3.9062e-07\n",
      "Epoch 175/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.6247 - regression_loss: 1.1787\n",
      "Epoch 175: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5489 - regression_loss: 1.0149 - val_loss: 3.1433 - val_regression_loss: 1.4412 - lr: 3.9062e-07\n",
      "Epoch 176/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5264 - regression_loss: 1.0149 - val_loss: 3.1432 - val_regression_loss: 1.4411 - lr: 1.9531e-07\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5171 - regression_loss: 1.0148 - val_loss: 3.1432 - val_regression_loss: 1.4411 - lr: 1.9531e-07\n",
      "Epoch 178/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5404 - regression_loss: 1.0148 - val_loss: 3.1431 - val_regression_loss: 1.4410 - lr: 1.9531e-07\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5354 - regression_loss: 1.0148 - val_loss: 3.1433 - val_regression_loss: 1.4411 - lr: 1.9531e-07\n",
      "Epoch 180/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.7740 - regression_loss: 1.3280\n",
      "Epoch 180: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5515 - regression_loss: 1.0148 - val_loss: 3.1432 - val_regression_loss: 1.4410 - lr: 1.9531e-07\n",
      "Epoch 181/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5235 - regression_loss: 1.0147 - val_loss: 3.1432 - val_regression_loss: 1.4410 - lr: 9.7656e-08\n",
      "Epoch 182/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5204 - regression_loss: 1.0147 - val_loss: 3.1431 - val_regression_loss: 1.4410 - lr: 9.7656e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5463 - regression_loss: 1.0147 - val_loss: 3.1432 - val_regression_loss: 1.4410 - lr: 9.7656e-08\n",
      "Epoch 184/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5239 - regression_loss: 1.0147 - val_loss: 3.1431 - val_regression_loss: 1.4410 - lr: 9.7656e-08\n",
      "Epoch 185/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.8445 - regression_loss: 1.3985\n",
      "Epoch 185: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5330 - regression_loss: 1.0147 - val_loss: 3.1431 - val_regression_loss: 1.4410 - lr: 9.7656e-08\n",
      "Epoch 186/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5425 - regression_loss: 1.0147 - val_loss: 3.1431 - val_regression_loss: 1.4410 - lr: 4.8828e-08\n",
      "Epoch 187/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5336 - regression_loss: 1.0147 - val_loss: 3.1430 - val_regression_loss: 1.4410 - lr: 4.8828e-08\n",
      "Epoch 188/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5263 - regression_loss: 1.0147 - val_loss: 3.1431 - val_regression_loss: 1.4410 - lr: 4.8828e-08\n",
      "Epoch 189/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5495 - regression_loss: 1.0147 - val_loss: 3.1431 - val_regression_loss: 1.4409 - lr: 4.8828e-08\n",
      "Epoch 190/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.6633 - regression_loss: 1.2173\n",
      "Epoch 190: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5320 - regression_loss: 1.0146 - val_loss: 3.1431 - val_regression_loss: 1.4409 - lr: 4.8828e-08\n",
      "Epoch 191/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5063 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 2.4414e-08\n",
      "Epoch 192/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5177 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 2.4414e-08\n",
      "Epoch 193/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5374 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 2.4414e-08\n",
      "Epoch 194/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5026 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 2.4414e-08\n",
      "Epoch 195/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.0646 - regression_loss: 1.6186\n",
      "Epoch 195: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5310 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 2.4414e-08\n",
      "Epoch 196/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5254 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 1.2207e-08\n",
      "Epoch 197/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5354 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 1.2207e-08\n",
      "Epoch 198/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5297 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 1.2207e-08\n",
      "Epoch 199/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5455 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 1.2207e-08\n",
      "Epoch 200/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.6232 - regression_loss: 1.1772\n",
      "Epoch 200: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5406 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 1.2207e-08\n",
      "Epoch 201/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5054 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 6.1035e-09\n",
      "Epoch 202/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5407 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 6.1035e-09\n",
      "Epoch 203/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5267 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 6.1035e-09\n",
      "Epoch 204/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5350 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 6.1035e-09\n",
      "Epoch 205/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.7672 - regression_loss: 1.3212\n",
      "Epoch 205: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5229 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 6.1035e-09\n",
      "Epoch 206/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5274 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 3.0518e-09\n",
      "Epoch 207/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5472 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 3.0518e-09\n",
      "Epoch 208/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5066 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 3.0518e-09\n",
      "Epoch 209/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5463 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 3.0518e-09\n",
      "Epoch 210/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.3640 - regression_loss: 0.9180\n",
      "Epoch 210: ReduceLROnPlateau reducing learning rate to 1.5258788677030566e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5114 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 3.0518e-09\n",
      "Epoch 211/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5338 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 1.5259e-09\n",
      "Epoch 212/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5372 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 1.5259e-09\n",
      "Epoch 213/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5409 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 1.5259e-09\n",
      "Epoch 214/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5386 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 1.5259e-09\n",
      "Epoch 215/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.4029 - regression_loss: 0.9569\n",
      "Epoch 215: ReduceLROnPlateau reducing learning rate to 7.629394338515283e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5464 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 1.5259e-09\n",
      "Epoch 216/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5299 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 7.6294e-10\n",
      "Epoch 217/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5305 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 7.6294e-10\n",
      "Epoch 218/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5304 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 7.6294e-10\n",
      "Epoch 219/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5089 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 7.6294e-10\n",
      "Epoch 220/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.3400 - regression_loss: 0.8940\n",
      "Epoch 220: ReduceLROnPlateau reducing learning rate to 3.8146971692576415e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5202 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 7.6294e-10\n",
      "Epoch 221/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5338 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 3.8147e-10\n",
      "Epoch 222/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5432 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 3.8147e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5305 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 3.8147e-10\n",
      "Epoch 224/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5317 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 3.8147e-10\n",
      "Epoch 225/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.5106 - regression_loss: 1.0646\n",
      "Epoch 225: ReduceLROnPlateau reducing learning rate to 1.9073485846288207e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5455 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 3.8147e-10\n",
      "Epoch 226/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5374 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 1.9073e-10\n",
      "Epoch 227/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5126 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 1.9073e-10\n",
      "Epoch 228/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5325 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 1.9073e-10\n",
      "Epoch 229/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5274 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 1.9073e-10\n",
      "Epoch 230/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.3937 - regression_loss: 0.9477\n",
      "Epoch 230: ReduceLROnPlateau reducing learning rate to 9.536742923144104e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5147 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 1.9073e-10\n",
      "Epoch 231/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4975 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 9.5367e-11\n",
      "Epoch 232/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5349 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 9.5367e-11\n",
      "Epoch 233/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5451 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 9.5367e-11\n",
      "Epoch 234/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5403 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 9.5367e-11\n",
      "Epoch 235/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.5625 - regression_loss: 1.1165\n",
      "Epoch 235: ReduceLROnPlateau reducing learning rate to 4.768371461572052e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4970 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 9.5367e-11\n",
      "Epoch 236/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5264 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 4.7684e-11\n",
      "Epoch 237/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5285 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 4.7684e-11\n",
      "Epoch 238/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5345 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 4.7684e-11\n",
      "Epoch 239/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5404 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 4.7684e-11\n",
      "Epoch 240/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.6020 - regression_loss: 1.1560\n",
      "Epoch 240: ReduceLROnPlateau reducing learning rate to 2.384185730786026e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5403 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 4.7684e-11\n",
      "Epoch 241/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5521 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 2.3842e-11\n",
      "Epoch 242/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5271 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 2.3842e-11\n",
      "Epoch 243/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5199 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 2.3842e-11\n",
      "Epoch 244/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5285 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 2.3842e-11\n",
      "Epoch 245/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.6778 - regression_loss: 1.2318\n",
      "Epoch 245: ReduceLROnPlateau reducing learning rate to 1.192092865393013e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5502 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 2.3842e-11\n",
      "Epoch 246/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5324 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 1.1921e-11\n",
      "Epoch 247/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5256 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 1.1921e-11\n",
      "Epoch 248/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5292 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 1.1921e-11\n",
      "Epoch 249/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5242 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 1.1921e-11\n",
      "Epoch 250/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.6365 - regression_loss: 1.1905\n",
      "Epoch 250: ReduceLROnPlateau reducing learning rate to 5.960464326965065e-12.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5337 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 1.1921e-11\n",
      "Epoch 251/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4821 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 5.9605e-12\n",
      "Epoch 252/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5091 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 5.9605e-12\n",
      "Epoch 253/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5244 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 5.9605e-12\n",
      "Epoch 254/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5451 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 5.9605e-12\n",
      "Epoch 255/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.4246 - regression_loss: 0.9787\n",
      "Epoch 255: ReduceLROnPlateau reducing learning rate to 2.9802321634825324e-12.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5341 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 5.9605e-12\n",
      "Epoch 256/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5443 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 2.9802e-12\n",
      "Epoch 257/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5271 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 2.9802e-12\n",
      "Epoch 258/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5399 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 2.9802e-12\n",
      "Epoch 259/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5076 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 2.9802e-12\n",
      "Epoch 260/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.1859 - regression_loss: 1.7399\n",
      "Epoch 260: ReduceLROnPlateau reducing learning rate to 1.4901160817412662e-12.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5358 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 2.9802e-12\n",
      "Epoch 261/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5264 - regression_loss: 1.0146 - val_loss: 3.1430 - val_regression_loss: 1.4409 - lr: 1.4901e-12\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 76.3167 - regression_loss: 70.2306 - val_loss: 41.4386 - val_regression_loss: 33.1487 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 54.1149 - regression_loss: 48.6601 - val_loss: 33.3813 - val_regression_loss: 25.9470 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 43.7717 - regression_loss: 40.3398 - val_loss: 27.3522 - val_regression_loss: 21.2127 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 36.8105 - regression_loss: 34.0922 - val_loss: 24.1947 - val_regression_loss: 18.1898 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31.9167 - regression_loss: 28.8177 - val_loss: 20.9906 - val_regression_loss: 15.4513 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.6146 - regression_loss: 24.0633 - val_loss: 18.6729 - val_regression_loss: 13.5526 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.2605 - regression_loss: 20.9556 - val_loss: 17.5988 - val_regression_loss: 12.5820 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.3203 - regression_loss: 18.4096 - val_loss: 15.4082 - val_regression_loss: 10.9080 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.6393 - regression_loss: 16.6663 - val_loss: 14.7683 - val_regression_loss: 10.3275 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8181 - regression_loss: 14.3050 - val_loss: 13.4227 - val_regression_loss: 9.2834 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.0281 - regression_loss: 12.7304 - val_loss: 12.4819 - val_regression_loss: 8.5588 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.0262 - regression_loss: 11.6262 - val_loss: 11.8197 - val_regression_loss: 8.0477 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8086 - regression_loss: 10.6392 - val_loss: 11.7460 - val_regression_loss: 8.0038 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2678 - regression_loss: 10.0917 - val_loss: 10.3224 - val_regression_loss: 6.9442 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4934 - regression_loss: 9.4826 - val_loss: 10.6998 - val_regression_loss: 7.2599 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0071 - regression_loss: 8.7267 - val_loss: 9.5125 - val_regression_loss: 6.3419 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2715 - regression_loss: 8.2433 - val_loss: 10.2558 - val_regression_loss: 6.9648 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3398 - regression_loss: 8.2118 - val_loss: 9.0141 - val_regression_loss: 5.9863 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7928 - regression_loss: 7.8086 - val_loss: 9.1735 - val_regression_loss: 6.1655 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4801 - regression_loss: 7.4155 - val_loss: 8.6905 - val_regression_loss: 5.7987 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0399 - regression_loss: 7.1190 - val_loss: 8.6715 - val_regression_loss: 5.8141 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9665 - regression_loss: 7.1440 - val_loss: 9.1320 - val_regression_loss: 6.2138 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0270 - regression_loss: 7.0961 - val_loss: 8.2209 - val_regression_loss: 5.4479 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4900 - regression_loss: 6.7166 - val_loss: 8.4374 - val_regression_loss: 5.6794 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3479 - regression_loss: 6.5160 - val_loss: 8.1044 - val_regression_loss: 5.4032 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1100 - regression_loss: 6.3320 - val_loss: 8.0770 - val_regression_loss: 5.4005 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2108 - regression_loss: 6.3212 - val_loss: 8.2524 - val_regression_loss: 5.5568 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0799 - regression_loss: 6.1575 - val_loss: 7.8321 - val_regression_loss: 5.1781 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1145 - regression_loss: 6.1151 - val_loss: 7.7324 - val_regression_loss: 5.1243 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7670 - regression_loss: 5.9231 - val_loss: 7.6630 - val_regression_loss: 5.0731 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6449 - regression_loss: 5.7792 - val_loss: 7.6753 - val_regression_loss: 5.0849 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6280 - regression_loss: 5.7358 - val_loss: 7.5211 - val_regression_loss: 4.9559 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5588 - regression_loss: 5.6735 - val_loss: 7.8541 - val_regression_loss: 5.2653 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4461 - regression_loss: 5.6677 - val_loss: 7.6824 - val_regression_loss: 5.1125 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.2775 - regression_loss: 5.4594 - val_loss: 7.5228 - val_regression_loss: 4.9636 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.2436 - regression_loss: 5.3885 - val_loss: 7.4405 - val_regression_loss: 4.9211 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.2158 - regression_loss: 5.3162 - val_loss: 7.5027 - val_regression_loss: 4.9872 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0895 - regression_loss: 5.2741 - val_loss: 7.4087 - val_regression_loss: 4.9143 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0112 - regression_loss: 5.1984 - val_loss: 7.3066 - val_regression_loss: 4.8146 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8242 - regression_loss: 5.1303 - val_loss: 7.2162 - val_regression_loss: 4.7180 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9078 - regression_loss: 5.1266 - val_loss: 7.2102 - val_regression_loss: 4.7472 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7849 - regression_loss: 5.0691 - val_loss: 7.3355 - val_regression_loss: 4.8642 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9580 - regression_loss: 5.1201 - val_loss: 7.2991 - val_regression_loss: 4.7389 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7995 - regression_loss: 5.0073 - val_loss: 7.6989 - val_regression_loss: 5.2006 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7647 - regression_loss: 4.9653 - val_loss: 7.1846 - val_regression_loss: 4.6883 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8257 - regression_loss: 4.9933 - val_loss: 7.0831 - val_regression_loss: 4.6106 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5046 - regression_loss: 4.7952 - val_loss: 7.5538 - val_regression_loss: 5.0719 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6606 - regression_loss: 4.9976 - val_loss: 6.9535 - val_regression_loss: 4.5012 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4835 - regression_loss: 4.8024 - val_loss: 7.0034 - val_regression_loss: 4.5281 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5456 - regression_loss: 4.6891 - val_loss: 7.3378 - val_regression_loss: 4.8876 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4031 - regression_loss: 4.6831 - val_loss: 7.0159 - val_regression_loss: 4.5905 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3196 - regression_loss: 4.5171 - val_loss: 7.0299 - val_regression_loss: 4.6001 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2050 - regression_loss: 4.4825 - val_loss: 6.9907 - val_regression_loss: 4.5616 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2685 - regression_loss: 4.4940 - val_loss: 6.9868 - val_regression_loss: 4.5005 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2874 - regression_loss: 4.5053 - val_loss: 7.0730 - val_regression_loss: 4.6674 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3108 - regression_loss: 4.5453 - val_loss: 7.3578 - val_regression_loss: 4.9360 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3222 - regression_loss: 4.6456 - val_loss: 7.0818 - val_regression_loss: 4.5479 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.7331 - regression_loss: 5.2888\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3282 - regression_loss: 4.6687 - val_loss: 6.8937 - val_regression_loss: 4.4277 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.1670 - regression_loss: 4.3696 - val_loss: 7.0149 - val_regression_loss: 4.6095 - lr: 5.0000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.0790 - regression_loss: 4.3197 - val_loss: 6.8661 - val_regression_loss: 4.4048 - lr: 5.0000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.0716 - regression_loss: 4.2983 - val_loss: 7.0160 - val_regression_loss: 4.6211 - lr: 5.0000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9870 - regression_loss: 4.2761 - val_loss: 6.8246 - val_regression_loss: 4.4067 - lr: 5.0000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9822 - regression_loss: 4.2294 - val_loss: 6.8263 - val_regression_loss: 4.4357 - lr: 5.0000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9352 - regression_loss: 4.2113 - val_loss: 6.7710 - val_regression_loss: 4.3723 - lr: 5.0000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9701 - regression_loss: 4.2102 - val_loss: 6.8322 - val_regression_loss: 4.4361 - lr: 5.0000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9504 - regression_loss: 4.2480 - val_loss: 6.8235 - val_regression_loss: 4.3981 - lr: 5.0000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9684 - regression_loss: 4.2358 - val_loss: 6.9453 - val_regression_loss: 4.5546 - lr: 5.0000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9080 - regression_loss: 4.1737 - val_loss: 6.7647 - val_regression_loss: 4.3433 - lr: 5.0000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8891 - regression_loss: 4.1488 - val_loss: 6.8377 - val_regression_loss: 4.4439 - lr: 5.0000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6924 - regression_loss: 4.1460 - val_loss: 6.7862 - val_regression_loss: 4.3899 - lr: 5.0000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8819 - regression_loss: 4.1080 - val_loss: 6.8168 - val_regression_loss: 4.4163 - lr: 5.0000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8707 - regression_loss: 4.1061 - val_loss: 6.7739 - val_regression_loss: 4.3667 - lr: 5.0000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7381 - regression_loss: 4.1127 - val_loss: 6.8140 - val_regression_loss: 4.4225 - lr: 5.0000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8750 - regression_loss: 4.1059 - val_loss: 6.7011 - val_regression_loss: 4.3037 - lr: 5.0000e-05\n",
      "Epoch 75/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.9215 - regression_loss: 4.4782\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6957 - regression_loss: 4.0703 - val_loss: 6.8167 - val_regression_loss: 4.4411 - lr: 5.0000e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8465 - regression_loss: 4.0720 - val_loss: 6.7304 - val_regression_loss: 4.3401 - lr: 2.5000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7880 - regression_loss: 4.0915 - val_loss: 6.7253 - val_regression_loss: 4.3307 - lr: 2.5000e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8522 - regression_loss: 4.0936 - val_loss: 6.7617 - val_regression_loss: 4.3808 - lr: 2.5000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6904 - regression_loss: 4.0230 - val_loss: 6.7076 - val_regression_loss: 4.3083 - lr: 2.5000e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7927 - regression_loss: 4.0596 - val_loss: 6.7409 - val_regression_loss: 4.3583 - lr: 2.5000e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7296 - regression_loss: 4.0173 - val_loss: 6.7193 - val_regression_loss: 4.3274 - lr: 2.5000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7738 - regression_loss: 4.0169 - val_loss: 6.7349 - val_regression_loss: 4.3450 - lr: 2.5000e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6858 - regression_loss: 3.9998 - val_loss: 6.7386 - val_regression_loss: 4.3494 - lr: 2.5000e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7182 - regression_loss: 4.0013 - val_loss: 6.7218 - val_regression_loss: 4.3278 - lr: 2.5000e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6416 - regression_loss: 3.9992 - val_loss: 6.7513 - val_regression_loss: 4.3653 - lr: 2.5000e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7261 - regression_loss: 4.0126 - val_loss: 6.7101 - val_regression_loss: 4.3084 - lr: 2.5000e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6893 - regression_loss: 3.9899 - val_loss: 6.7649 - val_regression_loss: 4.3792 - lr: 2.5000e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7023 - regression_loss: 3.9946 - val_loss: 6.7194 - val_regression_loss: 4.3202 - lr: 2.5000e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7253 - regression_loss: 3.9980 - val_loss: 6.7187 - val_regression_loss: 4.3335 - lr: 2.5000e-05\n",
      "Epoch 90/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.1960 - regression_loss: 3.7533\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7126 - regression_loss: 3.9675 - val_loss: 6.7078 - val_regression_loss: 4.3161 - lr: 2.5000e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7423 - regression_loss: 3.9567 - val_loss: 6.7009 - val_regression_loss: 4.3067 - lr: 1.2500e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7289 - regression_loss: 3.9535 - val_loss: 6.7168 - val_regression_loss: 4.3260 - lr: 1.2500e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6222 - regression_loss: 3.9506 - val_loss: 6.7282 - val_regression_loss: 4.3424 - lr: 1.2500e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6760 - regression_loss: 3.9486 - val_loss: 6.7166 - val_regression_loss: 4.3267 - lr: 1.2500e-05\n",
      "Epoch 95/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6492 - regression_loss: 3.9440 - val_loss: 6.7145 - val_regression_loss: 4.3239 - lr: 1.2500e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6789 - regression_loss: 3.9437 - val_loss: 6.7177 - val_regression_loss: 4.3270 - lr: 1.2500e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6501 - regression_loss: 3.9396 - val_loss: 6.7007 - val_regression_loss: 4.3111 - lr: 1.2500e-05\n",
      "Epoch 98/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.6419 - regression_loss: 4.1993\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6733 - regression_loss: 3.9436 - val_loss: 6.7245 - val_regression_loss: 4.3405 - lr: 1.2500e-05\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5937 - regression_loss: 3.9356 - val_loss: 6.7038 - val_regression_loss: 4.3156 - lr: 6.2500e-06\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5936 - regression_loss: 3.9300 - val_loss: 6.7029 - val_regression_loss: 4.3120 - lr: 6.2500e-06\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6435 - regression_loss: 3.9338 - val_loss: 6.6966 - val_regression_loss: 4.3036 - lr: 6.2500e-06\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6808 - regression_loss: 3.9306 - val_loss: 6.7119 - val_regression_loss: 4.3241 - lr: 6.2500e-06\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5746 - regression_loss: 3.9280 - val_loss: 6.7108 - val_regression_loss: 4.3249 - lr: 6.2500e-06\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7077 - regression_loss: 3.9295 - val_loss: 6.6979 - val_regression_loss: 4.3077 - lr: 6.2500e-06\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6273 - regression_loss: 3.9256 - val_loss: 6.7009 - val_regression_loss: 4.3111 - lr: 6.2500e-06\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6156 - regression_loss: 3.9203 - val_loss: 6.7026 - val_regression_loss: 4.3134 - lr: 6.2500e-06\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6583 - regression_loss: 3.9219 - val_loss: 6.6996 - val_regression_loss: 4.3110 - lr: 6.2500e-06\n",
      "Epoch 108/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.1054 - regression_loss: 5.6629\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6344 - regression_loss: 3.9192 - val_loss: 6.7001 - val_regression_loss: 4.3107 - lr: 6.2500e-06\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6608 - regression_loss: 3.9167 - val_loss: 6.7035 - val_regression_loss: 4.3150 - lr: 3.1250e-06\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6213 - regression_loss: 3.9169 - val_loss: 6.7054 - val_regression_loss: 4.3176 - lr: 3.1250e-06\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6972 - regression_loss: 3.9165 - val_loss: 6.6994 - val_regression_loss: 4.3107 - lr: 3.1250e-06\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6263 - regression_loss: 3.9151 - val_loss: 6.7018 - val_regression_loss: 4.3127 - lr: 3.1250e-06\n",
      "Epoch 113/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.3225 - regression_loss: 4.8800\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6286 - regression_loss: 3.9142 - val_loss: 6.6997 - val_regression_loss: 4.3108 - lr: 3.1250e-06\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6524 - regression_loss: 3.9129 - val_loss: 6.7001 - val_regression_loss: 4.3115 - lr: 1.5625e-06\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5584 - regression_loss: 3.9123 - val_loss: 6.6998 - val_regression_loss: 4.3110 - lr: 1.5625e-06\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6234 - regression_loss: 3.9123 - val_loss: 6.6998 - val_regression_loss: 4.3114 - lr: 1.5625e-06\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6451 - regression_loss: 3.9116 - val_loss: 6.6997 - val_regression_loss: 4.3108 - lr: 1.5625e-06\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6083 - regression_loss: 3.9111 - val_loss: 6.6979 - val_regression_loss: 4.3088 - lr: 1.5625e-06\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6274 - regression_loss: 3.9123 - val_loss: 6.6961 - val_regression_loss: 4.3061 - lr: 1.5625e-06\n",
      "Epoch 120/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.2504 - regression_loss: 4.8080\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6427 - regression_loss: 3.9115 - val_loss: 6.6961 - val_regression_loss: 4.3063 - lr: 1.5625e-06\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6275 - regression_loss: 3.9101 - val_loss: 6.6963 - val_regression_loss: 4.3069 - lr: 7.8125e-07\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5444 - regression_loss: 3.9100 - val_loss: 6.6967 - val_regression_loss: 4.3075 - lr: 7.8125e-07\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6132 - regression_loss: 3.9096 - val_loss: 6.6977 - val_regression_loss: 4.3089 - lr: 7.8125e-07\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6671 - regression_loss: 3.9094 - val_loss: 6.6983 - val_regression_loss: 4.3099 - lr: 7.8125e-07\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6799 - regression_loss: 3.9097 - val_loss: 6.6981 - val_regression_loss: 4.3094 - lr: 7.8125e-07\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6230 - regression_loss: 3.9096 - val_loss: 6.6982 - val_regression_loss: 4.3095 - lr: 7.8125e-07\n",
      "Epoch 127/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.6509 - regression_loss: 4.2084\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5788 - regression_loss: 3.9091 - val_loss: 6.6987 - val_regression_loss: 4.3101 - lr: 7.8125e-07\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6179 - regression_loss: 3.9090 - val_loss: 6.6994 - val_regression_loss: 4.3109 - lr: 3.9062e-07\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6239 - regression_loss: 3.9095 - val_loss: 6.7000 - val_regression_loss: 4.3118 - lr: 3.9062e-07\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5547 - regression_loss: 3.9089 - val_loss: 6.6990 - val_regression_loss: 4.3105 - lr: 3.9062e-07\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5879 - regression_loss: 3.9084 - val_loss: 6.6988 - val_regression_loss: 4.3101 - lr: 3.9062e-07\n",
      "Epoch 132/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.8832 - regression_loss: 4.4407\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5578 - regression_loss: 3.9085 - val_loss: 6.6991 - val_regression_loss: 4.3105 - lr: 3.9062e-07\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6667 - regression_loss: 3.9082 - val_loss: 6.6988 - val_regression_loss: 4.3103 - lr: 1.9531e-07\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5673 - regression_loss: 3.9083 - val_loss: 6.6985 - val_regression_loss: 4.3099 - lr: 1.9531e-07\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5848 - regression_loss: 3.9080 - val_loss: 6.6986 - val_regression_loss: 4.3100 - lr: 1.9531e-07\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6600 - regression_loss: 3.9081 - val_loss: 6.6983 - val_regression_loss: 4.3096 - lr: 1.9531e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.8186 - regression_loss: 5.3762\n",
      "Epoch 137: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6326 - regression_loss: 3.9079 - val_loss: 6.6985 - val_regression_loss: 4.3098 - lr: 1.9531e-07\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6154 - regression_loss: 3.9079 - val_loss: 6.6985 - val_regression_loss: 4.3099 - lr: 9.7656e-08\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6431 - regression_loss: 3.9080 - val_loss: 6.6987 - val_regression_loss: 4.3101 - lr: 9.7656e-08\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6296 - regression_loss: 3.9078 - val_loss: 6.6987 - val_regression_loss: 4.3101 - lr: 9.7656e-08\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5359 - regression_loss: 3.9078 - val_loss: 6.6986 - val_regression_loss: 4.3100 - lr: 9.7656e-08\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5698 - regression_loss: 3.9078 - val_loss: 6.6984 - val_regression_loss: 4.3097 - lr: 9.7656e-08\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6325 - regression_loss: 3.9078 - val_loss: 6.6982 - val_regression_loss: 4.3094 - lr: 9.7656e-08\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6293 - regression_loss: 3.9077 - val_loss: 6.6981 - val_regression_loss: 4.3094 - lr: 9.7656e-08\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6093 - regression_loss: 3.9078 - val_loss: 6.6982 - val_regression_loss: 4.3095 - lr: 9.7656e-08\n",
      "Epoch 146/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.1517 - regression_loss: 3.7093\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6572 - regression_loss: 3.9077 - val_loss: 6.6983 - val_regression_loss: 4.3096 - lr: 9.7656e-08\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6050 - regression_loss: 3.9077 - val_loss: 6.6983 - val_regression_loss: 4.3096 - lr: 4.8828e-08\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6579 - regression_loss: 3.9076 - val_loss: 6.6983 - val_regression_loss: 4.3097 - lr: 4.8828e-08\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7112 - regression_loss: 3.9076 - val_loss: 6.6983 - val_regression_loss: 4.3096 - lr: 4.8828e-08\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5432 - regression_loss: 3.9076 - val_loss: 6.6983 - val_regression_loss: 4.3096 - lr: 4.8828e-08\n",
      "Epoch 151/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.1434 - regression_loss: 4.7010\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5560 - regression_loss: 3.9076 - val_loss: 6.6983 - val_regression_loss: 4.3096 - lr: 4.8828e-08\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6038 - regression_loss: 3.9076 - val_loss: 6.6983 - val_regression_loss: 4.3096 - lr: 2.4414e-08\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6357 - regression_loss: 3.9076 - val_loss: 6.6983 - val_regression_loss: 4.3096 - lr: 2.4414e-08\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5419 - regression_loss: 3.9076 - val_loss: 6.6983 - val_regression_loss: 4.3096 - lr: 2.4414e-08\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5637 - regression_loss: 3.9075 - val_loss: 6.6983 - val_regression_loss: 4.3096 - lr: 2.4414e-08\n",
      "Epoch 156/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.3080 - regression_loss: 3.8656\n",
      "Epoch 156: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6717 - regression_loss: 3.9075 - val_loss: 6.6983 - val_regression_loss: 4.3096 - lr: 2.4414e-08\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6164 - regression_loss: 3.9075 - val_loss: 6.6983 - val_regression_loss: 4.3096 - lr: 1.2207e-08\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6526 - regression_loss: 3.9075 - val_loss: 6.6983 - val_regression_loss: 4.3096 - lr: 1.2207e-08\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6175 - regression_loss: 3.9075 - val_loss: 6.6983 - val_regression_loss: 4.3096 - lr: 1.2207e-08\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6431 - regression_loss: 3.9075 - val_loss: 6.6983 - val_regression_loss: 4.3096 - lr: 1.2207e-08\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 24ms/step - loss: 117.1061 - regression_loss: 105.5174 - val_loss: 73.2236 - val_regression_loss: 54.1891 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 79.6267 - regression_loss: 74.3648 - val_loss: 52.5833 - val_regression_loss: 37.6072 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 66.0373 - regression_loss: 58.8030 - val_loss: 42.1110 - val_regression_loss: 30.6263 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 54.8064 - regression_loss: 49.1806 - val_loss: 33.7184 - val_regression_loss: 23.8766 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 47.5346 - regression_loss: 42.1592 - val_loss: 28.3469 - val_regression_loss: 20.6466 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 41.6921 - regression_loss: 37.0004 - val_loss: 24.5902 - val_regression_loss: 17.5899 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 36.3468 - regression_loss: 33.0088 - val_loss: 22.2677 - val_regression_loss: 16.0668 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 34.8814 - regression_loss: 30.5873 - val_loss: 20.5845 - val_regression_loss: 14.7324 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.5321 - regression_loss: 28.1316 - val_loss: 19.1808 - val_regression_loss: 13.7223 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 30.3776 - regression_loss: 27.5809 - val_loss: 17.7877 - val_regression_loss: 12.8358 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.0925 - regression_loss: 24.3085 - val_loss: 17.0372 - val_regression_loss: 11.9934 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.5656 - regression_loss: 22.8775 - val_loss: 15.7661 - val_regression_loss: 11.3311 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.7144 - regression_loss: 20.6469 - val_loss: 14.9959 - val_regression_loss: 10.4569 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.6189 - regression_loss: 19.2840 - val_loss: 13.7561 - val_regression_loss: 9.8016 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7022 - regression_loss: 17.8187 - val_loss: 13.0504 - val_regression_loss: 9.1157 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.4644 - regression_loss: 16.4253 - val_loss: 12.2584 - val_regression_loss: 8.6287 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.0197 - regression_loss: 15.2895 - val_loss: 11.7562 - val_regression_loss: 8.1199 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5380 - regression_loss: 14.0132 - val_loss: 11.1505 - val_regression_loss: 7.7973 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.8946 - regression_loss: 13.1516 - val_loss: 10.6816 - val_regression_loss: 7.3503 - lr: 1.0000e-04\n",
      "Epoch 20/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 14.5630 - regression_loss: 12.1954 - val_loss: 10.2906 - val_regression_loss: 7.0783 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8572 - regression_loss: 11.3059 - val_loss: 9.9838 - val_regression_loss: 6.8235 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4962 - regression_loss: 10.4910 - val_loss: 9.6953 - val_regression_loss: 6.5642 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4361 - regression_loss: 9.7960 - val_loss: 9.4500 - val_regression_loss: 6.2845 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2210 - regression_loss: 9.1324 - val_loss: 9.3525 - val_regression_loss: 6.3489 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.8313 - regression_loss: 8.5271 - val_loss: 9.1571 - val_regression_loss: 6.0668 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9128 - regression_loss: 7.9004 - val_loss: 9.1193 - val_regression_loss: 6.0670 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5838 - regression_loss: 7.4741 - val_loss: 8.6808 - val_regression_loss: 5.5980 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0130 - regression_loss: 7.0154 - val_loss: 8.6702 - val_regression_loss: 5.6331 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6076 - regression_loss: 6.5067 - val_loss: 8.5490 - val_regression_loss: 5.5270 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0654 - regression_loss: 6.2006 - val_loss: 8.4773 - val_regression_loss: 5.4305 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5638 - regression_loss: 5.7808 - val_loss: 8.2587 - val_regression_loss: 5.2187 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4151 - regression_loss: 5.5231 - val_loss: 8.1636 - val_regression_loss: 5.1132 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0498 - regression_loss: 5.1247 - val_loss: 8.1986 - val_regression_loss: 5.1432 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7909 - regression_loss: 4.9620 - val_loss: 8.2172 - val_regression_loss: 5.1255 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3508 - regression_loss: 4.6262 - val_loss: 7.9695 - val_regression_loss: 4.8965 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4121 - regression_loss: 4.5254 - val_loss: 7.8021 - val_regression_loss: 4.7372 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8365 - regression_loss: 4.2469 - val_loss: 8.0076 - val_regression_loss: 4.9059 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8371 - regression_loss: 4.0093 - val_loss: 8.1370 - val_regression_loss: 5.0275 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.4937 - regression_loss: 3.7990 - val_loss: 7.8160 - val_regression_loss: 4.7152 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.3741 - regression_loss: 3.5833 - val_loss: 7.8850 - val_regression_loss: 4.7653 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2205 - regression_loss: 3.4547 - val_loss: 7.7710 - val_regression_loss: 4.6387 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8686 - regression_loss: 3.2704 - val_loss: 8.0674 - val_regression_loss: 4.8839 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9110 - regression_loss: 3.2194 - val_loss: 7.8020 - val_regression_loss: 4.6545 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7034 - regression_loss: 3.0107 - val_loss: 7.7907 - val_regression_loss: 4.6275 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5543 - regression_loss: 2.8906 - val_loss: 7.9431 - val_regression_loss: 4.7346 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.4060 - regression_loss: 2.7707 - val_loss: 7.7251 - val_regression_loss: 4.5360 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3613 - regression_loss: 2.6612 - val_loss: 7.8797 - val_regression_loss: 4.6468 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2430 - regression_loss: 2.5561 - val_loss: 7.9075 - val_regression_loss: 4.6581 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0770 - regression_loss: 2.4525 - val_loss: 7.7014 - val_regression_loss: 4.4767 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0394 - regression_loss: 2.3787 - val_loss: 7.8589 - val_regression_loss: 4.5937 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7529 - regression_loss: 2.3068 - val_loss: 7.5257 - val_regression_loss: 4.3286 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8734 - regression_loss: 2.2162 - val_loss: 7.9474 - val_regression_loss: 4.6283 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7541 - regression_loss: 2.1482 - val_loss: 8.0431 - val_regression_loss: 4.6974 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7321 - regression_loss: 2.1052 - val_loss: 7.7538 - val_regression_loss: 4.4643 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5957 - regression_loss: 2.0006 - val_loss: 8.0455 - val_regression_loss: 4.6849 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5101 - regression_loss: 1.9726 - val_loss: 7.5589 - val_regression_loss: 4.3108 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3890 - regression_loss: 1.8520 - val_loss: 7.9552 - val_regression_loss: 4.5848 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3823 - regression_loss: 1.7730 - val_loss: 7.9233 - val_regression_loss: 4.5573 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2680 - regression_loss: 1.6926 - val_loss: 7.8195 - val_regression_loss: 4.4738 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2330 - regression_loss: 1.6461 - val_loss: 7.9725 - val_regression_loss: 4.5813 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1433 - regression_loss: 1.5854 - val_loss: 7.7674 - val_regression_loss: 4.4244 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1255 - regression_loss: 1.5455 - val_loss: 8.0414 - val_regression_loss: 4.6214 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0743 - regression_loss: 1.5234 - val_loss: 7.8111 - val_regression_loss: 4.4432 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0283 - regression_loss: 1.4828 - val_loss: 7.6958 - val_regression_loss: 4.3597 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9520 - regression_loss: 1.4117 - val_loss: 7.8999 - val_regression_loss: 4.4989 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9602 - regression_loss: 1.4138 - val_loss: 7.8030 - val_regression_loss: 4.4296 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8589 - regression_loss: 1.3833 - val_loss: 8.0025 - val_regression_loss: 4.5595 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7861 - regression_loss: 1.3226 - val_loss: 7.7169 - val_regression_loss: 4.3655 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7943 - regression_loss: 1.2896 - val_loss: 7.6734 - val_regression_loss: 4.3292 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8242 - regression_loss: 1.2655 - val_loss: 7.9852 - val_regression_loss: 4.5356 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7782 - regression_loss: 1.2299 - val_loss: 7.6369 - val_regression_loss: 4.3013 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7763 - regression_loss: 1.2434 - val_loss: 8.0944 - val_regression_loss: 4.6115 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6932 - regression_loss: 1.1703 - val_loss: 7.6658 - val_regression_loss: 4.3187 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7191 - regression_loss: 1.1734 - val_loss: 7.7469 - val_regression_loss: 4.3669 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6657 - regression_loss: 1.1373 - val_loss: 8.0274 - val_regression_loss: 4.5568 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6363 - regression_loss: 1.1033 - val_loss: 7.7288 - val_regression_loss: 4.3556 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5743 - regression_loss: 1.0616 - val_loss: 7.8984 - val_regression_loss: 4.4689 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5900 - regression_loss: 1.0548 - val_loss: 7.7021 - val_regression_loss: 4.3285 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5385 - regression_loss: 1.0297 - val_loss: 7.8522 - val_regression_loss: 4.4275 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5045 - regression_loss: 0.9881 - val_loss: 7.6366 - val_regression_loss: 4.2859 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4886 - regression_loss: 1.0009 - val_loss: 7.6949 - val_regression_loss: 4.3215 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5118 - regression_loss: 0.9883 - val_loss: 7.8496 - val_regression_loss: 4.4177 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4628 - regression_loss: 0.9730 - val_loss: 7.6542 - val_regression_loss: 4.3017 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5422 - regression_loss: 1.0157 - val_loss: 7.6471 - val_regression_loss: 4.2844 - lr: 1.0000e-04\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4726 - regression_loss: 0.9573 - val_loss: 7.6554 - val_regression_loss: 4.2852 - lr: 1.0000e-04\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4417 - regression_loss: 0.9579 - val_loss: 7.7252 - val_regression_loss: 4.3566 - lr: 1.0000e-04\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5467 - regression_loss: 1.0315 - val_loss: 7.8779 - val_regression_loss: 4.4460 - lr: 1.0000e-04\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4162 - regression_loss: 0.9096 - val_loss: 7.5662 - val_regression_loss: 4.2293 - lr: 1.0000e-04\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4182 - regression_loss: 0.9078 - val_loss: 7.4056 - val_regression_loss: 4.1170 - lr: 1.0000e-04\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3810 - regression_loss: 0.8837 - val_loss: 7.7419 - val_regression_loss: 4.3466 - lr: 1.0000e-04\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4225 - regression_loss: 0.9036 - val_loss: 7.5202 - val_regression_loss: 4.1954 - lr: 1.0000e-04\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3315 - regression_loss: 0.8323 - val_loss: 7.4737 - val_regression_loss: 4.1608 - lr: 1.0000e-04\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3009 - regression_loss: 0.8201 - val_loss: 7.4641 - val_regression_loss: 4.1521 - lr: 1.0000e-04\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2932 - regression_loss: 0.8165 - val_loss: 7.4697 - val_regression_loss: 4.1499 - lr: 1.0000e-04\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2935 - regression_loss: 0.8116 - val_loss: 7.1975 - val_regression_loss: 3.9718 - lr: 1.0000e-04\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2842 - regression_loss: 0.7916 - val_loss: 7.5886 - val_regression_loss: 4.2248 - lr: 1.0000e-04\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3049 - regression_loss: 0.8184 - val_loss: 7.3314 - val_regression_loss: 4.0664 - lr: 1.0000e-04\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2469 - regression_loss: 0.7859 - val_loss: 7.2814 - val_regression_loss: 4.0251 - lr: 1.0000e-04\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2722 - regression_loss: 0.7757 - val_loss: 7.3829 - val_regression_loss: 4.0865 - lr: 1.0000e-04\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2768 - regression_loss: 0.7868 - val_loss: 7.2430 - val_regression_loss: 3.9987 - lr: 1.0000e-04\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2336 - regression_loss: 0.7593 - val_loss: 7.2472 - val_regression_loss: 3.9971 - lr: 1.0000e-04\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2793 - regression_loss: 0.8001 - val_loss: 7.4189 - val_regression_loss: 4.1134 - lr: 1.0000e-04\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3010 - regression_loss: 0.8248 - val_loss: 7.0096 - val_regression_loss: 3.8635 - lr: 1.0000e-04\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3381 - regression_loss: 0.8445 - val_loss: 7.0459 - val_regression_loss: 3.8697 - lr: 1.0000e-04\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2461 - regression_loss: 0.7586 - val_loss: 7.2638 - val_regression_loss: 4.0027 - lr: 1.0000e-04\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1866 - regression_loss: 0.7123 - val_loss: 7.0687 - val_regression_loss: 3.8758 - lr: 1.0000e-04\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1785 - regression_loss: 0.7042 - val_loss: 7.0484 - val_regression_loss: 3.8640 - lr: 1.0000e-04\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2111 - regression_loss: 0.7246 - val_loss: 7.2931 - val_regression_loss: 4.0277 - lr: 1.0000e-04\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2728 - regression_loss: 0.7808 - val_loss: 7.0774 - val_regression_loss: 3.8750 - lr: 1.0000e-04\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1913 - regression_loss: 0.7152 - val_loss: 6.8863 - val_regression_loss: 3.7834 - lr: 1.0000e-04\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2287 - regression_loss: 0.7513 - val_loss: 7.1130 - val_regression_loss: 3.9009 - lr: 1.0000e-04\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1463 - regression_loss: 0.6921 - val_loss: 6.8843 - val_regression_loss: 3.7519 - lr: 1.0000e-04\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1425 - regression_loss: 0.6603 - val_loss: 6.9675 - val_regression_loss: 3.8007 - lr: 1.0000e-04\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1534 - regression_loss: 0.6793 - val_loss: 6.7661 - val_regression_loss: 3.6787 - lr: 1.0000e-04\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1819 - regression_loss: 0.6933 - val_loss: 6.8924 - val_regression_loss: 3.7566 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1380 - regression_loss: 0.6569 - val_loss: 6.8569 - val_regression_loss: 3.7299 - lr: 1.0000e-04\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1212 - regression_loss: 0.6532 - val_loss: 6.7576 - val_regression_loss: 3.6650 - lr: 1.0000e-04\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1208 - regression_loss: 0.6600 - val_loss: 6.8437 - val_regression_loss: 3.7149 - lr: 1.0000e-04\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1257 - regression_loss: 0.6501 - val_loss: 6.4950 - val_regression_loss: 3.4984 - lr: 1.0000e-04\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1324 - regression_loss: 0.6646 - val_loss: 6.6517 - val_regression_loss: 3.5915 - lr: 1.0000e-04\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1400 - regression_loss: 0.6660 - val_loss: 6.7691 - val_regression_loss: 3.6638 - lr: 1.0000e-04\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1527 - regression_loss: 0.6759 - val_loss: 6.9080 - val_regression_loss: 3.7689 - lr: 1.0000e-04\n",
      "Epoch 123/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.0924 - regression_loss: 0.6661\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1973 - regression_loss: 0.7156 - val_loss: 6.7034 - val_regression_loss: 3.6500 - lr: 1.0000e-04\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1654 - regression_loss: 0.6889 - val_loss: 6.8208 - val_regression_loss: 3.7040 - lr: 5.0000e-05\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1170 - regression_loss: 0.6393 - val_loss: 6.4485 - val_regression_loss: 3.4841 - lr: 5.0000e-05\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1489 - regression_loss: 0.6739 - val_loss: 6.8854 - val_regression_loss: 3.7482 - lr: 5.0000e-05\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1626 - regression_loss: 0.6912 - val_loss: 6.5463 - val_regression_loss: 3.5445 - lr: 5.0000e-05\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1458 - regression_loss: 0.6764 - val_loss: 6.6410 - val_regression_loss: 3.5830 - lr: 5.0000e-05\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1292 - regression_loss: 0.6596 - val_loss: 6.3957 - val_regression_loss: 3.4337 - lr: 5.0000e-05\n",
      "Epoch 130/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.1288 - regression_loss: 0.7034\n",
      "Epoch 130: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1178 - regression_loss: 0.6406 - val_loss: 6.5938 - val_regression_loss: 3.5455 - lr: 5.0000e-05\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0919 - regression_loss: 0.6231 - val_loss: 6.4581 - val_regression_loss: 3.4593 - lr: 2.5000e-05\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0734 - regression_loss: 0.6155 - val_loss: 6.4424 - val_regression_loss: 3.4509 - lr: 2.5000e-05\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0616 - regression_loss: 0.6064 - val_loss: 6.5376 - val_regression_loss: 3.5086 - lr: 2.5000e-05\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0644 - regression_loss: 0.6002 - val_loss: 6.4603 - val_regression_loss: 3.4614 - lr: 2.5000e-05\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0678 - regression_loss: 0.6003 - val_loss: 6.4396 - val_regression_loss: 3.4480 - lr: 2.5000e-05\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0489 - regression_loss: 0.5943 - val_loss: 6.4900 - val_regression_loss: 3.4782 - lr: 2.5000e-05\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0649 - regression_loss: 0.5989 - val_loss: 6.4749 - val_regression_loss: 3.4693 - lr: 2.5000e-05\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0621 - regression_loss: 0.6021 - val_loss: 6.4382 - val_regression_loss: 3.4467 - lr: 2.5000e-05\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0607 - regression_loss: 0.5960 - val_loss: 6.5034 - val_regression_loss: 3.4857 - lr: 2.5000e-05\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0664 - regression_loss: 0.5955 - val_loss: 6.4220 - val_regression_loss: 3.4352 - lr: 2.5000e-05\n",
      "Epoch 141/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.0691 - regression_loss: 0.6445\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0594 - regression_loss: 0.5935 - val_loss: 6.4202 - val_regression_loss: 3.4330 - lr: 2.5000e-05\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0619 - regression_loss: 0.5892 - val_loss: 6.4294 - val_regression_loss: 3.4384 - lr: 1.2500e-05\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0517 - regression_loss: 0.5933 - val_loss: 6.4298 - val_regression_loss: 3.4391 - lr: 1.2500e-05\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0430 - regression_loss: 0.5882 - val_loss: 6.4408 - val_regression_loss: 3.4451 - lr: 1.2500e-05\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0429 - regression_loss: 0.5886 - val_loss: 6.4348 - val_regression_loss: 3.4411 - lr: 1.2500e-05\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0517 - regression_loss: 0.5888 - val_loss: 6.4137 - val_regression_loss: 3.4284 - lr: 1.2500e-05\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0627 - regression_loss: 0.5879 - val_loss: 6.4196 - val_regression_loss: 3.4319 - lr: 1.2500e-05\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0598 - regression_loss: 0.5872 - val_loss: 6.4215 - val_regression_loss: 3.4324 - lr: 1.2500e-05\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0611 - regression_loss: 0.5873 - val_loss: 6.3996 - val_regression_loss: 3.4186 - lr: 1.2500e-05\n",
      "Epoch 150/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.0652 - regression_loss: 0.6409\n",
      "Epoch 150: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0608 - regression_loss: 0.5881 - val_loss: 6.4069 - val_regression_loss: 3.4228 - lr: 1.2500e-05\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0502 - regression_loss: 0.5866 - val_loss: 6.3940 - val_regression_loss: 3.4153 - lr: 6.2500e-06\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0495 - regression_loss: 0.5859 - val_loss: 6.3952 - val_regression_loss: 3.4158 - lr: 6.2500e-06\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0472 - regression_loss: 0.5852 - val_loss: 6.3986 - val_regression_loss: 3.4177 - lr: 6.2500e-06\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0582 - regression_loss: 0.5848 - val_loss: 6.3977 - val_regression_loss: 3.4171 - lr: 6.2500e-06\n",
      "Epoch 155/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.9904 - regression_loss: 0.5662\n",
      "Epoch 155: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0496 - regression_loss: 0.5846 - val_loss: 6.4087 - val_regression_loss: 3.4240 - lr: 6.2500e-06\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0487 - regression_loss: 0.5842 - val_loss: 6.4079 - val_regression_loss: 3.4234 - lr: 3.1250e-06\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0489 - regression_loss: 0.5841 - val_loss: 6.4018 - val_regression_loss: 3.4197 - lr: 3.1250e-06\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0448 - regression_loss: 0.5839 - val_loss: 6.4032 - val_regression_loss: 3.4205 - lr: 3.1250e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0524 - regression_loss: 0.5837 - val_loss: 6.4016 - val_regression_loss: 3.4195 - lr: 3.1250e-06\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0331 - regression_loss: 0.5835 - val_loss: 6.3990 - val_regression_loss: 3.4178 - lr: 3.1250e-06\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0379 - regression_loss: 0.5836 - val_loss: 6.3987 - val_regression_loss: 3.4177 - lr: 3.1250e-06\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0456 - regression_loss: 0.5836 - val_loss: 6.3935 - val_regression_loss: 3.4142 - lr: 3.1250e-06\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0436 - regression_loss: 0.5835 - val_loss: 6.3938 - val_regression_loss: 3.4144 - lr: 3.1250e-06\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0444 - regression_loss: 0.5832 - val_loss: 6.3951 - val_regression_loss: 3.4151 - lr: 3.1250e-06\n",
      "Epoch 165/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.0782 - regression_loss: 0.6540\n",
      "Epoch 165: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0603 - regression_loss: 0.5835 - val_loss: 6.3885 - val_regression_loss: 3.4109 - lr: 3.1250e-06\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.0496 - regression_loss: 0.5829 - val_loss: 6.3900 - val_regression_loss: 3.4118 - lr: 1.5625e-06\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0514 - regression_loss: 0.5829 - val_loss: 6.3912 - val_regression_loss: 3.4125 - lr: 1.5625e-06\n",
      "Epoch 168/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0399 - regression_loss: 0.5828 - val_loss: 6.3925 - val_regression_loss: 3.4133 - lr: 1.5625e-06\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0531 - regression_loss: 0.5828 - val_loss: 6.3897 - val_regression_loss: 3.4116 - lr: 1.5625e-06\n",
      "Epoch 170/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.0699 - regression_loss: 0.6458\n",
      "Epoch 170: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.0468 - regression_loss: 0.5827 - val_loss: 6.3896 - val_regression_loss: 3.4115 - lr: 1.5625e-06\n",
      "Epoch 171/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0481 - regression_loss: 0.5826 - val_loss: 6.3893 - val_regression_loss: 3.4113 - lr: 7.8125e-07\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0535 - regression_loss: 0.5826 - val_loss: 6.3896 - val_regression_loss: 3.4114 - lr: 7.8125e-07\n",
      "Epoch 173/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0507 - regression_loss: 0.5826 - val_loss: 6.3880 - val_regression_loss: 3.4105 - lr: 7.8125e-07\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0429 - regression_loss: 0.5825 - val_loss: 6.3884 - val_regression_loss: 3.4107 - lr: 7.8125e-07\n",
      "Epoch 175/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.1216 - regression_loss: 0.6975\n",
      "Epoch 175: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0476 - regression_loss: 0.5825 - val_loss: 6.3870 - val_regression_loss: 3.4098 - lr: 7.8125e-07\n",
      "Epoch 176/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0557 - regression_loss: 0.5824 - val_loss: 6.3869 - val_regression_loss: 3.4098 - lr: 3.9062e-07\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0522 - regression_loss: 0.5824 - val_loss: 6.3874 - val_regression_loss: 3.4101 - lr: 3.9062e-07\n",
      "Epoch 178/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0514 - regression_loss: 0.5823 - val_loss: 6.3872 - val_regression_loss: 3.4099 - lr: 3.9062e-07\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0313 - regression_loss: 0.5824 - val_loss: 6.3876 - val_regression_loss: 3.4102 - lr: 3.9062e-07\n",
      "Epoch 180/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0548 - regression_loss: 0.5823 - val_loss: 6.3879 - val_regression_loss: 3.4104 - lr: 3.9062e-07\n",
      "Epoch 181/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0423 - regression_loss: 0.5823 - val_loss: 6.3876 - val_regression_loss: 3.4102 - lr: 3.9062e-07\n",
      "Epoch 182/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0467 - regression_loss: 0.5823 - val_loss: 6.3882 - val_regression_loss: 3.4105 - lr: 3.9062e-07\n",
      "Epoch 183/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0489 - regression_loss: 0.5823 - val_loss: 6.3875 - val_regression_loss: 3.4101 - lr: 3.9062e-07\n",
      "Epoch 184/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0294 - regression_loss: 0.5823 - val_loss: 6.3874 - val_regression_loss: 3.4100 - lr: 3.9062e-07\n",
      "Epoch 185/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0494 - regression_loss: 0.5822 - val_loss: 6.3870 - val_regression_loss: 3.4098 - lr: 3.9062e-07\n",
      "Epoch 186/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0306 - regression_loss: 0.5822 - val_loss: 6.3867 - val_regression_loss: 3.4096 - lr: 3.9062e-07\n",
      "Epoch 187/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0439 - regression_loss: 0.5822 - val_loss: 6.3872 - val_regression_loss: 3.4099 - lr: 3.9062e-07\n",
      "Epoch 188/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0475 - regression_loss: 0.5823 - val_loss: 6.3864 - val_regression_loss: 3.4094 - lr: 3.9062e-07\n",
      "Epoch 189/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.2699 - regression_loss: 0.8458\n",
      "Epoch 189: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0455 - regression_loss: 0.5822 - val_loss: 6.3865 - val_regression_loss: 3.4095 - lr: 3.9062e-07\n",
      "Epoch 190/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0501 - regression_loss: 0.5821 - val_loss: 6.3866 - val_regression_loss: 3.4095 - lr: 1.9531e-07\n",
      "Epoch 191/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0518 - regression_loss: 0.5821 - val_loss: 6.3865 - val_regression_loss: 3.4095 - lr: 1.9531e-07\n",
      "Epoch 192/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0465 - regression_loss: 0.5821 - val_loss: 6.3866 - val_regression_loss: 3.4095 - lr: 1.9531e-07\n",
      "Epoch 193/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0369 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 1.9531e-07\n",
      "Epoch 194/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.2474 - regression_loss: 0.8234\n",
      "Epoch 194: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0586 - regression_loss: 0.5821 - val_loss: 6.3862 - val_regression_loss: 3.4092 - lr: 1.9531e-07\n",
      "Epoch 195/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0574 - regression_loss: 0.5821 - val_loss: 6.3862 - val_regression_loss: 3.4092 - lr: 9.7656e-08\n",
      "Epoch 196/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0419 - regression_loss: 0.5821 - val_loss: 6.3861 - val_regression_loss: 3.4092 - lr: 9.7656e-08\n",
      "Epoch 197/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0514 - regression_loss: 0.5821 - val_loss: 6.3861 - val_regression_loss: 3.4092 - lr: 9.7656e-08\n",
      "Epoch 198/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0404 - regression_loss: 0.5821 - val_loss: 6.3865 - val_regression_loss: 3.4094 - lr: 9.7656e-08\n",
      "Epoch 199/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.1400 - regression_loss: 0.7159\n",
      "Epoch 199: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0429 - regression_loss: 0.5821 - val_loss: 6.3864 - val_regression_loss: 3.4094 - lr: 9.7656e-08\n",
      "Epoch 200/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0394 - regression_loss: 0.5821 - val_loss: 6.3864 - val_regression_loss: 3.4093 - lr: 4.8828e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0498 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 4.8828e-08\n",
      "Epoch 202/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0479 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 4.8828e-08\n",
      "Epoch 203/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0506 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 4.8828e-08\n",
      "Epoch 204/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.0492 - regression_loss: 0.6251\n",
      "Epoch 204: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.0426 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 4.8828e-08\n",
      "Epoch 205/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0531 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 2.4414e-08\n",
      "Epoch 206/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0452 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 2.4414e-08\n",
      "Epoch 207/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0542 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 2.4414e-08\n",
      "Epoch 208/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0429 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 2.4414e-08\n",
      "Epoch 209/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.1895 - regression_loss: 0.7654\n",
      "Epoch 209: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0481 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 2.4414e-08\n",
      "Epoch 210/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0535 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 1.2207e-08\n",
      "Epoch 211/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0415 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 1.2207e-08\n",
      "Epoch 212/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0407 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 1.2207e-08\n",
      "Epoch 213/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0519 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 1.2207e-08\n",
      "Epoch 214/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.0577 - regression_loss: 0.6336\n",
      "Epoch 214: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0418 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 1.2207e-08\n",
      "Epoch 215/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0421 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 6.1035e-09\n",
      "Epoch 216/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0432 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 6.1035e-09\n",
      "Epoch 217/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0538 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 6.1035e-09\n",
      "Epoch 218/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0509 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 6.1035e-09\n",
      "Epoch 219/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.0930 - regression_loss: 0.6690\n",
      "Epoch 219: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0449 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 6.1035e-09\n",
      "Epoch 220/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0440 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 3.0518e-09\n",
      "Epoch 221/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0569 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 3.0518e-09\n",
      "Epoch 222/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0487 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 3.0518e-09\n",
      "Epoch 223/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0389 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 3.0518e-09\n",
      "Epoch 224/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.0029 - regression_loss: 0.5789\n",
      "Epoch 224: ReduceLROnPlateau reducing learning rate to 1.5258788677030566e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0446 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 3.0518e-09\n",
      "Epoch 225/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0434 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 1.5259e-09\n",
      "Epoch 226/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0428 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 1.5259e-09\n",
      "Epoch 227/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0581 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 1.5259e-09\n",
      "Epoch 228/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0380 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 1.5259e-09\n",
      "Epoch 229/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.0330 - regression_loss: 0.6089\n",
      "Epoch 229: ReduceLROnPlateau reducing learning rate to 7.629394338515283e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0344 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 1.5259e-09\n",
      "Epoch 230/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0492 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 7.6294e-10\n",
      "Epoch 231/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0418 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 7.6294e-10\n",
      "Epoch 232/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0466 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 7.6294e-10\n",
      "Epoch 233/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0524 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 7.6294e-10\n",
      "Epoch 234/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.0927 - regression_loss: 0.6686\n",
      "Epoch 234: ReduceLROnPlateau reducing learning rate to 3.8146971692576415e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0297 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 7.6294e-10\n",
      "Epoch 235/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0575 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 3.8147e-10\n",
      "Epoch 236/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0519 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 3.8147e-10\n",
      "Epoch 237/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0454 - regression_loss: 0.5821 - val_loss: 6.3863 - val_regression_loss: 3.4093 - lr: 3.8147e-10\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 26ms/step - loss: 27.4838 - regression_loss: 24.6677 - val_loss: 25.2102 - val_regression_loss: 17.5581 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.9710 - regression_loss: 21.4498 - val_loss: 20.1395 - val_regression_loss: 13.6572 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.7279 - regression_loss: 16.5018 - val_loss: 18.7704 - val_regression_loss: 12.6639 - lr: 1.0000e-04\n",
      "Epoch 4/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7227 - regression_loss: 14.4882 - val_loss: 15.3324 - val_regression_loss: 10.0572 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.3562 - regression_loss: 11.8700 - val_loss: 14.2817 - val_regression_loss: 9.5750 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5217 - regression_loss: 10.5603 - val_loss: 11.8826 - val_regression_loss: 7.6922 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1778 - regression_loss: 9.0822 - val_loss: 11.5866 - val_regression_loss: 7.5566 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.1040 - regression_loss: 8.0425 - val_loss: 9.6995 - val_regression_loss: 6.0616 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9571 - regression_loss: 7.0975 - val_loss: 8.8072 - val_regression_loss: 5.4764 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9143 - regression_loss: 6.0576 - val_loss: 8.0661 - val_regression_loss: 4.9160 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8989 - regression_loss: 5.4446 - val_loss: 7.3282 - val_regression_loss: 4.3904 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7159 - regression_loss: 4.8750 - val_loss: 7.2469 - val_regression_loss: 4.3964 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.0028 - regression_loss: 4.4113 - val_loss: 6.4791 - val_regression_loss: 3.7654 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8956 - regression_loss: 4.0622 - val_loss: 6.8481 - val_regression_loss: 4.1530 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2831 - regression_loss: 3.5844 - val_loss: 5.6819 - val_regression_loss: 3.1674 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9575 - regression_loss: 3.2259 - val_loss: 6.1309 - val_regression_loss: 3.6249 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6089 - regression_loss: 2.9996 - val_loss: 5.0205 - val_regression_loss: 2.7016 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3176 - regression_loss: 2.6374 - val_loss: 5.0245 - val_regression_loss: 2.7543 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8987 - regression_loss: 2.3217 - val_loss: 4.6397 - val_regression_loss: 2.4313 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8425 - regression_loss: 2.1831 - val_loss: 4.5479 - val_regression_loss: 2.3650 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5650 - regression_loss: 1.9464 - val_loss: 4.3328 - val_regression_loss: 2.1966 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2746 - regression_loss: 1.8327 - val_loss: 4.2715 - val_regression_loss: 2.1631 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1991 - regression_loss: 1.6959 - val_loss: 4.1410 - val_regression_loss: 2.0578 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1091 - regression_loss: 1.5938 - val_loss: 4.0028 - val_regression_loss: 1.9477 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2310 - regression_loss: 1.6523 - val_loss: 4.7111 - val_regression_loss: 2.5495 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1513 - regression_loss: 1.6367 - val_loss: 4.0035 - val_regression_loss: 1.9540 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9440 - regression_loss: 1.3997 - val_loss: 3.8420 - val_regression_loss: 1.8190 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8155 - regression_loss: 1.3052 - val_loss: 3.7787 - val_regression_loss: 1.7730 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7259 - regression_loss: 1.2314 - val_loss: 3.8154 - val_regression_loss: 1.8112 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7250 - regression_loss: 1.1859 - val_loss: 3.8800 - val_regression_loss: 1.8685 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7199 - regression_loss: 1.2017 - val_loss: 3.6306 - val_regression_loss: 1.6607 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6050 - regression_loss: 1.0905 - val_loss: 3.7263 - val_regression_loss: 1.7478 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5384 - regression_loss: 1.0900 - val_loss: 3.5248 - val_regression_loss: 1.5699 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5383 - regression_loss: 1.0615 - val_loss: 3.5112 - val_regression_loss: 1.5661 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4909 - regression_loss: 1.0053 - val_loss: 3.8030 - val_regression_loss: 1.8123 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5366 - regression_loss: 1.0172 - val_loss: 3.4695 - val_regression_loss: 1.5389 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4399 - regression_loss: 0.9426 - val_loss: 3.5228 - val_regression_loss: 1.5832 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3230 - regression_loss: 0.9087 - val_loss: 3.5063 - val_regression_loss: 1.5682 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3609 - regression_loss: 0.8799 - val_loss: 3.4108 - val_regression_loss: 1.4954 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3362 - regression_loss: 0.8438 - val_loss: 3.4348 - val_regression_loss: 1.5140 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3288 - regression_loss: 0.8305 - val_loss: 3.5087 - val_regression_loss: 1.5776 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3126 - regression_loss: 0.8491 - val_loss: 3.4693 - val_regression_loss: 1.5460 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3050 - regression_loss: 0.8324 - val_loss: 3.2534 - val_regression_loss: 1.3587 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2623 - regression_loss: 0.8022 - val_loss: 3.2279 - val_regression_loss: 1.3478 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2604 - regression_loss: 0.7754 - val_loss: 3.2404 - val_regression_loss: 1.3559 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1757 - regression_loss: 0.7282 - val_loss: 3.5344 - val_regression_loss: 1.6090 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2532 - regression_loss: 0.7720 - val_loss: 3.2944 - val_regression_loss: 1.4026 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2084 - regression_loss: 0.7340 - val_loss: 3.1359 - val_regression_loss: 1.2655 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2887 - regression_loss: 0.8007 - val_loss: 3.1019 - val_regression_loss: 1.2427 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1453 - regression_loss: 0.6736 - val_loss: 3.3532 - val_regression_loss: 1.4552 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1054 - regression_loss: 0.6470 - val_loss: 3.2357 - val_regression_loss: 1.3580 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1035 - regression_loss: 0.6400 - val_loss: 3.0751 - val_regression_loss: 1.2278 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1080 - regression_loss: 0.6400 - val_loss: 3.1171 - val_regression_loss: 1.2599 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1014 - regression_loss: 0.6473 - val_loss: 3.2322 - val_regression_loss: 1.3646 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0601 - regression_loss: 0.6029 - val_loss: 3.2428 - val_regression_loss: 1.3697 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0684 - regression_loss: 0.6310 - val_loss: 3.0067 - val_regression_loss: 1.1742 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0468 - regression_loss: 0.5858 - val_loss: 3.0341 - val_regression_loss: 1.2005 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0557 - regression_loss: 0.5851 - val_loss: 3.0001 - val_regression_loss: 1.1775 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0477 - regression_loss: 0.5953 - val_loss: 3.0828 - val_regression_loss: 1.2457 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0062 - regression_loss: 0.5481 - val_loss: 3.2516 - val_regression_loss: 1.3802 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9960 - regression_loss: 0.5464 - val_loss: 3.1907 - val_regression_loss: 1.3373 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9860 - regression_loss: 0.5352 - val_loss: 2.9318 - val_regression_loss: 1.1134 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0316 - regression_loss: 0.5726 - val_loss: 2.9700 - val_regression_loss: 1.1496 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9715 - regression_loss: 0.5214 - val_loss: 3.2289 - val_regression_loss: 1.3691 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9486 - regression_loss: 0.5048 - val_loss: 2.9924 - val_regression_loss: 1.1733 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9248 - regression_loss: 0.4867 - val_loss: 2.8986 - val_regression_loss: 1.0972 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9147 - regression_loss: 0.4659 - val_loss: 3.0274 - val_regression_loss: 1.2058 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8768 - regression_loss: 0.4559 - val_loss: 2.8910 - val_regression_loss: 1.0937 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8937 - regression_loss: 0.4542 - val_loss: 2.8734 - val_regression_loss: 1.0840 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8963 - regression_loss: 0.4575 - val_loss: 2.9161 - val_regression_loss: 1.1208 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8918 - regression_loss: 0.4499 - val_loss: 2.9395 - val_regression_loss: 1.1408 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8649 - regression_loss: 0.4273 - val_loss: 2.8010 - val_regression_loss: 1.0266 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8825 - regression_loss: 0.4451 - val_loss: 2.8481 - val_regression_loss: 1.0652 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8467 - regression_loss: 0.4260 - val_loss: 2.8883 - val_regression_loss: 1.0998 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8844 - regression_loss: 0.4479 - val_loss: 3.2954 - val_regression_loss: 1.4281 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9299 - regression_loss: 0.4834 - val_loss: 2.8099 - val_regression_loss: 1.0378 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8748 - regression_loss: 0.4415 - val_loss: 2.7794 - val_regression_loss: 1.0082 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8987 - regression_loss: 0.4572 - val_loss: 2.7222 - val_regression_loss: 0.9640 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.9217 - regression_loss: 0.5177\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8525 - regression_loss: 0.4260 - val_loss: 2.9217 - val_regression_loss: 1.1326 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8202 - regression_loss: 0.3976 - val_loss: 2.7616 - val_regression_loss: 1.0026 - lr: 5.0000e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8179 - regression_loss: 0.3837 - val_loss: 2.7715 - val_regression_loss: 1.0119 - lr: 5.0000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8184 - regression_loss: 0.3847 - val_loss: 2.8787 - val_regression_loss: 1.1001 - lr: 5.0000e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8342 - regression_loss: 0.4039 - val_loss: 2.7374 - val_regression_loss: 0.9790 - lr: 5.0000e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8264 - regression_loss: 0.4028 - val_loss: 3.1176 - val_regression_loss: 1.2972 - lr: 5.0000e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8839 - regression_loss: 0.4514 - val_loss: 2.7491 - val_regression_loss: 0.9869 - lr: 5.0000e-05\n",
      "Epoch 86/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.9964 - regression_loss: 0.5940\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8703 - regression_loss: 0.4314 - val_loss: 3.0211 - val_regression_loss: 1.2177 - lr: 5.0000e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8209 - regression_loss: 0.3903 - val_loss: 2.7014 - val_regression_loss: 0.9550 - lr: 2.5000e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8255 - regression_loss: 0.3953 - val_loss: 2.8133 - val_regression_loss: 1.0513 - lr: 2.5000e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8039 - regression_loss: 0.3770 - val_loss: 2.7532 - val_regression_loss: 1.0019 - lr: 2.5000e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7991 - regression_loss: 0.3718 - val_loss: 2.7366 - val_regression_loss: 0.9863 - lr: 2.5000e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7814 - regression_loss: 0.3584 - val_loss: 2.8045 - val_regression_loss: 1.0433 - lr: 2.5000e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7905 - regression_loss: 0.3613 - val_loss: 2.7186 - val_regression_loss: 0.9729 - lr: 2.5000e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7856 - regression_loss: 0.3619 - val_loss: 2.7633 - val_regression_loss: 1.0101 - lr: 2.5000e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7835 - regression_loss: 0.3573 - val_loss: 2.7579 - val_regression_loss: 1.0062 - lr: 2.5000e-05\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7783 - regression_loss: 0.3572 - val_loss: 2.7356 - val_regression_loss: 0.9872 - lr: 2.5000e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7857 - regression_loss: 0.3580 - val_loss: 2.7345 - val_regression_loss: 0.9874 - lr: 2.5000e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7872 - regression_loss: 0.3544 - val_loss: 2.7152 - val_regression_loss: 0.9726 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7857 - regression_loss: 0.3578 - val_loss: 2.7463 - val_regression_loss: 0.9964 - lr: 2.5000e-05\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7813 - regression_loss: 0.3511 - val_loss: 2.7263 - val_regression_loss: 0.9812 - lr: 2.5000e-05\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7728 - regression_loss: 0.3508 - val_loss: 2.7284 - val_regression_loss: 0.9832 - lr: 2.5000e-05\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7774 - regression_loss: 0.3497 - val_loss: 2.7562 - val_regression_loss: 1.0060 - lr: 2.5000e-05\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7748 - regression_loss: 0.3484 - val_loss: 2.7315 - val_regression_loss: 0.9847 - lr: 2.5000e-05\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7810 - regression_loss: 0.3483 - val_loss: 2.7333 - val_regression_loss: 0.9870 - lr: 2.5000e-05\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7771 - regression_loss: 0.3469 - val_loss: 2.7184 - val_regression_loss: 0.9764 - lr: 2.5000e-05\n",
      "Epoch 105/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.6964 - regression_loss: 0.2960\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7731 - regression_loss: 0.3445 - val_loss: 2.6960 - val_regression_loss: 0.9575 - lr: 2.5000e-05\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7700 - regression_loss: 0.3444 - val_loss: 2.7165 - val_regression_loss: 0.9746 - lr: 1.2500e-05\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7622 - regression_loss: 0.3443 - val_loss: 2.7458 - val_regression_loss: 0.9989 - lr: 1.2500e-05\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7649 - regression_loss: 0.3422 - val_loss: 2.7159 - val_regression_loss: 0.9737 - lr: 1.2500e-05\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7736 - regression_loss: 0.3446 - val_loss: 2.7181 - val_regression_loss: 0.9755 - lr: 1.2500e-05\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7706 - regression_loss: 0.3420 - val_loss: 2.7226 - val_regression_loss: 0.9800 - lr: 1.2500e-05\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7682 - regression_loss: 0.3429 - val_loss: 2.7414 - val_regression_loss: 0.9955 - lr: 1.2500e-05\n",
      "Epoch 112/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.7354 - regression_loss: 0.3354\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7622 - regression_loss: 0.3404 - val_loss: 2.7092 - val_regression_loss: 0.9686 - lr: 1.2500e-05\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7608 - regression_loss: 0.3409 - val_loss: 2.7001 - val_regression_loss: 0.9618 - lr: 6.2500e-06\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7598 - regression_loss: 0.3391 - val_loss: 2.7233 - val_regression_loss: 0.9812 - lr: 6.2500e-06\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7649 - regression_loss: 0.3391 - val_loss: 2.7269 - val_regression_loss: 0.9842 - lr: 6.2500e-06\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7689 - regression_loss: 0.3384 - val_loss: 2.7125 - val_regression_loss: 0.9723 - lr: 6.2500e-06\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7693 - regression_loss: 0.3381 - val_loss: 2.7045 - val_regression_loss: 0.9656 - lr: 6.2500e-06\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7632 - regression_loss: 0.3393 - val_loss: 2.7047 - val_regression_loss: 0.9658 - lr: 6.2500e-06\n",
      "Epoch 119/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.8112 - regression_loss: 0.4115\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7661 - regression_loss: 0.3377 - val_loss: 2.7219 - val_regression_loss: 0.9802 - lr: 6.2500e-06\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7618 - regression_loss: 0.3378 - val_loss: 2.7151 - val_regression_loss: 0.9747 - lr: 3.1250e-06\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7560 - regression_loss: 0.3371 - val_loss: 2.7132 - val_regression_loss: 0.9731 - lr: 3.1250e-06\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7596 - regression_loss: 0.3373 - val_loss: 2.7149 - val_regression_loss: 0.9744 - lr: 3.1250e-06\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7646 - regression_loss: 0.3377 - val_loss: 2.7033 - val_regression_loss: 0.9650 - lr: 3.1250e-06\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7586 - regression_loss: 0.3369 - val_loss: 2.7072 - val_regression_loss: 0.9682 - lr: 3.1250e-06\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7558 - regression_loss: 0.3366 - val_loss: 2.7055 - val_regression_loss: 0.9668 - lr: 3.1250e-06\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7641 - regression_loss: 0.3365 - val_loss: 2.7079 - val_regression_loss: 0.9687 - lr: 3.1250e-06\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7513 - regression_loss: 0.3369 - val_loss: 2.7141 - val_regression_loss: 0.9739 - lr: 3.1250e-06\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7572 - regression_loss: 0.3365 - val_loss: 2.7135 - val_regression_loss: 0.9734 - lr: 3.1250e-06\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7595 - regression_loss: 0.3362 - val_loss: 2.7045 - val_regression_loss: 0.9661 - lr: 3.1250e-06\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7607 - regression_loss: 0.3359 - val_loss: 2.7056 - val_regression_loss: 0.9671 - lr: 3.1250e-06\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7606 - regression_loss: 0.3364 - val_loss: 2.7027 - val_regression_loss: 0.9649 - lr: 3.1250e-06\n",
      "Epoch 132/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.8583 - regression_loss: 0.4588\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7605 - regression_loss: 0.3357 - val_loss: 2.7053 - val_regression_loss: 0.9671 - lr: 3.1250e-06\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7520 - regression_loss: 0.3357 - val_loss: 2.7093 - val_regression_loss: 0.9703 - lr: 1.5625e-06\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7565 - regression_loss: 0.3355 - val_loss: 2.7080 - val_regression_loss: 0.9693 - lr: 1.5625e-06\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7633 - regression_loss: 0.3353 - val_loss: 2.7077 - val_regression_loss: 0.9690 - lr: 1.5625e-06\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7581 - regression_loss: 0.3353 - val_loss: 2.7047 - val_regression_loss: 0.9666 - lr: 1.5625e-06\n",
      "Epoch 137/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.7490 - regression_loss: 0.3495\n",
      "Epoch 137: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7574 - regression_loss: 0.3352 - val_loss: 2.7045 - val_regression_loss: 0.9663 - lr: 1.5625e-06\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7526 - regression_loss: 0.3352 - val_loss: 2.7050 - val_regression_loss: 0.9668 - lr: 7.8125e-07\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7547 - regression_loss: 0.3351 - val_loss: 2.7049 - val_regression_loss: 0.9667 - lr: 7.8125e-07\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7613 - regression_loss: 0.3351 - val_loss: 2.7036 - val_regression_loss: 0.9656 - lr: 7.8125e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7570 - regression_loss: 0.3350 - val_loss: 2.7042 - val_regression_loss: 0.9662 - lr: 7.8125e-07\n",
      "Epoch 142/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.7301 - regression_loss: 0.3306\n",
      "Epoch 142: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7627 - regression_loss: 0.3350 - val_loss: 2.7050 - val_regression_loss: 0.9669 - lr: 7.8125e-07\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7512 - regression_loss: 0.3349 - val_loss: 2.7046 - val_regression_loss: 0.9665 - lr: 3.9062e-07\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7596 - regression_loss: 0.3349 - val_loss: 2.7042 - val_regression_loss: 0.9662 - lr: 3.9062e-07\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7631 - regression_loss: 0.3349 - val_loss: 2.7043 - val_regression_loss: 0.9662 - lr: 3.9062e-07\n",
      "3/3 [==============================] - 0s 971us/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 26ms/step - loss: 161.0262 - regression_loss: 147.1543 - val_loss: 100.3777 - val_regression_loss: 82.9579 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 99.3619 - regression_loss: 91.9436 - val_loss: 74.7457 - val_regression_loss: 61.9757 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 84.6533 - regression_loss: 78.9029 - val_loss: 67.7012 - val_regression_loss: 55.0612 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 77.9350 - regression_loss: 70.0089 - val_loss: 62.6849 - val_regression_loss: 50.6332 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 68.1485 - regression_loss: 62.7510 - val_loss: 56.4565 - val_regression_loss: 45.2358 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 62.3083 - regression_loss: 55.4423 - val_loss: 50.2014 - val_regression_loss: 39.9043 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 51.8921 - regression_loss: 48.5166 - val_loss: 45.7117 - val_regression_loss: 35.4452 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 46.3518 - regression_loss: 42.1829 - val_loss: 39.9425 - val_regression_loss: 30.7824 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 40.0041 - regression_loss: 36.1905 - val_loss: 35.4094 - val_regression_loss: 27.0295 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 33.6273 - regression_loss: 30.6119 - val_loss: 31.4804 - val_regression_loss: 23.8489 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.8726 - regression_loss: 25.8818 - val_loss: 27.8517 - val_regression_loss: 21.0339 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.0069 - regression_loss: 21.8324 - val_loss: 25.0521 - val_regression_loss: 18.9166 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.2287 - regression_loss: 19.2704 - val_loss: 22.7312 - val_regression_loss: 17.2166 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.3767 - regression_loss: 16.9534 - val_loss: 21.0310 - val_regression_loss: 16.0026 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9336 - regression_loss: 14.9715 - val_loss: 19.3983 - val_regression_loss: 14.8175 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.8676 - regression_loss: 13.6336 - val_loss: 18.5046 - val_regression_loss: 14.1222 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.8388 - regression_loss: 12.3889 - val_loss: 16.8869 - val_regression_loss: 12.8310 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6261 - regression_loss: 11.4540 - val_loss: 15.9150 - val_regression_loss: 12.0176 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5723 - regression_loss: 10.5188 - val_loss: 14.8126 - val_regression_loss: 11.1092 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3866 - regression_loss: 10.0374 - val_loss: 14.2271 - val_regression_loss: 10.5970 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9191 - regression_loss: 9.6315 - val_loss: 13.4264 - val_regression_loss: 9.9739 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9777 - regression_loss: 8.8732 - val_loss: 12.6567 - val_regression_loss: 9.3459 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3528 - regression_loss: 8.3589 - val_loss: 12.0483 - val_regression_loss: 8.8497 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6181 - regression_loss: 7.9554 - val_loss: 11.4877 - val_regression_loss: 8.3642 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4345 - regression_loss: 7.3729 - val_loss: 11.2034 - val_regression_loss: 8.1014 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1571 - regression_loss: 7.1246 - val_loss: 10.7332 - val_regression_loss: 7.7407 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5109 - regression_loss: 6.8117 - val_loss: 10.1578 - val_regression_loss: 7.2299 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1465 - regression_loss: 6.5815 - val_loss: 9.8968 - val_regression_loss: 7.0322 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1612 - regression_loss: 6.2130 - val_loss: 9.5252 - val_regression_loss: 6.6829 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7216 - regression_loss: 6.0022 - val_loss: 9.0622 - val_regression_loss: 6.3207 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5837 - regression_loss: 5.7014 - val_loss: 8.8259 - val_regression_loss: 6.1004 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.1821 - regression_loss: 5.5695 - val_loss: 8.4115 - val_regression_loss: 5.7705 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6361 - regression_loss: 5.2230 - val_loss: 8.1074 - val_regression_loss: 5.4883 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7724 - regression_loss: 4.9701 - val_loss: 7.8691 - val_regression_loss: 5.2947 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4506 - regression_loss: 4.8038 - val_loss: 7.6896 - val_regression_loss: 5.1580 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2512 - regression_loss: 4.5606 - val_loss: 7.4659 - val_regression_loss: 4.9478 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.0503 - regression_loss: 4.3900 - val_loss: 7.2013 - val_regression_loss: 4.7239 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.0609 - regression_loss: 4.3089 - val_loss: 7.1226 - val_regression_loss: 4.6263 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8303 - regression_loss: 4.1402 - val_loss: 6.9461 - val_regression_loss: 4.4954 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.5833 - regression_loss: 3.9046 - val_loss: 6.6562 - val_regression_loss: 4.2301 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.3809 - regression_loss: 3.6898 - val_loss: 6.5483 - val_regression_loss: 4.1491 - lr: 1.0000e-04\n",
      "Epoch 42/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 5.3405 - regression_loss: 3.6678 - val_loss: 6.3024 - val_regression_loss: 3.9175 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0524 - regression_loss: 3.4252 - val_loss: 6.2106 - val_regression_loss: 3.8503 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9258 - regression_loss: 3.3034 - val_loss: 5.9198 - val_regression_loss: 3.5889 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6798 - regression_loss: 3.1480 - val_loss: 5.9205 - val_regression_loss: 3.6037 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5831 - regression_loss: 3.0175 - val_loss: 5.6987 - val_regression_loss: 3.4055 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5134 - regression_loss: 2.9209 - val_loss: 5.9554 - val_regression_loss: 3.6255 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5043 - regression_loss: 2.9402 - val_loss: 5.3959 - val_regression_loss: 3.1498 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0386 - regression_loss: 2.6676 - val_loss: 5.4462 - val_regression_loss: 3.1924 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.1956 - regression_loss: 2.6106 - val_loss: 5.1407 - val_regression_loss: 2.9341 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0453 - regression_loss: 2.4941 - val_loss: 4.9973 - val_regression_loss: 2.8084 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9513 - regression_loss: 2.3699 - val_loss: 5.0510 - val_regression_loss: 2.8561 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8213 - regression_loss: 2.2861 - val_loss: 4.9217 - val_regression_loss: 2.7460 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8551 - regression_loss: 2.2350 - val_loss: 4.7930 - val_regression_loss: 2.6394 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6186 - regression_loss: 2.1610 - val_loss: 5.0732 - val_regression_loss: 2.8646 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7173 - regression_loss: 2.1760 - val_loss: 4.6527 - val_regression_loss: 2.5178 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6002 - regression_loss: 2.1725 - val_loss: 4.6395 - val_regression_loss: 2.5208 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5451 - regression_loss: 1.9636 - val_loss: 4.4992 - val_regression_loss: 2.4037 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5049 - regression_loss: 1.9644 - val_loss: 4.7594 - val_regression_loss: 2.6104 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4362 - regression_loss: 1.9057 - val_loss: 4.3063 - val_regression_loss: 2.2461 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4217 - regression_loss: 1.8814 - val_loss: 4.5210 - val_regression_loss: 2.4233 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2644 - regression_loss: 1.7584 - val_loss: 4.2693 - val_regression_loss: 2.2293 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3520 - regression_loss: 1.8071 - val_loss: 4.4361 - val_regression_loss: 2.3600 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.2829 - regression_loss: 1.7605 - val_loss: 4.1038 - val_regression_loss: 2.0956 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2474 - regression_loss: 1.6854 - val_loss: 4.0455 - val_regression_loss: 2.0470 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1518 - regression_loss: 1.6429 - val_loss: 4.2506 - val_regression_loss: 2.2137 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1006 - regression_loss: 1.6226 - val_loss: 3.9328 - val_regression_loss: 1.9616 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0681 - regression_loss: 1.5550 - val_loss: 4.2094 - val_regression_loss: 2.1816 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0133 - regression_loss: 1.5218 - val_loss: 3.8534 - val_regression_loss: 1.9058 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0774 - regression_loss: 1.5498 - val_loss: 3.9656 - val_regression_loss: 1.9961 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8216 - regression_loss: 1.4495 - val_loss: 3.8082 - val_regression_loss: 1.8722 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9100 - regression_loss: 1.4390 - val_loss: 4.0192 - val_regression_loss: 2.0364 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9828 - regression_loss: 1.4675 - val_loss: 3.8073 - val_regression_loss: 1.8737 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8399 - regression_loss: 1.3943 - val_loss: 3.6933 - val_regression_loss: 1.7895 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9290 - regression_loss: 1.4068 - val_loss: 3.7169 - val_regression_loss: 1.8121 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.0669 - regression_loss: 1.6721\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8509 - regression_loss: 1.4024 - val_loss: 4.1055 - val_regression_loss: 2.1191 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8929 - regression_loss: 1.3982 - val_loss: 3.5890 - val_regression_loss: 1.7152 - lr: 5.0000e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8678 - regression_loss: 1.4039 - val_loss: 3.7813 - val_regression_loss: 1.8600 - lr: 5.0000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8269 - regression_loss: 1.3164 - val_loss: 3.6097 - val_regression_loss: 1.7261 - lr: 5.0000e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7684 - regression_loss: 1.2754 - val_loss: 3.6680 - val_regression_loss: 1.7725 - lr: 5.0000e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7506 - regression_loss: 1.2671 - val_loss: 3.5589 - val_regression_loss: 1.6885 - lr: 5.0000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7607 - regression_loss: 1.2650 - val_loss: 3.6755 - val_regression_loss: 1.7806 - lr: 5.0000e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7454 - regression_loss: 1.2461 - val_loss: 3.5223 - val_regression_loss: 1.6625 - lr: 5.0000e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7204 - regression_loss: 1.2357 - val_loss: 3.5670 - val_regression_loss: 1.6984 - lr: 5.0000e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7490 - regression_loss: 1.2406 - val_loss: 3.5185 - val_regression_loss: 1.6606 - lr: 5.0000e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7168 - regression_loss: 1.2149 - val_loss: 3.4992 - val_regression_loss: 1.6476 - lr: 5.0000e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6681 - regression_loss: 1.2042 - val_loss: 3.5771 - val_regression_loss: 1.7069 - lr: 5.0000e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6910 - regression_loss: 1.2014 - val_loss: 3.4548 - val_regression_loss: 1.6126 - lr: 5.0000e-05\n",
      "Epoch 89/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6910 - regression_loss: 1.1819 - val_loss: 3.4893 - val_regression_loss: 1.6417 - lr: 5.0000e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6627 - regression_loss: 1.1783 - val_loss: 3.4205 - val_regression_loss: 1.5912 - lr: 5.0000e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6321 - regression_loss: 1.1819 - val_loss: 3.4297 - val_regression_loss: 1.5983 - lr: 5.0000e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6199 - regression_loss: 1.1598 - val_loss: 3.4200 - val_regression_loss: 1.5895 - lr: 5.0000e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6215 - regression_loss: 1.1464 - val_loss: 3.4347 - val_regression_loss: 1.5972 - lr: 5.0000e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6268 - regression_loss: 1.1387 - val_loss: 3.4111 - val_regression_loss: 1.5805 - lr: 5.0000e-05\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4670 - regression_loss: 1.1235 - val_loss: 3.4201 - val_regression_loss: 1.5889 - lr: 5.0000e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5925 - regression_loss: 1.1153 - val_loss: 3.3802 - val_regression_loss: 1.5593 - lr: 5.0000e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6042 - regression_loss: 1.1170 - val_loss: 3.3523 - val_regression_loss: 1.5411 - lr: 5.0000e-05\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6057 - regression_loss: 1.1227 - val_loss: 3.4228 - val_regression_loss: 1.5946 - lr: 5.0000e-05\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5564 - regression_loss: 1.1021 - val_loss: 3.2828 - val_regression_loss: 1.4881 - lr: 5.0000e-05\n",
      "Epoch 100/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.8107 - regression_loss: 1.4177\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5616 - regression_loss: 1.0903 - val_loss: 3.3729 - val_regression_loss: 1.5557 - lr: 5.0000e-05\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5399 - regression_loss: 1.0776 - val_loss: 3.3192 - val_regression_loss: 1.5141 - lr: 2.5000e-05\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5639 - regression_loss: 1.0693 - val_loss: 3.2980 - val_regression_loss: 1.4986 - lr: 2.5000e-05\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5420 - regression_loss: 1.0704 - val_loss: 3.3276 - val_regression_loss: 1.5217 - lr: 2.5000e-05\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5429 - regression_loss: 1.0594 - val_loss: 3.3015 - val_regression_loss: 1.5019 - lr: 2.5000e-05\n",
      "Epoch 105/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.2986 - regression_loss: 0.9059\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5140 - regression_loss: 1.0568 - val_loss: 3.2926 - val_regression_loss: 1.4957 - lr: 2.5000e-05\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5451 - regression_loss: 1.0513 - val_loss: 3.3005 - val_regression_loss: 1.5017 - lr: 1.2500e-05\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5272 - regression_loss: 1.0527 - val_loss: 3.3042 - val_regression_loss: 1.5049 - lr: 1.2500e-05\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5308 - regression_loss: 1.0512 - val_loss: 3.2724 - val_regression_loss: 1.4806 - lr: 1.2500e-05\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5343 - regression_loss: 1.0463 - val_loss: 3.2893 - val_regression_loss: 1.4934 - lr: 1.2500e-05\n",
      "Epoch 110/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.3040 - regression_loss: 0.9114\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4900 - regression_loss: 1.0456 - val_loss: 3.3141 - val_regression_loss: 1.5125 - lr: 1.2500e-05\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5286 - regression_loss: 1.0452 - val_loss: 3.2910 - val_regression_loss: 1.4946 - lr: 6.2500e-06\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5166 - regression_loss: 1.0406 - val_loss: 3.2793 - val_regression_loss: 1.4860 - lr: 6.2500e-06\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5013 - regression_loss: 1.0390 - val_loss: 3.2780 - val_regression_loss: 1.4851 - lr: 6.2500e-06\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5106 - regression_loss: 1.0382 - val_loss: 3.2806 - val_regression_loss: 1.4872 - lr: 6.2500e-06\n",
      "Epoch 115/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.1638 - regression_loss: 0.7713\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5083 - regression_loss: 1.0381 - val_loss: 3.2783 - val_regression_loss: 1.4855 - lr: 6.2500e-06\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4573 - regression_loss: 1.0359 - val_loss: 3.2791 - val_regression_loss: 1.4862 - lr: 3.1250e-06\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4908 - regression_loss: 1.0357 - val_loss: 3.2764 - val_regression_loss: 1.4841 - lr: 3.1250e-06\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5211 - regression_loss: 1.0363 - val_loss: 3.2683 - val_regression_loss: 1.4779 - lr: 3.1250e-06\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4936 - regression_loss: 1.0344 - val_loss: 3.2699 - val_regression_loss: 1.4792 - lr: 3.1250e-06\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4752 - regression_loss: 1.0341 - val_loss: 3.2700 - val_regression_loss: 1.4791 - lr: 3.1250e-06\n",
      "Epoch 121/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.4895 - regression_loss: 1.0971\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4656 - regression_loss: 1.0334 - val_loss: 3.2736 - val_regression_loss: 1.4818 - lr: 3.1250e-06\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5141 - regression_loss: 1.0328 - val_loss: 3.2753 - val_regression_loss: 1.4831 - lr: 1.5625e-06\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5255 - regression_loss: 1.0325 - val_loss: 3.2761 - val_regression_loss: 1.4838 - lr: 1.5625e-06\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4914 - regression_loss: 1.0330 - val_loss: 3.2718 - val_regression_loss: 1.4805 - lr: 1.5625e-06\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5184 - regression_loss: 1.0322 - val_loss: 3.2694 - val_regression_loss: 1.4787 - lr: 1.5625e-06\n",
      "Epoch 126/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.9273 - regression_loss: 1.5349\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5096 - regression_loss: 1.0323 - val_loss: 3.2736 - val_regression_loss: 1.4819 - lr: 1.5625e-06\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5223 - regression_loss: 1.0313 - val_loss: 3.2729 - val_regression_loss: 1.4814 - lr: 7.8125e-07\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5121 - regression_loss: 1.0314 - val_loss: 3.2707 - val_regression_loss: 1.4798 - lr: 7.8125e-07\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4974 - regression_loss: 1.0312 - val_loss: 3.2711 - val_regression_loss: 1.4801 - lr: 7.8125e-07\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5065 - regression_loss: 1.0310 - val_loss: 3.2706 - val_regression_loss: 1.4797 - lr: 7.8125e-07\n",
      "Epoch 131/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 [====>.........................] - ETA: 0s - loss: 2.7152 - regression_loss: 1.3228\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5147 - regression_loss: 1.0309 - val_loss: 3.2699 - val_regression_loss: 1.4791 - lr: 7.8125e-07\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5079 - regression_loss: 1.0308 - val_loss: 3.2688 - val_regression_loss: 1.4783 - lr: 3.9062e-07\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5105 - regression_loss: 1.0307 - val_loss: 3.2686 - val_regression_loss: 1.4782 - lr: 3.9062e-07\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5092 - regression_loss: 1.0307 - val_loss: 3.2696 - val_regression_loss: 1.4790 - lr: 3.9062e-07\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4927 - regression_loss: 1.0305 - val_loss: 3.2695 - val_regression_loss: 1.4789 - lr: 3.9062e-07\n",
      "Epoch 136/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.7218 - regression_loss: 1.3294\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4809 - regression_loss: 1.0306 - val_loss: 3.2684 - val_regression_loss: 1.4780 - lr: 3.9062e-07\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5083 - regression_loss: 1.0303 - val_loss: 3.2684 - val_regression_loss: 1.4781 - lr: 1.9531e-07\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4951 - regression_loss: 1.0303 - val_loss: 3.2687 - val_regression_loss: 1.4782 - lr: 1.9531e-07\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5032 - regression_loss: 1.0302 - val_loss: 3.2687 - val_regression_loss: 1.4783 - lr: 1.9531e-07\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5247 - regression_loss: 1.0302 - val_loss: 3.2683 - val_regression_loss: 1.4780 - lr: 1.9531e-07\n",
      "Epoch 141/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.7626 - regression_loss: 1.3702\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4818 - regression_loss: 1.0302 - val_loss: 3.2686 - val_regression_loss: 1.4782 - lr: 1.9531e-07\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5042 - regression_loss: 1.0301 - val_loss: 3.2684 - val_regression_loss: 1.4781 - lr: 9.7656e-08\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5002 - regression_loss: 1.0301 - val_loss: 3.2684 - val_regression_loss: 1.4781 - lr: 9.7656e-08\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4865 - regression_loss: 1.0301 - val_loss: 3.2683 - val_regression_loss: 1.4780 - lr: 9.7656e-08\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5097 - regression_loss: 1.0301 - val_loss: 3.2684 - val_regression_loss: 1.4780 - lr: 9.7656e-08\n",
      "Epoch 146/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.3739 - regression_loss: 1.9815\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5119 - regression_loss: 1.0301 - val_loss: 3.2683 - val_regression_loss: 1.4780 - lr: 9.7656e-08\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5166 - regression_loss: 1.0300 - val_loss: 3.2682 - val_regression_loss: 1.4779 - lr: 4.8828e-08\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4940 - regression_loss: 1.0300 - val_loss: 3.2683 - val_regression_loss: 1.4779 - lr: 4.8828e-08\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4621 - regression_loss: 1.0300 - val_loss: 3.2682 - val_regression_loss: 1.4779 - lr: 4.8828e-08\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5110 - regression_loss: 1.0300 - val_loss: 3.2682 - val_regression_loss: 1.4779 - lr: 4.8828e-08\n",
      "Epoch 151/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.5687 - regression_loss: 1.1763\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5002 - regression_loss: 1.0300 - val_loss: 3.2682 - val_regression_loss: 1.4779 - lr: 4.8828e-08\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5044 - regression_loss: 1.0300 - val_loss: 3.2682 - val_regression_loss: 1.4779 - lr: 2.4414e-08\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5062 - regression_loss: 1.0300 - val_loss: 3.2682 - val_regression_loss: 1.4779 - lr: 2.4414e-08\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4971 - regression_loss: 1.0300 - val_loss: 3.2682 - val_regression_loss: 1.4779 - lr: 2.4414e-08\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5013 - regression_loss: 1.0300 - val_loss: 3.2682 - val_regression_loss: 1.4779 - lr: 2.4414e-08\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4536 - regression_loss: 1.0300 - val_loss: 3.2682 - val_regression_loss: 1.4779 - lr: 2.4414e-08\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5082 - regression_loss: 1.0300 - val_loss: 3.2682 - val_regression_loss: 1.4779 - lr: 2.4414e-08\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4771 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 2.4414e-08\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4701 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 2.4414e-08\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5265 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 2.4414e-08\n",
      "Epoch 161/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.6061 - regression_loss: 1.2137\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5209 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 2.4414e-08\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5026 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.2207e-08\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4721 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.2207e-08\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5118 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.2207e-08\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4861 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.2207e-08\n",
      "Epoch 166/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.7977 - regression_loss: 1.4053\n",
      "Epoch 166: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4943 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.2207e-08\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5023 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 6.1035e-09\n",
      "Epoch 168/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5011 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 6.1035e-09\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4797 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 6.1035e-09\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4996 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 6.1035e-09\n",
      "Epoch 171/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.1910 - regression_loss: 1.7986\n",
      "Epoch 171: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-09.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4994 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 6.1035e-09\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4512 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 3.0518e-09\n",
      "Epoch 173/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4942 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 3.0518e-09\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5077 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 3.0518e-09\n",
      "Epoch 175/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5031 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 3.0518e-09\n",
      "Epoch 176/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4502 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 3.0518e-09\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5051 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 3.0518e-09\n",
      "Epoch 178/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4866 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 3.0518e-09\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5073 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 3.0518e-09\n",
      "Epoch 180/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5185 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 3.0518e-09\n",
      "Epoch 181/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.8574 - regression_loss: 1.4650\n",
      "Epoch 181: ReduceLROnPlateau reducing learning rate to 1.5258788677030566e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5125 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 3.0518e-09\n",
      "Epoch 182/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4641 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.5259e-09\n",
      "Epoch 183/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4974 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.5259e-09\n",
      "Epoch 184/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4982 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.5259e-09\n",
      "Epoch 185/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4309 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.5259e-09\n",
      "Epoch 186/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4875 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.5259e-09\n",
      "Epoch 187/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5114 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.5259e-09\n",
      "Epoch 188/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4682 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.5259e-09\n",
      "Epoch 189/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5038 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.5259e-09\n",
      "Epoch 190/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4234 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.5259e-09\n",
      "Epoch 191/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5053 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.5259e-09\n",
      "Epoch 192/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4987 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.5259e-09\n",
      "Epoch 193/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5026 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.5259e-09\n",
      "Epoch 194/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4424 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.5259e-09\n",
      "Epoch 195/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.4368 - regression_loss: 1.0444\n",
      "Epoch 195: ReduceLROnPlateau reducing learning rate to 7.629394338515283e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4931 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.5259e-09\n",
      "Epoch 196/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4983 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 7.6294e-10\n",
      "Epoch 197/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5268 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 7.6294e-10\n",
      "Epoch 198/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4581 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 7.6294e-10\n",
      "Epoch 199/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5132 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 7.6294e-10\n",
      "Epoch 200/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.2044 - regression_loss: 0.8120\n",
      "Epoch 200: ReduceLROnPlateau reducing learning rate to 3.8146971692576415e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5155 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 7.6294e-10\n",
      "Epoch 201/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4957 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 3.8147e-10\n",
      "Epoch 202/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5133 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 3.8147e-10\n",
      "Epoch 203/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5118 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 3.8147e-10\n",
      "Epoch 204/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5051 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 3.8147e-10\n",
      "Epoch 205/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.3035 - regression_loss: 1.9111\n",
      "Epoch 205: ReduceLROnPlateau reducing learning rate to 1.9073485846288207e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5096 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 3.8147e-10\n",
      "Epoch 206/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4841 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.9073e-10\n",
      "Epoch 207/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4766 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.9073e-10\n",
      "Epoch 208/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.4816 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.9073e-10\n",
      "Epoch 209/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5105 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.9073e-10\n",
      "Epoch 210/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.5784 - regression_loss: 1.1860\n",
      "Epoch 210: ReduceLROnPlateau reducing learning rate to 9.536742923144104e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4946 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.9073e-10\n",
      "Epoch 211/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4942 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 9.5367e-11\n",
      "Epoch 212/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4982 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 9.5367e-11\n",
      "Epoch 213/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5197 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 9.5367e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4965 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 9.5367e-11\n",
      "Epoch 215/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.4747 - regression_loss: 1.0823\n",
      "Epoch 215: ReduceLROnPlateau reducing learning rate to 4.768371461572052e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5182 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 9.5367e-11\n",
      "Epoch 216/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5090 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 4.7684e-11\n",
      "Epoch 217/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5216 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 4.7684e-11\n",
      "Epoch 218/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5159 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 4.7684e-11\n",
      "Epoch 219/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4942 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 4.7684e-11\n",
      "Epoch 220/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.9740 - regression_loss: 1.5816\n",
      "Epoch 220: ReduceLROnPlateau reducing learning rate to 2.384185730786026e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5052 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 4.7684e-11\n",
      "Epoch 221/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4963 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 2.3842e-11\n",
      "Epoch 222/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5106 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 2.3842e-11\n",
      "Epoch 223/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4817 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 2.3842e-11\n",
      "Epoch 224/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4951 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 2.3842e-11\n",
      "Epoch 225/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.1578 - regression_loss: 0.7654\n",
      "Epoch 225: ReduceLROnPlateau reducing learning rate to 1.192092865393013e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4998 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 2.3842e-11\n",
      "Epoch 226/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5087 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.1921e-11\n",
      "Epoch 227/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5004 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.1921e-11\n",
      "Epoch 228/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5209 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.1921e-11\n",
      "Epoch 229/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4891 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.1921e-11\n",
      "Epoch 230/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.7718 - regression_loss: 1.3794\n",
      "Epoch 230: ReduceLROnPlateau reducing learning rate to 5.960464326965065e-12.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5120 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.1921e-11\n",
      "Epoch 231/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5210 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 5.9605e-12\n",
      "Epoch 232/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4888 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 5.9605e-12\n",
      "Epoch 233/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5023 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 5.9605e-12\n",
      "Epoch 234/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5031 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 5.9605e-12\n",
      "Epoch 235/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.3838 - regression_loss: 0.9914\n",
      "Epoch 235: ReduceLROnPlateau reducing learning rate to 2.9802321634825324e-12.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4426 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 5.9605e-12\n",
      "Epoch 236/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5045 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 2.9802e-12\n",
      "Epoch 237/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5090 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 2.9802e-12\n",
      "Epoch 238/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4961 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 2.9802e-12\n",
      "Epoch 239/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5070 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 2.9802e-12\n",
      "Epoch 240/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.7332 - regression_loss: 1.3408\n",
      "Epoch 240: ReduceLROnPlateau reducing learning rate to 1.4901160817412662e-12.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4664 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 2.9802e-12\n",
      "Epoch 241/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5007 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.4901e-12\n",
      "Epoch 242/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.4883 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.4901e-12\n",
      "Epoch 243/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5084 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.4901e-12\n",
      "Epoch 244/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4840 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.4901e-12\n",
      "Epoch 245/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.7950 - regression_loss: 1.4026\n",
      "Epoch 245: ReduceLROnPlateau reducing learning rate to 7.450580408706331e-13.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5091 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 1.4901e-12\n",
      "Epoch 246/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4982 - regression_loss: 1.0300 - val_loss: 3.2681 - val_regression_loss: 1.4778 - lr: 7.4506e-13\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 26ms/step - loss: 95.6305 - regression_loss: 86.9954 - val_loss: 62.1175 - val_regression_loss: 50.5975 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 63.3069 - regression_loss: 57.5941 - val_loss: 43.1839 - val_regression_loss: 35.6546 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 47.7164 - regression_loss: 43.0645 - val_loss: 35.0254 - val_regression_loss: 29.8545 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 41.3121 - regression_loss: 36.7397 - val_loss: 29.5077 - val_regression_loss: 25.1580 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 35.6352 - regression_loss: 31.9336 - val_loss: 26.7008 - val_regression_loss: 22.7498 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.2656 - regression_loss: 28.1136 - val_loss: 24.4837 - val_regression_loss: 20.7313 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.3747 - regression_loss: 24.9831 - val_loss: 22.0089 - val_regression_loss: 18.2002 - lr: 1.0000e-04\n",
      "Epoch 8/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 25.9984 - regression_loss: 22.5681 - val_loss: 20.1013 - val_regression_loss: 16.4803 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.3809 - regression_loss: 20.3676 - val_loss: 18.5334 - val_regression_loss: 14.9388 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.0088 - regression_loss: 19.0512 - val_loss: 17.3005 - val_regression_loss: 13.9374 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.8382 - regression_loss: 17.3757 - val_loss: 16.1077 - val_regression_loss: 12.8287 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.3938 - regression_loss: 15.8116 - val_loss: 15.2456 - val_regression_loss: 12.0868 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.1776 - regression_loss: 14.8689 - val_loss: 14.4434 - val_regression_loss: 11.3740 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.3259 - regression_loss: 13.9127 - val_loss: 14.0562 - val_regression_loss: 10.9353 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.0391 - regression_loss: 13.1641 - val_loss: 13.1825 - val_regression_loss: 10.3357 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.7820 - regression_loss: 12.5124 - val_loss: 12.7990 - val_regression_loss: 9.8523 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8764 - regression_loss: 11.6607 - val_loss: 12.2485 - val_regression_loss: 9.4071 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.5986 - regression_loss: 11.2092 - val_loss: 12.0247 - val_regression_loss: 9.1455 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8248 - regression_loss: 10.8068 - val_loss: 11.7372 - val_regression_loss: 8.9388 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6001 - regression_loss: 10.4625 - val_loss: 11.8545 - val_regression_loss: 8.8688 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3981 - regression_loss: 10.1676 - val_loss: 11.3176 - val_regression_loss: 8.5227 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7678 - regression_loss: 9.8023 - val_loss: 11.7762 - val_regression_loss: 8.7137 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7912 - regression_loss: 9.6749 - val_loss: 11.0802 - val_regression_loss: 8.2484 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1584 - regression_loss: 9.1460 - val_loss: 11.6187 - val_regression_loss: 8.5252 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3571 - regression_loss: 9.2954 - val_loss: 11.3822 - val_regression_loss: 8.4977 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4463 - regression_loss: 9.2568 - val_loss: 11.6318 - val_regression_loss: 8.4695 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8941 - regression_loss: 8.8346 - val_loss: 10.7490 - val_regression_loss: 7.8509 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.4956 - regression_loss: 8.5034 - val_loss: 10.9492 - val_regression_loss: 7.9226 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3778 - regression_loss: 8.4277 - val_loss: 10.6857 - val_regression_loss: 7.7665 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.5923 - regression_loss: 8.5532 - val_loss: 10.6925 - val_regression_loss: 7.7158 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.1810 - regression_loss: 8.2812 - val_loss: 10.7705 - val_regression_loss: 7.7373 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9010 - regression_loss: 8.0761 - val_loss: 10.8478 - val_regression_loss: 7.7746 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9207 - regression_loss: 8.0039 - val_loss: 10.7423 - val_regression_loss: 7.7325 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9619 - regression_loss: 7.9695 - val_loss: 11.2124 - val_regression_loss: 8.0305 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9341 - regression_loss: 7.9147 - val_loss: 10.5702 - val_regression_loss: 7.5704 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6731 - regression_loss: 7.8616 - val_loss: 10.5094 - val_regression_loss: 7.4618 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4952 - regression_loss: 7.6096 - val_loss: 10.6554 - val_regression_loss: 7.5635 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6807 - regression_loss: 7.6710 - val_loss: 10.5054 - val_regression_loss: 7.4896 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6424 - regression_loss: 7.7070 - val_loss: 10.6534 - val_regression_loss: 7.5455 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5427 - regression_loss: 7.5904 - val_loss: 10.6098 - val_regression_loss: 7.4921 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0948 - regression_loss: 7.3370 - val_loss: 10.3441 - val_regression_loss: 7.3034 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0305 - regression_loss: 7.2774 - val_loss: 10.4209 - val_regression_loss: 7.3316 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1163 - regression_loss: 7.2533 - val_loss: 10.4840 - val_regression_loss: 7.3704 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0237 - regression_loss: 7.1828 - val_loss: 10.4785 - val_regression_loss: 7.3540 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0456 - regression_loss: 7.1778 - val_loss: 10.4725 - val_regression_loss: 7.3967 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8669 - regression_loss: 7.2377 - val_loss: 11.1835 - val_regression_loss: 7.8742 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2458 - regression_loss: 7.3307 - val_loss: 10.2127 - val_regression_loss: 7.1379 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8286 - regression_loss: 7.0228 - val_loss: 10.2761 - val_regression_loss: 7.1619 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6893 - regression_loss: 6.9063 - val_loss: 10.2087 - val_regression_loss: 7.1006 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7889 - regression_loss: 6.8482 - val_loss: 10.2169 - val_regression_loss: 7.1066 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7512 - regression_loss: 6.8466 - val_loss: 10.1396 - val_regression_loss: 7.0602 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5891 - regression_loss: 6.8435 - val_loss: 10.1639 - val_regression_loss: 7.0485 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6156 - regression_loss: 6.7682 - val_loss: 10.1736 - val_regression_loss: 7.0429 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6516 - regression_loss: 6.7144 - val_loss: 10.1314 - val_regression_loss: 7.0126 - lr: 1.0000e-04\n",
      "Epoch 55/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4797 - regression_loss: 6.6652 - val_loss: 10.0809 - val_regression_loss: 6.9872 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4892 - regression_loss: 6.7255 - val_loss: 10.2471 - val_regression_loss: 7.0965 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3613 - regression_loss: 6.5655 - val_loss: 10.0454 - val_regression_loss: 6.9693 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6704 - regression_loss: 6.8210 - val_loss: 9.9723 - val_regression_loss: 6.8793 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4281 - regression_loss: 6.5642 - val_loss: 10.0733 - val_regression_loss: 6.9475 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5117 - regression_loss: 6.5654 - val_loss: 9.9575 - val_regression_loss: 6.8497 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2840 - regression_loss: 6.4479 - val_loss: 10.0045 - val_regression_loss: 6.8891 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4014 - regression_loss: 6.4842 - val_loss: 9.9803 - val_regression_loss: 6.8620 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2244 - regression_loss: 6.4322 - val_loss: 10.1316 - val_regression_loss: 6.9675 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2653 - regression_loss: 6.3981 - val_loss: 9.9230 - val_regression_loss: 6.8178 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2001 - regression_loss: 6.4103 - val_loss: 9.8783 - val_regression_loss: 6.7721 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4573 - regression_loss: 6.5114 - val_loss: 10.3212 - val_regression_loss: 7.0973 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3196 - regression_loss: 6.4340 - val_loss: 10.0488 - val_regression_loss: 6.9204 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2702 - regression_loss: 6.3926 - val_loss: 9.9540 - val_regression_loss: 6.8275 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0507 - regression_loss: 6.2323 - val_loss: 9.8646 - val_regression_loss: 6.7724 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1664 - regression_loss: 6.3248 - val_loss: 9.9981 - val_regression_loss: 6.8275 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0505 - regression_loss: 6.2347 - val_loss: 9.7917 - val_regression_loss: 6.6929 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9802 - regression_loss: 6.1919 - val_loss: 9.9071 - val_regression_loss: 6.7839 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0974 - regression_loss: 6.2839 - val_loss: 10.1184 - val_regression_loss: 6.9411 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1763 - regression_loss: 6.3119 - val_loss: 10.0643 - val_regression_loss: 6.9203 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0824 - regression_loss: 6.3896 - val_loss: 10.0835 - val_regression_loss: 6.8857 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2464 - regression_loss: 6.3701 - val_loss: 10.1497 - val_regression_loss: 6.9449 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.8949 - regression_loss: 6.5072\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1162 - regression_loss: 6.3508 - val_loss: 9.8583 - val_regression_loss: 6.7222 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8100 - regression_loss: 6.0989 - val_loss: 10.0117 - val_regression_loss: 6.8222 - lr: 5.0000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0215 - regression_loss: 6.1158 - val_loss: 9.7623 - val_regression_loss: 6.6515 - lr: 5.0000e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0233 - regression_loss: 6.1639 - val_loss: 9.7687 - val_regression_loss: 6.6503 - lr: 5.0000e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9380 - regression_loss: 6.0576 - val_loss: 9.7097 - val_regression_loss: 6.6037 - lr: 5.0000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8822 - regression_loss: 6.0643 - val_loss: 9.7467 - val_regression_loss: 6.6340 - lr: 5.0000e-05\n",
      "Epoch 83/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.1403 - regression_loss: 6.7528\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8789 - regression_loss: 6.0878 - val_loss: 9.8349 - val_regression_loss: 6.6957 - lr: 5.0000e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8643 - regression_loss: 6.0082 - val_loss: 9.7787 - val_regression_loss: 6.6502 - lr: 2.5000e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7907 - regression_loss: 5.9981 - val_loss: 9.7552 - val_regression_loss: 6.6299 - lr: 2.5000e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7988 - regression_loss: 5.9897 - val_loss: 9.8008 - val_regression_loss: 6.6598 - lr: 2.5000e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6940 - regression_loss: 5.9853 - val_loss: 9.7527 - val_regression_loss: 6.6274 - lr: 2.5000e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7348 - regression_loss: 5.9710 - val_loss: 9.7441 - val_regression_loss: 6.6247 - lr: 2.5000e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7636 - regression_loss: 5.9831 - val_loss: 9.7771 - val_regression_loss: 6.6453 - lr: 2.5000e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.5880 - regression_loss: 5.9805 - val_loss: 9.7358 - val_regression_loss: 6.6162 - lr: 2.5000e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8821 - regression_loss: 6.0098 - val_loss: 9.7771 - val_regression_loss: 6.6431 - lr: 2.5000e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7784 - regression_loss: 5.9831 - val_loss: 9.7387 - val_regression_loss: 6.6214 - lr: 2.5000e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7715 - regression_loss: 6.0124 - val_loss: 9.7611 - val_regression_loss: 6.6288 - lr: 2.5000e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7279 - regression_loss: 5.9742 - val_loss: 9.7310 - val_regression_loss: 6.6108 - lr: 2.5000e-05\n",
      "Epoch 95/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.0014 - regression_loss: 6.6142\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6744 - regression_loss: 5.9825 - val_loss: 9.7922 - val_regression_loss: 6.6516 - lr: 2.5000e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6498 - regression_loss: 5.9588 - val_loss: 9.7249 - val_regression_loss: 6.6048 - lr: 1.2500e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6489 - regression_loss: 5.9449 - val_loss: 9.7313 - val_regression_loss: 6.6113 - lr: 1.2500e-05\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7133 - regression_loss: 5.9476 - val_loss: 9.7842 - val_regression_loss: 6.6443 - lr: 1.2500e-05\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7022 - regression_loss: 5.9285 - val_loss: 9.7485 - val_regression_loss: 6.6192 - lr: 1.2500e-05\n",
      "Epoch 100/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 [====>.........................] - ETA: 0s - loss: 7.2712 - regression_loss: 5.8841\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7076 - regression_loss: 5.9198 - val_loss: 9.7318 - val_regression_loss: 6.6107 - lr: 1.2500e-05\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8197 - regression_loss: 5.9195 - val_loss: 9.7304 - val_regression_loss: 6.6098 - lr: 6.2500e-06\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6889 - regression_loss: 5.9261 - val_loss: 9.7468 - val_regression_loss: 6.6182 - lr: 6.2500e-06\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7276 - regression_loss: 5.9132 - val_loss: 9.7447 - val_regression_loss: 6.6161 - lr: 6.2500e-06\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6556 - regression_loss: 5.9137 - val_loss: 9.7406 - val_regression_loss: 6.6130 - lr: 6.2500e-06\n",
      "Epoch 105/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.2521 - regression_loss: 7.8650\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7248 - regression_loss: 5.9112 - val_loss: 9.7410 - val_regression_loss: 6.6136 - lr: 6.2500e-06\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6305 - regression_loss: 5.9079 - val_loss: 9.7405 - val_regression_loss: 6.6134 - lr: 3.1250e-06\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7539 - regression_loss: 5.9095 - val_loss: 9.7418 - val_regression_loss: 6.6140 - lr: 3.1250e-06\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6165 - regression_loss: 5.9081 - val_loss: 9.7425 - val_regression_loss: 6.6149 - lr: 3.1250e-06\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7162 - regression_loss: 5.9064 - val_loss: 9.7392 - val_regression_loss: 6.6123 - lr: 3.1250e-06\n",
      "Epoch 110/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.6422 - regression_loss: 8.2551\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7862 - regression_loss: 5.9101 - val_loss: 9.7400 - val_regression_loss: 6.6132 - lr: 3.1250e-06\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6804 - regression_loss: 5.9058 - val_loss: 9.7372 - val_regression_loss: 6.6111 - lr: 1.5625e-06\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6752 - regression_loss: 5.9043 - val_loss: 9.7375 - val_regression_loss: 6.6114 - lr: 1.5625e-06\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6892 - regression_loss: 5.9047 - val_loss: 9.7388 - val_regression_loss: 6.6121 - lr: 1.5625e-06\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7623 - regression_loss: 5.9044 - val_loss: 9.7402 - val_regression_loss: 6.6129 - lr: 1.5625e-06\n",
      "Epoch 115/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.4587 - regression_loss: 7.0716\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7127 - regression_loss: 5.9038 - val_loss: 9.7375 - val_regression_loss: 6.6109 - lr: 1.5625e-06\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7486 - regression_loss: 5.9048 - val_loss: 9.7355 - val_regression_loss: 6.6096 - lr: 7.8125e-07\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6360 - regression_loss: 5.9031 - val_loss: 9.7363 - val_regression_loss: 6.6101 - lr: 7.8125e-07\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6885 - regression_loss: 5.9028 - val_loss: 9.7354 - val_regression_loss: 6.6094 - lr: 7.8125e-07\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8040 - regression_loss: 5.9030 - val_loss: 9.7368 - val_regression_loss: 6.6104 - lr: 7.8125e-07\n",
      "Epoch 120/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.0755 - regression_loss: 6.6884\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6039 - regression_loss: 5.9023 - val_loss: 9.7372 - val_regression_loss: 6.6107 - lr: 7.8125e-07\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6963 - regression_loss: 5.9022 - val_loss: 9.7376 - val_regression_loss: 6.6110 - lr: 3.9062e-07\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 26ms/step - loss: 87.6544 - regression_loss: 80.3290 - val_loss: 46.9560 - val_regression_loss: 34.1152 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 53.0846 - regression_loss: 49.3499 - val_loss: 41.2593 - val_regression_loss: 30.3259 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 46.2330 - regression_loss: 41.5768 - val_loss: 41.8487 - val_regression_loss: 31.3116 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 43.3390 - regression_loss: 39.0809 - val_loss: 39.1353 - val_regression_loss: 29.4589 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 40.0030 - regression_loss: 36.4273 - val_loss: 35.4163 - val_regression_loss: 26.4168 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 37.8784 - regression_loss: 33.9618 - val_loss: 33.0783 - val_regression_loss: 24.5983 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 36.3666 - regression_loss: 32.5100 - val_loss: 31.9044 - val_regression_loss: 23.6504 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 35.5961 - regression_loss: 31.7020 - val_loss: 31.0625 - val_regression_loss: 22.9585 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 34.6797 - regression_loss: 31.0450 - val_loss: 30.5823 - val_regression_loss: 22.6443 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 34.0305 - regression_loss: 30.5773 - val_loss: 30.3062 - val_regression_loss: 22.5143 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 33.4102 - regression_loss: 29.9296 - val_loss: 30.3446 - val_regression_loss: 22.5095 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.6153 - regression_loss: 29.1285 - val_loss: 30.0493 - val_regression_loss: 22.4237 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.2346 - regression_loss: 29.0940 - val_loss: 29.7181 - val_regression_loss: 22.0486 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.7982 - regression_loss: 28.4393 - val_loss: 29.6371 - val_regression_loss: 22.1064 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31.8775 - regression_loss: 28.3680 - val_loss: 29.1257 - val_regression_loss: 21.5999 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.7487 - regression_loss: 28.2499 - val_loss: 28.9712 - val_regression_loss: 21.4884 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.3672 - regression_loss: 27.9129 - val_loss: 28.9389 - val_regression_loss: 21.5066 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.9289 - regression_loss: 27.3756 - val_loss: 28.7218 - val_regression_loss: 21.3272 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.8042 - regression_loss: 27.0598 - val_loss: 28.7779 - val_regression_loss: 21.4626 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.7248 - regression_loss: 27.0009 - val_loss: 28.5943 - val_regression_loss: 21.2620 - lr: 1.0000e-04\n",
      "Epoch 21/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 29.9854 - regression_loss: 26.7166 - val_loss: 28.4062 - val_regression_loss: 21.0821 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.4285 - regression_loss: 26.4931 - val_loss: 28.2539 - val_regression_loss: 20.9512 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.4633 - regression_loss: 26.4326 - val_loss: 28.3790 - val_regression_loss: 21.1249 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.6255 - regression_loss: 26.2219 - val_loss: 28.4564 - val_regression_loss: 21.1684 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.9058 - regression_loss: 26.0572 - val_loss: 28.2230 - val_regression_loss: 20.9615 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.9764 - regression_loss: 25.9179 - val_loss: 28.0980 - val_regression_loss: 20.8400 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.2024 - regression_loss: 25.7494 - val_loss: 28.2065 - val_regression_loss: 20.9606 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.3814 - regression_loss: 25.7974 - val_loss: 28.1471 - val_regression_loss: 20.8936 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.9383 - regression_loss: 25.7197 - val_loss: 28.0145 - val_regression_loss: 20.7585 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.6240 - regression_loss: 25.3361 - val_loss: 28.0891 - val_regression_loss: 20.8992 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.4903 - regression_loss: 25.2571 - val_loss: 27.9628 - val_regression_loss: 20.7236 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.1893 - regression_loss: 25.2805 - val_loss: 28.3030 - val_regression_loss: 21.0654 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.8538 - regression_loss: 25.0612 - val_loss: 28.2943 - val_regression_loss: 21.0514 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.2155 - regression_loss: 25.0125 - val_loss: 28.1006 - val_regression_loss: 20.9009 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.4408 - regression_loss: 24.9359 - val_loss: 28.2666 - val_regression_loss: 20.9238 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.9263 - regression_loss: 24.9213 - val_loss: 28.3057 - val_regression_loss: 21.0379 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.1747 - regression_loss: 24.7875 - val_loss: 28.3110 - val_regression_loss: 20.9771 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.8461 - regression_loss: 24.8469 - val_loss: 28.0159 - val_regression_loss: 20.7824 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.0778 - regression_loss: 24.6613 - val_loss: 28.1283 - val_regression_loss: 20.8499 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.6941 - regression_loss: 24.5340 - val_loss: 28.3505 - val_regression_loss: 21.0212 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.4422 - regression_loss: 24.4717 - val_loss: 28.4093 - val_regression_loss: 21.0984 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.0537 - regression_loss: 24.5715 - val_loss: 28.6574 - val_regression_loss: 21.3033 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.7353 - regression_loss: 24.5711 - val_loss: 28.5873 - val_regression_loss: 21.1069 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.5430 - regression_loss: 24.4775 - val_loss: 28.8088 - val_regression_loss: 21.4419 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.6061 - regression_loss: 24.4806 - val_loss: 28.7957 - val_regression_loss: 21.2837 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.5840 - regression_loss: 24.4902 - val_loss: 28.8179 - val_regression_loss: 21.4390 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 27.9846 - regression_loss: 26.6015\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.8413 - regression_loss: 24.4674 - val_loss: 28.6349 - val_regression_loss: 21.1589 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.8898 - regression_loss: 24.1747 - val_loss: 28.3206 - val_regression_loss: 20.9483 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.1991 - regression_loss: 23.9548 - val_loss: 28.4685 - val_regression_loss: 21.0637 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.9427 - regression_loss: 24.0298 - val_loss: 28.4932 - val_regression_loss: 21.0600 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.6599 - regression_loss: 23.9301 - val_loss: 28.5650 - val_regression_loss: 21.0999 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.9626 - regression_loss: 23.9065 - val_loss: 28.6819 - val_regression_loss: 21.2474 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.2542 - regression_loss: 24.0216 - val_loss: 28.5704 - val_regression_loss: 21.1132 - lr: 5.0000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.7332 - regression_loss: 23.9118 - val_loss: 28.4902 - val_regression_loss: 21.0244 - lr: 5.0000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.0977 - regression_loss: 23.8152 - val_loss: 28.6444 - val_regression_loss: 21.1747 - lr: 5.0000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.4992 - regression_loss: 23.7912 - val_loss: 28.5820 - val_regression_loss: 21.1170 - lr: 5.0000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.8771 - regression_loss: 23.7871 - val_loss: 28.7490 - val_regression_loss: 21.2371 - lr: 5.0000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.8486 - regression_loss: 23.7483 - val_loss: 28.7458 - val_regression_loss: 21.2399 - lr: 5.0000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.1104 - regression_loss: 23.7679 - val_loss: 28.6944 - val_regression_loss: 21.2156 - lr: 5.0000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.9530 - regression_loss: 23.7305 - val_loss: 28.6678 - val_regression_loss: 21.1542 - lr: 5.0000e-05\n",
      "Epoch 61/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 36.0699 - regression_loss: 34.6868\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.1773 - regression_loss: 23.9482 - val_loss: 28.6908 - val_regression_loss: 21.1630 - lr: 5.0000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.5889 - regression_loss: 23.6206 - val_loss: 28.7432 - val_regression_loss: 21.2259 - lr: 2.5000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.0101 - regression_loss: 23.7104 - val_loss: 28.8393 - val_regression_loss: 21.2976 - lr: 2.5000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.8090 - regression_loss: 23.6114 - val_loss: 28.8006 - val_regression_loss: 21.2439 - lr: 2.5000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.0578 - regression_loss: 23.6595 - val_loss: 28.7128 - val_regression_loss: 21.1729 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.8201 - regression_loss: 23.5896 - val_loss: 28.7657 - val_regression_loss: 21.2204 - lr: 2.5000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.4334 - regression_loss: 23.6822 - val_loss: 28.8695 - val_regression_loss: 21.3264 - lr: 2.5000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.4830 - regression_loss: 23.5966 - val_loss: 28.7353 - val_regression_loss: 21.1835 - lr: 2.5000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.7523 - regression_loss: 23.6527 - val_loss: 28.7888 - val_regression_loss: 21.2260 - lr: 2.5000e-05\n",
      "Epoch 70/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 27.5498 - regression_loss: 26.1667\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.4641 - regression_loss: 23.5660 - val_loss: 28.8583 - val_regression_loss: 21.3113 - lr: 2.5000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.6743 - regression_loss: 23.5503 - val_loss: 28.8589 - val_regression_loss: 21.3150 - lr: 1.2500e-05\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 69.4093 - regression_loss: 63.2616 - val_loss: 66.1152 - val_regression_loss: 48.8897 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 48.6915 - regression_loss: 43.6331 - val_loss: 52.8992 - val_regression_loss: 39.0239 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 39.6068 - regression_loss: 35.4513 - val_loss: 44.9375 - val_regression_loss: 32.6738 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 33.6684 - regression_loss: 29.8207 - val_loss: 41.2817 - val_regression_loss: 29.4784 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.0914 - regression_loss: 25.6719 - val_loss: 40.1341 - val_regression_loss: 28.2657 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.2442 - regression_loss: 22.3261 - val_loss: 37.5791 - val_regression_loss: 26.2794 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.9363 - regression_loss: 20.0945 - val_loss: 34.1144 - val_regression_loss: 23.7855 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.5336 - regression_loss: 18.5345 - val_loss: 30.4628 - val_regression_loss: 21.1907 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.4892 - regression_loss: 16.8556 - val_loss: 30.7145 - val_regression_loss: 21.3462 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.0913 - regression_loss: 15.5865 - val_loss: 26.5595 - val_regression_loss: 18.3869 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.6860 - regression_loss: 14.3587 - val_loss: 26.7806 - val_regression_loss: 18.5203 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.8492 - regression_loss: 13.5062 - val_loss: 23.2969 - val_regression_loss: 16.0518 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.9402 - regression_loss: 12.6249 - val_loss: 23.5974 - val_regression_loss: 16.2859 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8867 - regression_loss: 11.8740 - val_loss: 20.2186 - val_regression_loss: 13.8722 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6949 - regression_loss: 11.4202 - val_loss: 21.8864 - val_regression_loss: 15.0795 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0808 - regression_loss: 10.9521 - val_loss: 17.8031 - val_regression_loss: 12.1510 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3084 - regression_loss: 10.2950 - val_loss: 18.8007 - val_regression_loss: 12.8895 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0380 - regression_loss: 10.0091 - val_loss: 16.8066 - val_regression_loss: 11.4518 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8184 - regression_loss: 9.7706 - val_loss: 15.6864 - val_regression_loss: 10.6666 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5277 - regression_loss: 9.3136 - val_loss: 17.8222 - val_regression_loss: 12.2559 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3970 - regression_loss: 9.3490 - val_loss: 14.1807 - val_regression_loss: 9.6031 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6491 - regression_loss: 8.7421 - val_loss: 14.8045 - val_regression_loss: 10.0531 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.5735 - regression_loss: 8.4990 - val_loss: 13.8549 - val_regression_loss: 9.3757 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2777 - regression_loss: 8.3339 - val_loss: 13.8846 - val_regression_loss: 9.4125 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2026 - regression_loss: 8.1678 - val_loss: 14.2857 - val_regression_loss: 9.7230 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0453 - regression_loss: 8.0110 - val_loss: 12.7863 - val_regression_loss: 8.6328 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9782 - regression_loss: 7.9622 - val_loss: 12.2604 - val_regression_loss: 8.2704 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9372 - regression_loss: 7.9079 - val_loss: 15.2421 - val_regression_loss: 10.5424 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8252 - regression_loss: 7.9014 - val_loss: 11.9000 - val_regression_loss: 8.0308 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5252 - regression_loss: 7.6204 - val_loss: 13.6544 - val_regression_loss: 9.3253 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4539 - regression_loss: 7.5495 - val_loss: 12.0080 - val_regression_loss: 8.1086 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1094 - regression_loss: 7.4104 - val_loss: 11.5397 - val_regression_loss: 7.7732 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2171 - regression_loss: 7.2688 - val_loss: 12.6372 - val_regression_loss: 8.5838 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9808 - regression_loss: 7.1445 - val_loss: 11.3470 - val_regression_loss: 7.6368 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8428 - regression_loss: 6.9882 - val_loss: 12.8097 - val_regression_loss: 8.7354 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8635 - regression_loss: 7.0795 - val_loss: 11.1250 - val_regression_loss: 7.4865 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9370 - regression_loss: 7.1263 - val_loss: 12.7144 - val_regression_loss: 8.6730 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8985 - regression_loss: 7.0646 - val_loss: 11.9065 - val_regression_loss: 8.0678 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6295 - regression_loss: 6.7929 - val_loss: 11.1125 - val_regression_loss: 7.4843 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7320 - regression_loss: 6.8811 - val_loss: 10.9729 - val_regression_loss: 7.3865 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8598 - regression_loss: 7.0107 - val_loss: 11.8573 - val_regression_loss: 8.0638 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8461 - regression_loss: 6.9820 - val_loss: 12.4379 - val_regression_loss: 8.5348 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8690 - regression_loss: 6.9126 - val_loss: 10.7835 - val_regression_loss: 7.3042 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.7482 - regression_loss: 10.3695\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9474 - regression_loss: 7.0831 - val_loss: 11.2263 - val_regression_loss: 7.6115 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3531 - regression_loss: 6.4818 - val_loss: 11.0462 - val_regression_loss: 7.4736 - lr: 5.0000e-05\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2690 - regression_loss: 6.5096 - val_loss: 11.1082 - val_regression_loss: 7.5210 - lr: 5.0000e-05\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1872 - regression_loss: 6.3989 - val_loss: 11.0097 - val_regression_loss: 7.4523 - lr: 5.0000e-05\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0970 - regression_loss: 6.3599 - val_loss: 11.3104 - val_regression_loss: 7.6896 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0556 - regression_loss: 6.3214 - val_loss: 10.7474 - val_regression_loss: 7.2621 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0241 - regression_loss: 6.3096 - val_loss: 11.2545 - val_regression_loss: 7.6562 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1210 - regression_loss: 6.2984 - val_loss: 10.8587 - val_regression_loss: 7.3507 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0628 - regression_loss: 6.2570 - val_loss: 11.0888 - val_regression_loss: 7.5258 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9412 - regression_loss: 6.2324 - val_loss: 10.9016 - val_regression_loss: 7.3854 - lr: 5.0000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0840 - regression_loss: 6.2427 - val_loss: 10.8538 - val_regression_loss: 7.3541 - lr: 5.0000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8971 - regression_loss: 6.1628 - val_loss: 10.9769 - val_regression_loss: 7.4520 - lr: 5.0000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1021 - regression_loss: 6.2845 - val_loss: 10.7700 - val_regression_loss: 7.2994 - lr: 5.0000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0802 - regression_loss: 6.2558 - val_loss: 10.7414 - val_regression_loss: 7.2761 - lr: 5.0000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0211 - regression_loss: 6.1366 - val_loss: 11.1157 - val_regression_loss: 7.5670 - lr: 5.0000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9394 - regression_loss: 6.2018 - val_loss: 10.4805 - val_regression_loss: 7.0858 - lr: 5.0000e-05\n",
      "Epoch 60/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.7086 - regression_loss: 6.3307\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9487 - regression_loss: 6.2179 - val_loss: 10.9216 - val_regression_loss: 7.4246 - lr: 5.0000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8954 - regression_loss: 6.1063 - val_loss: 10.8582 - val_regression_loss: 7.3766 - lr: 2.5000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9183 - regression_loss: 6.0914 - val_loss: 10.5647 - val_regression_loss: 7.1526 - lr: 2.5000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9713 - regression_loss: 6.0682 - val_loss: 10.8376 - val_regression_loss: 7.3613 - lr: 2.5000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9469 - regression_loss: 6.0850 - val_loss: 10.8272 - val_regression_loss: 7.3566 - lr: 2.5000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8049 - regression_loss: 6.0470 - val_loss: 10.5825 - val_regression_loss: 7.1715 - lr: 2.5000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7950 - regression_loss: 6.0357 - val_loss: 10.8089 - val_regression_loss: 7.3450 - lr: 2.5000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8338 - regression_loss: 6.0370 - val_loss: 10.6099 - val_regression_loss: 7.1953 - lr: 2.5000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8811 - regression_loss: 6.0226 - val_loss: 10.5977 - val_regression_loss: 7.1844 - lr: 2.5000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8157 - regression_loss: 6.0197 - val_loss: 10.5663 - val_regression_loss: 7.1621 - lr: 2.5000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8319 - regression_loss: 6.0025 - val_loss: 10.6121 - val_regression_loss: 7.2028 - lr: 2.5000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7130 - regression_loss: 5.9952 - val_loss: 10.6997 - val_regression_loss: 7.2679 - lr: 2.5000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6104 - regression_loss: 5.9998 - val_loss: 10.7417 - val_regression_loss: 7.2994 - lr: 2.5000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8651 - regression_loss: 5.9987 - val_loss: 10.4917 - val_regression_loss: 7.1109 - lr: 2.5000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7902 - regression_loss: 5.9763 - val_loss: 10.6253 - val_regression_loss: 7.2162 - lr: 2.5000e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6924 - regression_loss: 5.9630 - val_loss: 10.6246 - val_regression_loss: 7.2168 - lr: 2.5000e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5725 - regression_loss: 5.9684 - val_loss: 10.6172 - val_regression_loss: 7.2102 - lr: 2.5000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8022 - regression_loss: 5.9554 - val_loss: 10.5559 - val_regression_loss: 7.1650 - lr: 2.5000e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6207 - regression_loss: 5.9369 - val_loss: 10.4909 - val_regression_loss: 7.1190 - lr: 2.5000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7848 - regression_loss: 5.9814 - val_loss: 10.5714 - val_regression_loss: 7.1784 - lr: 2.5000e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7229 - regression_loss: 5.9850 - val_loss: 10.4315 - val_regression_loss: 7.0701 - lr: 2.5000e-05\n",
      "Epoch 81/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.3805 - regression_loss: 7.0030\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8009 - regression_loss: 5.9028 - val_loss: 10.7220 - val_regression_loss: 7.2975 - lr: 2.5000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8005 - regression_loss: 5.9278 - val_loss: 10.6383 - val_regression_loss: 7.2340 - lr: 1.2500e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6056 - regression_loss: 5.9153 - val_loss: 10.4659 - val_regression_loss: 7.0999 - lr: 1.2500e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6087 - regression_loss: 5.8946 - val_loss: 10.5153 - val_regression_loss: 7.1386 - lr: 1.2500e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5128 - regression_loss: 5.8902 - val_loss: 10.5494 - val_regression_loss: 7.1670 - lr: 1.2500e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7067 - regression_loss: 5.8861 - val_loss: 10.5261 - val_regression_loss: 7.1499 - lr: 1.2500e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6573 - regression_loss: 5.8834 - val_loss: 10.4582 - val_regression_loss: 7.0984 - lr: 1.2500e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5529 - regression_loss: 5.8861 - val_loss: 10.4759 - val_regression_loss: 7.1109 - lr: 1.2500e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6571 - regression_loss: 5.8775 - val_loss: 10.5052 - val_regression_loss: 7.1348 - lr: 1.2500e-05\n",
      "Epoch 90/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.4601 - regression_loss: 7.0827\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6232 - regression_loss: 5.8788 - val_loss: 10.5260 - val_regression_loss: 7.1526 - lr: 1.2500e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6803 - regression_loss: 5.8707 - val_loss: 10.5170 - val_regression_loss: 7.1456 - lr: 6.2500e-06\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6075 - regression_loss: 5.8652 - val_loss: 10.4929 - val_regression_loss: 7.1267 - lr: 6.2500e-06\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6843 - regression_loss: 5.8613 - val_loss: 10.4716 - val_regression_loss: 7.1103 - lr: 6.2500e-06\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6143 - regression_loss: 5.8762 - val_loss: 10.4044 - val_regression_loss: 7.0586 - lr: 6.2500e-06\n",
      "Epoch 95/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.8074 - regression_loss: 7.4301\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6094 - regression_loss: 5.8601 - val_loss: 10.4723 - val_regression_loss: 7.1110 - lr: 6.2500e-06\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6211 - regression_loss: 5.8528 - val_loss: 10.4966 - val_regression_loss: 7.1299 - lr: 3.1250e-06\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6955 - regression_loss: 5.8576 - val_loss: 10.5348 - val_regression_loss: 7.1595 - lr: 3.1250e-06\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6925 - regression_loss: 5.8571 - val_loss: 10.4991 - val_regression_loss: 7.1318 - lr: 3.1250e-06\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5933 - regression_loss: 5.8550 - val_loss: 10.4789 - val_regression_loss: 7.1161 - lr: 3.1250e-06\n",
      "Epoch 100/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.5319 - regression_loss: 5.1546\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7149 - regression_loss: 5.8530 - val_loss: 10.4929 - val_regression_loss: 7.1272 - lr: 3.1250e-06\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6524 - regression_loss: 5.8497 - val_loss: 10.4826 - val_regression_loss: 7.1194 - lr: 1.5625e-06\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6732 - regression_loss: 5.8499 - val_loss: 10.4839 - val_regression_loss: 7.1206 - lr: 1.5625e-06\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6254 - regression_loss: 5.8500 - val_loss: 10.4694 - val_regression_loss: 7.1095 - lr: 1.5625e-06\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6805 - regression_loss: 5.8484 - val_loss: 10.4748 - val_regression_loss: 7.1138 - lr: 1.5625e-06\n",
      "Epoch 105/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.6678 - regression_loss: 6.2905\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7207 - regression_loss: 5.8475 - val_loss: 10.4722 - val_regression_loss: 7.1118 - lr: 1.5625e-06\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6822 - regression_loss: 5.8471 - val_loss: 10.4674 - val_regression_loss: 7.1081 - lr: 7.8125e-07\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6441 - regression_loss: 5.8467 - val_loss: 10.4700 - val_regression_loss: 7.1101 - lr: 7.8125e-07\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6739 - regression_loss: 5.8462 - val_loss: 10.4724 - val_regression_loss: 7.1121 - lr: 7.8125e-07\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6014 - regression_loss: 5.8468 - val_loss: 10.4683 - val_regression_loss: 7.1090 - lr: 7.8125e-07\n",
      "Epoch 110/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.9867 - regression_loss: 5.6095\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6077 - regression_loss: 5.8459 - val_loss: 10.4677 - val_regression_loss: 7.1085 - lr: 7.8125e-07\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6629 - regression_loss: 5.8455 - val_loss: 10.4719 - val_regression_loss: 7.1117 - lr: 3.9062e-07\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6517 - regression_loss: 5.8454 - val_loss: 10.4733 - val_regression_loss: 7.1129 - lr: 3.9062e-07\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6644 - regression_loss: 5.8455 - val_loss: 10.4695 - val_regression_loss: 7.1100 - lr: 3.9062e-07\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6566 - regression_loss: 5.8452 - val_loss: 10.4726 - val_regression_loss: 7.1123 - lr: 3.9062e-07\n",
      "Epoch 115/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.3193 - regression_loss: 6.9420\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6696 - regression_loss: 5.8450 - val_loss: 10.4747 - val_regression_loss: 7.1139 - lr: 3.9062e-07\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7314 - regression_loss: 5.8447 - val_loss: 10.4738 - val_regression_loss: 7.1133 - lr: 1.9531e-07\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6602 - regression_loss: 5.8448 - val_loss: 10.4747 - val_regression_loss: 7.1140 - lr: 1.9531e-07\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5036 - regression_loss: 5.8446 - val_loss: 10.4738 - val_regression_loss: 7.1133 - lr: 1.9531e-07\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5360 - regression_loss: 5.8444 - val_loss: 10.4729 - val_regression_loss: 7.1126 - lr: 1.9531e-07\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6395 - regression_loss: 5.8446 - val_loss: 10.4736 - val_regression_loss: 7.1132 - lr: 1.9531e-07\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5699 - regression_loss: 5.8444 - val_loss: 10.4721 - val_regression_loss: 7.1121 - lr: 1.9531e-07\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6843 - regression_loss: 5.8444 - val_loss: 10.4722 - val_regression_loss: 7.1121 - lr: 1.9531e-07\n",
      "Epoch 123/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.4451 - regression_loss: 5.0678\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6234 - regression_loss: 5.8442 - val_loss: 10.4713 - val_regression_loss: 7.1114 - lr: 1.9531e-07\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5322 - regression_loss: 5.8440 - val_loss: 10.4713 - val_regression_loss: 7.1114 - lr: 9.7656e-08\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6118 - regression_loss: 5.8442 - val_loss: 10.4705 - val_regression_loss: 7.1108 - lr: 9.7656e-08\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7086 - regression_loss: 5.8440 - val_loss: 10.4711 - val_regression_loss: 7.1113 - lr: 9.7656e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7066 - regression_loss: 5.8442 - val_loss: 10.4721 - val_regression_loss: 7.1121 - lr: 9.7656e-08\n",
      "Epoch 128/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.0397 - regression_loss: 7.6624\n",
      "Epoch 128: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6006 - regression_loss: 5.8440 - val_loss: 10.4722 - val_regression_loss: 7.1122 - lr: 9.7656e-08\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6109 - regression_loss: 5.8439 - val_loss: 10.4720 - val_regression_loss: 7.1120 - lr: 4.8828e-08\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6592 - regression_loss: 5.8439 - val_loss: 10.4717 - val_regression_loss: 7.1117 - lr: 4.8828e-08\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6884 - regression_loss: 5.8439 - val_loss: 10.4715 - val_regression_loss: 7.1116 - lr: 4.8828e-08\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5622 - regression_loss: 5.8439 - val_loss: 10.4712 - val_regression_loss: 7.1114 - lr: 4.8828e-08\n",
      "Epoch 133/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.4948 - regression_loss: 8.1175\n",
      "Epoch 133: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6282 - regression_loss: 5.8439 - val_loss: 10.4716 - val_regression_loss: 7.1117 - lr: 4.8828e-08\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6267 - regression_loss: 5.8438 - val_loss: 10.4715 - val_regression_loss: 7.1116 - lr: 2.4414e-08\n",
      "3/3 [==============================] - 0s 996us/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 24ms/step - loss: 107.7258 - regression_loss: 98.6085 - val_loss: 60.3356 - val_regression_loss: 48.2207 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 85.4614 - regression_loss: 78.1520 - val_loss: 50.6901 - val_regression_loss: 40.3713 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 72.8679 - regression_loss: 67.5249 - val_loss: 46.3315 - val_regression_loss: 36.4453 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 66.0975 - regression_loss: 60.3497 - val_loss: 44.8111 - val_regression_loss: 34.4779 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 60.7315 - regression_loss: 55.0876 - val_loss: 42.0879 - val_regression_loss: 32.1772 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 56.1330 - regression_loss: 51.0130 - val_loss: 39.1594 - val_regression_loss: 29.8524 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 51.6889 - regression_loss: 47.3677 - val_loss: 36.5637 - val_regression_loss: 27.7844 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 48.0067 - regression_loss: 43.6580 - val_loss: 33.3426 - val_regression_loss: 25.4865 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 45.4571 - regression_loss: 40.7688 - val_loss: 31.4184 - val_regression_loss: 23.9253 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 41.1497 - regression_loss: 37.6026 - val_loss: 29.5579 - val_regression_loss: 22.3470 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 38.0346 - regression_loss: 34.6703 - val_loss: 27.6945 - val_regression_loss: 20.8634 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 35.6510 - regression_loss: 32.1229 - val_loss: 26.2570 - val_regression_loss: 19.5808 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 33.6673 - regression_loss: 29.7648 - val_loss: 23.8463 - val_regression_loss: 17.8160 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.4339 - regression_loss: 26.9543 - val_loss: 21.7081 - val_regression_loss: 16.2886 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.6653 - regression_loss: 24.7753 - val_loss: 20.8597 - val_regression_loss: 15.4453 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.0167 - regression_loss: 22.9645 - val_loss: 19.8179 - val_regression_loss: 14.6101 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.9759 - regression_loss: 21.2615 - val_loss: 18.5672 - val_regression_loss: 13.7091 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.8136 - regression_loss: 20.0187 - val_loss: 17.4745 - val_regression_loss: 12.9597 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.3708 - regression_loss: 18.9279 - val_loss: 17.0081 - val_regression_loss: 12.5396 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.3641 - regression_loss: 18.0388 - val_loss: 16.6420 - val_regression_loss: 12.2271 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.7945 - regression_loss: 17.3207 - val_loss: 16.1822 - val_regression_loss: 11.9701 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.3003 - regression_loss: 16.8965 - val_loss: 16.0000 - val_regression_loss: 11.8878 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1003 - regression_loss: 16.5792 - val_loss: 15.8853 - val_regression_loss: 11.6709 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.6655 - regression_loss: 16.3555 - val_loss: 15.8002 - val_regression_loss: 11.6181 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.2331 - regression_loss: 16.1529 - val_loss: 15.5904 - val_regression_loss: 11.4728 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.7225 - regression_loss: 16.1609 - val_loss: 15.6977 - val_regression_loss: 11.6402 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.1487 - regression_loss: 15.7105 - val_loss: 15.3942 - val_regression_loss: 11.3799 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.4271 - regression_loss: 15.8508 - val_loss: 15.6896 - val_regression_loss: 11.8065 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.9968 - regression_loss: 15.5926 - val_loss: 16.8459 - val_regression_loss: 12.2163 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.2146 - regression_loss: 15.6905 - val_loss: 17.2457 - val_regression_loss: 13.2778 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.8463 - regression_loss: 16.1873 - val_loss: 17.1623 - val_regression_loss: 12.4738 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.1106 - regression_loss: 15.9114 - val_loss: 15.8967 - val_regression_loss: 12.1488 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.8131 - regression_loss: 15.2987 - val_loss: 15.9787 - val_regression_loss: 11.6302 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3913 - regression_loss: 15.0269 - val_loss: 15.6451 - val_regression_loss: 11.6657 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3931 - regression_loss: 15.0857 - val_loss: 15.5125 - val_regression_loss: 11.3175 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3233 - regression_loss: 14.7394 - val_loss: 15.3809 - val_regression_loss: 11.2817 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8407 - regression_loss: 14.6963 - val_loss: 15.3763 - val_regression_loss: 11.4342 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.0166 - regression_loss: 14.5069 - val_loss: 15.1881 - val_regression_loss: 11.1139 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8585 - regression_loss: 14.4900 - val_loss: 15.3402 - val_regression_loss: 11.3691 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7971 - regression_loss: 14.5185 - val_loss: 15.5597 - val_regression_loss: 11.3505 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9232 - regression_loss: 14.3542 - val_loss: 15.1455 - val_regression_loss: 11.1692 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.4774 - regression_loss: 14.1532 - val_loss: 15.0623 - val_regression_loss: 11.0856 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.3767 - regression_loss: 14.1011 - val_loss: 15.2301 - val_regression_loss: 11.1962 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.2467 - regression_loss: 14.0606 - val_loss: 15.4454 - val_regression_loss: 11.3243 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5110 - regression_loss: 13.9992 - val_loss: 15.1887 - val_regression_loss: 11.1445 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.3817 - regression_loss: 13.8723 - val_loss: 14.9258 - val_regression_loss: 10.9652 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.2579 - regression_loss: 13.8390 - val_loss: 15.2357 - val_regression_loss: 11.1572 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.3611 - regression_loss: 13.8409 - val_loss: 15.6037 - val_regression_loss: 11.5964 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.9256 - regression_loss: 14.0325 - val_loss: 15.8686 - val_regression_loss: 11.5172 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.3732 - regression_loss: 13.8006 - val_loss: 15.7350 - val_regression_loss: 11.8357 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5133 - regression_loss: 14.0669 - val_loss: 16.1188 - val_regression_loss: 11.7229 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.4071 - regression_loss: 14.2989 - val_loss: 15.3193 - val_regression_loss: 11.3937 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.0906 - regression_loss: 13.7978 - val_loss: 15.1101 - val_regression_loss: 11.1026 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.6816 - regression_loss: 13.4030 - val_loss: 15.2983 - val_regression_loss: 11.1931 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.7979 - regression_loss: 13.4709 - val_loss: 15.2602 - val_regression_loss: 11.3707 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.7814 - regression_loss: 13.4367 - val_loss: 15.5207 - val_regression_loss: 11.3225 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.7809 - regression_loss: 13.4091 - val_loss: 16.1969 - val_regression_loss: 12.3169 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5249 - regression_loss: 14.0882 - val_loss: 16.3748 - val_regression_loss: 11.9267 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.9064 - regression_loss: 17.5340\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.8382 - regression_loss: 13.6475 - val_loss: 15.2952 - val_regression_loss: 11.3912 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5993 - regression_loss: 13.3457 - val_loss: 15.1997 - val_regression_loss: 11.0992 - lr: 5.0000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.3946 - regression_loss: 13.0862 - val_loss: 14.9949 - val_regression_loss: 11.0881 - lr: 5.0000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.3935 - regression_loss: 13.1916 - val_loss: 15.1096 - val_regression_loss: 11.0584 - lr: 5.0000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5519 - regression_loss: 13.2202 - val_loss: 15.0890 - val_regression_loss: 11.0699 - lr: 5.0000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.3055 - regression_loss: 12.9366 - val_loss: 15.2083 - val_regression_loss: 11.2071 - lr: 5.0000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.0301 - regression_loss: 12.9934 - val_loss: 15.0839 - val_regression_loss: 11.0668 - lr: 5.0000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.2239 - regression_loss: 12.9734 - val_loss: 14.9897 - val_regression_loss: 11.0022 - lr: 5.0000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.3087 - regression_loss: 12.9607 - val_loss: 15.0841 - val_regression_loss: 11.0846 - lr: 5.0000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.0868 - regression_loss: 12.8750 - val_loss: 15.2162 - val_regression_loss: 11.1296 - lr: 5.0000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.8063 - regression_loss: 12.9187 - val_loss: 15.1422 - val_regression_loss: 11.1584 - lr: 5.0000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.2698 - regression_loss: 12.8713 - val_loss: 15.0604 - val_regression_loss: 11.0903 - lr: 5.0000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.1679 - regression_loss: 12.8254 - val_loss: 15.1310 - val_regression_loss: 11.0877 - lr: 5.0000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.0561 - regression_loss: 12.8416 - val_loss: 15.0925 - val_regression_loss: 11.1356 - lr: 5.0000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.9526 - regression_loss: 12.8186 - val_loss: 15.0619 - val_regression_loss: 11.0489 - lr: 5.0000e-05\n",
      "Epoch 74/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.7374 - regression_loss: 14.3649\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.8350 - regression_loss: 12.8009 - val_loss: 15.1485 - val_regression_loss: 11.1020 - lr: 5.0000e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.0291 - regression_loss: 12.7283 - val_loss: 15.1446 - val_regression_loss: 11.1364 - lr: 2.5000e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.1312 - regression_loss: 12.7464 - val_loss: 15.1267 - val_regression_loss: 11.0940 - lr: 2.5000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.7960 - regression_loss: 12.7417 - val_loss: 15.0655 - val_regression_loss: 11.0740 - lr: 2.5000e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.9253 - regression_loss: 12.6951 - val_loss: 15.0316 - val_regression_loss: 11.0687 - lr: 2.5000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.7751 - regression_loss: 12.6912 - val_loss: 15.0324 - val_regression_loss: 11.0427 - lr: 2.5000e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.2165 - regression_loss: 12.7852 - val_loss: 15.0868 - val_regression_loss: 11.0636 - lr: 2.5000e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.6166 - regression_loss: 12.7152 - val_loss: 15.1220 - val_regression_loss: 11.1636 - lr: 2.5000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.0440 - regression_loss: 12.7019 - val_loss: 15.1213 - val_regression_loss: 11.1041 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.8621 - regression_loss: 12.6935 - val_loss: 15.1945 - val_regression_loss: 11.1309 - lr: 2.5000e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.7671 - regression_loss: 12.6452 - val_loss: 15.1167 - val_regression_loss: 11.1145 - lr: 2.5000e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.0491 - regression_loss: 12.6872 - val_loss: 15.0954 - val_regression_loss: 11.1119 - lr: 2.5000e-05\n",
      "Epoch 86/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.5934 - regression_loss: 17.2210\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.0663 - regression_loss: 12.6216 - val_loss: 15.0696 - val_regression_loss: 11.0673 - lr: 2.5000e-05\n",
      "3/3 [==============================] - 0s 958us/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 26ms/step - loss: 88.0690 - regression_loss: 81.0910 - val_loss: 71.2858 - val_regression_loss: 56.7276 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 67.5267 - regression_loss: 61.8137 - val_loss: 62.4860 - val_regression_loss: 48.6158 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 57.2629 - regression_loss: 51.8027 - val_loss: 55.0265 - val_regression_loss: 42.7968 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 48.7848 - regression_loss: 45.2108 - val_loss: 48.9021 - val_regression_loss: 37.9723 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 45.1141 - regression_loss: 41.0551 - val_loss: 45.2400 - val_regression_loss: 34.8566 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 42.0000 - regression_loss: 37.6848 - val_loss: 41.6160 - val_regression_loss: 32.6696 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 40.1938 - regression_loss: 35.9828 - val_loss: 39.6737 - val_regression_loss: 30.6592 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 38.4301 - regression_loss: 34.5935 - val_loss: 37.5316 - val_regression_loss: 29.3252 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 37.1109 - regression_loss: 32.8013 - val_loss: 36.5170 - val_regression_loss: 28.6744 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 35.4046 - regression_loss: 31.6793 - val_loss: 35.5464 - val_regression_loss: 27.7093 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 34.5198 - regression_loss: 30.8671 - val_loss: 34.2699 - val_regression_loss: 27.0868 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 34.1841 - regression_loss: 30.2510 - val_loss: 33.0515 - val_regression_loss: 26.0614 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.8282 - regression_loss: 29.4430 - val_loss: 32.5500 - val_regression_loss: 25.6554 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.6531 - regression_loss: 29.0558 - val_loss: 31.9335 - val_regression_loss: 25.2062 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.3770 - regression_loss: 28.6218 - val_loss: 31.4350 - val_regression_loss: 24.9628 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.9222 - regression_loss: 28.0731 - val_loss: 31.0590 - val_regression_loss: 24.6197 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.1280 - regression_loss: 27.7312 - val_loss: 30.3572 - val_regression_loss: 24.2780 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.9903 - regression_loss: 27.5764 - val_loss: 29.9929 - val_regression_loss: 23.9805 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.8262 - regression_loss: 27.2062 - val_loss: 29.9580 - val_regression_loss: 23.8812 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.1465 - regression_loss: 26.9252 - val_loss: 29.6128 - val_regression_loss: 23.6993 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.9374 - regression_loss: 26.7131 - val_loss: 29.4678 - val_regression_loss: 23.5339 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.7078 - regression_loss: 26.4876 - val_loss: 28.9647 - val_regression_loss: 23.3099 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.9830 - regression_loss: 26.2461 - val_loss: 29.1662 - val_regression_loss: 23.3056 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.2736 - regression_loss: 26.2677 - val_loss: 29.3097 - val_regression_loss: 23.7355 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.6246 - regression_loss: 26.3563 - val_loss: 29.2380 - val_regression_loss: 23.3459 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.0034 - regression_loss: 25.8492 - val_loss: 28.7873 - val_regression_loss: 23.2093 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.0019 - regression_loss: 25.7127 - val_loss: 28.5674 - val_regression_loss: 23.0704 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.1298 - regression_loss: 25.7852 - val_loss: 28.9978 - val_regression_loss: 23.1305 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.1110 - regression_loss: 25.9095 - val_loss: 28.3957 - val_regression_loss: 22.9100 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.7273 - regression_loss: 25.3226 - val_loss: 28.7323 - val_regression_loss: 23.0448 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.4849 - regression_loss: 25.3957 - val_loss: 28.8895 - val_regression_loss: 23.2480 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.3755 - regression_loss: 25.1771 - val_loss: 28.8341 - val_regression_loss: 23.0555 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.4647 - regression_loss: 25.0578 - val_loss: 28.5465 - val_regression_loss: 23.2043 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.3970 - regression_loss: 25.1259 - val_loss: 28.8467 - val_regression_loss: 23.0778 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.4267 - regression_loss: 25.0988 - val_loss: 28.5629 - val_regression_loss: 23.0299 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.9901 - regression_loss: 25.1401 - val_loss: 28.5593 - val_regression_loss: 22.9612 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.3673 - regression_loss: 25.1019 - val_loss: 28.5194 - val_regression_loss: 23.1176 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.2003 - regression_loss: 25.1886 - val_loss: 28.9375 - val_regression_loss: 23.1434 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.7942 - regression_loss: 24.6809 - val_loss: 28.5593 - val_regression_loss: 22.9991 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.5872 - regression_loss: 24.9472 - val_loss: 29.0212 - val_regression_loss: 23.0780 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.2172 - regression_loss: 25.1710 - val_loss: 28.4190 - val_regression_loss: 22.9522 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.0093 - regression_loss: 24.8193 - val_loss: 28.7299 - val_regression_loss: 23.0785 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.8341 - regression_loss: 24.8375 - val_loss: 29.1134 - val_regression_loss: 23.2427 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.1348 - regression_loss: 24.7879 - val_loss: 28.8319 - val_regression_loss: 23.3941 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 30.4746 - regression_loss: 29.1046\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.2973 - regression_loss: 25.2092 - val_loss: 29.1701 - val_regression_loss: 23.2093 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.4802 - regression_loss: 24.5582 - val_loss: 28.6862 - val_regression_loss: 23.0560 - lr: 5.0000e-05\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.1666 - regression_loss: 24.6885 - val_loss: 28.8199 - val_regression_loss: 23.2927 - lr: 5.0000e-05\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.2134 - regression_loss: 24.2124 - val_loss: 29.1563 - val_regression_loss: 23.2480 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.7936 - regression_loss: 24.1932 - val_loss: 28.7962 - val_regression_loss: 23.2504 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.7391 - regression_loss: 24.2573 - val_loss: 28.8551 - val_regression_loss: 23.2101 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.1831 - regression_loss: 24.0598 - val_loss: 28.7515 - val_regression_loss: 23.0279 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.8513 - regression_loss: 24.0210 - val_loss: 28.6822 - val_regression_loss: 23.1838 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.8499 - regression_loss: 23.9891 - val_loss: 28.5491 - val_regression_loss: 22.9554 - lr: 5.0000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.8469 - regression_loss: 23.9664 - val_loss: 28.7183 - val_regression_loss: 23.0537 - lr: 5.0000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.6441 - regression_loss: 23.8722 - val_loss: 28.8289 - val_regression_loss: 23.2439 - lr: 5.0000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.2428 - regression_loss: 23.9122 - val_loss: 28.8408 - val_regression_loss: 23.1597 - lr: 5.0000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.8333 - regression_loss: 23.8083 - val_loss: 28.8064 - val_regression_loss: 23.1886 - lr: 5.0000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.4297 - regression_loss: 23.7838 - val_loss: 28.7867 - val_regression_loss: 23.1649 - lr: 5.0000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.4794 - regression_loss: 23.8004 - val_loss: 28.7704 - val_regression_loss: 23.0716 - lr: 5.0000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.2243 - regression_loss: 23.8071 - val_loss: 28.8741 - val_regression_loss: 23.2496 - lr: 5.0000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.9032 - regression_loss: 23.8157 - val_loss: 29.0062 - val_regression_loss: 23.2381 - lr: 5.0000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.4756 - regression_loss: 24.0995 - val_loss: 28.6951 - val_regression_loss: 23.1689 - lr: 5.0000e-05\n",
      "Epoch 63/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 25.4284 - regression_loss: 24.0578\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.0305 - regression_loss: 23.8445 - val_loss: 28.7519 - val_regression_loss: 22.9921 - lr: 5.0000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.6948 - regression_loss: 23.6334 - val_loss: 28.7996 - val_regression_loss: 23.1570 - lr: 2.5000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.2103 - regression_loss: 23.5761 - val_loss: 28.9338 - val_regression_loss: 23.3763 - lr: 2.5000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.4720 - regression_loss: 23.6056 - val_loss: 28.8658 - val_regression_loss: 23.2237 - lr: 2.5000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.0063 - regression_loss: 23.5147 - val_loss: 28.9404 - val_regression_loss: 23.2211 - lr: 2.5000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.3329 - regression_loss: 23.5343 - val_loss: 28.9279 - val_regression_loss: 23.2712 - lr: 2.5000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.4524 - regression_loss: 23.4963 - val_loss: 28.9015 - val_regression_loss: 23.2940 - lr: 2.5000e-05\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 72.9013 - regression_loss: 66.7529 - val_loss: 84.0780 - val_regression_loss: 68.0043 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 55.6926 - regression_loss: 50.8063 - val_loss: 68.4929 - val_regression_loss: 54.6481 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 50.7229 - regression_loss: 45.7057 - val_loss: 64.9933 - val_regression_loss: 53.2321 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 44.7717 - regression_loss: 40.4674 - val_loss: 53.3713 - val_regression_loss: 43.3666 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 38.3128 - regression_loss: 35.4309 - val_loss: 48.1461 - val_regression_loss: 39.7281 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 35.8196 - regression_loss: 31.9640 - val_loss: 41.9600 - val_regression_loss: 34.6805 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.4090 - regression_loss: 29.1481 - val_loss: 38.0954 - val_regression_loss: 31.5784 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.6133 - regression_loss: 26.2059 - val_loss: 34.1884 - val_regression_loss: 28.3056 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.5150 - regression_loss: 23.5572 - val_loss: 31.9892 - val_regression_loss: 26.6186 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.7471 - regression_loss: 21.4399 - val_loss: 27.9576 - val_regression_loss: 22.9003 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.3795 - regression_loss: 19.2412 - val_loss: 26.2549 - val_regression_loss: 21.4864 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.8624 - regression_loss: 17.2374 - val_loss: 23.5825 - val_regression_loss: 19.1472 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.7837 - regression_loss: 15.6776 - val_loss: 21.6132 - val_regression_loss: 17.4427 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.3346 - regression_loss: 14.3098 - val_loss: 19.7224 - val_regression_loss: 15.7852 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.7145 - regression_loss: 13.2273 - val_loss: 17.6962 - val_regression_loss: 13.9924 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1744 - regression_loss: 12.2793 - val_loss: 17.2199 - val_regression_loss: 13.5720 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8023 - regression_loss: 11.5410 - val_loss: 15.4683 - val_regression_loss: 11.9690 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.2450 - regression_loss: 11.0704 - val_loss: 15.4468 - val_regression_loss: 11.9356 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0083 - regression_loss: 11.0658 - val_loss: 16.2239 - val_regression_loss: 12.5723 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2751 - regression_loss: 10.1858 - val_loss: 13.9237 - val_regression_loss: 10.5255 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0924 - regression_loss: 10.7910 - val_loss: 14.0923 - val_regression_loss: 10.6863 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9553 - regression_loss: 9.9038 - val_loss: 14.8581 - val_regression_loss: 11.3284 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8271 - regression_loss: 9.7405 - val_loss: 12.9279 - val_regression_loss: 9.6667 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1858 - regression_loss: 9.0750 - val_loss: 13.8227 - val_regression_loss: 10.3858 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1075 - regression_loss: 9.1375 - val_loss: 12.5207 - val_regression_loss: 9.2606 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2152 - regression_loss: 9.3767 - val_loss: 15.0061 - val_regression_loss: 11.3184 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1873 - regression_loss: 9.1929 - val_loss: 12.3861 - val_regression_loss: 9.1152 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8386 - regression_loss: 8.8161 - val_loss: 12.5631 - val_regression_loss: 9.2334 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6403 - regression_loss: 8.5892 - val_loss: 13.2344 - val_regression_loss: 9.8052 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2089 - regression_loss: 8.3319 - val_loss: 12.0149 - val_regression_loss: 8.7741 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.5045 - regression_loss: 8.5574 - val_loss: 12.3913 - val_regression_loss: 9.0571 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9127 - regression_loss: 8.1558 - val_loss: 12.4727 - val_regression_loss: 9.1293 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8093 - regression_loss: 8.0498 - val_loss: 12.1573 - val_regression_loss: 8.8661 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9421 - regression_loss: 7.9506 - val_loss: 12.5862 - val_regression_loss: 9.2043 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8821 - regression_loss: 7.9431 - val_loss: 11.4472 - val_regression_loss: 8.2189 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0641 - regression_loss: 8.1255 - val_loss: 11.9837 - val_regression_loss: 8.7019 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7323 - regression_loss: 7.7691 - val_loss: 11.9112 - val_regression_loss: 8.6207 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5642 - regression_loss: 7.6428 - val_loss: 11.3823 - val_regression_loss: 8.1447 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7650 - regression_loss: 7.7875 - val_loss: 11.6398 - val_regression_loss: 8.3560 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5277 - regression_loss: 7.5982 - val_loss: 12.2030 - val_regression_loss: 8.8247 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2573 - regression_loss: 7.4147 - val_loss: 10.9584 - val_regression_loss: 7.7704 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6122 - regression_loss: 7.7297 - val_loss: 11.5764 - val_regression_loss: 8.2910 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1191 - regression_loss: 7.4412 - val_loss: 12.3794 - val_regression_loss: 8.9399 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2882 - regression_loss: 7.3640 - val_loss: 10.8119 - val_regression_loss: 7.6215 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1741 - regression_loss: 7.2762 - val_loss: 11.3012 - val_regression_loss: 8.0257 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0339 - regression_loss: 7.1837 - val_loss: 11.4709 - val_regression_loss: 8.1696 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.1266 - regression_loss: 7.1924 - val_loss: 10.6973 - val_regression_loss: 7.4722 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7169 - regression_loss: 7.1278 - val_loss: 10.8849 - val_regression_loss: 7.6725 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9235 - regression_loss: 7.0018 - val_loss: 11.7300 - val_regression_loss: 8.3540 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8228 - regression_loss: 7.0802 - val_loss: 10.6026 - val_regression_loss: 7.4087 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8683 - regression_loss: 7.0482 - val_loss: 10.7701 - val_regression_loss: 7.5384 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8972 - regression_loss: 6.9008 - val_loss: 10.3620 - val_regression_loss: 7.2030 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.3755 - regression_loss: 8.0093\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8151 - regression_loss: 6.9619 - val_loss: 13.7872 - val_regression_loss: 9.9910 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2264 - regression_loss: 7.4377 - val_loss: 10.4522 - val_regression_loss: 7.2468 - lr: 5.0000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1108 - regression_loss: 7.2368 - val_loss: 12.5628 - val_regression_loss: 8.9952 - lr: 5.0000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7441 - regression_loss: 6.8260 - val_loss: 10.2732 - val_regression_loss: 7.0769 - lr: 5.0000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9738 - regression_loss: 7.0888 - val_loss: 12.0707 - val_regression_loss: 8.5759 - lr: 5.0000e-05\n",
      "Epoch 58/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.9302 - regression_loss: 7.5643\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8127 - regression_loss: 6.9522 - val_loss: 10.2521 - val_regression_loss: 7.0702 - lr: 5.0000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7010 - regression_loss: 6.9350 - val_loss: 10.5252 - val_regression_loss: 7.3129 - lr: 2.5000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5065 - regression_loss: 6.6649 - val_loss: 11.5571 - val_regression_loss: 8.1589 - lr: 2.5000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5893 - regression_loss: 6.6791 - val_loss: 10.4570 - val_regression_loss: 7.2444 - lr: 2.5000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6873 - regression_loss: 6.7123 - val_loss: 10.6683 - val_regression_loss: 7.4238 - lr: 2.5000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4657 - regression_loss: 6.5699 - val_loss: 11.0495 - val_regression_loss: 7.7411 - lr: 2.5000e-05\n",
      "Epoch 64/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3958 - regression_loss: 6.5591 - val_loss: 10.5858 - val_regression_loss: 7.3466 - lr: 2.5000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3854 - regression_loss: 6.5434 - val_loss: 10.5526 - val_regression_loss: 7.3189 - lr: 2.5000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3771 - regression_loss: 6.5286 - val_loss: 10.7241 - val_regression_loss: 7.4615 - lr: 2.5000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4823 - regression_loss: 6.5221 - val_loss: 10.5331 - val_regression_loss: 7.3061 - lr: 2.5000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3993 - regression_loss: 6.5196 - val_loss: 10.5907 - val_regression_loss: 7.3538 - lr: 2.5000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3795 - regression_loss: 6.5233 - val_loss: 10.8139 - val_regression_loss: 7.5333 - lr: 2.5000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2443 - regression_loss: 6.4784 - val_loss: 10.4792 - val_regression_loss: 7.2507 - lr: 2.5000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3781 - regression_loss: 6.5178 - val_loss: 10.5895 - val_regression_loss: 7.3357 - lr: 2.5000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3764 - regression_loss: 6.5008 - val_loss: 10.9319 - val_regression_loss: 7.6238 - lr: 2.5000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3314 - regression_loss: 6.4651 - val_loss: 10.3576 - val_regression_loss: 7.1436 - lr: 2.5000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4149 - regression_loss: 6.4734 - val_loss: 10.6690 - val_regression_loss: 7.3969 - lr: 2.5000e-05\n",
      "Epoch 75/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.8116 - regression_loss: 6.4461\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2504 - regression_loss: 6.4915 - val_loss: 10.6322 - val_regression_loss: 7.3685 - lr: 2.5000e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2975 - regression_loss: 6.4337 - val_loss: 10.4795 - val_regression_loss: 7.2414 - lr: 1.2500e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2936 - regression_loss: 6.4175 - val_loss: 10.6072 - val_regression_loss: 7.3472 - lr: 1.2500e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.1243 - regression_loss: 6.4106 - val_loss: 10.5694 - val_regression_loss: 7.3146 - lr: 1.2500e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1951 - regression_loss: 6.4037 - val_loss: 10.5855 - val_regression_loss: 7.3276 - lr: 1.2500e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1152 - regression_loss: 6.4083 - val_loss: 10.4924 - val_regression_loss: 7.2476 - lr: 1.2500e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2669 - regression_loss: 6.3942 - val_loss: 10.5873 - val_regression_loss: 7.3282 - lr: 1.2500e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1173 - regression_loss: 6.3923 - val_loss: 10.5857 - val_regression_loss: 7.3238 - lr: 1.2500e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1142 - regression_loss: 6.3878 - val_loss: 10.5325 - val_regression_loss: 7.2827 - lr: 1.2500e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2204 - regression_loss: 6.3794 - val_loss: 10.5285 - val_regression_loss: 7.2767 - lr: 1.2500e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2535 - regression_loss: 6.3757 - val_loss: 10.5224 - val_regression_loss: 7.2699 - lr: 1.2500e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9752 - regression_loss: 6.3722 - val_loss: 10.5294 - val_regression_loss: 7.2751 - lr: 1.2500e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2520 - regression_loss: 6.3665 - val_loss: 10.5507 - val_regression_loss: 7.2929 - lr: 1.2500e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1592 - regression_loss: 6.3697 - val_loss: 10.4298 - val_regression_loss: 7.1926 - lr: 1.2500e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1716 - regression_loss: 6.3607 - val_loss: 10.5468 - val_regression_loss: 7.2862 - lr: 1.2500e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9846 - regression_loss: 6.3583 - val_loss: 10.4962 - val_regression_loss: 7.2444 - lr: 1.2500e-05\n",
      "Epoch 91/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.4876 - regression_loss: 6.1223\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.1286 - regression_loss: 6.3448 - val_loss: 10.5182 - val_regression_loss: 7.2627 - lr: 1.2500e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1761 - regression_loss: 6.3435 - val_loss: 10.4809 - val_regression_loss: 7.2324 - lr: 6.2500e-06\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1623 - regression_loss: 6.3412 - val_loss: 10.4952 - val_regression_loss: 7.2417 - lr: 6.2500e-06\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2248 - regression_loss: 6.3398 - val_loss: 10.5113 - val_regression_loss: 7.2548 - lr: 6.2500e-06\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1825 - regression_loss: 6.3423 - val_loss: 10.4345 - val_regression_loss: 7.1931 - lr: 6.2500e-06\n",
      "Epoch 96/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.1030 - regression_loss: 7.7376\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1879 - regression_loss: 6.3409 - val_loss: 10.4518 - val_regression_loss: 7.2051 - lr: 6.2500e-06\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9932 - regression_loss: 6.3268 - val_loss: 10.5301 - val_regression_loss: 7.2693 - lr: 3.1250e-06\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1039 - regression_loss: 6.3259 - val_loss: 10.5558 - val_regression_loss: 7.2891 - lr: 3.1250e-06\n",
      "3/3 [==============================] - 0s 985us/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 26ms/step - loss: 143.2682 - regression_loss: 130.8732 - val_loss: 135.0451 - val_regression_loss: 103.1267 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 100.3299 - regression_loss: 90.6223 - val_loss: 95.4201 - val_regression_loss: 73.5448 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 74.2647 - regression_loss: 66.3273 - val_loss: 80.8960 - val_regression_loss: 61.1143 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 61.8485 - regression_loss: 55.7291 - val_loss: 69.9030 - val_regression_loss: 52.3117 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 54.0916 - regression_loss: 48.2722 - val_loss: 61.5399 - val_regression_loss: 45.7205 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 48.1730 - regression_loss: 42.7027 - val_loss: 55.1937 - val_regression_loss: 40.6367 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 42.4677 - regression_loss: 37.6681 - val_loss: 49.8723 - val_regression_loss: 36.4748 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 38.3609 - regression_loss: 34.0303 - val_loss: 45.1217 - val_regression_loss: 32.8500 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 33.8246 - regression_loss: 30.9773 - val_loss: 41.1379 - val_regression_loss: 29.8240 - lr: 1.0000e-04\n",
      "Epoch 10/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 31.8482 - regression_loss: 28.5947 - val_loss: 37.7413 - val_regression_loss: 27.2239 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.6517 - regression_loss: 26.5228 - val_loss: 34.7013 - val_regression_loss: 24.9008 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.5147 - regression_loss: 24.7273 - val_loss: 32.2292 - val_regression_loss: 22.9915 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.5354 - regression_loss: 23.0654 - val_loss: 30.2531 - val_regression_loss: 21.4345 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.4975 - regression_loss: 20.9471 - val_loss: 28.3881 - val_regression_loss: 19.9591 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.4238 - regression_loss: 19.4381 - val_loss: 26.5177 - val_regression_loss: 18.5188 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8294 - regression_loss: 18.0088 - val_loss: 24.8422 - val_regression_loss: 17.2400 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.7597 - regression_loss: 16.8062 - val_loss: 23.4728 - val_regression_loss: 16.2040 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.8003 - regression_loss: 15.8562 - val_loss: 22.1843 - val_regression_loss: 15.2747 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3945 - regression_loss: 14.8343 - val_loss: 20.5990 - val_regression_loss: 14.0672 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.2111 - regression_loss: 13.8942 - val_loss: 19.4086 - val_regression_loss: 13.1822 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.7121 - regression_loss: 13.2169 - val_loss: 18.3716 - val_regression_loss: 12.4145 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.0251 - regression_loss: 12.4289 - val_loss: 17.4628 - val_regression_loss: 11.7694 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.2164 - regression_loss: 11.6807 - val_loss: 16.4916 - val_regression_loss: 11.0777 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.4001 - regression_loss: 11.0342 - val_loss: 15.6084 - val_regression_loss: 10.4265 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7488 - regression_loss: 10.4954 - val_loss: 14.8720 - val_regression_loss: 9.8898 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1537 - regression_loss: 9.9586 - val_loss: 14.2056 - val_regression_loss: 9.3992 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6772 - regression_loss: 9.5702 - val_loss: 13.7138 - val_regression_loss: 9.0651 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3375 - regression_loss: 9.1189 - val_loss: 13.0159 - val_regression_loss: 8.5561 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8735 - regression_loss: 8.7593 - val_loss: 12.5223 - val_regression_loss: 8.1888 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7114 - regression_loss: 8.4244 - val_loss: 12.5046 - val_regression_loss: 8.1848 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2092 - regression_loss: 8.2452 - val_loss: 11.8088 - val_regression_loss: 7.6805 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2987 - regression_loss: 8.1145 - val_loss: 11.8901 - val_regression_loss: 7.7678 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2910 - regression_loss: 7.5478 - val_loss: 11.1734 - val_regression_loss: 7.2277 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6547 - regression_loss: 7.5935 - val_loss: 11.6088 - val_regression_loss: 7.5862 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0483 - regression_loss: 7.3591 - val_loss: 10.4660 - val_regression_loss: 6.6784 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1964 - regression_loss: 7.0880 - val_loss: 10.5594 - val_regression_loss: 6.7564 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9745 - regression_loss: 6.9657 - val_loss: 9.7586 - val_regression_loss: 6.1589 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5677 - regression_loss: 6.6102 - val_loss: 9.5249 - val_regression_loss: 5.9906 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4225 - regression_loss: 6.4663 - val_loss: 9.6031 - val_regression_loss: 6.0552 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.7163 - regression_loss: 3.3562\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4063 - regression_loss: 6.3883 - val_loss: 9.3175 - val_regression_loss: 5.8476 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9605 - regression_loss: 6.4907 - val_loss: 9.5449 - val_regression_loss: 6.0125 - lr: 5.0000e-05\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2748 - regression_loss: 6.3464 - val_loss: 8.9432 - val_regression_loss: 5.5763 - lr: 5.0000e-05\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0593 - regression_loss: 6.1140 - val_loss: 8.8484 - val_regression_loss: 5.4963 - lr: 5.0000e-05\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6684 - regression_loss: 6.0072 - val_loss: 8.6830 - val_regression_loss: 5.3576 - lr: 5.0000e-05\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7139 - regression_loss: 5.9441 - val_loss: 8.6585 - val_regression_loss: 5.3389 - lr: 5.0000e-05\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7941 - regression_loss: 5.8922 - val_loss: 8.4956 - val_regression_loss: 5.2153 - lr: 5.0000e-05\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0349 - regression_loss: 5.7773 - val_loss: 8.4158 - val_regression_loss: 5.1569 - lr: 5.0000e-05\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5191 - regression_loss: 5.7183 - val_loss: 8.2724 - val_regression_loss: 5.0628 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6497 - regression_loss: 5.7701 - val_loss: 8.2576 - val_regression_loss: 5.0516 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.2772 - regression_loss: 5.7959 - val_loss: 8.1557 - val_regression_loss: 4.9683 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6844 - regression_loss: 5.7278 - val_loss: 8.1200 - val_regression_loss: 4.9374 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.5946 - regression_loss: 3.2357\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.3633 - regression_loss: 5.5288 - val_loss: 7.9445 - val_regression_loss: 4.8024 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.1676 - regression_loss: 5.4101 - val_loss: 7.8825 - val_regression_loss: 4.7620 - lr: 2.5000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.2552 - regression_loss: 5.3724 - val_loss: 7.8431 - val_regression_loss: 4.7352 - lr: 2.5000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7978 - regression_loss: 5.3391 - val_loss: 7.8483 - val_regression_loss: 4.7371 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.1531 - regression_loss: 5.3077 - val_loss: 7.7946 - val_regression_loss: 4.6925 - lr: 2.5000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.1221 - regression_loss: 5.2717 - val_loss: 7.7460 - val_regression_loss: 4.6560 - lr: 2.5000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.1249 - regression_loss: 5.2534 - val_loss: 7.6900 - val_regression_loss: 4.6204 - lr: 2.5000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0073 - regression_loss: 5.2131 - val_loss: 7.6501 - val_regression_loss: 4.5905 - lr: 2.5000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0682 - regression_loss: 5.2090 - val_loss: 7.6326 - val_regression_loss: 4.5743 - lr: 2.5000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9881 - regression_loss: 5.1538 - val_loss: 7.5801 - val_regression_loss: 4.5362 - lr: 2.5000e-05\n",
      "Epoch 62/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.4025 - regression_loss: 3.0441\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0115 - regression_loss: 5.1289 - val_loss: 7.5558 - val_regression_loss: 4.5171 - lr: 2.5000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8912 - regression_loss: 5.0929 - val_loss: 7.5432 - val_regression_loss: 4.5074 - lr: 1.2500e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7731 - regression_loss: 5.0716 - val_loss: 7.5037 - val_regression_loss: 4.4782 - lr: 1.2500e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9225 - regression_loss: 5.0569 - val_loss: 7.4703 - val_regression_loss: 4.4555 - lr: 1.2500e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9262 - regression_loss: 5.0501 - val_loss: 7.4492 - val_regression_loss: 4.4408 - lr: 1.2500e-05\n",
      "Epoch 67/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.5790 - regression_loss: 2.2207\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8694 - regression_loss: 5.0204 - val_loss: 7.4456 - val_regression_loss: 4.4351 - lr: 1.2500e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4948 - regression_loss: 5.0273 - val_loss: 7.4574 - val_regression_loss: 4.4419 - lr: 6.2500e-06\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6344 - regression_loss: 5.0053 - val_loss: 7.4268 - val_regression_loss: 4.4196 - lr: 6.2500e-06\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8641 - regression_loss: 5.0005 - val_loss: 7.4081 - val_regression_loss: 4.4064 - lr: 6.2500e-06\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8381 - regression_loss: 4.9885 - val_loss: 7.3957 - val_regression_loss: 4.3973 - lr: 6.2500e-06\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8607 - regression_loss: 4.9809 - val_loss: 7.3832 - val_regression_loss: 4.3886 - lr: 6.2500e-06\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7925 - regression_loss: 4.9720 - val_loss: 7.3744 - val_regression_loss: 4.3821 - lr: 6.2500e-06\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7804 - regression_loss: 4.9773 - val_loss: 7.3785 - val_regression_loss: 4.3826 - lr: 6.2500e-06\n",
      "Epoch 75/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 14.9448 - regression_loss: 13.5866\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7578 - regression_loss: 4.9644 - val_loss: 7.3545 - val_regression_loss: 4.3669 - lr: 6.2500e-06\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7300 - regression_loss: 4.9493 - val_loss: 7.3480 - val_regression_loss: 4.3627 - lr: 3.1250e-06\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7012 - regression_loss: 4.9494 - val_loss: 7.3414 - val_regression_loss: 4.3582 - lr: 3.1250e-06\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7394 - regression_loss: 4.9475 - val_loss: 7.3463 - val_regression_loss: 4.3600 - lr: 3.1250e-06\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7994 - regression_loss: 4.9393 - val_loss: 7.3362 - val_regression_loss: 4.3528 - lr: 3.1250e-06\n",
      "Epoch 80/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.3505 - regression_loss: 2.9924\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7424 - regression_loss: 4.9336 - val_loss: 7.3339 - val_regression_loss: 4.3506 - lr: 3.1250e-06\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6679 - regression_loss: 4.9286 - val_loss: 7.3327 - val_regression_loss: 4.3495 - lr: 1.5625e-06\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7987 - regression_loss: 4.9264 - val_loss: 7.3299 - val_regression_loss: 4.3473 - lr: 1.5625e-06\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8267 - regression_loss: 4.9261 - val_loss: 7.3293 - val_regression_loss: 4.3466 - lr: 1.5625e-06\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6279 - regression_loss: 4.9224 - val_loss: 7.3236 - val_regression_loss: 4.3428 - lr: 1.5625e-06\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3225 - regression_loss: 4.9216 - val_loss: 7.3231 - val_regression_loss: 4.3420 - lr: 1.5625e-06\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7677 - regression_loss: 4.9192 - val_loss: 7.3198 - val_regression_loss: 4.3397 - lr: 1.5625e-06\n",
      "Epoch 87/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.8774 - regression_loss: 3.5193\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3610 - regression_loss: 4.9159 - val_loss: 7.3160 - val_regression_loss: 4.3368 - lr: 1.5625e-06\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7055 - regression_loss: 4.9139 - val_loss: 7.3139 - val_regression_loss: 4.3354 - lr: 7.8125e-07\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7311 - regression_loss: 4.9131 - val_loss: 7.3122 - val_regression_loss: 4.3342 - lr: 7.8125e-07\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5532 - regression_loss: 4.9119 - val_loss: 7.3112 - val_regression_loss: 4.3334 - lr: 7.8125e-07\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7930 - regression_loss: 4.9111 - val_loss: 7.3102 - val_regression_loss: 4.3326 - lr: 7.8125e-07\n",
      "Epoch 92/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.6009 - regression_loss: 12.2428\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7009 - regression_loss: 4.9114 - val_loss: 7.3075 - val_regression_loss: 4.3308 - lr: 7.8125e-07\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6972 - regression_loss: 4.9091 - val_loss: 7.3071 - val_regression_loss: 4.3304 - lr: 3.9062e-07\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7562 - regression_loss: 4.9086 - val_loss: 7.3068 - val_regression_loss: 4.3301 - lr: 3.9062e-07\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7064 - regression_loss: 4.9083 - val_loss: 7.3065 - val_regression_loss: 4.3299 - lr: 3.9062e-07\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5382 - regression_loss: 4.9075 - val_loss: 7.3062 - val_regression_loss: 4.3296 - lr: 3.9062e-07\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7920 - regression_loss: 4.9068 - val_loss: 7.3054 - val_regression_loss: 4.3290 - lr: 3.9062e-07\n",
      "Epoch 98/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7956 - regression_loss: 4.9063 - val_loss: 7.3047 - val_regression_loss: 4.3284 - lr: 3.9062e-07\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6424 - regression_loss: 4.9059 - val_loss: 7.3045 - val_regression_loss: 4.3282 - lr: 3.9062e-07\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7145 - regression_loss: 4.9056 - val_loss: 7.3030 - val_regression_loss: 4.3272 - lr: 3.9062e-07\n",
      "Epoch 101/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.0662 - regression_loss: 5.7081\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6999 - regression_loss: 4.9048 - val_loss: 7.3019 - val_regression_loss: 4.3266 - lr: 3.9062e-07\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6510 - regression_loss: 4.9042 - val_loss: 7.3018 - val_regression_loss: 4.3264 - lr: 1.9531e-07\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6728 - regression_loss: 4.9040 - val_loss: 7.3017 - val_regression_loss: 4.3263 - lr: 1.9531e-07\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6681 - regression_loss: 4.9038 - val_loss: 7.3016 - val_regression_loss: 4.3262 - lr: 1.9531e-07\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7579 - regression_loss: 4.9032 - val_loss: 7.3011 - val_regression_loss: 4.3258 - lr: 1.9531e-07\n",
      "Epoch 106/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.7464 - regression_loss: 4.3883\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7318 - regression_loss: 4.9031 - val_loss: 7.3004 - val_regression_loss: 4.3254 - lr: 1.9531e-07\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7495 - regression_loss: 4.9028 - val_loss: 7.3001 - val_regression_loss: 4.3252 - lr: 9.7656e-08\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7635 - regression_loss: 4.9027 - val_loss: 7.3002 - val_regression_loss: 4.3252 - lr: 9.7656e-08\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7748 - regression_loss: 4.9025 - val_loss: 7.3000 - val_regression_loss: 4.3251 - lr: 9.7656e-08\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4623 - regression_loss: 4.9023 - val_loss: 7.2997 - val_regression_loss: 4.3249 - lr: 9.7656e-08\n",
      "Epoch 111/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.9167 - regression_loss: 5.5586\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7573 - regression_loss: 4.9023 - val_loss: 7.2997 - val_regression_loss: 4.3248 - lr: 9.7656e-08\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6918 - regression_loss: 4.9021 - val_loss: 7.2995 - val_regression_loss: 4.3247 - lr: 4.8828e-08\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6883 - regression_loss: 4.9019 - val_loss: 7.2995 - val_regression_loss: 4.3246 - lr: 4.8828e-08\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4033 - regression_loss: 4.9019 - val_loss: 7.2994 - val_regression_loss: 4.3246 - lr: 4.8828e-08\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.7221 - regression_loss: 4.9018 - val_loss: 7.2994 - val_regression_loss: 4.3246 - lr: 4.8828e-08\n",
      "Epoch 116/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.3441 - regression_loss: 4.9860\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.7366 - regression_loss: 4.9017 - val_loss: 7.2992 - val_regression_loss: 4.3245 - lr: 4.8828e-08\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7021 - regression_loss: 4.9017 - val_loss: 7.2992 - val_regression_loss: 4.3245 - lr: 2.4414e-08\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7310 - regression_loss: 4.9016 - val_loss: 7.2992 - val_regression_loss: 4.3244 - lr: 2.4414e-08\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6718 - regression_loss: 4.9016 - val_loss: 7.2991 - val_regression_loss: 4.3244 - lr: 2.4414e-08\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6875 - regression_loss: 4.9016 - val_loss: 7.2991 - val_regression_loss: 4.3243 - lr: 2.4414e-08\n",
      "Epoch 121/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.3677 - regression_loss: 4.0096\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7329 - regression_loss: 4.9016 - val_loss: 7.2991 - val_regression_loss: 4.3243 - lr: 2.4414e-08\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6197 - regression_loss: 4.9015 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 1.2207e-08\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7439 - regression_loss: 4.9015 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 1.2207e-08\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6628 - regression_loss: 4.9015 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 1.2207e-08\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7330 - regression_loss: 4.9015 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 1.2207e-08\n",
      "Epoch 126/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.2604 - regression_loss: 2.9023\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7191 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 1.2207e-08\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.7267 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 6.1035e-09\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5636 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 6.1035e-09\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7510 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 6.1035e-09\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4261 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 6.1035e-09\n",
      "Epoch 131/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.6685 - regression_loss: 3.3104\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7723 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 6.1035e-09\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7339 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 3.0518e-09\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6859 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 3.0518e-09\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3651 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 3.0518e-09\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6844 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 3.0518e-09\n",
      "Epoch 136/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.6430 - regression_loss: 12.2849\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 1.5258788677030566e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6086 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 3.0518e-09\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6605 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 1.5259e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7503 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 1.5259e-09\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6851 - regression_loss: 4.9014 - val_loss: 7.2989 - val_regression_loss: 4.3243 - lr: 1.5259e-09\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7259 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 1.5259e-09\n",
      "Epoch 141/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.4007 - regression_loss: 3.0426\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 7.629394338515283e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6527 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 1.5259e-09\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7282 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 7.6294e-10\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7138 - regression_loss: 4.9014 - val_loss: 7.2989 - val_regression_loss: 4.3243 - lr: 7.6294e-10\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7493 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 7.6294e-10\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7080 - regression_loss: 4.9014 - val_loss: 7.2989 - val_regression_loss: 4.3243 - lr: 7.6294e-10\n",
      "Epoch 146/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.9266 - regression_loss: 5.5685\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 3.8146971692576415e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2833 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 7.6294e-10\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7527 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 3.8147e-10\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2223 - regression_loss: 4.9014 - val_loss: 7.2989 - val_regression_loss: 4.3243 - lr: 3.8147e-10\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3868 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 3.8147e-10\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6640 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 3.8147e-10\n",
      "Epoch 151/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.0337 - regression_loss: 5.6756\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 1.9073485846288207e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7909 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 3.8147e-10\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7966 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 1.9073e-10\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7121 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 1.9073e-10\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7127 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 1.9073e-10\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6297 - regression_loss: 4.9014 - val_loss: 7.2989 - val_regression_loss: 4.3243 - lr: 1.9073e-10\n",
      "Epoch 156/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.4975 - regression_loss: 14.1394\n",
      "Epoch 156: ReduceLROnPlateau reducing learning rate to 9.536742923144104e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6701 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 1.9073e-10\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7282 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 9.5367e-11\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6004 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 9.5367e-11\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6408 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 9.5367e-11\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6250 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 9.5367e-11\n",
      "Epoch 161/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.7294 - regression_loss: 11.3713\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 4.768371461572052e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5621 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 9.5367e-11\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7371 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 4.7684e-11\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7482 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 4.7684e-11\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5802 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 4.7684e-11\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7084 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 4.7684e-11\n",
      "Epoch 166/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.5098 - regression_loss: 2.1517\n",
      "Epoch 166: ReduceLROnPlateau reducing learning rate to 2.384185730786026e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4864 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 4.7684e-11\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7447 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 2.3842e-11\n",
      "Epoch 168/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7009 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 2.3842e-11\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7270 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 2.3842e-11\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7680 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 2.3842e-11\n",
      "Epoch 171/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.4164 - regression_loss: 12.0583\n",
      "Epoch 171: ReduceLROnPlateau reducing learning rate to 1.192092865393013e-11.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.2907 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 2.3842e-11\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6718 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 1.1921e-11\n",
      "Epoch 173/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7693 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 1.1921e-11\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6342 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 1.1921e-11\n",
      "Epoch 175/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6978 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 1.1921e-11\n",
      "Epoch 176/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.4627 - regression_loss: 2.1046\n",
      "Epoch 176: ReduceLROnPlateau reducing learning rate to 5.960464326965065e-12.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6954 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 1.1921e-11\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6807 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 5.9605e-12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6592 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 5.9605e-12\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6299 - regression_loss: 4.9014 - val_loss: 7.2990 - val_regression_loss: 4.3243 - lr: 5.9605e-12\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 89.6024 - regression_loss: 81.3015 - val_loss: 61.5000 - val_regression_loss: 52.4561 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 52.8011 - regression_loss: 48.0616 - val_loss: 49.8467 - val_regression_loss: 43.0539 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 44.4192 - regression_loss: 40.0833 - val_loss: 48.0871 - val_regression_loss: 41.7439 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 39.2704 - regression_loss: 35.5068 - val_loss: 42.0353 - val_regression_loss: 36.0195 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 35.1663 - regression_loss: 31.9930 - val_loss: 40.6410 - val_regression_loss: 34.8732 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 33.4586 - regression_loss: 29.7005 - val_loss: 37.9231 - val_regression_loss: 32.3417 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.3310 - regression_loss: 27.3393 - val_loss: 35.7080 - val_regression_loss: 30.3733 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.5256 - regression_loss: 25.3752 - val_loss: 33.0309 - val_regression_loss: 27.8814 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.2339 - regression_loss: 23.9387 - val_loss: 31.8520 - val_regression_loss: 26.9037 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.7052 - regression_loss: 22.7161 - val_loss: 29.7623 - val_regression_loss: 24.7361 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.9590 - regression_loss: 22.0296 - val_loss: 33.7969 - val_regression_loss: 28.7119 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.9995 - regression_loss: 21.2812 - val_loss: 28.2463 - val_regression_loss: 22.6160 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.6204 - regression_loss: 20.8470 - val_loss: 31.0318 - val_regression_loss: 25.9412 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.8721 - regression_loss: 19.2408 - val_loss: 26.7936 - val_regression_loss: 21.4983 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.2530 - regression_loss: 18.6352 - val_loss: 27.2883 - val_regression_loss: 22.2094 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.8807 - regression_loss: 17.6158 - val_loss: 26.3385 - val_regression_loss: 21.0518 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.0785 - regression_loss: 17.4426 - val_loss: 26.1771 - val_regression_loss: 20.9117 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.5899 - regression_loss: 16.9222 - val_loss: 26.1618 - val_regression_loss: 20.9478 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1242 - regression_loss: 16.5049 - val_loss: 25.1151 - val_regression_loss: 19.7187 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.6865 - regression_loss: 16.2828 - val_loss: 25.9864 - val_regression_loss: 20.7012 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.6320 - regression_loss: 16.0723 - val_loss: 24.8464 - val_regression_loss: 19.1389 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.9071 - regression_loss: 16.2525 - val_loss: 25.6175 - val_regression_loss: 20.2567 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.3567 - regression_loss: 15.7037 - val_loss: 24.8026 - val_regression_loss: 19.3585 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.7901 - regression_loss: 15.5004 - val_loss: 24.6688 - val_regression_loss: 19.1595 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.0683 - regression_loss: 15.3990 - val_loss: 24.4194 - val_regression_loss: 18.8090 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6726 - regression_loss: 15.3225 - val_loss: 24.6805 - val_regression_loss: 19.2079 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3176 - regression_loss: 15.2807 - val_loss: 25.5339 - val_regression_loss: 20.1165 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4402 - regression_loss: 15.1888 - val_loss: 24.0580 - val_regression_loss: 18.5493 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2390 - regression_loss: 14.9940 - val_loss: 23.9312 - val_regression_loss: 18.3471 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2798 - regression_loss: 15.0006 - val_loss: 24.7349 - val_regression_loss: 19.2607 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.0726 - regression_loss: 14.8322 - val_loss: 23.8916 - val_regression_loss: 18.2511 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.0646 - regression_loss: 14.6452 - val_loss: 24.8824 - val_regression_loss: 19.4259 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3104 - regression_loss: 14.8016 - val_loss: 24.0274 - val_regression_loss: 18.5381 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.1672 - regression_loss: 14.6446 - val_loss: 23.8232 - val_regression_loss: 18.3639 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7906 - regression_loss: 14.5578 - val_loss: 24.0510 - val_regression_loss: 18.6314 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2861 - regression_loss: 14.8434 - val_loss: 24.1368 - val_regression_loss: 18.0303 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.8046 - regression_loss: 15.2374 - val_loss: 24.9220 - val_regression_loss: 19.4384 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.0002 - regression_loss: 14.4150 - val_loss: 23.5565 - val_regression_loss: 18.0104 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5764 - regression_loss: 14.2971 - val_loss: 24.4105 - val_regression_loss: 18.9626 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5326 - regression_loss: 14.1985 - val_loss: 23.4222 - val_regression_loss: 17.5728 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.4435 - regression_loss: 14.8995 - val_loss: 23.4127 - val_regression_loss: 17.8483 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.6184 - regression_loss: 14.2394 - val_loss: 25.3263 - val_regression_loss: 19.8287 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5156 - regression_loss: 14.2409 - val_loss: 23.3269 - val_regression_loss: 17.5571 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5006 - regression_loss: 14.2384 - val_loss: 25.8880 - val_regression_loss: 20.4537 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8295 - regression_loss: 14.5146 - val_loss: 23.3782 - val_regression_loss: 17.8121 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.2028 - regression_loss: 13.9668 - val_loss: 23.2138 - val_regression_loss: 17.5997 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.4366 - regression_loss: 14.0091 - val_loss: 24.6433 - val_regression_loss: 19.2675 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.1889 - regression_loss: 14.1211 - val_loss: 23.2971 - val_regression_loss: 17.8330 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.0788 - regression_loss: 13.8267 - val_loss: 24.3201 - val_regression_loss: 18.8554 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.4900 - regression_loss: 14.2408 - val_loss: 24.5642 - val_regression_loss: 19.1143 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5848 - regression_loss: 14.2692 - val_loss: 23.2236 - val_regression_loss: 17.4939 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.4056 - regression_loss: 13.8989 - val_loss: 23.8065 - val_regression_loss: 18.3726 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.8770 - regression_loss: 13.6619 - val_loss: 23.6692 - val_regression_loss: 18.1811 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.6371 - regression_loss: 13.6186 - val_loss: 24.1539 - val_regression_loss: 18.6441 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.8906 - regression_loss: 13.5863 - val_loss: 23.2827 - val_regression_loss: 17.7144 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.9127 - regression_loss: 13.5540 - val_loss: 23.6330 - val_regression_loss: 18.0929 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.7384 - regression_loss: 13.4527 - val_loss: 24.0709 - val_regression_loss: 18.5793 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.7446 - regression_loss: 13.5646 - val_loss: 24.4453 - val_regression_loss: 18.9912 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 16.3515 - regression_loss: 14.9978\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.0010 - regression_loss: 13.6416 - val_loss: 23.9141 - val_regression_loss: 18.4117 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.7815 - regression_loss: 13.4325 - val_loss: 23.4658 - val_regression_loss: 17.8620 - lr: 5.0000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.4329 - regression_loss: 13.3818 - val_loss: 23.9596 - val_regression_loss: 18.4368 - lr: 5.0000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.8482 - regression_loss: 13.4499 - val_loss: 23.4183 - val_regression_loss: 17.7113 - lr: 5.0000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.1147 - regression_loss: 13.5506 - val_loss: 24.0945 - val_regression_loss: 18.5653 - lr: 5.0000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.3021 - regression_loss: 13.2978 - val_loss: 23.6517 - val_regression_loss: 18.0763 - lr: 5.0000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.6654 - regression_loss: 13.2089 - val_loss: 23.5917 - val_regression_loss: 18.0238 - lr: 5.0000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5755 - regression_loss: 13.1659 - val_loss: 23.8520 - val_regression_loss: 18.3149 - lr: 5.0000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.4831 - regression_loss: 13.2400 - val_loss: 23.5127 - val_regression_loss: 17.8913 - lr: 5.0000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5563 - regression_loss: 13.1594 - val_loss: 23.9310 - val_regression_loss: 18.3822 - lr: 5.0000e-05\n",
      "Epoch 69/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.3419 - regression_loss: 13.9881\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.3102 - regression_loss: 13.1126 - val_loss: 23.7892 - val_regression_loss: 18.2176 - lr: 5.0000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.4393 - regression_loss: 13.0739 - val_loss: 23.7066 - val_regression_loss: 18.1177 - lr: 2.5000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.1916 - regression_loss: 13.0474 - val_loss: 23.8332 - val_regression_loss: 18.2559 - lr: 2.5000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5831 - regression_loss: 13.0294 - val_loss: 23.8638 - val_regression_loss: 18.2813 - lr: 2.5000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5125 - regression_loss: 13.0460 - val_loss: 23.7881 - val_regression_loss: 18.1965 - lr: 2.5000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.3145 - regression_loss: 13.0129 - val_loss: 23.9326 - val_regression_loss: 18.3586 - lr: 2.5000e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.4655 - regression_loss: 13.0318 - val_loss: 23.7795 - val_regression_loss: 18.1814 - lr: 2.5000e-05\n",
      "Epoch 76/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.6881 - regression_loss: 14.3343\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.4439 - regression_loss: 12.9986 - val_loss: 23.8206 - val_regression_loss: 18.2280 - lr: 2.5000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.1403 - regression_loss: 12.9838 - val_loss: 23.8827 - val_regression_loss: 18.2969 - lr: 1.2500e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.2827 - regression_loss: 12.9774 - val_loss: 23.8702 - val_regression_loss: 18.2797 - lr: 1.2500e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.2827 - regression_loss: 12.9681 - val_loss: 23.9140 - val_regression_loss: 18.3310 - lr: 1.2500e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.1716 - regression_loss: 12.9827 - val_loss: 23.8072 - val_regression_loss: 18.1980 - lr: 1.2500e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.3730 - regression_loss: 12.9682 - val_loss: 23.9431 - val_regression_loss: 18.3523 - lr: 1.2500e-05\n",
      "Epoch 82/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 17.0014 - regression_loss: 15.6476\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.3001 - regression_loss: 12.9685 - val_loss: 23.8506 - val_regression_loss: 18.2326 - lr: 1.2500e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.2334 - regression_loss: 12.9470 - val_loss: 23.8381 - val_regression_loss: 18.2207 - lr: 6.2500e-06\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.2071 - regression_loss: 12.9494 - val_loss: 23.9162 - val_regression_loss: 18.3144 - lr: 6.2500e-06\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.3434 - regression_loss: 12.9480 - val_loss: 23.9376 - val_regression_loss: 18.3388 - lr: 6.2500e-06\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.0430 - regression_loss: 12.9352 - val_loss: 23.8979 - val_regression_loss: 18.2922 - lr: 6.2500e-06\n",
      "3/3 [==============================] - 0s 984us/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 26ms/step - loss: 84.5324 - regression_loss: 77.1065 - val_loss: 50.5564 - val_regression_loss: 42.2846 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 61.6925 - regression_loss: 57.0396 - val_loss: 40.1273 - val_regression_loss: 33.8922 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 50.6798 - regression_loss: 46.1198 - val_loss: 31.6212 - val_regression_loss: 27.0169 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 40.8785 - regression_loss: 37.7139 - val_loss: 26.2403 - val_regression_loss: 22.4662 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 37.0082 - regression_loss: 32.7471 - val_loss: 23.1461 - val_regression_loss: 19.8045 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.5491 - regression_loss: 28.3673 - val_loss: 21.1099 - val_regression_loss: 17.9485 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.9286 - regression_loss: 25.5253 - val_loss: 20.3950 - val_regression_loss: 17.1630 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.2337 - regression_loss: 23.0194 - val_loss: 18.8153 - val_regression_loss: 15.5886 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.7415 - regression_loss: 20.8309 - val_loss: 18.5943 - val_regression_loss: 15.2103 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.3708 - regression_loss: 18.8215 - val_loss: 17.8248 - val_regression_loss: 14.3565 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.8994 - regression_loss: 17.6693 - val_loss: 18.6233 - val_regression_loss: 14.8255 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.7571 - regression_loss: 16.4676 - val_loss: 17.5996 - val_regression_loss: 13.8100 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.8907 - regression_loss: 15.5255 - val_loss: 18.5510 - val_regression_loss: 14.4156 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.0190 - regression_loss: 15.0094 - val_loss: 17.5355 - val_regression_loss: 13.5087 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.0367 - regression_loss: 14.7583 - val_loss: 17.5470 - val_regression_loss: 13.4136 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5653 - regression_loss: 14.1207 - val_loss: 17.7027 - val_regression_loss: 13.4581 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.3085 - regression_loss: 13.7911 - val_loss: 17.1038 - val_regression_loss: 12.9373 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.7781 - regression_loss: 13.5013 - val_loss: 18.2719 - val_regression_loss: 13.7705 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.8429 - regression_loss: 13.7278 - val_loss: 17.8742 - val_regression_loss: 13.4880 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.6563 - regression_loss: 13.3641 - val_loss: 20.0897 - val_regression_loss: 15.1634 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.1826 - regression_loss: 13.7990 - val_loss: 16.8549 - val_regression_loss: 12.6328 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.7315 - regression_loss: 13.4182 - val_loss: 17.8110 - val_regression_loss: 13.3057 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.0163 - regression_loss: 13.5104 - val_loss: 18.1596 - val_regression_loss: 13.5712 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.9888 - regression_loss: 12.7394 - val_loss: 16.9244 - val_regression_loss: 12.6073 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.4385 - regression_loss: 12.4642 - val_loss: 17.1426 - val_regression_loss: 12.7632 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.3265 - regression_loss: 12.3499 - val_loss: 16.5472 - val_regression_loss: 12.3112 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.6071 - regression_loss: 12.2267 - val_loss: 17.3478 - val_regression_loss: 12.9295 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.6010 - regression_loss: 12.4852 - val_loss: 16.6829 - val_regression_loss: 12.4220 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.6787 - regression_loss: 12.2836 - val_loss: 16.4005 - val_regression_loss: 12.1948 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.4404 - regression_loss: 12.1485 - val_loss: 17.6461 - val_regression_loss: 13.1597 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 17.7562 - regression_loss: 16.4071\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.3396 - regression_loss: 12.1442 - val_loss: 16.5694 - val_regression_loss: 12.3267 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1570 - regression_loss: 11.9406 - val_loss: 17.1569 - val_regression_loss: 12.7636 - lr: 5.0000e-05\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1229 - regression_loss: 11.9263 - val_loss: 16.3403 - val_regression_loss: 12.1541 - lr: 5.0000e-05\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.9927 - regression_loss: 11.8153 - val_loss: 16.8714 - val_regression_loss: 12.5482 - lr: 5.0000e-05\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1189 - regression_loss: 11.8342 - val_loss: 16.3557 - val_regression_loss: 12.1603 - lr: 5.0000e-05\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8318 - regression_loss: 11.6527 - val_loss: 16.3334 - val_regression_loss: 12.1461 - lr: 5.0000e-05\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6912 - regression_loss: 11.6711 - val_loss: 16.5161 - val_regression_loss: 12.2777 - lr: 5.0000e-05\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8210 - regression_loss: 11.6510 - val_loss: 16.4488 - val_regression_loss: 12.2232 - lr: 5.0000e-05\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7030 - regression_loss: 11.4727 - val_loss: 16.6840 - val_regression_loss: 12.4054 - lr: 5.0000e-05\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8671 - regression_loss: 11.5344 - val_loss: 16.3178 - val_regression_loss: 12.1279 - lr: 5.0000e-05\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.5282 - regression_loss: 11.4762 - val_loss: 16.3474 - val_regression_loss: 12.1498 - lr: 5.0000e-05\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6349 - regression_loss: 11.4267 - val_loss: 16.3997 - val_regression_loss: 12.1879 - lr: 5.0000e-05\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.3094 - regression_loss: 11.3532 - val_loss: 16.2040 - val_regression_loss: 12.0379 - lr: 5.0000e-05\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.5211 - regression_loss: 11.3933 - val_loss: 16.4815 - val_regression_loss: 12.2482 - lr: 5.0000e-05\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.3185 - regression_loss: 11.3561 - val_loss: 16.3513 - val_regression_loss: 12.1512 - lr: 5.0000e-05\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.4329 - regression_loss: 11.3376 - val_loss: 16.3307 - val_regression_loss: 12.1341 - lr: 5.0000e-05\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.2619 - regression_loss: 11.2469 - val_loss: 16.3629 - val_regression_loss: 12.1653 - lr: 5.0000e-05\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.5291 - regression_loss: 11.2412 - val_loss: 16.2666 - val_regression_loss: 12.0921 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.3237 - regression_loss: 11.2643 - val_loss: 16.4200 - val_regression_loss: 12.2051 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.5206 - regression_loss: 11.1715 - val_loss: 16.2530 - val_regression_loss: 12.0824 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.2906 - regression_loss: 11.1732 - val_loss: 16.0521 - val_regression_loss: 11.9308 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.4055 - regression_loss: 10.0566\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.4600 - regression_loss: 11.0973 - val_loss: 16.3414 - val_regression_loss: 12.1521 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1490 - regression_loss: 11.0700 - val_loss: 16.1930 - val_regression_loss: 12.0389 - lr: 2.5000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.3499 - regression_loss: 11.0185 - val_loss: 16.1371 - val_regression_loss: 11.9914 - lr: 2.5000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.2379 - regression_loss: 11.0806 - val_loss: 16.1072 - val_regression_loss: 11.9722 - lr: 2.5000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1358 - regression_loss: 11.0465 - val_loss: 16.4529 - val_regression_loss: 12.2303 - lr: 2.5000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1432 - regression_loss: 11.0532 - val_loss: 16.0793 - val_regression_loss: 11.9483 - lr: 2.5000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.1559 - regression_loss: 10.9845 - val_loss: 16.1399 - val_regression_loss: 11.9934 - lr: 2.5000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1224 - regression_loss: 11.0109 - val_loss: 16.1427 - val_regression_loss: 11.9948 - lr: 2.5000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1590 - regression_loss: 10.9494 - val_loss: 16.1656 - val_regression_loss: 12.0121 - lr: 2.5000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9922 - regression_loss: 10.9957 - val_loss: 15.9741 - val_regression_loss: 11.8710 - lr: 2.5000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1797 - regression_loss: 10.9760 - val_loss: 16.2455 - val_regression_loss: 12.0705 - lr: 2.5000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9790 - regression_loss: 10.9300 - val_loss: 16.0617 - val_regression_loss: 11.9327 - lr: 2.5000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0387 - regression_loss: 10.9164 - val_loss: 15.9949 - val_regression_loss: 11.8813 - lr: 2.5000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7847 - regression_loss: 10.8757 - val_loss: 16.2454 - val_regression_loss: 12.0748 - lr: 2.5000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9243 - regression_loss: 10.9101 - val_loss: 15.9962 - val_regression_loss: 11.8887 - lr: 2.5000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0500 - regression_loss: 10.8637 - val_loss: 16.0844 - val_regression_loss: 11.9520 - lr: 2.5000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9049 - regression_loss: 10.8428 - val_loss: 16.0705 - val_regression_loss: 11.9387 - lr: 2.5000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0918 - regression_loss: 10.8552 - val_loss: 16.0694 - val_regression_loss: 11.9387 - lr: 2.5000e-05\n",
      "Epoch 70/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.2552 - regression_loss: 9.9062\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0168 - regression_loss: 10.9193 - val_loss: 15.9763 - val_regression_loss: 11.8730 - lr: 2.5000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9153 - regression_loss: 10.7882 - val_loss: 16.1517 - val_regression_loss: 12.0041 - lr: 1.2500e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9605 - regression_loss: 10.7861 - val_loss: 16.0676 - val_regression_loss: 11.9393 - lr: 1.2500e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9130 - regression_loss: 10.7664 - val_loss: 15.9822 - val_regression_loss: 11.8755 - lr: 1.2500e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9525 - regression_loss: 10.7618 - val_loss: 15.9459 - val_regression_loss: 11.8476 - lr: 1.2500e-05\n",
      "Epoch 75/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.1123 - regression_loss: 13.7634\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9497 - regression_loss: 10.7956 - val_loss: 15.9578 - val_regression_loss: 11.8575 - lr: 1.2500e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9706 - regression_loss: 10.7426 - val_loss: 15.9973 - val_regression_loss: 11.8869 - lr: 6.2500e-06\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8640 - regression_loss: 10.7379 - val_loss: 16.0620 - val_regression_loss: 11.9361 - lr: 6.2500e-06\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9217 - regression_loss: 10.7415 - val_loss: 16.0639 - val_regression_loss: 11.9373 - lr: 6.2500e-06\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6753 - regression_loss: 10.7334 - val_loss: 16.0287 - val_regression_loss: 11.9109 - lr: 6.2500e-06\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7253 - regression_loss: 10.7335 - val_loss: 15.9666 - val_regression_loss: 11.8649 - lr: 6.2500e-06\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8577 - regression_loss: 10.7254 - val_loss: 15.9901 - val_regression_loss: 11.8810 - lr: 6.2500e-06\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8289 - regression_loss: 10.7357 - val_loss: 16.0406 - val_regression_loss: 11.9185 - lr: 6.2500e-06\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6643 - regression_loss: 10.7198 - val_loss: 15.9895 - val_regression_loss: 11.8797 - lr: 6.2500e-06\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8928 - regression_loss: 10.7360 - val_loss: 15.9350 - val_regression_loss: 11.8396 - lr: 6.2500e-06\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7806 - regression_loss: 10.7158 - val_loss: 15.9854 - val_regression_loss: 11.8770 - lr: 6.2500e-06\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5731 - regression_loss: 10.7179 - val_loss: 16.0189 - val_regression_loss: 11.9029 - lr: 6.2500e-06\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7457 - regression_loss: 10.7083 - val_loss: 16.0095 - val_regression_loss: 11.8960 - lr: 6.2500e-06\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6359 - regression_loss: 10.7045 - val_loss: 15.9806 - val_regression_loss: 11.8737 - lr: 6.2500e-06\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8887 - regression_loss: 10.7274 - val_loss: 15.9260 - val_regression_loss: 11.8337 - lr: 6.2500e-06\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4811 - regression_loss: 10.6941 - val_loss: 15.9870 - val_regression_loss: 11.8786 - lr: 6.2500e-06\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7601 - regression_loss: 10.7034 - val_loss: 16.0579 - val_regression_loss: 11.9312 - lr: 6.2500e-06\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9742 - regression_loss: 10.7008 - val_loss: 15.9991 - val_regression_loss: 11.8872 - lr: 6.2500e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9185 - regression_loss: 10.6856 - val_loss: 15.9969 - val_regression_loss: 11.8854 - lr: 6.2500e-06\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5821 - regression_loss: 10.6886 - val_loss: 15.9640 - val_regression_loss: 11.8606 - lr: 6.2500e-06\n",
      "Epoch 95/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 14.8064 - regression_loss: 13.4575\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8167 - regression_loss: 10.6788 - val_loss: 15.9522 - val_regression_loss: 11.8525 - lr: 6.2500e-06\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6270 - regression_loss: 10.6712 - val_loss: 15.9714 - val_regression_loss: 11.8669 - lr: 3.1250e-06\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8506 - regression_loss: 10.6811 - val_loss: 15.9505 - val_regression_loss: 11.8513 - lr: 3.1250e-06\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7145 - regression_loss: 10.6786 - val_loss: 15.9951 - val_regression_loss: 11.8846 - lr: 3.1250e-06\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9184 - regression_loss: 10.6694 - val_loss: 15.9779 - val_regression_loss: 11.8720 - lr: 3.1250e-06\n",
      "Epoch 100/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.5612 - regression_loss: 9.2123\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7489 - regression_loss: 10.6679 - val_loss: 15.9542 - val_regression_loss: 11.8541 - lr: 3.1250e-06\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7074 - regression_loss: 10.6641 - val_loss: 15.9526 - val_regression_loss: 11.8529 - lr: 1.5625e-06\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8510 - regression_loss: 10.6620 - val_loss: 15.9516 - val_regression_loss: 11.8521 - lr: 1.5625e-06\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9528 - regression_loss: 10.6586 - val_loss: 15.9579 - val_regression_loss: 11.8566 - lr: 1.5625e-06\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7357 - regression_loss: 10.6581 - val_loss: 15.9584 - val_regression_loss: 11.8571 - lr: 1.5625e-06\n",
      "Epoch 105/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.7647 - regression_loss: 12.4158\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6309 - regression_loss: 10.6592 - val_loss: 15.9558 - val_regression_loss: 11.8554 - lr: 1.5625e-06\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5785 - regression_loss: 10.6560 - val_loss: 15.9575 - val_regression_loss: 11.8567 - lr: 7.8125e-07\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.7622 - regression_loss: 10.6560 - val_loss: 15.9574 - val_regression_loss: 11.8564 - lr: 7.8125e-07\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6922 - regression_loss: 10.6565 - val_loss: 15.9567 - val_regression_loss: 11.8559 - lr: 7.8125e-07\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7367 - regression_loss: 10.6549 - val_loss: 15.9590 - val_regression_loss: 11.8576 - lr: 7.8125e-07\n",
      "Epoch 110/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.4505 - regression_loss: 14.1017\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8585 - regression_loss: 10.6547 - val_loss: 15.9569 - val_regression_loss: 11.8561 - lr: 7.8125e-07\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7855 - regression_loss: 10.6533 - val_loss: 15.9564 - val_regression_loss: 11.8557 - lr: 3.9062e-07\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7854 - regression_loss: 10.6532 - val_loss: 15.9559 - val_regression_loss: 11.8553 - lr: 3.9062e-07\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8950 - regression_loss: 10.6527 - val_loss: 15.9533 - val_regression_loss: 11.8534 - lr: 3.9062e-07\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6079 - regression_loss: 10.6526 - val_loss: 15.9526 - val_regression_loss: 11.8529 - lr: 3.9062e-07\n",
      "Epoch 115/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 14.8518 - regression_loss: 13.5029\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8651 - regression_loss: 10.6528 - val_loss: 15.9537 - val_regression_loss: 11.8537 - lr: 3.9062e-07\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8194 - regression_loss: 10.6520 - val_loss: 15.9526 - val_regression_loss: 11.8528 - lr: 1.9531e-07\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8317 - regression_loss: 10.6519 - val_loss: 15.9525 - val_regression_loss: 11.8528 - lr: 1.9531e-07\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7245 - regression_loss: 10.6519 - val_loss: 15.9529 - val_regression_loss: 11.8531 - lr: 1.9531e-07\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8982 - regression_loss: 10.6516 - val_loss: 15.9523 - val_regression_loss: 11.8526 - lr: 1.9531e-07\n",
      "Epoch 120/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.6599 - regression_loss: 11.3110\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9659 - regression_loss: 10.6516 - val_loss: 15.9525 - val_regression_loss: 11.8528 - lr: 1.9531e-07\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8501 - regression_loss: 10.6516 - val_loss: 15.9523 - val_regression_loss: 11.8527 - lr: 9.7656e-08\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7355 - regression_loss: 10.6513 - val_loss: 15.9523 - val_regression_loss: 11.8526 - lr: 9.7656e-08\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6364 - regression_loss: 10.6514 - val_loss: 15.9528 - val_regression_loss: 11.8530 - lr: 9.7656e-08\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7385 - regression_loss: 10.6513 - val_loss: 15.9524 - val_regression_loss: 11.8527 - lr: 9.7656e-08\n",
      "Epoch 125/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.9985 - regression_loss: 9.6496\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8295 - regression_loss: 10.6514 - val_loss: 15.9520 - val_regression_loss: 11.8524 - lr: 9.7656e-08\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7587 - regression_loss: 10.6511 - val_loss: 15.9526 - val_regression_loss: 11.8528 - lr: 4.8828e-08\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5345 - regression_loss: 10.6511 - val_loss: 15.9527 - val_regression_loss: 11.8529 - lr: 4.8828e-08\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6660 - regression_loss: 10.6510 - val_loss: 15.9525 - val_regression_loss: 11.8527 - lr: 4.8828e-08\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7903 - regression_loss: 10.6509 - val_loss: 15.9525 - val_regression_loss: 11.8528 - lr: 4.8828e-08\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 24ms/step - loss: 123.7872 - regression_loss: 111.7922 - val_loss: 67.0226 - val_regression_loss: 51.7262 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 82.4919 - regression_loss: 73.8060 - val_loss: 46.4222 - val_regression_loss: 35.6148 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 60.8000 - regression_loss: 55.7463 - val_loss: 41.4559 - val_regression_loss: 32.1186 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 48.7882 - regression_loss: 44.5314 - val_loss: 34.0525 - val_regression_loss: 26.3830 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 40.6638 - regression_loss: 37.4353 - val_loss: 34.5889 - val_regression_loss: 26.9745 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 37.0577 - regression_loss: 33.1784 - val_loss: 28.5259 - val_regression_loss: 22.0804 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.5550 - regression_loss: 29.4692 - val_loss: 28.5435 - val_regression_loss: 22.1206 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.3305 - regression_loss: 26.9464 - val_loss: 24.4935 - val_regression_loss: 18.6992 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.8881 - regression_loss: 24.8604 - val_loss: 25.3655 - val_regression_loss: 19.4415 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.4168 - regression_loss: 22.3412 - val_loss: 21.3690 - val_regression_loss: 16.0764 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.5781 - regression_loss: 20.7094 - val_loss: 21.6850 - val_regression_loss: 16.3957 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.8098 - regression_loss: 19.1030 - val_loss: 19.4782 - val_regression_loss: 14.5222 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8645 - regression_loss: 17.9540 - val_loss: 19.4420 - val_regression_loss: 14.5483 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.2495 - regression_loss: 16.8530 - val_loss: 18.5998 - val_regression_loss: 13.8785 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.6724 - regression_loss: 16.1223 - val_loss: 18.2934 - val_regression_loss: 13.6739 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.0184 - regression_loss: 15.6975 - val_loss: 17.4416 - val_regression_loss: 12.9968 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3850 - regression_loss: 15.0709 - val_loss: 17.1093 - val_regression_loss: 12.7482 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8175 - regression_loss: 14.5510 - val_loss: 17.0612 - val_regression_loss: 12.7573 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.2265 - regression_loss: 14.1510 - val_loss: 17.0028 - val_regression_loss: 12.7277 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.3364 - regression_loss: 13.8528 - val_loss: 16.2269 - val_regression_loss: 12.1160 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.1373 - regression_loss: 13.6714 - val_loss: 16.7645 - val_regression_loss: 12.6164 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.4778 - regression_loss: 13.6201 - val_loss: 15.6130 - val_regression_loss: 11.6073 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5466 - regression_loss: 13.1514 - val_loss: 16.5124 - val_regression_loss: 12.3950 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5124 - regression_loss: 13.2815 - val_loss: 15.8017 - val_regression_loss: 11.7379 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.4971 - regression_loss: 13.2574 - val_loss: 17.3982 - val_regression_loss: 13.2367 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.2994 - regression_loss: 13.1682 - val_loss: 15.2120 - val_regression_loss: 11.2349 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.2868 - regression_loss: 13.0634 - val_loss: 16.0634 - val_regression_loss: 12.0522 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.4856 - regression_loss: 12.5996 - val_loss: 15.0669 - val_regression_loss: 11.1885 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.6205 - regression_loss: 12.4170 - val_loss: 15.2358 - val_regression_loss: 11.3689 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.2275 - regression_loss: 12.2916 - val_loss: 14.6974 - val_regression_loss: 10.9285 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.0333 - regression_loss: 12.1240 - val_loss: 15.1602 - val_regression_loss: 11.2946 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.2411 - regression_loss: 12.0525 - val_loss: 15.0254 - val_regression_loss: 11.2100 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.0548 - regression_loss: 11.9518 - val_loss: 14.6178 - val_regression_loss: 10.8657 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.9502 - regression_loss: 11.9216 - val_loss: 14.7323 - val_regression_loss: 10.8955 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.0393 - regression_loss: 11.7866 - val_loss: 14.7394 - val_regression_loss: 10.9342 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8793 - regression_loss: 11.6367 - val_loss: 14.4449 - val_regression_loss: 10.6804 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.4487 - regression_loss: 11.6366 - val_loss: 14.6152 - val_regression_loss: 10.8317 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7588 - regression_loss: 11.6727 - val_loss: 14.9241 - val_regression_loss: 11.1110 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.5407 - regression_loss: 11.5171 - val_loss: 14.0225 - val_regression_loss: 10.3231 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.9285 - regression_loss: 11.5766 - val_loss: 14.4785 - val_regression_loss: 10.6938 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.5117 - regression_loss: 11.2458 - val_loss: 14.0620 - val_regression_loss: 10.3052 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.3286 - regression_loss: 11.3238 - val_loss: 14.9716 - val_regression_loss: 11.1135 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.4643 - regression_loss: 11.2136 - val_loss: 14.3148 - val_regression_loss: 10.5081 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6469 - regression_loss: 11.3837 - val_loss: 15.3216 - val_regression_loss: 11.3553 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.4470 - regression_loss: 11.1960 - val_loss: 13.9417 - val_regression_loss: 10.2015 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1068 - regression_loss: 11.0457 - val_loss: 15.6778 - val_regression_loss: 11.6476 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7626 - regression_loss: 10.8855 - val_loss: 13.8337 - val_regression_loss: 10.0599 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.1179 - regression_loss: 11.2617 - val_loss: 15.0451 - val_regression_loss: 11.0998 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7792 - regression_loss: 10.6640 - val_loss: 13.9593 - val_regression_loss: 10.1690 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1504 - regression_loss: 11.2072 - val_loss: 15.4191 - val_regression_loss: 11.4642 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.2825 - regression_loss: 11.3387 - val_loss: 14.0137 - val_regression_loss: 10.2076 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7421 - regression_loss: 10.6857 - val_loss: 14.2089 - val_regression_loss: 10.3477 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5872 - regression_loss: 10.6273 - val_loss: 13.9673 - val_regression_loss: 10.2031 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4127 - regression_loss: 10.5152 - val_loss: 13.8417 - val_regression_loss: 10.0919 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6259 - regression_loss: 10.4357 - val_loss: 13.7221 - val_regression_loss: 9.9476 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4215 - regression_loss: 10.3903 - val_loss: 15.0159 - val_regression_loss: 10.9914 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2945 - regression_loss: 10.3405 - val_loss: 13.5742 - val_regression_loss: 9.8214 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6500 - regression_loss: 10.4912 - val_loss: 14.1961 - val_regression_loss: 10.3114 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4543 - regression_loss: 10.2842 - val_loss: 13.5174 - val_regression_loss: 9.7828 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0073 - regression_loss: 10.1721 - val_loss: 14.3171 - val_regression_loss: 10.4134 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0631 - regression_loss: 10.1420 - val_loss: 13.8020 - val_regression_loss: 9.9671 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3370 - regression_loss: 10.1364 - val_loss: 13.4818 - val_regression_loss: 9.7290 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2157 - regression_loss: 10.0153 - val_loss: 13.9359 - val_regression_loss: 10.0788 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0161 - regression_loss: 9.9117 - val_loss: 13.6242 - val_regression_loss: 9.8005 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.6965 - regression_loss: 10.3534\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0842 - regression_loss: 9.9751 - val_loss: 13.5621 - val_regression_loss: 9.7708 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9428 - regression_loss: 9.8870 - val_loss: 13.8552 - val_regression_loss: 10.0125 - lr: 5.0000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8767 - regression_loss: 9.8368 - val_loss: 13.7822 - val_regression_loss: 9.9471 - lr: 5.0000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9279 - regression_loss: 9.8394 - val_loss: 13.5557 - val_regression_loss: 9.7685 - lr: 5.0000e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7714 - regression_loss: 9.8502 - val_loss: 13.8118 - val_regression_loss: 9.9450 - lr: 5.0000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7632 - regression_loss: 9.7755 - val_loss: 13.6915 - val_regression_loss: 9.8568 - lr: 5.0000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7044 - regression_loss: 9.7688 - val_loss: 13.4422 - val_regression_loss: 9.6716 - lr: 5.0000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8697 - regression_loss: 9.7597 - val_loss: 13.7902 - val_regression_loss: 9.9359 - lr: 5.0000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8128 - regression_loss: 9.7192 - val_loss: 13.7122 - val_regression_loss: 9.8837 - lr: 5.0000e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4597 - regression_loss: 9.6903 - val_loss: 13.6422 - val_regression_loss: 9.8188 - lr: 5.0000e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5615 - regression_loss: 9.6639 - val_loss: 13.7507 - val_regression_loss: 9.9052 - lr: 5.0000e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7604 - regression_loss: 9.6877 - val_loss: 13.5403 - val_regression_loss: 9.7386 - lr: 5.0000e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6229 - regression_loss: 9.6680 - val_loss: 13.6392 - val_regression_loss: 9.8028 - lr: 5.0000e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4351 - regression_loss: 9.6342 - val_loss: 13.6660 - val_regression_loss: 9.8128 - lr: 5.0000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6150 - regression_loss: 9.5970 - val_loss: 13.9029 - val_regression_loss: 10.0142 - lr: 5.0000e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6684 - regression_loss: 9.6281 - val_loss: 13.5384 - val_regression_loss: 9.7330 - lr: 5.0000e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5476 - regression_loss: 9.6740 - val_loss: 13.6129 - val_regression_loss: 9.7719 - lr: 5.0000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7275 - regression_loss: 9.5739 - val_loss: 13.7208 - val_regression_loss: 9.8609 - lr: 5.0000e-05\n",
      "Epoch 83/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.0782 - regression_loss: 10.7351\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6027 - regression_loss: 9.6176 - val_loss: 13.4745 - val_regression_loss: 9.6678 - lr: 5.0000e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6228 - regression_loss: 9.5439 - val_loss: 13.8327 - val_regression_loss: 9.9580 - lr: 2.5000e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6230 - regression_loss: 9.5428 - val_loss: 13.6473 - val_regression_loss: 9.8119 - lr: 2.5000e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4422 - regression_loss: 9.5237 - val_loss: 13.7514 - val_regression_loss: 9.8908 - lr: 2.5000e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4564 - regression_loss: 9.4950 - val_loss: 13.6926 - val_regression_loss: 9.8377 - lr: 2.5000e-05\n",
      "Epoch 88/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.6287 - regression_loss: 10.2857\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4647 - regression_loss: 9.5135 - val_loss: 13.6185 - val_regression_loss: 9.7827 - lr: 2.5000e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5827 - regression_loss: 9.4903 - val_loss: 13.4968 - val_regression_loss: 9.6836 - lr: 1.2500e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4830 - regression_loss: 9.4702 - val_loss: 13.5638 - val_regression_loss: 9.7358 - lr: 1.2500e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3706 - regression_loss: 9.4680 - val_loss: 13.6885 - val_regression_loss: 9.8350 - lr: 1.2500e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5296 - regression_loss: 9.4668 - val_loss: 13.6694 - val_regression_loss: 9.8123 - lr: 1.2500e-05\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4303 - regression_loss: 9.4568 - val_loss: 13.6368 - val_regression_loss: 9.7865 - lr: 1.2500e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1771 - regression_loss: 9.4584 - val_loss: 13.5628 - val_regression_loss: 9.7301 - lr: 1.2500e-05\n",
      "Epoch 95/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4586 - regression_loss: 9.4604 - val_loss: 13.6339 - val_regression_loss: 9.7879 - lr: 1.2500e-05\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4402 - regression_loss: 9.4508 - val_loss: 13.6873 - val_regression_loss: 9.8327 - lr: 1.2500e-05\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5335 - regression_loss: 9.4463 - val_loss: 13.5901 - val_regression_loss: 9.7529 - lr: 1.2500e-05\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4150 - regression_loss: 9.4416 - val_loss: 13.5440 - val_regression_loss: 9.7173 - lr: 1.2500e-05\n",
      "Epoch 99/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.8152 - regression_loss: 10.4722\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2824 - regression_loss: 9.4437 - val_loss: 13.5966 - val_regression_loss: 9.7601 - lr: 1.2500e-05\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6497 - regression_loss: 9.4264 - val_loss: 13.6088 - val_regression_loss: 9.7688 - lr: 6.2500e-06\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4316 - regression_loss: 9.4281 - val_loss: 13.6364 - val_regression_loss: 9.7899 - lr: 6.2500e-06\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4297 - regression_loss: 9.4251 - val_loss: 13.6032 - val_regression_loss: 9.7629 - lr: 6.2500e-06\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0956 - regression_loss: 9.4225 - val_loss: 13.6242 - val_regression_loss: 9.7786 - lr: 6.2500e-06\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3476 - regression_loss: 9.4216 - val_loss: 13.6237 - val_regression_loss: 9.7796 - lr: 6.2500e-06\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4117 - regression_loss: 9.4293 - val_loss: 13.5556 - val_regression_loss: 9.7243 - lr: 6.2500e-06\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.5500 - regression_loss: 9.4197 - val_loss: 13.5685 - val_regression_loss: 9.7344 - lr: 6.2500e-06\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4151 - regression_loss: 9.4340 - val_loss: 13.6987 - val_regression_loss: 9.8385 - lr: 6.2500e-06\n",
      "Epoch 108/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.5627 - regression_loss: 10.2197\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3319 - regression_loss: 9.4252 - val_loss: 13.6997 - val_regression_loss: 9.8393 - lr: 6.2500e-06\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3529 - regression_loss: 9.4092 - val_loss: 13.6603 - val_regression_loss: 9.8075 - lr: 3.1250e-06\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3952 - regression_loss: 9.4099 - val_loss: 13.5929 - val_regression_loss: 9.7535 - lr: 3.1250e-06\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4854 - regression_loss: 9.4114 - val_loss: 13.5686 - val_regression_loss: 9.7347 - lr: 3.1250e-06\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 2s 26ms/step - loss: 138.3190 - regression_loss: 126.5311 - val_loss: 89.4189 - val_regression_loss: 77.4637 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 85.4470 - regression_loss: 77.8409 - val_loss: 60.1457 - val_regression_loss: 51.8976 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 55.9586 - regression_loss: 50.5855 - val_loss: 45.6035 - val_regression_loss: 39.1005 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 41.4321 - regression_loss: 36.6333 - val_loss: 35.2603 - val_regression_loss: 29.8184 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 33.6966 - regression_loss: 29.7919 - val_loss: 30.5046 - val_regression_loss: 25.4455 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.7890 - regression_loss: 26.5588 - val_loss: 27.7198 - val_regression_loss: 23.1499 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.0164 - regression_loss: 24.8237 - val_loss: 25.9310 - val_regression_loss: 21.0831 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.5063 - regression_loss: 23.4851 - val_loss: 23.8803 - val_regression_loss: 19.6717 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.8888 - regression_loss: 21.9191 - val_loss: 22.8903 - val_regression_loss: 18.3614 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.8119 - regression_loss: 20.9851 - val_loss: 21.3967 - val_regression_loss: 17.3826 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.6492 - regression_loss: 19.8078 - val_loss: 20.7920 - val_regression_loss: 16.6002 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.9707 - regression_loss: 19.2286 - val_loss: 19.7254 - val_regression_loss: 15.8727 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.5992 - regression_loss: 18.0741 - val_loss: 19.0363 - val_regression_loss: 15.2419 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.9484 - regression_loss: 17.3494 - val_loss: 18.6216 - val_regression_loss: 14.9081 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.5452 - regression_loss: 16.8574 - val_loss: 17.9405 - val_regression_loss: 14.3443 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.2417 - regression_loss: 16.6053 - val_loss: 17.4431 - val_regression_loss: 13.7883 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.9051 - regression_loss: 16.1734 - val_loss: 17.0155 - val_regression_loss: 13.3992 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5224 - regression_loss: 15.2288 - val_loss: 16.6773 - val_regression_loss: 13.0336 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.1786 - regression_loss: 14.8606 - val_loss: 16.3261 - val_regression_loss: 12.8091 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.6630 - regression_loss: 14.4738 - val_loss: 15.9871 - val_regression_loss: 12.3764 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.5762 - regression_loss: 14.0854 - val_loss: 15.7116 - val_regression_loss: 12.2448 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.2975 - regression_loss: 13.8796 - val_loss: 15.4485 - val_regression_loss: 11.8375 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.7006 - regression_loss: 13.4461 - val_loss: 15.2331 - val_regression_loss: 11.7345 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.9372 - regression_loss: 13.3895 - val_loss: 15.7569 - val_regression_loss: 11.8677 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.1150 - regression_loss: 13.6213 - val_loss: 15.3007 - val_regression_loss: 11.7966 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.4873 - regression_loss: 13.3916 - val_loss: 15.6882 - val_regression_loss: 11.7373 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.3674 - regression_loss: 13.1397 - val_loss: 14.8068 - val_regression_loss: 11.2799 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.5357 - regression_loss: 12.4639 - val_loss: 14.7663 - val_regression_loss: 10.9978 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.8084 - regression_loss: 12.4962 - val_loss: 14.4564 - val_regression_loss: 10.9215 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.5407 - regression_loss: 12.2638 - val_loss: 14.2963 - val_regression_loss: 10.6339 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1766 - regression_loss: 11.9495 - val_loss: 14.2221 - val_regression_loss: 10.5361 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1021 - regression_loss: 11.8036 - val_loss: 14.0010 - val_regression_loss: 10.3490 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6385 - regression_loss: 11.6923 - val_loss: 14.0914 - val_regression_loss: 10.3874 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7944 - regression_loss: 11.5468 - val_loss: 13.9961 - val_regression_loss: 10.2727 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7284 - regression_loss: 11.4143 - val_loss: 13.8495 - val_regression_loss: 10.1966 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.7854 - regression_loss: 11.5068 - val_loss: 13.7393 - val_regression_loss: 10.0924 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.3734 - regression_loss: 11.2467 - val_loss: 13.8091 - val_regression_loss: 10.0556 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.9713 - regression_loss: 11.1200 - val_loss: 13.7656 - val_regression_loss: 10.0673 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.3574 - regression_loss: 11.1279 - val_loss: 13.7204 - val_regression_loss: 9.9437 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8940 - regression_loss: 10.9167 - val_loss: 13.8394 - val_regression_loss: 10.1125 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9251 - regression_loss: 10.8648 - val_loss: 13.7294 - val_regression_loss: 9.8959 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0538 - regression_loss: 10.9568 - val_loss: 13.9478 - val_regression_loss: 10.2129 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7159 - regression_loss: 11.3653 - val_loss: 14.6942 - val_regression_loss: 10.5524 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.2004 - regression_loss: 11.0249 - val_loss: 14.7966 - val_regression_loss: 10.9044 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.5707 - regression_loss: 14.2308\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7599 - regression_loss: 11.5262 - val_loss: 15.1129 - val_regression_loss: 10.8174 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.2751 - regression_loss: 11.1823 - val_loss: 13.9036 - val_regression_loss: 10.1196 - lr: 5.0000e-05\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1469 - regression_loss: 10.8924 - val_loss: 13.6882 - val_regression_loss: 9.7896 - lr: 5.0000e-05\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6102 - regression_loss: 10.5789 - val_loss: 13.5342 - val_regression_loss: 9.7058 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5188 - regression_loss: 10.3476 - val_loss: 13.4807 - val_regression_loss: 9.6978 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1228 - regression_loss: 10.2759 - val_loss: 13.4316 - val_regression_loss: 9.6075 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4482 - regression_loss: 10.4122 - val_loss: 13.3842 - val_regression_loss: 9.5770 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2461 - regression_loss: 10.2432 - val_loss: 13.4829 - val_regression_loss: 9.6073 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3124 - regression_loss: 10.2291 - val_loss: 13.3880 - val_regression_loss: 9.5942 - lr: 5.0000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2532 - regression_loss: 10.1802 - val_loss: 13.5006 - val_regression_loss: 9.6178 - lr: 5.0000e-05\n",
      "Epoch 55/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.9897 - regression_loss: 10.6499\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4868 - regression_loss: 10.2654 - val_loss: 13.3470 - val_regression_loss: 9.5176 - lr: 5.0000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2992 - regression_loss: 10.1266 - val_loss: 13.3404 - val_regression_loss: 9.5391 - lr: 2.5000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2811 - regression_loss: 10.0828 - val_loss: 13.3759 - val_regression_loss: 9.5271 - lr: 2.5000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9642 - regression_loss: 10.0409 - val_loss: 13.4143 - val_regression_loss: 9.5530 - lr: 2.5000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9871 - regression_loss: 10.0351 - val_loss: 13.3825 - val_regression_loss: 9.5360 - lr: 2.5000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9209 - regression_loss: 10.0064 - val_loss: 13.3287 - val_regression_loss: 9.5121 - lr: 2.5000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9552 - regression_loss: 10.0284 - val_loss: 13.3570 - val_regression_loss: 9.5112 - lr: 2.5000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8092 - regression_loss: 10.0112 - val_loss: 13.2955 - val_regression_loss: 9.4782 - lr: 2.5000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0993 - regression_loss: 10.0196 - val_loss: 13.3834 - val_regression_loss: 9.5193 - lr: 2.5000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0585 - regression_loss: 9.9594 - val_loss: 13.3018 - val_regression_loss: 9.4714 - lr: 2.5000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8694 - regression_loss: 9.9518 - val_loss: 13.3332 - val_regression_loss: 9.4861 - lr: 2.5000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0539 - regression_loss: 9.9240 - val_loss: 13.3293 - val_regression_loss: 9.4895 - lr: 2.5000e-05\n",
      "Epoch 67/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.1685 - regression_loss: 9.8289\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9840 - regression_loss: 9.9304 - val_loss: 13.3851 - val_regression_loss: 9.5154 - lr: 2.5000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8725 - regression_loss: 9.8945 - val_loss: 13.3544 - val_regression_loss: 9.5014 - lr: 1.2500e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0375 - regression_loss: 9.8946 - val_loss: 13.3391 - val_regression_loss: 9.4946 - lr: 1.2500e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0583 - regression_loss: 9.8799 - val_loss: 13.3197 - val_regression_loss: 9.4747 - lr: 1.2500e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9469 - regression_loss: 9.8797 - val_loss: 13.3119 - val_regression_loss: 9.4687 - lr: 1.2500e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7636 - regression_loss: 9.8614 - val_loss: 13.3425 - val_regression_loss: 9.4814 - lr: 1.2500e-05\n",
      "Epoch 73/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8277 - regression_loss: 9.8653 - val_loss: 13.3311 - val_regression_loss: 9.4712 - lr: 1.2500e-05\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6891 - regression_loss: 9.8519 - val_loss: 13.3141 - val_regression_loss: 9.4586 - lr: 1.2500e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5834 - regression_loss: 9.8733 - val_loss: 13.2730 - val_regression_loss: 9.4429 - lr: 1.2500e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7648 - regression_loss: 9.8379 - val_loss: 13.2810 - val_regression_loss: 9.4366 - lr: 1.2500e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6328 - regression_loss: 9.8350 - val_loss: 13.3262 - val_regression_loss: 9.4665 - lr: 1.2500e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9454 - regression_loss: 9.8251 - val_loss: 13.3149 - val_regression_loss: 9.4579 - lr: 1.2500e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.8545 - regression_loss: 9.8189 - val_loss: 13.3331 - val_regression_loss: 9.4715 - lr: 1.2500e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5518 - regression_loss: 9.8017 - val_loss: 13.3091 - val_regression_loss: 9.4578 - lr: 1.2500e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8684 - regression_loss: 9.8184 - val_loss: 13.2934 - val_regression_loss: 9.4481 - lr: 1.2500e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8269 - regression_loss: 9.7984 - val_loss: 13.3342 - val_regression_loss: 9.4665 - lr: 1.2500e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6708 - regression_loss: 9.7901 - val_loss: 13.3168 - val_regression_loss: 9.4517 - lr: 1.2500e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7370 - regression_loss: 9.7835 - val_loss: 13.2986 - val_regression_loss: 9.4391 - lr: 1.2500e-05\n",
      "Epoch 85/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.8919 - regression_loss: 9.5523\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.7711 - regression_loss: 9.7695 - val_loss: 13.2940 - val_regression_loss: 9.4365 - lr: 1.2500e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.9581 - regression_loss: 9.7638 - val_loss: 13.2919 - val_regression_loss: 9.4346 - lr: 6.2500e-06\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5147 - regression_loss: 9.7578 - val_loss: 13.2883 - val_regression_loss: 9.4332 - lr: 6.2500e-06\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8091 - regression_loss: 9.7548 - val_loss: 13.2947 - val_regression_loss: 9.4369 - lr: 6.2500e-06\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.8125 - regression_loss: 9.7515 - val_loss: 13.2871 - val_regression_loss: 9.4295 - lr: 6.2500e-06\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7931 - regression_loss: 9.7513 - val_loss: 13.2967 - val_regression_loss: 9.4373 - lr: 6.2500e-06\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8623 - regression_loss: 9.7445 - val_loss: 13.2933 - val_regression_loss: 9.4331 - lr: 6.2500e-06\n",
      "Epoch 92/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.6590 - regression_loss: 10.3194\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.6618 - regression_loss: 9.7468 - val_loss: 13.3041 - val_regression_loss: 9.4380 - lr: 6.2500e-06\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.9113 - regression_loss: 9.7408 - val_loss: 13.3057 - val_regression_loss: 9.4386 - lr: 3.1250e-06\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7610 - regression_loss: 9.7346 - val_loss: 13.2953 - val_regression_loss: 9.4341 - lr: 3.1250e-06\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8086 - regression_loss: 9.7330 - val_loss: 13.2908 - val_regression_loss: 9.4318 - lr: 3.1250e-06\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8150 - regression_loss: 9.7328 - val_loss: 13.2864 - val_regression_loss: 9.4276 - lr: 3.1250e-06\n",
      "Epoch 97/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.0284 - regression_loss: 9.6889\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6373 - regression_loss: 9.7348 - val_loss: 13.2833 - val_regression_loss: 9.4270 - lr: 3.1250e-06\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6570 - regression_loss: 9.7283 - val_loss: 13.2855 - val_regression_loss: 9.4267 - lr: 1.5625e-06\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7816 - regression_loss: 9.7254 - val_loss: 13.2882 - val_regression_loss: 9.4278 - lr: 1.5625e-06\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7405 - regression_loss: 9.7259 - val_loss: 13.2865 - val_regression_loss: 9.4273 - lr: 1.5625e-06\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9101 - regression_loss: 9.7292 - val_loss: 13.2924 - val_regression_loss: 9.4295 - lr: 1.5625e-06\n",
      "Epoch 102/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.2100 - regression_loss: 8.8705\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6321 - regression_loss: 9.7240 - val_loss: 13.2893 - val_regression_loss: 9.4278 - lr: 1.5625e-06\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7520 - regression_loss: 9.7215 - val_loss: 13.2909 - val_regression_loss: 9.4292 - lr: 7.8125e-07\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7627 - regression_loss: 9.7232 - val_loss: 13.2924 - val_regression_loss: 9.4294 - lr: 7.8125e-07\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.6918 - regression_loss: 9.7207 - val_loss: 13.2913 - val_regression_loss: 9.4288 - lr: 7.8125e-07\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.8942 - regression_loss: 9.7202 - val_loss: 13.2899 - val_regression_loss: 9.4282 - lr: 7.8125e-07\n",
      "Epoch 107/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.6646 - regression_loss: 9.3251\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6692 - regression_loss: 9.7196 - val_loss: 13.2900 - val_regression_loss: 9.4281 - lr: 7.8125e-07\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7638 - regression_loss: 9.7191 - val_loss: 13.2902 - val_regression_loss: 9.4282 - lr: 3.9062e-07\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6783 - regression_loss: 9.7187 - val_loss: 13.2905 - val_regression_loss: 9.4283 - lr: 3.9062e-07\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7408 - regression_loss: 9.7185 - val_loss: 13.2902 - val_regression_loss: 9.4282 - lr: 3.9062e-07\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5767 - regression_loss: 9.7187 - val_loss: 13.2905 - val_regression_loss: 9.4282 - lr: 3.9062e-07\n",
      "Epoch 112/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.7623 - regression_loss: 10.4228\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7342 - regression_loss: 9.7182 - val_loss: 13.2904 - val_regression_loss: 9.4283 - lr: 3.9062e-07\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7943 - regression_loss: 9.7179 - val_loss: 13.2908 - val_regression_loss: 9.4285 - lr: 1.9531e-07\n",
      "Epoch 114/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7260 - regression_loss: 9.7177 - val_loss: 13.2903 - val_regression_loss: 9.4283 - lr: 1.9531e-07\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.9229 - regression_loss: 9.7176 - val_loss: 13.2907 - val_regression_loss: 9.4284 - lr: 1.9531e-07\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 54.9984 - regression_loss: 49.5483 - val_loss: 25.2336 - val_regression_loss: 20.7817 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 40.9891 - regression_loss: 36.5566 - val_loss: 19.7262 - val_regression_loss: 16.1549 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.7636 - regression_loss: 29.1451 - val_loss: 18.2416 - val_regression_loss: 14.7916 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.1179 - regression_loss: 25.5156 - val_loss: 15.5264 - val_regression_loss: 12.4817 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 24.7105 - regression_loss: 22.0054 - val_loss: 13.7569 - val_regression_loss: 10.9191 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8900 - regression_loss: 18.6029 - val_loss: 12.1277 - val_regression_loss: 9.5490 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.8967 - regression_loss: 15.9918 - val_loss: 10.9500 - val_regression_loss: 8.4996 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.4770 - regression_loss: 13.9967 - val_loss: 10.1072 - val_regression_loss: 7.7048 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.3436 - regression_loss: 11.9317 - val_loss: 9.1916 - val_regression_loss: 6.8456 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4909 - regression_loss: 10.2379 - val_loss: 8.5561 - val_regression_loss: 6.2042 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7866 - regression_loss: 8.8988 - val_loss: 7.8712 - val_regression_loss: 5.5616 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8135 - regression_loss: 7.7903 - val_loss: 7.4008 - val_regression_loss: 5.1258 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6182 - regression_loss: 6.8535 - val_loss: 7.0744 - val_regression_loss: 4.7850 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6188 - regression_loss: 5.8959 - val_loss: 6.4126 - val_regression_loss: 4.2262 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9780 - regression_loss: 5.3794 - val_loss: 6.0445 - val_regression_loss: 3.8526 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3743 - regression_loss: 4.7372 - val_loss: 5.7252 - val_regression_loss: 3.5658 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9409 - regression_loss: 4.2844 - val_loss: 5.4035 - val_regression_loss: 3.2598 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5140 - regression_loss: 3.9177 - val_loss: 5.1221 - val_regression_loss: 3.0124 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1680 - regression_loss: 3.5967 - val_loss: 4.8011 - val_regression_loss: 2.7502 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9021 - regression_loss: 3.3359 - val_loss: 4.5776 - val_regression_loss: 2.5504 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6742 - regression_loss: 3.0958 - val_loss: 4.4239 - val_regression_loss: 2.4145 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3746 - regression_loss: 2.8450 - val_loss: 4.3016 - val_regression_loss: 2.3606 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2604 - regression_loss: 2.7966 - val_loss: 4.3032 - val_regression_loss: 2.3108 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.1309 - regression_loss: 2.5656 - val_loss: 3.8832 - val_regression_loss: 2.0268 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8221 - regression_loss: 2.3238 - val_loss: 3.6330 - val_regression_loss: 1.7910 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6497 - regression_loss: 2.1285 - val_loss: 3.4708 - val_regression_loss: 1.6971 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5355 - regression_loss: 2.0594 - val_loss: 3.3568 - val_regression_loss: 1.5943 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4201 - regression_loss: 1.9675 - val_loss: 3.3275 - val_regression_loss: 1.5849 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3517 - regression_loss: 1.8658 - val_loss: 3.1701 - val_regression_loss: 1.4430 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1719 - regression_loss: 1.7219 - val_loss: 3.0672 - val_regression_loss: 1.3665 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1009 - regression_loss: 1.6496 - val_loss: 3.0278 - val_regression_loss: 1.3610 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0701 - regression_loss: 1.6088 - val_loss: 2.9184 - val_regression_loss: 1.2565 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9340 - regression_loss: 1.4871 - val_loss: 2.8702 - val_regression_loss: 1.2168 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8644 - regression_loss: 1.4441 - val_loss: 2.8214 - val_regression_loss: 1.1820 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8221 - regression_loss: 1.3846 - val_loss: 2.7812 - val_regression_loss: 1.1840 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7875 - regression_loss: 1.3831 - val_loss: 2.8512 - val_regression_loss: 1.1971 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7627 - regression_loss: 1.3167 - val_loss: 2.7674 - val_regression_loss: 1.1729 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7773 - regression_loss: 1.3310 - val_loss: 2.6192 - val_regression_loss: 1.0313 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6481 - regression_loss: 1.2179 - val_loss: 2.5995 - val_regression_loss: 1.0202 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5615 - regression_loss: 1.1796 - val_loss: 2.5980 - val_regression_loss: 1.0438 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6130 - regression_loss: 1.1919 - val_loss: 2.5622 - val_regression_loss: 0.9916 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6119 - regression_loss: 1.1871 - val_loss: 2.5777 - val_regression_loss: 1.0027 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5418 - regression_loss: 1.1445 - val_loss: 2.8418 - val_regression_loss: 1.2817 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6738 - regression_loss: 1.2513 - val_loss: 2.4527 - val_regression_loss: 0.9162 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4724 - regression_loss: 1.0464 - val_loss: 2.4513 - val_regression_loss: 0.9066 - lr: 1.0000e-04\n",
      "Epoch 46/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4212 - regression_loss: 1.0147 - val_loss: 2.5755 - val_regression_loss: 1.0427 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5331 - regression_loss: 1.1480 - val_loss: 2.4210 - val_regression_loss: 0.8939 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4419 - regression_loss: 1.0354 - val_loss: 2.4645 - val_regression_loss: 0.9323 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4263 - regression_loss: 1.0172 - val_loss: 2.4295 - val_regression_loss: 0.9136 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3725 - regression_loss: 0.9611 - val_loss: 2.3529 - val_regression_loss: 0.8488 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3184 - regression_loss: 0.9292 - val_loss: 2.3418 - val_regression_loss: 0.8318 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2946 - regression_loss: 0.9018 - val_loss: 2.3668 - val_regression_loss: 0.8640 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2848 - regression_loss: 0.9026 - val_loss: 2.3355 - val_regression_loss: 0.8278 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3329 - regression_loss: 0.9257 - val_loss: 2.3602 - val_regression_loss: 0.8405 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3170 - regression_loss: 0.9286 - val_loss: 2.3736 - val_regression_loss: 0.8792 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3057 - regression_loss: 0.9136 - val_loss: 2.3495 - val_regression_loss: 0.8540 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2925 - regression_loss: 0.8928 - val_loss: 2.3126 - val_regression_loss: 0.8237 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2615 - regression_loss: 0.8619 - val_loss: 2.3622 - val_regression_loss: 0.8509 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2714 - regression_loss: 0.9130 - val_loss: 2.2917 - val_regression_loss: 0.7917 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2532 - regression_loss: 0.8509 - val_loss: 2.2773 - val_regression_loss: 0.7978 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2441 - regression_loss: 0.8425 - val_loss: 2.3661 - val_regression_loss: 0.8761 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3187 - regression_loss: 0.9294 - val_loss: 2.3939 - val_regression_loss: 0.8974 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2956 - regression_loss: 0.8957 - val_loss: 2.3300 - val_regression_loss: 0.8206 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2346 - regression_loss: 0.8435 - val_loss: 2.2752 - val_regression_loss: 0.7808 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1973 - regression_loss: 0.7935 - val_loss: 2.2484 - val_regression_loss: 0.7704 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1753 - regression_loss: 0.7928 - val_loss: 2.2899 - val_regression_loss: 0.7936 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2171 - regression_loss: 0.8175 - val_loss: 2.2620 - val_regression_loss: 0.7726 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2311 - regression_loss: 0.8442 - val_loss: 2.2943 - val_regression_loss: 0.8202 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1820 - regression_loss: 0.7950 - val_loss: 2.2399 - val_regression_loss: 0.7674 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1844 - regression_loss: 0.7930 - val_loss: 2.3121 - val_regression_loss: 0.8128 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1494 - regression_loss: 0.7698 - val_loss: 2.2894 - val_regression_loss: 0.8226 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1966 - regression_loss: 0.8067 - val_loss: 2.2751 - val_regression_loss: 0.7986 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1771 - regression_loss: 0.7857 - val_loss: 2.2030 - val_regression_loss: 0.7246 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1516 - regression_loss: 0.7676 - val_loss: 2.3090 - val_regression_loss: 0.8027 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2285 - regression_loss: 0.8429 - val_loss: 2.2061 - val_regression_loss: 0.7354 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1266 - regression_loss: 0.7501 - val_loss: 2.2292 - val_regression_loss: 0.7534 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0912 - regression_loss: 0.7281 - val_loss: 2.2253 - val_regression_loss: 0.7595 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1090 - regression_loss: 0.7302 - val_loss: 2.2003 - val_regression_loss: 0.7329 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1202 - regression_loss: 0.7293 - val_loss: 2.2111 - val_regression_loss: 0.7448 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0904 - regression_loss: 0.7164 - val_loss: 2.1972 - val_regression_loss: 0.7271 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1004 - regression_loss: 0.7224 - val_loss: 2.2135 - val_regression_loss: 0.7391 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1132 - regression_loss: 0.7204 - val_loss: 2.2036 - val_regression_loss: 0.7352 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1052 - regression_loss: 0.7426 - val_loss: 2.2298 - val_regression_loss: 0.7427 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1447 - regression_loss: 0.7710 - val_loss: 2.1949 - val_regression_loss: 0.7210 - lr: 1.0000e-04\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0895 - regression_loss: 0.7126 - val_loss: 2.1958 - val_regression_loss: 0.7259 - lr: 1.0000e-04\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0803 - regression_loss: 0.6978 - val_loss: 2.2032 - val_regression_loss: 0.7413 - lr: 1.0000e-04\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0792 - regression_loss: 0.6987 - val_loss: 2.1901 - val_regression_loss: 0.7252 - lr: 1.0000e-04\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0538 - regression_loss: 0.6889 - val_loss: 2.1909 - val_regression_loss: 0.7294 - lr: 1.0000e-04\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0584 - regression_loss: 0.6930 - val_loss: 2.1763 - val_regression_loss: 0.7175 - lr: 1.0000e-04\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0642 - regression_loss: 0.6974 - val_loss: 2.1851 - val_regression_loss: 0.7116 - lr: 1.0000e-04\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0464 - regression_loss: 0.6784 - val_loss: 2.1770 - val_regression_loss: 0.7056 - lr: 1.0000e-04\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0647 - regression_loss: 0.6851 - val_loss: 2.2622 - val_regression_loss: 0.8016 - lr: 1.0000e-04\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1263 - regression_loss: 0.7494 - val_loss: 2.2201 - val_regression_loss: 0.7622 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0688 - regression_loss: 0.6976 - val_loss: 2.1959 - val_regression_loss: 0.7270 - lr: 1.0000e-04\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0419 - regression_loss: 0.6707 - val_loss: 2.1760 - val_regression_loss: 0.7168 - lr: 1.0000e-04\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0466 - regression_loss: 0.6760 - val_loss: 2.1902 - val_regression_loss: 0.7346 - lr: 1.0000e-04\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0907 - regression_loss: 0.7196 - val_loss: 2.1891 - val_regression_loss: 0.7205 - lr: 1.0000e-04\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0471 - regression_loss: 0.6725 - val_loss: 2.1967 - val_regression_loss: 0.7482 - lr: 1.0000e-04\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0434 - regression_loss: 0.6708 - val_loss: 2.1568 - val_regression_loss: 0.7030 - lr: 1.0000e-04\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0327 - regression_loss: 0.6618 - val_loss: 2.1783 - val_regression_loss: 0.7151 - lr: 1.0000e-04\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0367 - regression_loss: 0.6592 - val_loss: 2.1930 - val_regression_loss: 0.7210 - lr: 1.0000e-04\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0339 - regression_loss: 0.6731 - val_loss: 2.1590 - val_regression_loss: 0.6965 - lr: 1.0000e-04\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0220 - regression_loss: 0.6568 - val_loss: 2.1638 - val_regression_loss: 0.7054 - lr: 1.0000e-04\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0108 - regression_loss: 0.6405 - val_loss: 2.1474 - val_regression_loss: 0.6974 - lr: 1.0000e-04\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0252 - regression_loss: 0.6514 - val_loss: 2.1700 - val_regression_loss: 0.7152 - lr: 1.0000e-04\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0214 - regression_loss: 0.6445 - val_loss: 2.1388 - val_regression_loss: 0.6954 - lr: 1.0000e-04\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0051 - regression_loss: 0.6390 - val_loss: 2.1584 - val_regression_loss: 0.7117 - lr: 1.0000e-04\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0353 - regression_loss: 0.6562 - val_loss: 2.2680 - val_regression_loss: 0.7741 - lr: 1.0000e-04\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1069 - regression_loss: 0.7250 - val_loss: 2.3156 - val_regression_loss: 0.8191 - lr: 1.0000e-04\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1558 - regression_loss: 0.7834 - val_loss: 2.1894 - val_regression_loss: 0.7192 - lr: 1.0000e-04\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0292 - regression_loss: 0.6502 - val_loss: 2.1479 - val_regression_loss: 0.7041 - lr: 1.0000e-04\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9947 - regression_loss: 0.6319 - val_loss: 2.1949 - val_regression_loss: 0.7246 - lr: 1.0000e-04\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9942 - regression_loss: 0.6218 - val_loss: 2.1372 - val_regression_loss: 0.6860 - lr: 1.0000e-04\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0218 - regression_loss: 0.6565 - val_loss: 2.1619 - val_regression_loss: 0.7038 - lr: 1.0000e-04\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0375 - regression_loss: 0.6629 - val_loss: 2.2061 - val_regression_loss: 0.7668 - lr: 1.0000e-04\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0355 - regression_loss: 0.6644 - val_loss: 2.1860 - val_regression_loss: 0.7279 - lr: 1.0000e-04\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0163 - regression_loss: 0.6543 - val_loss: 2.1932 - val_regression_loss: 0.7552 - lr: 1.0000e-04\n",
      "Epoch 118/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.1304 - regression_loss: 0.8098\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0125 - regression_loss: 0.6521 - val_loss: 2.1958 - val_regression_loss: 0.7402 - lr: 1.0000e-04\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9816 - regression_loss: 0.6196 - val_loss: 2.1900 - val_regression_loss: 0.7170 - lr: 5.0000e-05\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0006 - regression_loss: 0.6352 - val_loss: 2.1423 - val_regression_loss: 0.6910 - lr: 5.0000e-05\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9798 - regression_loss: 0.6118 - val_loss: 2.1443 - val_regression_loss: 0.7031 - lr: 5.0000e-05\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9892 - regression_loss: 0.6306 - val_loss: 2.2825 - val_regression_loss: 0.7895 - lr: 5.0000e-05\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0582 - regression_loss: 0.6970 - val_loss: 2.3105 - val_regression_loss: 0.8556 - lr: 5.0000e-05\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0273 - regression_loss: 0.6620 - val_loss: 2.2201 - val_regression_loss: 0.7410 - lr: 5.0000e-05\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0039 - regression_loss: 0.6513 - val_loss: 2.1549 - val_regression_loss: 0.7109 - lr: 5.0000e-05\n",
      "Epoch 126/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.0637 - regression_loss: 0.7440\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9904 - regression_loss: 0.6245 - val_loss: 2.1240 - val_regression_loss: 0.6765 - lr: 5.0000e-05\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9607 - regression_loss: 0.5964 - val_loss: 2.1292 - val_regression_loss: 0.6819 - lr: 2.5000e-05\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9516 - regression_loss: 0.5895 - val_loss: 2.1454 - val_regression_loss: 0.6876 - lr: 2.5000e-05\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9404 - regression_loss: 0.5878 - val_loss: 2.1363 - val_regression_loss: 0.6852 - lr: 2.5000e-05\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9488 - regression_loss: 0.5871 - val_loss: 2.1296 - val_regression_loss: 0.6816 - lr: 2.5000e-05\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9518 - regression_loss: 0.5849 - val_loss: 2.1355 - val_regression_loss: 0.6829 - lr: 2.5000e-05\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9425 - regression_loss: 0.5861 - val_loss: 2.1305 - val_regression_loss: 0.6826 - lr: 2.5000e-05\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9406 - regression_loss: 0.5831 - val_loss: 2.1314 - val_regression_loss: 0.6811 - lr: 2.5000e-05\n",
      "Epoch 134/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.0191 - regression_loss: 0.6999\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9435 - regression_loss: 0.5845 - val_loss: 2.1295 - val_regression_loss: 0.6817 - lr: 2.5000e-05\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9431 - regression_loss: 0.5822 - val_loss: 2.1322 - val_regression_loss: 0.6836 - lr: 1.2500e-05\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9448 - regression_loss: 0.5823 - val_loss: 2.1299 - val_regression_loss: 0.6815 - lr: 1.2500e-05\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9504 - regression_loss: 0.5819 - val_loss: 2.1305 - val_regression_loss: 0.6804 - lr: 1.2500e-05\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9402 - regression_loss: 0.5840 - val_loss: 2.1298 - val_regression_loss: 0.6806 - lr: 1.2500e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9363 - regression_loss: 0.5808 - val_loss: 2.1302 - val_regression_loss: 0.6804 - lr: 1.2500e-05\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9457 - regression_loss: 0.5817 - val_loss: 2.1318 - val_regression_loss: 0.6821 - lr: 1.2500e-05\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9373 - regression_loss: 0.5804 - val_loss: 2.1330 - val_regression_loss: 0.6833 - lr: 1.2500e-05\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9404 - regression_loss: 0.5809 - val_loss: 2.1324 - val_regression_loss: 0.6820 - lr: 1.2500e-05\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9362 - regression_loss: 0.5806 - val_loss: 2.1262 - val_regression_loss: 0.6795 - lr: 1.2500e-05\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9460 - regression_loss: 0.5838 - val_loss: 2.1282 - val_regression_loss: 0.6807 - lr: 1.2500e-05\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9333 - regression_loss: 0.5793 - val_loss: 2.1308 - val_regression_loss: 0.6812 - lr: 1.2500e-05\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9371 - regression_loss: 0.5790 - val_loss: 2.1320 - val_regression_loss: 0.6819 - lr: 1.2500e-05\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9375 - regression_loss: 0.5792 - val_loss: 2.1304 - val_regression_loss: 0.6822 - lr: 1.2500e-05\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9345 - regression_loss: 0.5787 - val_loss: 2.1295 - val_regression_loss: 0.6805 - lr: 1.2500e-05\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9381 - regression_loss: 0.5806 - val_loss: 2.1275 - val_regression_loss: 0.6797 - lr: 1.2500e-05\n",
      "Epoch 150/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.0208 - regression_loss: 0.7021\n",
      "Epoch 150: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9430 - regression_loss: 0.5836 - val_loss: 2.1329 - val_regression_loss: 0.6843 - lr: 1.2500e-05\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9241 - regression_loss: 0.5766 - val_loss: 2.1343 - val_regression_loss: 0.6834 - lr: 6.2500e-06\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9449 - regression_loss: 0.5790 - val_loss: 2.1334 - val_regression_loss: 0.6821 - lr: 6.2500e-06\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9298 - regression_loss: 0.5776 - val_loss: 2.1310 - val_regression_loss: 0.6820 - lr: 6.2500e-06\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9390 - regression_loss: 0.5773 - val_loss: 2.1295 - val_regression_loss: 0.6811 - lr: 6.2500e-06\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9399 - regression_loss: 0.5767 - val_loss: 2.1289 - val_regression_loss: 0.6803 - lr: 6.2500e-06\n",
      "Epoch 156/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.9293 - regression_loss: 0.6107\n",
      "Epoch 156: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9358 - regression_loss: 0.5761 - val_loss: 2.1285 - val_regression_loss: 0.6798 - lr: 6.2500e-06\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9413 - regression_loss: 0.5764 - val_loss: 2.1282 - val_regression_loss: 0.6800 - lr: 3.1250e-06\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9464 - regression_loss: 0.5758 - val_loss: 2.1283 - val_regression_loss: 0.6799 - lr: 3.1250e-06\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9414 - regression_loss: 0.5757 - val_loss: 2.1280 - val_regression_loss: 0.6799 - lr: 3.1250e-06\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9342 - regression_loss: 0.5760 - val_loss: 2.1283 - val_regression_loss: 0.6802 - lr: 3.1250e-06\n",
      "Epoch 161/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.0970 - regression_loss: 0.7784\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9297 - regression_loss: 0.5759 - val_loss: 2.1293 - val_regression_loss: 0.6805 - lr: 3.1250e-06\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9250 - regression_loss: 0.5758 - val_loss: 2.1290 - val_regression_loss: 0.6806 - lr: 1.5625e-06\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9377 - regression_loss: 0.5754 - val_loss: 2.1290 - val_regression_loss: 0.6805 - lr: 1.5625e-06\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9176 - regression_loss: 0.5754 - val_loss: 2.1288 - val_regression_loss: 0.6803 - lr: 1.5625e-06\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9359 - regression_loss: 0.5754 - val_loss: 2.1291 - val_regression_loss: 0.6805 - lr: 1.5625e-06\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9390 - regression_loss: 0.5755 - val_loss: 2.1294 - val_regression_loss: 0.6809 - lr: 1.5625e-06\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 24ms/step - loss: 172.0739 - regression_loss: 158.4126 - val_loss: 108.2186 - val_regression_loss: 97.4649 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 96.3464 - regression_loss: 87.5271 - val_loss: 72.7708 - val_regression_loss: 65.3415 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 69.2735 - regression_loss: 62.3615 - val_loss: 60.7868 - val_regression_loss: 54.0408 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 59.8244 - regression_loss: 53.5789 - val_loss: 52.8435 - val_regression_loss: 46.6980 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 52.4492 - regression_loss: 47.0874 - val_loss: 46.6693 - val_regression_loss: 41.3053 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 45.2554 - regression_loss: 41.1956 - val_loss: 42.3335 - val_regression_loss: 37.6144 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 40.6676 - regression_loss: 36.5540 - val_loss: 38.5660 - val_regression_loss: 34.3354 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 36.7443 - regression_loss: 33.0121 - val_loss: 35.4931 - val_regression_loss: 31.4935 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 34.2870 - regression_loss: 29.9907 - val_loss: 32.6930 - val_regression_loss: 28.9331 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.5016 - regression_loss: 27.6051 - val_loss: 30.2683 - val_regression_loss: 26.6406 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 28.3837 - regression_loss: 24.9041 - val_loss: 27.5094 - val_regression_loss: 24.1111 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.7138 - regression_loss: 22.4499 - val_loss: 25.1032 - val_regression_loss: 21.9760 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.8816 - regression_loss: 20.3178 - val_loss: 23.1742 - val_regression_loss: 20.2874 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.9799 - regression_loss: 18.3555 - val_loss: 21.1726 - val_regression_loss: 18.4935 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1970 - regression_loss: 16.8619 - val_loss: 19.6989 - val_regression_loss: 17.2440 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.0519 - regression_loss: 15.5227 - val_loss: 18.0748 - val_regression_loss: 15.7037 - lr: 1.0000e-04\n",
      "Epoch 17/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5890 - regression_loss: 14.3295 - val_loss: 17.0776 - val_regression_loss: 14.9116 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.8431 - regression_loss: 13.5255 - val_loss: 15.7272 - val_regression_loss: 13.6801 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.0027 - regression_loss: 12.6805 - val_loss: 14.8776 - val_regression_loss: 12.9432 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.3294 - regression_loss: 12.0543 - val_loss: 14.2742 - val_regression_loss: 12.4155 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1616 - regression_loss: 11.7459 - val_loss: 14.2271 - val_regression_loss: 12.4268 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.4039 - regression_loss: 11.2311 - val_loss: 13.4001 - val_regression_loss: 11.4240 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8649 - regression_loss: 11.0397 - val_loss: 13.3007 - val_regression_loss: 11.6019 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9726 - regression_loss: 10.7265 - val_loss: 12.6423 - val_regression_loss: 10.8465 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6087 - regression_loss: 10.4248 - val_loss: 12.4189 - val_regression_loss: 10.6714 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4512 - regression_loss: 10.2374 - val_loss: 12.6257 - val_regression_loss: 10.8863 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0782 - regression_loss: 9.9593 - val_loss: 12.3176 - val_regression_loss: 10.3870 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8742 - regression_loss: 9.8790 - val_loss: 12.3975 - val_regression_loss: 10.6949 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8093 - regression_loss: 9.6345 - val_loss: 12.2233 - val_regression_loss: 10.2456 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7639 - regression_loss: 9.7962 - val_loss: 12.5377 - val_regression_loss: 10.7757 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5059 - regression_loss: 9.4039 - val_loss: 11.9213 - val_regression_loss: 9.9740 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8961 - regression_loss: 9.3158 - val_loss: 11.9826 - val_regression_loss: 10.1602 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3993 - regression_loss: 9.4177 - val_loss: 12.1286 - val_regression_loss: 10.2336 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5795 - regression_loss: 9.4451 - val_loss: 12.4694 - val_regression_loss: 10.1850 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.5024 - regression_loss: 8.1829\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3536 - regression_loss: 9.3929 - val_loss: 13.6088 - val_regression_loss: 11.6488 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6590 - regression_loss: 9.6984 - val_loss: 12.2363 - val_regression_loss: 9.9537 - lr: 5.0000e-05\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1837 - regression_loss: 9.1051 - val_loss: 11.9026 - val_regression_loss: 10.0143 - lr: 5.0000e-05\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3701 - regression_loss: 9.3495 - val_loss: 11.5250 - val_regression_loss: 9.5833 - lr: 5.0000e-05\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.4612 - regression_loss: 8.7214 - val_loss: 11.5926 - val_regression_loss: 9.5482 - lr: 5.0000e-05\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.4049 - regression_loss: 8.4829 - val_loss: 11.7981 - val_regression_loss: 9.9022 - lr: 5.0000e-05\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3816 - regression_loss: 8.4290 - val_loss: 11.4511 - val_regression_loss: 9.4336 - lr: 5.0000e-05\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3211 - regression_loss: 8.3391 - val_loss: 11.4097 - val_regression_loss: 9.4692 - lr: 5.0000e-05\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3079 - regression_loss: 8.3088 - val_loss: 11.3364 - val_regression_loss: 9.3546 - lr: 5.0000e-05\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0940 - regression_loss: 8.2363 - val_loss: 11.3104 - val_regression_loss: 9.3165 - lr: 5.0000e-05\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9526 - regression_loss: 8.1190 - val_loss: 11.3589 - val_regression_loss: 9.3987 - lr: 5.0000e-05\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4607 - regression_loss: 8.1459 - val_loss: 11.2647 - val_regression_loss: 9.2330 - lr: 5.0000e-05\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8212 - regression_loss: 8.0335 - val_loss: 11.3597 - val_regression_loss: 9.3766 - lr: 5.0000e-05\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9861 - regression_loss: 8.0494 - val_loss: 11.2569 - val_regression_loss: 9.2073 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9694 - regression_loss: 7.9622 - val_loss: 11.2449 - val_regression_loss: 9.1965 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7897 - regression_loss: 7.8748 - val_loss: 11.1401 - val_regression_loss: 9.1155 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.4065 - regression_loss: 6.0875\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7130 - regression_loss: 7.8575 - val_loss: 11.1355 - val_regression_loss: 9.0829 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7052 - regression_loss: 7.7920 - val_loss: 11.1170 - val_regression_loss: 9.0628 - lr: 2.5000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6718 - regression_loss: 7.7562 - val_loss: 11.0963 - val_regression_loss: 9.0523 - lr: 2.5000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7195 - regression_loss: 7.7677 - val_loss: 11.0986 - val_regression_loss: 9.0441 - lr: 2.5000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6412 - regression_loss: 7.7229 - val_loss: 11.0716 - val_regression_loss: 8.9792 - lr: 2.5000e-05\n",
      "Epoch 56/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.4598 - regression_loss: 11.1409\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6320 - regression_loss: 7.6839 - val_loss: 11.0658 - val_regression_loss: 9.0223 - lr: 2.5000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7149 - regression_loss: 7.6810 - val_loss: 11.0528 - val_regression_loss: 9.0185 - lr: 1.2500e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4387 - regression_loss: 7.6417 - val_loss: 11.0442 - val_regression_loss: 8.9722 - lr: 1.2500e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6267 - regression_loss: 7.6236 - val_loss: 11.0383 - val_regression_loss: 8.9546 - lr: 1.2500e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3729 - regression_loss: 7.6153 - val_loss: 11.0494 - val_regression_loss: 8.9618 - lr: 1.2500e-05\n",
      "Epoch 61/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5114 - regression_loss: 7.6041 - val_loss: 11.0341 - val_regression_loss: 8.9676 - lr: 1.2500e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5337 - regression_loss: 7.6175 - val_loss: 11.0019 - val_regression_loss: 8.9179 - lr: 1.2500e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4860 - regression_loss: 7.5722 - val_loss: 10.9922 - val_regression_loss: 8.9290 - lr: 1.2500e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4142 - regression_loss: 7.5905 - val_loss: 11.0049 - val_regression_loss: 8.9419 - lr: 1.2500e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2438 - regression_loss: 7.5611 - val_loss: 10.9908 - val_regression_loss: 8.9195 - lr: 1.2500e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4791 - regression_loss: 7.5456 - val_loss: 10.9930 - val_regression_loss: 8.8906 - lr: 1.2500e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3895 - regression_loss: 7.5374 - val_loss: 10.9715 - val_regression_loss: 8.8750 - lr: 1.2500e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3150 - regression_loss: 7.5135 - val_loss: 10.9810 - val_regression_loss: 8.9170 - lr: 1.2500e-05\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3655 - regression_loss: 7.5091 - val_loss: 10.9606 - val_regression_loss: 8.8830 - lr: 1.2500e-05\n",
      "Epoch 70/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.6635 - regression_loss: 12.3447\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3954 - regression_loss: 7.5036 - val_loss: 10.9505 - val_regression_loss: 8.8512 - lr: 1.2500e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2634 - regression_loss: 7.4801 - val_loss: 10.9547 - val_regression_loss: 8.8704 - lr: 6.2500e-06\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1657 - regression_loss: 7.4646 - val_loss: 10.9548 - val_regression_loss: 8.8679 - lr: 6.2500e-06\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2115 - regression_loss: 7.4662 - val_loss: 10.9386 - val_regression_loss: 8.8447 - lr: 6.2500e-06\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2810 - regression_loss: 7.4535 - val_loss: 10.9362 - val_regression_loss: 8.8394 - lr: 6.2500e-06\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3133 - regression_loss: 7.4547 - val_loss: 10.9439 - val_regression_loss: 8.8551 - lr: 6.2500e-06\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3396 - regression_loss: 7.4442 - val_loss: 10.9392 - val_regression_loss: 8.8414 - lr: 6.2500e-06\n",
      "Epoch 77/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 14.2644 - regression_loss: 12.9457\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3383 - regression_loss: 7.4391 - val_loss: 10.9294 - val_regression_loss: 8.8247 - lr: 6.2500e-06\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2225 - regression_loss: 7.4302 - val_loss: 10.9271 - val_regression_loss: 8.8294 - lr: 3.1250e-06\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2692 - regression_loss: 7.4247 - val_loss: 10.9231 - val_regression_loss: 8.8213 - lr: 3.1250e-06\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1773 - regression_loss: 7.4218 - val_loss: 10.9246 - val_regression_loss: 8.8284 - lr: 3.1250e-06\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3757 - regression_loss: 7.4180 - val_loss: 10.9180 - val_regression_loss: 8.8200 - lr: 3.1250e-06\n",
      "Epoch 82/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.7046 - regression_loss: 8.3858\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4014 - regression_loss: 7.4144 - val_loss: 10.9147 - val_regression_loss: 8.8157 - lr: 3.1250e-06\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3208 - regression_loss: 7.4100 - val_loss: 10.9165 - val_regression_loss: 8.8188 - lr: 1.5625e-06\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2785 - regression_loss: 7.4075 - val_loss: 10.9176 - val_regression_loss: 8.8199 - lr: 1.5625e-06\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3576 - regression_loss: 7.4060 - val_loss: 10.9142 - val_regression_loss: 8.8155 - lr: 1.5625e-06\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2311 - regression_loss: 7.4055 - val_loss: 10.9111 - val_regression_loss: 8.8109 - lr: 1.5625e-06\n",
      "Epoch 87/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.2855 - regression_loss: 10.9667\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4306 - regression_loss: 7.4034 - val_loss: 10.9100 - val_regression_loss: 8.8085 - lr: 1.5625e-06\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7771 - regression_loss: 7.4023 - val_loss: 10.9105 - val_regression_loss: 8.8109 - lr: 7.8125e-07\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2531 - regression_loss: 7.4004 - val_loss: 10.9111 - val_regression_loss: 8.8102 - lr: 7.8125e-07\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3097 - regression_loss: 7.3994 - val_loss: 10.9109 - val_regression_loss: 8.8106 - lr: 7.8125e-07\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0765 - regression_loss: 7.3983 - val_loss: 10.9096 - val_regression_loss: 8.8086 - lr: 7.8125e-07\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3481 - regression_loss: 7.3974 - val_loss: 10.9093 - val_regression_loss: 8.8081 - lr: 7.8125e-07\n",
      "Epoch 93/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.4567 - regression_loss: 6.1380\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.2128 - regression_loss: 7.3963 - val_loss: 10.9090 - val_regression_loss: 8.8088 - lr: 7.8125e-07\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2706 - regression_loss: 7.3956 - val_loss: 10.9086 - val_regression_loss: 8.8080 - lr: 3.9062e-07\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3161 - regression_loss: 7.3953 - val_loss: 10.9077 - val_regression_loss: 8.8068 - lr: 3.9062e-07\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2844 - regression_loss: 7.3946 - val_loss: 10.9075 - val_regression_loss: 8.8069 - lr: 3.9062e-07\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7770 - regression_loss: 7.3939 - val_loss: 10.9072 - val_regression_loss: 8.8068 - lr: 3.9062e-07\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3905 - regression_loss: 7.3935 - val_loss: 10.9068 - val_regression_loss: 8.8063 - lr: 3.9062e-07\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2371 - regression_loss: 7.3936 - val_loss: 10.9064 - val_regression_loss: 8.8052 - lr: 3.9062e-07\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0588 - regression_loss: 7.3928 - val_loss: 10.9070 - val_regression_loss: 8.8055 - lr: 3.9062e-07\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2272 - regression_loss: 7.3925 - val_loss: 10.9064 - val_regression_loss: 8.8048 - lr: 3.9062e-07\n",
      "Epoch 102/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.6079 - regression_loss: 7.2892\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3873 - regression_loss: 7.3921 - val_loss: 10.9058 - val_regression_loss: 8.8049 - lr: 3.9062e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2576 - regression_loss: 7.3912 - val_loss: 10.9057 - val_regression_loss: 8.8048 - lr: 1.9531e-07\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2331 - regression_loss: 7.3912 - val_loss: 10.9058 - val_regression_loss: 8.8049 - lr: 1.9531e-07\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2875 - regression_loss: 7.3909 - val_loss: 10.9060 - val_regression_loss: 8.8052 - lr: 1.9531e-07\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1082 - regression_loss: 7.3907 - val_loss: 10.9057 - val_regression_loss: 8.8048 - lr: 1.9531e-07\n",
      "Epoch 107/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.6904 - regression_loss: 8.3717\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2356 - regression_loss: 7.3903 - val_loss: 10.9053 - val_regression_loss: 8.8047 - lr: 1.9531e-07\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0258 - regression_loss: 7.3902 - val_loss: 10.9052 - val_regression_loss: 8.8043 - lr: 9.7656e-08\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1587 - regression_loss: 7.3900 - val_loss: 10.9051 - val_regression_loss: 8.8043 - lr: 9.7656e-08\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1920 - regression_loss: 7.3900 - val_loss: 10.9049 - val_regression_loss: 8.8040 - lr: 9.7656e-08\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1350 - regression_loss: 7.3897 - val_loss: 10.9049 - val_regression_loss: 8.8041 - lr: 9.7656e-08\n",
      "Epoch 112/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.0311 - regression_loss: 8.7123\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3389 - regression_loss: 7.3896 - val_loss: 10.9048 - val_regression_loss: 8.8041 - lr: 9.7656e-08\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1729 - regression_loss: 7.3895 - val_loss: 10.9049 - val_regression_loss: 8.8042 - lr: 4.8828e-08\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3585 - regression_loss: 7.3894 - val_loss: 10.9048 - val_regression_loss: 8.8040 - lr: 4.8828e-08\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2104 - regression_loss: 7.3894 - val_loss: 10.9047 - val_regression_loss: 8.8040 - lr: 4.8828e-08\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2734 - regression_loss: 7.3894 - val_loss: 10.9048 - val_regression_loss: 8.8039 - lr: 4.8828e-08\n",
      "Epoch 117/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.7097 - regression_loss: 12.3909\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2428 - regression_loss: 7.3892 - val_loss: 10.9048 - val_regression_loss: 8.8039 - lr: 4.8828e-08\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7440 - regression_loss: 7.3892 - val_loss: 10.9047 - val_regression_loss: 8.8039 - lr: 2.4414e-08\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3916 - regression_loss: 7.3891 - val_loss: 10.9047 - val_regression_loss: 8.8039 - lr: 2.4414e-08\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3098 - regression_loss: 7.3891 - val_loss: 10.9047 - val_regression_loss: 8.8038 - lr: 2.4414e-08\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2603 - regression_loss: 7.3891 - val_loss: 10.9047 - val_regression_loss: 8.8038 - lr: 2.4414e-08\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2971 - regression_loss: 7.3891 - val_loss: 10.9047 - val_regression_loss: 8.8038 - lr: 2.4414e-08\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6725 - regression_loss: 7.3890 - val_loss: 10.9046 - val_regression_loss: 8.8038 - lr: 2.4414e-08\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3278 - regression_loss: 7.3890 - val_loss: 10.9046 - val_regression_loss: 8.8037 - lr: 2.4414e-08\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2257 - regression_loss: 7.3890 - val_loss: 10.9046 - val_regression_loss: 8.8037 - lr: 2.4414e-08\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2060 - regression_loss: 7.3890 - val_loss: 10.9046 - val_regression_loss: 8.8037 - lr: 2.4414e-08\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4047 - regression_loss: 7.3889 - val_loss: 10.9046 - val_regression_loss: 8.8037 - lr: 2.4414e-08\n",
      "Epoch 128/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.8289 - regression_loss: 7.5102\n",
      "Epoch 128: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8230 - regression_loss: 7.3889 - val_loss: 10.9046 - val_regression_loss: 8.8037 - lr: 2.4414e-08\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2206 - regression_loss: 7.3889 - val_loss: 10.9045 - val_regression_loss: 8.8037 - lr: 1.2207e-08\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1189 - regression_loss: 7.3889 - val_loss: 10.9045 - val_regression_loss: 8.8037 - lr: 1.2207e-08\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3013 - regression_loss: 7.3888 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 1.2207e-08\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3170 - regression_loss: 7.3888 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 1.2207e-08\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6575 - regression_loss: 7.3888 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 1.2207e-08\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2741 - regression_loss: 7.3888 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 1.2207e-08\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2333 - regression_loss: 7.3888 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 1.2207e-08\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2083 - regression_loss: 7.3888 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 1.2207e-08\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3700 - regression_loss: 7.3888 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 1.2207e-08\n",
      "Epoch 138/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.1602 - regression_loss: 5.8415\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0804 - regression_loss: 7.3888 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 1.2207e-08\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3000 - regression_loss: 7.3888 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 6.1035e-09\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3600 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 6.1035e-09\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2324 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 6.1035e-09\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3655 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 6.1035e-09\n",
      "Epoch 143/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.7243 - regression_loss: 8.4056\n",
      "Epoch 143: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3194 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 6.1035e-09\n",
      "Epoch 144/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3977 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 3.0518e-09\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3146 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 3.0518e-09\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2283 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 3.0518e-09\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2664 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 3.0518e-09\n",
      "Epoch 148/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.0770 - regression_loss: 5.7582\n",
      "Epoch 148: ReduceLROnPlateau reducing learning rate to 1.5258788677030566e-09.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3185 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 3.0518e-09\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3210 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 1.5259e-09\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3977 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 1.5259e-09\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4017 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 1.5259e-09\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2905 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 1.5259e-09\n",
      "Epoch 153/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.1591 - regression_loss: 6.8403\n",
      "Epoch 153: ReduceLROnPlateau reducing learning rate to 7.629394338515283e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3540 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 1.5259e-09\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1787 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 7.6294e-10\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2128 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 7.6294e-10\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2661 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 7.6294e-10\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1357 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 7.6294e-10\n",
      "Epoch 158/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.7880 - regression_loss: 9.4692\n",
      "Epoch 158: ReduceLROnPlateau reducing learning rate to 3.8146971692576415e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2813 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 7.6294e-10\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2929 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 3.8147e-10\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2512 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 3.8147e-10\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5390 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 3.8147e-10\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2596 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 3.8147e-10\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3823 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 3.8147e-10\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2841 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 3.8147e-10\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0717 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 3.8147e-10\n",
      "Epoch 166/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.8386 - regression_loss: 8.5199\n",
      "Epoch 166: ReduceLROnPlateau reducing learning rate to 1.9073485846288207e-10.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3688 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 3.8147e-10\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2096 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 1.9073e-10\n",
      "Epoch 168/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3513 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 1.9073e-10\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3966 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 1.9073e-10\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2322 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 1.9073e-10\n",
      "Epoch 171/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.0148 - regression_loss: 7.6961\n",
      "Epoch 171: ReduceLROnPlateau reducing learning rate to 9.536742923144104e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3303 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 1.9073e-10\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2993 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 9.5367e-11\n",
      "Epoch 173/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2152 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 9.5367e-11\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2908 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 9.5367e-11\n",
      "Epoch 175/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3016 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 9.5367e-11\n",
      "Epoch 176/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.8734 - regression_loss: 8.5547\n",
      "Epoch 176: ReduceLROnPlateau reducing learning rate to 4.768371461572052e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3726 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 9.5367e-11\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3556 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 4.7684e-11\n",
      "Epoch 178/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2340 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 4.7684e-11\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7594 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 4.7684e-11\n",
      "Epoch 180/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2404 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 4.7684e-11\n",
      "Epoch 181/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.7134 - regression_loss: 8.3946\n",
      "Epoch 181: ReduceLROnPlateau reducing learning rate to 2.384185730786026e-11.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3176 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 4.7684e-11\n",
      "Epoch 182/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7653 - regression_loss: 7.3887 - val_loss: 10.9045 - val_regression_loss: 8.8036 - lr: 2.3842e-11\n",
      "3/3 [==============================] - 0s 960us/step\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 25ms/step - loss: 112.0852 - regression_loss: 104.3094 - val_loss: 81.3316 - val_regression_loss: 62.3073 - lr: 1.0000e-04\n",
      "Epoch 2/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 82.2285 - regression_loss: 74.7404 - val_loss: 61.0761 - val_regression_loss: 47.7505 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 64.5518 - regression_loss: 61.0362 - val_loss: 53.7827 - val_regression_loss: 42.1966 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 57.0836 - regression_loss: 53.7415 - val_loss: 48.3034 - val_regression_loss: 38.0529 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 51.7005 - regression_loss: 48.4312 - val_loss: 45.7408 - val_regression_loss: 35.9088 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 48.7419 - regression_loss: 44.1817 - val_loss: 43.1657 - val_regression_loss: 33.5953 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 44.5454 - regression_loss: 40.3163 - val_loss: 41.1799 - val_regression_loss: 31.7631 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 41.8648 - regression_loss: 37.6455 - val_loss: 39.6961 - val_regression_loss: 30.3564 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 38.8438 - regression_loss: 34.8139 - val_loss: 37.3745 - val_regression_loss: 28.2644 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 36.8497 - regression_loss: 32.5691 - val_loss: 35.8540 - val_regression_loss: 27.0009 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 33.6843 - regression_loss: 30.3365 - val_loss: 33.2803 - val_regression_loss: 24.8382 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.3635 - regression_loss: 28.6884 - val_loss: 31.1829 - val_regression_loss: 23.0853 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.2137 - regression_loss: 26.4348 - val_loss: 29.6877 - val_regression_loss: 22.0204 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.9551 - regression_loss: 24.9968 - val_loss: 27.4371 - val_regression_loss: 20.0719 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.7992 - regression_loss: 23.5158 - val_loss: 27.2133 - val_regression_loss: 20.1209 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.0732 - regression_loss: 21.8960 - val_loss: 24.6431 - val_regression_loss: 17.7458 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.3056 - regression_loss: 20.7362 - val_loss: 24.5219 - val_regression_loss: 18.0459 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.2026 - regression_loss: 20.2980 - val_loss: 22.0193 - val_regression_loss: 15.6917 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.8656 - regression_loss: 19.0743 - val_loss: 22.3405 - val_regression_loss: 16.3657 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6496 - regression_loss: 17.9499 - val_loss: 21.5241 - val_regression_loss: 15.2951 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6609 - regression_loss: 17.7159 - val_loss: 22.3119 - val_regression_loss: 16.4850 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.6833 - regression_loss: 17.1338 - val_loss: 20.8441 - val_regression_loss: 14.8178 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.9401 - regression_loss: 16.6511 - val_loss: 19.5282 - val_regression_loss: 14.2798 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6962 - regression_loss: 15.2838 - val_loss: 18.5469 - val_regression_loss: 13.1819 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6719 - regression_loss: 15.3901 - val_loss: 18.1297 - val_regression_loss: 13.2075 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.2854 - regression_loss: 13.9389 - val_loss: 16.9506 - val_regression_loss: 12.0284 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7893 - regression_loss: 14.2987 - val_loss: 16.6187 - val_regression_loss: 12.0410 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.4039 - regression_loss: 13.2259 - val_loss: 15.3002 - val_regression_loss: 10.8017 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.9412 - regression_loss: 12.7136 - val_loss: 15.2882 - val_regression_loss: 10.9466 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.5653 - regression_loss: 12.4256 - val_loss: 14.6142 - val_regression_loss: 10.3090 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1680 - regression_loss: 12.0030 - val_loss: 14.5475 - val_regression_loss: 10.3766 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.0844 - regression_loss: 11.9915 - val_loss: 13.9183 - val_regression_loss: 9.8099 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6713 - regression_loss: 11.4992 - val_loss: 14.3211 - val_regression_loss: 10.1935 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1447 - regression_loss: 11.2370 - val_loss: 14.2414 - val_regression_loss: 10.0477 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6740 - regression_loss: 11.5891 - val_loss: 14.4221 - val_regression_loss: 10.2751 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9428 - regression_loss: 11.3005 - val_loss: 13.9357 - val_regression_loss: 9.8181 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.2349 - regression_loss: 11.0600 - val_loss: 14.2267 - val_regression_loss: 10.1156 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.2991 - regression_loss: 11.2309 - val_loss: 13.9640 - val_regression_loss: 9.8385 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0256 - regression_loss: 10.9700 - val_loss: 14.2211 - val_regression_loss: 10.1083 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6618 - regression_loss: 10.7067 - val_loss: 13.7129 - val_regression_loss: 9.6464 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8796 - regression_loss: 10.7666 - val_loss: 14.3519 - val_regression_loss: 10.2008 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9653 - regression_loss: 10.9081 - val_loss: 14.5236 - val_regression_loss: 10.2531 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9103 - regression_loss: 10.8775 - val_loss: 15.2753 - val_regression_loss: 10.9486 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.4831 - regression_loss: 11.3414 - val_loss: 14.7904 - val_regression_loss: 10.4869 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.6304 - regression_loss: 10.3120\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9920 - regression_loss: 11.0212 - val_loss: 14.7924 - val_regression_loss: 10.5423 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1434 - regression_loss: 10.9869 - val_loss: 13.6895 - val_regression_loss: 9.6107 - lr: 5.0000e-05\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7864 - regression_loss: 10.5551 - val_loss: 13.4362 - val_regression_loss: 9.4339 - lr: 5.0000e-05\n",
      "Epoch 48/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3944 - regression_loss: 10.3923 - val_loss: 13.4642 - val_regression_loss: 9.4482 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3149 - regression_loss: 10.2430 - val_loss: 13.4212 - val_regression_loss: 9.4061 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0481 - regression_loss: 10.1187 - val_loss: 13.4501 - val_regression_loss: 9.4360 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2174 - regression_loss: 10.2220 - val_loss: 13.4445 - val_regression_loss: 9.4184 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3864 - regression_loss: 10.1853 - val_loss: 13.4104 - val_regression_loss: 9.3938 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2734 - regression_loss: 10.0565 - val_loss: 13.4097 - val_regression_loss: 9.4012 - lr: 5.0000e-05\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1216 - regression_loss: 10.0708 - val_loss: 13.3631 - val_regression_loss: 9.3591 - lr: 5.0000e-05\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0173 - regression_loss: 9.9870 - val_loss: 13.4036 - val_regression_loss: 9.3902 - lr: 5.0000e-05\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9912 - regression_loss: 9.9394 - val_loss: 13.3573 - val_regression_loss: 9.3501 - lr: 5.0000e-05\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0066 - regression_loss: 9.9109 - val_loss: 13.3383 - val_regression_loss: 9.3359 - lr: 5.0000e-05\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9248 - regression_loss: 9.8997 - val_loss: 13.3507 - val_regression_loss: 9.3464 - lr: 5.0000e-05\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0207 - regression_loss: 9.8693 - val_loss: 13.4050 - val_regression_loss: 9.3841 - lr: 5.0000e-05\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7434 - regression_loss: 9.8864 - val_loss: 13.3976 - val_regression_loss: 9.3797 - lr: 5.0000e-05\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8562 - regression_loss: 9.9056 - val_loss: 13.4323 - val_regression_loss: 9.4057 - lr: 5.0000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8002 - regression_loss: 9.8373 - val_loss: 13.3471 - val_regression_loss: 9.3388 - lr: 5.0000e-05\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8645 - regression_loss: 9.7947 - val_loss: 13.4572 - val_regression_loss: 9.4262 - lr: 5.0000e-05\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5121 - regression_loss: 9.8327 - val_loss: 13.4328 - val_regression_loss: 9.3931 - lr: 5.0000e-05\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9294 - regression_loss: 9.8222 - val_loss: 13.4101 - val_regression_loss: 9.3786 - lr: 5.0000e-05\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7702 - regression_loss: 9.7357 - val_loss: 13.4326 - val_regression_loss: 9.3939 - lr: 5.0000e-05\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7997 - regression_loss: 9.7147 - val_loss: 13.4003 - val_regression_loss: 9.3636 - lr: 5.0000e-05\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6418 - regression_loss: 9.6564 - val_loss: 13.4083 - val_regression_loss: 9.3770 - lr: 5.0000e-05\n",
      "Epoch 69/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.6444 - regression_loss: 10.3257\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.5558 - regression_loss: 9.6410 - val_loss: 13.3324 - val_regression_loss: 9.3230 - lr: 5.0000e-05\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7144 - regression_loss: 9.6166 - val_loss: 13.3508 - val_regression_loss: 9.3321 - lr: 2.5000e-05\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5383 - regression_loss: 9.5737 - val_loss: 13.4032 - val_regression_loss: 9.3714 - lr: 2.5000e-05\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7597 - regression_loss: 9.6185 - val_loss: 13.4089 - val_regression_loss: 9.3725 - lr: 2.5000e-05\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5571 - regression_loss: 9.5910 - val_loss: 13.3994 - val_regression_loss: 9.3636 - lr: 2.5000e-05\n",
      "Epoch 74/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 14.8877 - regression_loss: 13.5690\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7189 - regression_loss: 9.5829 - val_loss: 13.3844 - val_regression_loss: 9.3542 - lr: 2.5000e-05\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5602 - regression_loss: 9.5336 - val_loss: 13.3757 - val_regression_loss: 9.3456 - lr: 1.2500e-05\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5302 - regression_loss: 9.5375 - val_loss: 13.4062 - val_regression_loss: 9.3687 - lr: 1.2500e-05\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4610 - regression_loss: 9.5103 - val_loss: 13.3960 - val_regression_loss: 9.3584 - lr: 1.2500e-05\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3948 - regression_loss: 9.5088 - val_loss: 13.4012 - val_regression_loss: 9.3635 - lr: 1.2500e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4035 - regression_loss: 9.5047 - val_loss: 13.4021 - val_regression_loss: 9.3627 - lr: 1.2500e-05\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5664 - regression_loss: 9.5009 - val_loss: 13.4041 - val_regression_loss: 9.3662 - lr: 1.2500e-05\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5390 - regression_loss: 9.4881 - val_loss: 13.3974 - val_regression_loss: 9.3597 - lr: 1.2500e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4292 - regression_loss: 9.4825 - val_loss: 13.3927 - val_regression_loss: 9.3561 - lr: 1.2500e-05\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3068 - regression_loss: 9.4800 - val_loss: 13.3924 - val_regression_loss: 9.3574 - lr: 1.2500e-05\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4495 - regression_loss: 9.4810 - val_loss: 13.4113 - val_regression_loss: 9.3696 - lr: 1.2500e-05\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3754 - regression_loss: 9.4642 - val_loss: 13.4283 - val_regression_loss: 9.3797 - lr: 1.2500e-05\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5144 - regression_loss: 9.4663 - val_loss: 13.4227 - val_regression_loss: 9.3768 - lr: 1.2500e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3924 - regression_loss: 9.4529 - val_loss: 13.4183 - val_regression_loss: 9.3724 - lr: 1.2500e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2614 - regression_loss: 9.4557 - val_loss: 13.3950 - val_regression_loss: 9.3558 - lr: 1.2500e-05\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4791 - regression_loss: 9.4561 - val_loss: 13.3919 - val_regression_loss: 9.3535 - lr: 1.2500e-05\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3344 - regression_loss: 9.4313 - val_loss: 13.4206 - val_regression_loss: 9.3768 - lr: 1.2500e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4907 - regression_loss: 9.4485 - val_loss: 13.4212 - val_regression_loss: 9.3752 - lr: 1.2500e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3466 - regression_loss: 9.4198 - val_loss: 13.4119 - val_regression_loss: 9.3664 - lr: 1.2500e-05\n",
      "Epoch 93/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 [====>.........................] - ETA: 0s - loss: 11.0500 - regression_loss: 9.7312\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2894 - regression_loss: 9.4210 - val_loss: 13.4090 - val_regression_loss: 9.3640 - lr: 1.2500e-05\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5642 - regression_loss: 9.4069 - val_loss: 13.4099 - val_regression_loss: 9.3635 - lr: 6.2500e-06\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4616 - regression_loss: 9.4071 - val_loss: 13.4088 - val_regression_loss: 9.3638 - lr: 6.2500e-06\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4060 - regression_loss: 9.4051 - val_loss: 13.4126 - val_regression_loss: 9.3667 - lr: 6.2500e-06\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4097 - regression_loss: 9.3987 - val_loss: 13.4192 - val_regression_loss: 9.3725 - lr: 6.2500e-06\n",
      "Epoch 98/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.7958 - regression_loss: 9.4770\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3368 - regression_loss: 9.4076 - val_loss: 13.4196 - val_regression_loss: 9.3712 - lr: 6.2500e-06\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4010 - regression_loss: 9.3909 - val_loss: 13.4212 - val_regression_loss: 9.3721 - lr: 3.1250e-06\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4045 - regression_loss: 9.3891 - val_loss: 13.4204 - val_regression_loss: 9.3718 - lr: 3.1250e-06\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3091 - regression_loss: 9.3900 - val_loss: 13.4227 - val_regression_loss: 9.3739 - lr: 3.1250e-06\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4066 - regression_loss: 9.3893 - val_loss: 13.4253 - val_regression_loss: 9.3755 - lr: 3.1250e-06\n",
      "Epoch 103/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.3675 - regression_loss: 10.0487\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4285 - regression_loss: 9.3901 - val_loss: 13.4226 - val_regression_loss: 9.3731 - lr: 3.1250e-06\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3894 - regression_loss: 9.3829 - val_loss: 13.4233 - val_regression_loss: 9.3737 - lr: 1.5625e-06\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2526 - regression_loss: 9.3825 - val_loss: 13.4211 - val_regression_loss: 9.3721 - lr: 1.5625e-06\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2981 - regression_loss: 9.3823 - val_loss: 13.4197 - val_regression_loss: 9.3711 - lr: 1.5625e-06\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1051 - regression_loss: 9.3809 - val_loss: 13.4224 - val_regression_loss: 9.3733 - lr: 1.5625e-06\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1432 - regression_loss: 9.3796 - val_loss: 13.4231 - val_regression_loss: 9.3739 - lr: 1.5625e-06\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3778 - regression_loss: 9.3789 - val_loss: 13.4227 - val_regression_loss: 9.3737 - lr: 1.5625e-06\n",
      "3/3 [==============================] - 0s 975us/step\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 100 is out of bounds for axis 1 with size 100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 30\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#tarnet_model.compile(optimizer=SGD(lr=sgd_lr, momentum=momentum, nesterov=True),\u001b[39;00m\n\u001b[1;32m     22\u001b[0m                     \u001b[38;5;66;03m#loss=regression_loss,\u001b[39;00m\n\u001b[1;32m     23\u001b[0m                     \u001b[38;5;66;03m#metrics=regression_loss)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m101\u001b[39m):\n\u001b[0;32m---> 30\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[43mload_IHDP_data_n\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./ihdp_npci_1-100.train.npz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./ihdp_npci_1-100.test.npz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     tarnet_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m),\n\u001b[1;32m     32\u001b[0m                     loss\u001b[38;5;241m=\u001b[39mregression_loss,\n\u001b[1;32m     33\u001b[0m                     metrics\u001b[38;5;241m=\u001b[39mregression_loss)\n\u001b[1;32m     35\u001b[0m     yt \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mys\u001b[39m\u001b[38;5;124m'\u001b[39m], data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m'\u001b[39m]], \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[78], line 27\u001b[0m, in \u001b[0;36mload_IHDP_data_n\u001b[0;34m(training_data, testing_data, i, dt)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(training_data,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m trf, \u001b[38;5;28mopen\u001b[39m(testing_data,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tef:\n\u001b[1;32m     26\u001b[0m     train_data\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mload(trf); test_data\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mload(tef)\n\u001b[0;32m---> 27\u001b[0m     y\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#most GPUs only compute 32-bit floats\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     t\u001b[38;5;241m=\u001b[39mtrain_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m'\u001b[39m][:,i]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m     x\u001b[38;5;241m=\u001b[39mtrain_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m][:,:,i]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 100 is out of bounds for axis 1 with size 100"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "pehe_loss=[]\n",
    "val_split=0.2\n",
    "batch_size=100\n",
    "verbose=1\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    " #we'll use both y and t to compute the loss\n",
    "sgd_callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='val_loss', patience=40, min_delta=0.), \n",
    "        #40 is Shi's recommendation for this dataset, but you should tune for your data \n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=0., cooldown=0, min_lr=0),\n",
    "    ]\n",
    "#optimzier hyperparameters\n",
    "sgd_lr = 1e-5\n",
    "momentum = 0.9\n",
    "#tarnet_model.compile(optimizer=SGD(lr=sgd_lr, momentum=momentum, nesterov=True),\n",
    "                    #loss=regression_loss,\n",
    "                    #metrics=regression_loss)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1,101):\n",
    "   \n",
    "    \n",
    "    data=load_IHDP_data_n('./ihdp_npci_1-100.train.npz','./ihdp_npci_1-100.test.npz',i,'train')\n",
    "    tarnet_model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                    loss=regression_loss,\n",
    "                    metrics=regression_loss)\n",
    "    \n",
    "    yt = np.concatenate([data['ys'], data['t']], 1)\n",
    "\n",
    "    tarnet_model.fit(x=data['x'],y=yt,\n",
    "                    callbacks=sgd_callbacks,\n",
    "                    validation_split=val_split,\n",
    "                    epochs=300,\n",
    "                    batch_size=batch_size,\n",
    "                    verbose=verbose)\n",
    "    \n",
    "    pehe_loss.append(cal_pehe(i,tarnet_model))\n",
    "    \n",
    "print(\"DONE!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6c31298e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.2017565\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(pehe_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da341930",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
